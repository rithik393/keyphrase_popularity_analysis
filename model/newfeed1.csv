year,month,day,title,link,description
2021,7,9,An A/B Test Loses Its Luster If A/A Tests Fail,https://towardsdatascience.com/an-a-b-test-loses-its-luster-if-a-a-tests-fail-2dd11fa6d241?source=rss----7f60cf5620c9---4,A statistical approach to A/A testsContinue reading on Towards Data Science »
2021,7,9,Top 3 Lessons Learned from Leaving a Job I Loved,https://towardsdatascience.com/top-3-lessons-learned-from-leaving-a-job-i-loved-6c71a76cae7?source=rss----7f60cf5620c9---4,Job hopping to the next opportunity: becoming a data science consultantContinue reading on Towards Data Science »
2021,7,9,Be a Proteomic Detective with Python,https://towardsdatascience.com/be-a-proteomic-detective-with-python-6ea18ef3f8f3?source=rss----7f60cf5620c9---4,Inspecting the quality of isobaric labeling proteomics results in a Jupyter notebookWe should follow the footsteps of Holmes and Watson with our mass spectrometry data. Illustration from The Strand Magazine (1892) by Sidney Paget currently in the public domainIsobaric labeling quantitative proteomics is complex requiring extensive sample preparation mass spectrometry (MS) acquisition and data analysis. The samples are lysed and solubilized cysteine residues are normally reduced and alkylated proteins are digested into peptides peptides are labeled with isotopically labeled reagents usually fractionated and desalted [1]. And that is only the path from the sample vial to mass spectrometer! Too many things can go wrong so it is important to scrutinize the quality of the data that becomes a basis for biological and medical conclusions.Today I would like to share a Jupyter notebook containing the Python code for a range of processing and visualization steps that I find handy when inspecting an output from the Proteome Discoverer (PD) suite. It’s PD because we use that in our lab but of course other data processing pipelines can be used given a comprehensive enough output.The complete notebook has been uploaded to the Github repository while here I will show a few of the plots that I consider particularly useful for getting an insight into the quality of a data set.The example data consists if 10 E. coli samples labeled with tandem mass tag (TMT) reagents it is a part of a recent study by Thulin and Andersson [2]. The MS files are publicly available in the project PXD007647 from PRIDE archive; I have downloaded the raw files and re-processed them using PD 2.4 with the Spectrum Exporter node in the workflow. With the said node activated the output consists of a bunch of tab-delimited text files:/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_QuanSpectra.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_SpecializedTraces.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_Proteins.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_PeptideGroups.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_MSMSSpectrumInfo.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_PSMs.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_ResultStatistics.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_ProteinGroups.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_InputFiles.txt/PD_Out/PXD007647_Reproc_TMT-set-2_8fracs_PrSMs.txtLet’s take a look at the selected favorites in the order of appearance in the notebook (arbitrary order that is).File MSMSSpectrumInfoThe labeling batch in the study has been separated into fractions each of them is analyzed on MS resulting in a separate file. It is interesting to check if the fractions were alright for example by looking at how many fragmentation spectra are acquired per minute:https://medium.com/media/0df782bab9fe51523a461f4ac9f6d551/hrefImage by authorBut what’s probably more important is the number of identifications per minute. Irregular distributions and lacking identifications might suggest among other options to modify the fractionation scheme. The picture is quite decent in this project no file is devoid of identifications to a significant degree even though the profiles look a bit skewed for some:https://medium.com/media/c1a969c634654e406732eb7e1d282e9b/hrefImage by authorFile QuanSpectraAs it is a quantitative study I would definitely want to check the distributions of values in the quan channels before and after normalization (done in PD). We would like to see the normalized channels lined up in a nice and uniformly looking range:https://medium.com/media/051b4329f32b671b9586079566451af6/hrefImage by authorUsing square roots of SN values instead of the log-values helps to preserve zero values and thus give the more complete overview of the intensity distribution. This data set doesn’t have too many zero-intensity spectra and the median intensities seem to be around a hundred which is high. At the same time scales on the y-axes show that many of the high-intensity spectra have no assigned peptide IDs (one might select “sharey=True” to scale the subplots equally and emphasize that point).https://medium.com/media/c3f1f605240b7d071172ba67c4817c4b/hrefImage by authorFile PSMsAs there are normally many thousands of peptide-to-spectrum matches (PSMs) it can be a good idea to use hexbin plots when trying to represent the density of points see the example of a mass error plot below. We could have plotted a scatter with very small and highly transparent individual points but the hexagonal bins represent the distribution very clearly. The mass accuracy was in order in this project concentrating well within 5 parts per million (ppm):https://medium.com/media/705bf7f6b18aed2a27ef4fa1e37d1dc9/hrefImage by authorFile PeptideGroupsWhen exploring isobaric data sets I often check the relative standard deviation (RSD) of quantitative values for peptides within each protein:https://medium.com/media/c500e47c880e038d28df706bc8c51aa1/hrefIn the simplest scenario we expect peptides originating from a protein to repeat the quantitative profile of that protein. High coefficient of variance for peptides within most proteins across the data set may be an evidence of low signal intensity (and thus high relative noise level). In our case the mean and median RSDs for different samples fall between 8 and 19% which is quite reasonable:https://medium.com/media/825715b71d1409739c0087c8f04f46a3/hrefImage by authorCysteine derivatization can influence the structure of proteins and have impact on the amounts of some peptides in the final data set. If something goes wrong the relative levels of modified cysteine peptides may be low (log-ratio &lt;&lt; 0) on a global scale for the affected samples. One of the ways to express that is a clustered heatmap which is conveniently available via seaborn library. In our case different peptides have various abundance profiles but there are no samples that would have low abundance of cysteine-containing peptides on the scale of the whole sample:https://medium.com/media/ee151950638c020e25cb2e9021fb72dc/hrefImage by authorFile ProteinsGood old principal component analysis (PCA) is worth looking at as well. It can help spot outlier samples and check if the sample group as expected (I have not pulled that information up for the project). Let’s plot the two main principal components slightly modifying the default matplotlib scatter plot and adding the percentage of explained variance to the axis labels:https://medium.com/media/d58ba086808bc2746ab5f61dc372b53f/hrefImage by authorSummaryThis post introduces the Jupyter notebook that I use to inspect the quality of isobaric labeling MS proteomic data from Proteome Discoverer. Code and example data can be found in the Github repository.References[1] N. Rauniyar and J. R. Yates III. Isobaric Labeling-Based Relative Quantification in Shotgun Proteomics (2014) J. Proteome Res. 13 12 5293–5309. Open access.[2] E. Thulin and D.I. Andersson. Upegulation of PBP1B and LpoB in cysB Mutants Confers Mecillinam (Amdinocillin) Resistance in Escherichia coli (2019) Antimicrob. Agents Chemother. 63 10 e00612–19. Open access.Be a Proteomic Detective with Python was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,9,Six Guidelines for Good Visualizations,https://towardsdatascience.com/six-guidelines-for-good-visualizations-7c1831cda29f?source=rss----7f60cf5620c9---4,Key takeaways from the works of DataViz legends.Continue reading on Towards Data Science »
2021,7,9,Pythonic Counting: An Overview,https://towardsdatascience.com/pythonic-counting-an-overview-4266edbb0001?source=rss----7f60cf5620c9---4,Counting elements is critical! Learn how to do it eloquently!Continue reading on Towards Data Science »
2021,7,9,What They Don’t Tell You — 4 Ways Humans Still Vastly Outperform AI,https://towardsdatascience.com/what-they-dont-tell-you-4-ways-humans-still-vastly-outperform-ai-ba640aae0d4?source=rss----7f60cf5620c9---4,They portray AI as more intelligent than it is.Continue reading on Towards Data Science »
2021,7,9,5 tips to Customize the Display of your Pandas Data Frame,https://towardsdatascience.com/6-tips-to-customize-the-display-of-your-pandas-data-frame-ce5a8caa7783?source=rss----7f60cf5620c9---4,Make the data frame more intuitive in the Jupyter NotebookContinue reading on Towards Data Science »
2021,7,9,Assyrian or Babylonian? Language Identification in Cuneiform Texts,https://towardsdatascience.com/assyrian-or-babylonian-language-identification-in-cuneiform-texts-4f15a14a5d70?source=rss----7f60cf5620c9---4,A language model for ancient Mesopotamian dialectsContinue reading on Towards Data Science »
2021,7,9,Determining SLA for On-time Deliveries: An Application of Descriptive Statistics,https://towardsdatascience.com/determining-sla-for-on-time-deliveries-an-application-of-descriptive-statistics-801be0ad11bc?source=rss----7f60cf5620c9---4,How to interpret percentiles in making a business decisionAs a data analyst responsible for the logistics service in an e-commerce platform we need to determine ways to make our customers satisfied with the delivery service. One of the essential factors is ensuring on-time deliveries. With the spirit of “make it better” we want to eliminate the loopholes during the delivery journey and maximize sellers’ visibility about what is happening during the journey. A few months ago we initiated to determine the service level agreement (SLA) with third-party logistics (3PL) partners regarding the duration for finding drivers.Sellers’ journey in waiting driver to pick-up a package (Image by author)Before implementation our sellers can only be certain on how long should a driver come to the seller’s address to pick up a package (SLA Pick-up). However there was still a loophole between the process where the 3PL’s system needs to assign a driver first before directing the driver to pick up the package. Therefore to ensure a smooth picking-up process we want to keep track of finding driver activity by determining how long the SLA Finding Driver should be. To achieve that we then proceeded with the descriptive analytics method.Why using descriptive analytics?We want to determine the SLA based on historical conditions.The historical condition is assumed can represent conditions in the future (e.g. using the same 3PL partners with constant service quality same driver capacity).The historical performance data has shown a constant pattern during a certain period of time.Responding to the current analytics trend that promotes the power of predictive analytics we would like to argue that descriptive analytics is often being underrated. Descriptive analytics is still very handy in solving real-life problems especially during a limited amount of time (and cost). Therefore we need to be considerate in implementing analytics in order to be both effective and efficient while still understand when to use a certain approach.When should we use predictive analytics?We want to simulate the future condition with changes on several variables (e.g. using different 3PL partners with different characteristics targeting different customer segments).We are detecting changes in the current performance and want to forecast the future trend.Step 1: Determining the metricsThere are 2 metrics that we want to analyze in determining SLA Finding Driver:Duration from a seller requests for pick-up until a driver is assigned (RPU to Driver Assigned).Duration from a seller requests for pick-up until CS ticket is created (RPU to Ticket Created).The first metric is for analyzing our 3PL partner’s performance: how fast is their system capability in assigning a driver for our seller. On the other hand the second metric is for analyzing sellers’ expectations toward our service: how long are they willing to wait before they submit a complaint ticket. By analyzing both metrics we aim to optimize the experience for both stakeholders.Step 2: Exploratory Data Analysis (EDA)Before dive deeper into the dataset we need to understand first the distribution of the data by performing EDA. We want to know which measure of central tendency (mean or median) that we should look through for further analysis. By visualizing both metrics into boxplots we can get the idea of whether the data are skewed or not.Examining data distribution using boxplots (Image by author)From the boxplots above we find that both RPU to Driver Assigned and RPU to Ticket Created data are very right-skewed which means that the mean value can be much higher than the median due to outliers. Therefore we continue the analysis by analyzing the median and percentiles values.Step 3: Analyzing the percentilesAfter EDA we break down the percentiles for both RPU to Driver Assigned and RPU to Ticket Created durations into 10th 25th 50th 75th and 90th percentiles.Percentiles of each duration measured (dummy data)The duration table above can be interpreted as:If we set the SLA for 51 minutes (according to the 90th percentile of RPU to Driver Assigned duration) there will be about 90% of pick-up requests that can be successfully assigned with a driver before the SLA is breached. However we can only partly tackle CS tickets related to finding driver issues since there is still 10–25% of tickets that will be created before the SLA.If we adjust the SLA for 26 minutes there will be only 50% of pick-up requests that can be successfully assigned with a driver before the SLA is breached while the number of CS tickets can be decreased from 10–25% to &lt;10% of the current number of tickets.Step 4: Determining the optimum SLA for both 3PL &amp; sellersFrom the analysis we recognize that in determining the SLA there is a trade-off between 3PL’s assigning driver system capacity and CS tickets created by sellers. Therefore we may need to add another factor in the analysis such as how many pick-up orders and CS tickets are created in a month. We also need to dive deeper into what is the consequence of a tardy finding driver system for the seller.If the number of tickets created is not too high we can consider extending the SLA Finding Driver longer. Conversely if the tickets are often created with hard complaints we need to consider shortening the SLA. Considering both 3PL parties and sellers as our valuable customers the optimum SLA should be determined to give the best experience and maximize our customers’ satisfaction.By determining the SLA Finding Driver now our sellers have the visibility and certainty to know not only when are their orders are going to be picked up but also whether if there is a driver assigned for them or not. If the SLA has been breached and the system has not found a driver for them yet sellers also have the capability to retry the pick-up process. With this new feature development sellers are expected to have a better experience by having more control to have their orders delivered on-time.Determining SLA for On-time Deliveries: An Application of Descriptive Statistics was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,9,How to Join and Win Hackathons with AI,https://towardsdatascience.com/how-to-join-and-win-hackathons-with-ai-ec055e2b5482?source=rss----7f60cf5620c9---4,My story of entering hackathons and how I learned to win at a high rateContinue reading on Towards Data Science »
2021,7,8,My unfair advantage in computing resources that are accessible to most people,https://towardsdatascience.com/my-unfair-advantage-in-computing-resources-that-are-accessible-to-most-people-5bb95932f247?source=rss----7f60cf5620c9---4,How to use Google Colab in a very advantageous styleContinue reading on Towards Data Science »
2021,7,8,Recent Developments and Views on Computer Vision x Transformer,https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654?source=rss----7f60cf5620c9---4,On the differences between Transformer and CNN why Transformer matters and what its weaknesses are.Continue reading on Towards Data Science »
2021,7,8,"Object localization using pre-trained CNN models such as MobileNet, ResNet & Xception",https://towardsdatascience.com/object-localization-using-pre-trained-cnn-models-such-as-mobilenet-resnet-xception-f8a5f6a0228d?source=rss----7f60cf5620c9---4,Object localization to locate animal faces in an image from the Oxford Pet Dataset using different Pre-trained CNN ModelsPhoto by Kevin Bhagat on UnsplashThis work will introduce you to single object localization using pre-trained CNN and a few additional interesting adaptations to find the best performing model in them according to the context.Let&#39;s start by explaining localization. In simple words to locate an object in an image. Localization means entailing an object to a bounding box or precisely to a rectangle for localizing the concerned object after identifying it. There are single-object localization multiple object localization and semantic segmentation for doing things of similar means with different forms of doing the purpose.Here I will stick to single-object localization which will identify the needed object in the image and then locate the same using CNN. Also notice that I will use Mobile Net ResNet and Xception separately as pre-trained convolutional neural networks and will perform the whole classifier and localizer for each. On the way IOU (Intersection over Union) will be made familiarise and will print out the same for each of them and finally we will see which pre-trained network performs well for the dataset we use.For this project the Oxford Pet dataset is suitable: You can download the same from the link below.http://www.robots.ox.ac.uk/~vgg/data/pets/Now let&#39;s analyse the dataset. The dataset contains images of animals and each image contains a single animal. We could see that these animals are cats and dogs having different types. Notice that the image&#39;s alignment position and structure are different in each image which can help us have a good training set for more accurate results. Going with the above link we can download the dataset and the ground truth data. Once we download the data we will end up in two files: images and annotations. We can get the xml annotation class lists and all in the annotations folder. Once we have all these in hand let&#39;s move into object localization using different pre-trained CNN models.Before we typically commence we will introduce IOU as a metric here itself. Intersection over Union(IOU) helps to understand how much the predicted bounding box vary from the real one. It’s a good measure for understanding how our prediction goes…PS: for all the pictures printed with bounding boxes after training separately with pre-trained networks we will print IOU below each picture…..First we need to import all the necessary libraries and packages.from collections import namedtupleimport csvimport tensorflow as tffrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 preprocess_inputfrom tensorflow.keras.applications.resnet50 import ResNet50 preprocess_inputfrom tensorflow.keras.applications.xception import Xception preprocess_inputfrom tensorflow.keras import backend as Kfrom tensorflow.keras.layers import Dense GlobalAveragePooling2D Dropout Flattenfrom tensorflow.keras.models import Model Sequentialfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.utils import to_categoricalimport matplotlib.pyplot as pltimport matplotlib.patches as patchesimport numpy as npimport os# import the necessary packagesfrom collections import namedtupleimport numpy as npimport cv2# define the `Detection` object for IOU(Detection = namedtuple(&quot;Detection&quot; [&quot;image_path&quot; &quot;gt&quot; &quot;pred&quot;])from PIL import Image ImageOps# importing XML parsing library for parsing the dataimport xml.etree.ElementTree as ETNow let&#39;s import the data - I normally use google colaboratory. Thus I mount my google drive to colab( You can import the data in any way by saving it wherever as per your convenience).Also here we can set the target size as (224224) and we will use Mobile Net ResNet and Xception as pre-trained networks to compare each of them.from google.colab import drivedrive.mount(&#39;/content/drive&#39;)data_images = &#39;/content/drive/MyDrive/AI_dataset_pets/images&#39;data_ClassList = &#39;/content/drive/MyDrive/AI_dataset_pets/annotations/list.txt&#39;data_xmlAnnotations = &#39;/content/drive/MyDrive/AI_dataset_pets/annotations/xmls&#39;TARGET_SIZE = (224 224)Now define the bounding box to locate the animal in the image to it.#BoundingBoxBounding_Box = namedtuple(&#39;Bounding_Box&#39; &#39;xmin ymin xmax ymax&#39;)# The following function will read the xml and return the values for xmin ymin xmax ymax for formulating the bounding boxdef building_bounding_box(path_to_xml_annotation):tree = ET.parse(path_to_xml_annotation)root = tree.getroot()path_to_box = &#39;./object/bndbox/&#39;xmin = int(root.find(path_to_box + &quot;xmin&quot;).text)ymin = int(root.find(path_to_box + &quot;ymin&quot;).text)xmax = int(root.find(path_to_box + &quot;xmax&quot;).text)ymax = int(root.find(path_to_box + &quot;ymax&quot;).text)return Bounding_Box(xmin ymin xmax ymax)So let&#39;s do padding for making the image to a perfect square and apply the needed changes to the bounding box according to the padding and rescaling.In the following code normalising has also been done.def resize_image_with_bounds(path_to_image bounding_box=None target_size=None):image = Image.open(path_to_image)width height = image.sizew_pad = 0h_pad = 0bonus_h_pad = 0bonus_w_pad = 0#the following code helps determining where to pad or is it not necessary for the images we have.# If the difference between the width and height was odd((height&lt;width)case) we add one pixel on one side# If the difference between the height and width was odd((height&gt;width)case) then we add one pixel on one side.#if both of these are not the case then pads=0 no padding is needed since the image is already a square itself.if width &gt; height:pix_diff = (width - height)h_pad = pix_diff // 2bonus_h_pad = pix_diff % 2elif height &gt; width:pix_diff = (height - width)w_pad = pix_diff // 2bonus_w_pad = pix_diff % 2# When we pad the image to square we need to adjust all the bounding box values by the amounts we added on the left or top.#The &quot;bonus&quot; pads are always done on the bottom and right so we can ignore them in terms of the box.image = ImageOps.expand(image (w_pad h_pad w_pad+bonus_w_pad h_pad+bonus_h_pad))if bounding_box is not None:new_xmin = bounding_box.xmin + w_padnew_xmax = bounding_box.xmax + w_padnew_ymin = bounding_box.ymin + h_padnew_ymax = bounding_box.ymax + h_pad# We need to also apply the scalr to the bounding box which we used in resizing the imageif target_size is not None:# So width and height have changed due to the padding resize.width height = image.sizeimage = image.resize(target_size)width_scale = target_size[0] / widthheight_scale = target_size[1] / heightif bounding_box is not None:new_xmin = new_xmin * width_scalenew_xmax = new_xmax * width_scalenew_ymin = new_ymin * height_scalenew_ymax = new_ymax * height_scaleimage_data = np.array(image.getdata()).reshape(image.size[0] image.size[1] 3)# The image data is a 3D array such that 3 channels RGB of target_size.(RGB values are 0-255)if bounding_box is None:return image_data Nonereturn (image_data Bounding_Box(new_xmin new_ymin new_xmax new_ymax))So from the input data we have reshaped the image as well as the bounding box.def setting_sample_from_name(sample_name):path_to_image = os.path.join(data_images sample_name + &#39;.jpg&#39;)path_to_xml = os.path.join(data_xmlAnnotations sample_name + &#39;.xml&#39;)original_bounding_box = get_bounding_box(path_to_xml)image_data bounding_box = resize_image_with_bounds(path_to_image original_bounding_box TARGET_SIZE)return (image_data bounding_box)Notice that The yellow box is the predicted bounding box and the blue box is the ground-truth bounding box is the real one.Now let&#39;s write the function to plot the image data along with the bounding box and find the Intersection over Union-IOU of the two boxes. It can be computed as IOU = Area of Overlap/Area of Union.The code is below in the function ‘plot_with_box’.def plot_with_box(image_data bounding_box compare_box=None):figax = plt.subplots(1)ax.imshow(image_data)# Creating a Rectangle patch for the changed oneboxA = patches.Rectangle((bounding_box.xmin bounding_box.ymin)bounding_box.xmax - bounding_box.xminbounding_box.ymax - bounding_box.yminlinewidth=3 edgecolor=&#39;y&#39; facecolor=&#39;none&#39;)# Add the patch to the Axesax.add_patch(boxA)#Creating another Rectangular patch for the real oneif compare_box is not None:boxB = patches.Rectangle((compare_box.xmin compare_box.ymin)compare_box.xmax - compare_box.xmincompare_box.ymax - compare_box.yminlinewidth=2 edgecolor=&#39;b&#39; facecolor=&#39;none&#39;)# Add the patch to the Axesax.add_patch(boxB)#FOR FINDING INTERSECTION OVER UNIONxA = max(bounding_box.xmin compare_box.xmin)yA = max(bounding_box.ymin compare_box.ymin)xB = min(bounding_box.xmax compare_box.xmax)yB = max(bounding_box.ymax compare_box.ymax)interArea = max(0 xB - xA + 1) * max(0 yB - yA + 1)boxAArea = (bounding_box.xmax - bounding_box.xmin + 1) * (bounding_box.ymax - bounding_box.ymin + 1)boxBArea = (compare_box.xmax - compare_box.xmin + 1) * (compare_box.ymax - compare_box.ymin + 1)iou =interArea/float(boxAArea+boxBArea-interArea)#By intersection of union I mean intersection over union(IOU) #itselfprint(&#39;intersection of union =&#39;iou)plt.show()Now let&#39;s plot a random image and see what all have happened and check the working of the predicted bounding box.sample_name = &#39;Abyssinian_10&#39;image bounding_box = setting_sample_from_name(sample_name)plot_with_box(image bounding_box)Image by AuthorThus we have the bounding box in the object.Now let&#39;s make all of our data gets processed. Also let&#39;s remove all those images which have no annotations. And turn it into a Numpy Array.data_pros = []with open(data_ClassList) as csv_list_file:csv_reader = csv.reader(csv_list_file delimiter=&#39; &#39;)for row in csv_reader:if row[0].startswith(&#39;#&#39;): continue# Unpack for readabilitysample_name class_id species breed_id = row# Not every image has a bounding box some files are missing.So lets ignore those by the following linestry:image bounding_box = setting_sample_from_name(sample_name)except FileNotFoundError:# This actually happens quite a lot as you can see in the output.# we end up with 7349 samples.print(f&#39;cannot find annotations for {sample_name}: so skipped it&#39;)continue# cat = 0 and dog = 1.data_tuple = (image int(species) - 1 bounding_box)data_pros.append(data_tuple)print(f&#39;Processed {len(data_pros)} samples&#39;)data_pros = np.array(data_pros)Now once we get this done let&#39;s test the whole with 6 random images#for checking lets print 6 of themfor _ in range(6):i = np.random.randint(len(data_pros))image species bounding_box = data_pros[i]if species == 0:print(i &quot;it is cat&quot;)elif species == 1:print(i &quot;it is dog&quot;)else:print(&quot;ERROR FOUND: This is of invalid species type&quot;)plot_with_box(image bounding_box)And the results look like this.Image by AuthorSplitting the given data for bounding box predictions.x_train = []y_class_train = []y_box_train = []x_validation = []y_class_validation = []y_box_validation = []validation_split = 0.2for image species bounding_box in processed_data:if np.random.random() &gt; validation_split:x_train.append(preprocess_input(image))y_class_train.append(species)y_box_train.append(bounding_box)else:x_validation.append(preprocess_input(image))y_class_validation.append(species)y_box_validation.append(bounding_box)x_train = np.array(x_train)y_class_train = np.array(y_class_train)y_box_train = np.array(y_box_train)x_validation = np.array(x_validation)y_class_validation = np.array(y_class_validation)y_box_validation = np.array(y_box_validation)Going to use some pre-trained models using transfer learning.First I’m using Mobile Net and I&#39;m going to perform both classifier and localizer.base_model = MobileNetV2(weights=&#39;imagenet&#39; include_top=False input_shape=(TARGET_SIZE[0] TARGET_SIZE[1] 3))chopped_mobilenet = Model(inputs=[base_model.input] outputs=[base_model.layers[90].output])classification_output = GlobalAveragePooling2D()(chopped_mobilenet.output)classification_output = Dense(units=1 activation=&#39;sigmoid&#39;)(classification_output)localization_output = Flatten()(chopped_mobilenet.output)localization_output = Dense(units=4 activation=&#39;relu&#39;)(localization_output)model = Model(inputs=[chopped_mobilenet.input] outputs=[classification_output localization_output])model.summary()Once printing the above we will obtain a detailed summary of the model build using MobileNet.Now plotting the accuracy and loss of the model in every epoch.plot_training_history(history1 model)And the plots are:Image by AuthorThe true box is in blue and the predicted box is in yellowfor _ in range(18):i = np.random.randint(len(processed_data))img species true_bounding_box = processed_data[i]pred = model.predict(np.array([preprocess_input(img)]))if pred[0][0] &lt; .5:print(&quot;it is a Cat&quot;)else:print(&quot;it is a dog&quot;)plot_with_box(img Bounding_Box(*pred[1][0]) true_bounding_box)Results are:Images by AuthorNotice that here all the images are detected as cats using MobileNet. Taking some random samples for checking to know how well the model is:some_random_samples = [&#39;Abyssinian_174&#39;&#39;american_bulldog_59&#39;]for sample_name in some_random_samples:path_to_image = os.path.join(data_images sample_name + &#39;.jpg&#39;)print(path_to_image)img _ = resize_image_with_bounds(path_to_image target_size=TARGET_SIZE)pred = model.predict(np.array([preprocess_input(img)]))if pred[0][0] &lt; .5:print(&quot;YesIts a Cat&quot;)else:print(&quot;Yes Its a dog&quot;)plot_with_box(img Bounding_Box(*pred[1][0])true_bounding_box)Image by AuthorIOU values while using MobileNet are not bad….But in pictures that have some small anonymity The IOU values are too small…..Let&#39;s see how it is using ResNet and Xception.Now try the same by using ResNet pre-trained networkbase_model1 = ResNet50(weights=&#39;imagenet&#39; include_top=False input_shape=(TARGET_SIZE[0] TARGET_SIZE[1] 3))chopped_resnet1 = Model(inputs=[base_model1.input] outputs=[base_model1.layers[90].output])classification_output1 = GlobalAveragePooling2D()(chopped_resnet1.output)classification_output1 = Dense(units=1 activation=&#39;sigmoid&#39;)(classification_output1)localization_output1 = Flatten()(chopped_resnet1.output)localization_output1 = Dense(units=4 activation=&#39;relu&#39;)(localization_output1)model1 = Model(inputs=[chopped_resnet1.input] outputs=[classification_output1 localization_output1])model1.summary()Please go through the summary once you get it printed by this; it will help you to gain a clear understanding of the network.Now we will move on to compiling and fitting the model made with Resnet.model1.compile(optimizer=&#39;adam&#39; metrics=[&#39;accuracy&#39;]loss=[&#39;binary_crossentropy&#39; &#39;mse&#39;]loss_weights=[800 1]  )#lets run it through 10 epochshistory2=model1.fit(x_train [y_class_train y_box_train] validation_data=(x_validation [y_class_validation y_box_validation])epochs=10verbose=True)history2I do not include the summary and the validation accuracy and loss in every epoch. You can see them once you implement them and the plots of accuracy and loss are given below for every epoch.def plot_training_history(history model):plt.plot(history.history[&#39;dense_3_accuracy&#39;])plt.plot(history.history[&#39;val_dense_3_accuracy&#39;])plt.ylabel(&#39;accuracy&#39;)plt.xlabel(&#39;epoch&#39;)plt.legend([&#39;training&#39; &#39;validation&#39;] loc=&#39;best&#39;)plt.show()plt.plot(history.history[&#39;dense_3_loss&#39;])plt.plot(history.history[&#39;val_dense_3_loss&#39;])plt.title(&#39;model loss&#39;)plt.ylabel(&#39;loss&#39;)plt.xlabel(&#39;epoch&#39;)plt.legend([&#39;training&#39; &#39;validation&#39;] loc=&#39;best&#39;)plt.show()plot_training_history(history2 model1)Image by AuthorLet&#39;s print the images after the changes and trained using ResNet and print the IOU (Intersection over Union) of the two boxes to see how good our prediction is.for _ in range(3):i = np.random.randint(len(processed_data))img species true_bounding_box = processed_data[i]pred = model1.predict(np.array([preprocess_input(img)]))if pred[0][0] &lt; .5:print(&quot;it is a Cat by ResNet&quot;)else:print(&quot;it is a dog by ResNet&quot;)plot_with_box(img Bounding_Box(*pred[1][0]) true_bounding_box)Image by AuthorIt is detecting a dog as a cat but IOU values are pretty good!Let&#39;s try out Xception pre-trained network now. The code for its implementation is given below.base_model2 = Xception(weights=&#39;imagenet&#39; include_top=False input_shape=(TARGET_SIZE[0] TARGET_SIZE[1] 3))chopped_Xception = Model(inputs=[base_model2.input] outputs=[base_model2.layers[90].output])classification_output2 = GlobalAveragePooling2D()(chopped_Xception.output)classification_output2 = Dense(units=1 activation=&#39;sigmoid&#39;)(classification_output2)localization_output2 = Flatten()(chopped_Xception.output)localization_output2 = Dense(units=4 activation=&#39;relu&#39;)(localization_output2)model2 = Model(inputs=[chopped_Xception.input] outputs=[classification_output2 localization_output2])model2.summary()Compiling and fitting the model by Xception network:model2.compile(optimizer=&#39;adam&#39; metrics=[&#39;accuracy&#39;]loss=[&#39;binary_crossentropy&#39; &#39;mse&#39;]loss_weights=[800 1]  )#lets run it through 10 epochshistory3=model2.fit(x_train [y_class_train y_box_train] validation_data=(x_validation [y_class_validation y_box_validation])epochs=10verbose=True)history3Plotting the accuracy and loss for it.def plot_training_history(history model):plt.plot(history.history[&#39;dense_9_accuracy&#39;])plt.plot(history.history[&#39;val_dense_9_accuracy&#39;])plt.ylabel(&#39;accuracy&#39;)plt.xlabel(&#39;epoch&#39;)plt.legend([&#39;training&#39; &#39;validation&#39;] loc=&#39;best&#39;)plt.show()plt.plot(history.history[&#39;dense_9_loss&#39;])plt.plot(history.history[&#39;val_dense_9_loss&#39;])plt.title(&#39;model loss&#39;)plt.ylabel(&#39;loss&#39;)plt.xlabel(&#39;epoch&#39;)plt.legend([&#39;training&#39; &#39;validation&#39;] loc=&#39;best&#39;)plt.show()plot_training_history(history3 model2)Image by AuthorLet&#39;s print the images after the changes and trained using Xception and print the IOU of the two boxes.Let’s see the images processed using Xception:for _ in range(6):i = np.random.randint(len(processed_data))img species true_bounding_box = processed_data[i]pred = model2.predict(np.array([preprocess_input(img)]))if pred[0][0] &lt; .5:print(&quot;it is a Cat by Xception&quot;)else:print(&quot;it is a dog by Xception&quot;)plot_with_box(img BoundingBox(*pred[1][0]) true_bounding_box)Results:Image by AuthorNow we will test the model with few random samples.#testing with randSome_Random_samples = [&#39;Abyssinian_174&#39;&#39;american_bulldog_59&#39;]for sample_name in Some_Random_samples:path_to_image = os.path.join(data_images sample_name + &#39;.jpg&#39;)print(path_to_image)img _ = resize_image_with_bounds(path_to_image target_size=TARGET_SIZE)pred = model2.predict(np.array([preprocess_input(img)]))if pred[0][0] &lt; .5:print(&quot;YesIts a Cat by Xception&quot;)else:print(&quot;Yes Its a dog by Xception&quot;)plot_with_box(img Bounding_Box(*pred[1][0])true_bounding_box)Results:Image by AuthorXception performs well and is giving quite accurate predictions.The IOU values look good while trying with Xception and MobileNet and Resnet.The final validation accuracy at the final layer obtained for MobilNet= 0.8125The final validation accuracy at the final layer obtained for ResNet= 0.7969The final validation accuracy at the final layer obtained for Xception= 0.8438(You can get the exact final validation accuracy when you obtain the results for each epoch in the training of each model).All the pre-trained networks such as MobileNet ResNet and Xception performed satisfactorily.But accuracy-wise MobileNet and Xception did well but in terms of IoU the prediction fluctuated in all these networks.When it comes to pictures that have some sort of anonymity it varies so.But the IOU was quite good in most of the pictures.The measurement using IOU clearly makes us understand in which picture we got bad IOU and up to what extent the predicted bounding box differs from the real one.Observations from accuracy plots and loss plots of each modelFor the first pre-trained model MobileNet I found that the training accuracy goes above the validation accuracy after the initial epochs while plotting the accuracy and loss in every epoch. The validation loss was also high in the model.For the second pre-trained model ResNet while plotting from the initial or the starting epoch itself the training accuracy was exceedingly above the validation accuracy. The validation loss was too high!For the third pre-trained model Xception while plotting the training accuracy was found above the validation accuracy from the 4th epoch itself. Also the validation loss was high.That is the models are getting overtrained.I felt that according to IOU all these models performed quite well except for a few confusing images.And in Xception some pictures give good IOU values too!Overall Xception was observed to be performing well for this dataset.I hope that from this article you would have got an idea of how to do object localization in a dataset and experiment with various pre-trained networks such as Mobile Net ResNet and Xception.The results might vary according to the dataset you will be taking but this will definitely help you make use of the data by performing various experiments or tests and coming up with the best pre-trained network according to the context of your study.Object localization using pre-trained CNN models such as MobileNet ResNet &amp; Xception was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,8,Is Data Science Truly Dead?,https://towardsdatascience.com/is-data-science-truly-dead-b11cd76eafc8?source=rss----7f60cf5620c9---4,OpinionFrom Data Scientist to AI PractitionerImage by Steve Bidmead from PixabayPerhaps you’ve heard it or read about it or both. The data scientist is dying and there is little we can do to hold on to our cushy salaries rock-star-like images and inflated egos. Obviously I am overstating things here for dramatic effect but the message is still anxiety-provoking for many data science professionals who have begun to smell blood in their industrial waters as concepts like “citizen data scientist” “democratization of analytics” and “automated machine learning” are being thrown around by more and more executive teams. Such fears were stoked a few years back when Matt Tucker’s article “The Death of the Data Scientist” was published on Data Science Central though it wasn’t the first to make such a claim. But are data scientists as we know them today truly a breed bound for extinction? In the remainder of this post I explore this idea while offering an alternative perspective on what the future may look like for the current data science professional.The Rise of the MachinesAt its most fundamental level the argument goes something like this: many of the activities of the data scientist are quantifiable or statistical in nature and are as a result automatable. Therefore the better we can orchestrate statistical models together in an automated fashion the less need there is for a data scientist to be pulling on those levers that select optimize and deploy their data-driven insights. Indeed companies and products such as DataRobot Google’s AutoML and the ever-expanding access to pre-trained service-based data science models (Azure Cognitive Services Google Ai Services AWS Watson) have made significant strides to achieve just that an artificial data scientist.Image by Stefan Keller from PixabayFrom Popularity to NecessityDespite this dire prognosis of a field that was always poorly defined anyway those who claim the title of data scientist have nonetheless developed ample skill to evolve with the coming wave of artificial data science. Thus we must replace our conjured-up images of the Skynets of the world rising to overthrow the last remaining strongholds of human data scientists with images of explorers riding the hype-wave of artificial intelligence technologies that are fundamentally embedded with skills that only the human data scientist truly understands. To achieve such evolution there are three areas the practicing data scientist must focus on and that the employer of the future must inspire: an ever evolving/expanding toolkit the importance of the user experience and the evangelism of a trade.The Ever Evolving/Expanding ToolkitIf there is one thing that data scientists are good at it’s catching a buzz (and a few new tools along the way). The concept of data science itself is a buzz term that many professionals with any statistical understanding in business attached to themselves in order to improve their marketability and to good effect. Why should we expect the building wave of artificial intelligence to be any different? As the concept of data scientist has evolved so too have the tools associated with it and thus the professionals in this field have been caught in a constant race to remain relevant by exposing themselves to the newest tools being made available. Although the rate of change has been near to overwhelming those who have survived and been able to demonstrate competence around the core functionality of these data science technologies are well poised to take advantage of the tools of artificial intelligence. Thus the data scientists who learn to evolve will learn how to rebrand themselves as practitioners of artificial intelligence. But to be able to convince others of this rebrand such professionals will need to continue to expand on their toolkits. Whereas the early 2000’s brought us Hadoop NoSQL IoT Python’s scikit-learn Tensorflow and Spark the next generation will be leveraging AI-as-a-Service cloud computing intelligent automation and containerization for analytics. This means that data scientists must continue to learn how to leverage API calls architect cloud environments that support data science and deploy analytics to expose API endpoints.Image by Ed Zilch from PixabayThe Importance of the User ExperienceAs you can see from above statistical tools are not the only tools that will help data scientists to survive in this quickly changing landscape. Artificial intelligence is not merely statistical technologies but rather it is the embedding of those statistical technologies into user experiences. Thus the savvy data science survivalist will identify opportunities to solve problems using embedded statistical analytics. Such efforts will require a greater understanding of software programming concepts which the data scientist is already well-poised for through the acquisition of open source scripting tools and the ability to work more closely with application development teams. There are many ways to tackle the user experience problem from both a technical as well as a theoretical (see our previous blog post as one example) perspective and what works will always depend on satisfying the user but the key is to identify strategies whereby statistical models improve the user experience. In this way data scientists will need to continue to evolve their approach to problem solving. Where once we focused on using cutting edge modeling techniques to extract insights from data we now need to focus on their utility within an application.Image by Gerd Altmann from PixabayEvangelizing a TradeAnd finally because the true test of our data science products depends on the user’s ability to get value from them we must be prepared to take our specialized understanding of these AI-enabling technologies and empower the citizen data scientist rather than pontificate over the sacredness of our special anointed knowledge. Despite the apparent ease-of-use promised by the onslaught of automated data science products citizen data scientists will still lack understanding of their application. As one Reddit user so elegantly put it “most people can barely use Excel and even most data/business analysts have a hard time understanding anything beyond basic aggregation and statistics.” Thus businesses will look to data scientists to train the citizen data scientist of the future to use those tools as use cases permit. The reason that data scientists will be required is because data science is not a tool but rather it is a way of thinking and tackling problems. Tools certainly enable new ways of thinking but people need to be trained on how to think about the tool in order for the tool to change their approach to solving problems. In short we must evangelize the tools that enable the artificial data scientist. In this vain data scientists become the hub of both artificial and human data science products within an organization and the citizen data scientists the spokes.Image by Comfreak from PixabayFrom Data Scientist to AI PractitionerIn conclusion the data scientist is not dead or dying for that matter but is instead in need of a coming evolution. Those who are most successful in continuing to expand their tool kits to leverage AI services expose results to and interact with applications and impart their way of thinking to enable others will be the most confidently poised to meet the coming needs of the AI practitioner for the future of digital enterprise.Like engaging to learn more about data science? Join me.Is Data Science Truly Dead? was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,8,Forecasting wind power from multiple numerical weather predictions,https://towardsdatascience.com/forecasting-wind-power-from-multiple-numerical-weather-predictions-63c9a67cfc5?source=rss----7f60cf5620c9---4,Forecasting Wind Power from Multiple Numerical Weather PredictionsAn application of data science techniques in the renewable energy space.With renewed interest in wind energy production in the United States and across Europe it is timely to revisit important factors such as variability and predictability of the renewable energy resource.Photo by Waldemar Brandt on UnsplashIn this article I discuss the following:Variability and predictability of wind energyChallenges of using multiple weather predictionsTips for handling messy weather dataFeature engineeringNon-time series approach to predicting wind powerVariability and predictability of wind energyA dependable energy resource needs to have low variability and high predictability. While moderate variation is tolerable poor predictability is unacceptable and can result in huge revenue loss.Wind energy variability is due to strong dependence on weather which changes during the course of the day and seasonally. So accurate weather prediction is necessary to achieve useful wind power forecast.It is well known that accuracy of weather predictions improves with shorter forecast horizon. In addition combining predictions from models using different numerical techniques can be beneficial. Hence wind farms rely on multiple sources of weather predictions across models generated at different time of day or day of week.Although weather cannot be controlled the wind industry can tap into the advancement of artificial intelligence technologies to improve predictability of the energy resource.Challenges of using multiple weather predictionsLet’s discuss some challenges encountered when using multiple weather predictions for wind power forecast.Variables may be predicted at different heights:Wind speed varies with height. Different models may provide speed data at different heights making it difficult to infer missing values across models without additional wind farm specific parameters or formulas.Massive missing data at inference time: Weather data may consist of predictions from different models generated at different times. Each column may have several missing data because not all weather predictions will be available at the time of wind power forecast. This kind of missing data cannot be imputed by simple known strategies such as column wise mean of all observations or by row removal.The missing values in a sample training and test data are shown in yellow below.Image by authorImage by authorTips for handling messy weather dataSome tips for processing weather data from multiple models and at different times are outlined below.Fill missing values with mean of most recent available predictions from same model (column wise say within recent three hours).Fill other missing values with mean across models starting with most recent available predictions.If there are still more missing values use wind farm specific parameters and standard formulas to map wind speed from known height to another with missing speed.Weather predictions obtained at different times of the day may be aggregated by model to reduce the feature space and number of missing values.It is important to have at least two of the above tips implemented in the data pipeline for robustness of the machine learning model in production.Finally standard row removal procedure may be applied if any missing values are remaining.Feature engineeringBased on domain knowledge the magnitude of wind velocity computed from horizontal and vertical components is very important for wind power forecast. This variable should be calculated for each model.Adding features such as standard deviation of velocity magnitude at different heights may improve forecast accuracy. In addition training a model on one-hot-encoded data from multiple wind farms may yield better results than separately training on fewer datasets from each farm.Important features relating to wind farm operations such as the fraction of turbines in service could boost forecast performance. This feature is not available for the forecast example in this article. As a result some power forecasts were obtained even when a wind farm appears to be shutdown.Sweetviz library was used to explore the variable distributions and relationship between variables. The distributions can be found here and associations between variables are shown below:Image by author using sweetviz libraryVelocity magnitude shows the strongest association with the target variable (produced power) as expected based on scientific knowledge from the field of fluid dynamics. This emphasizes the importance of physics when applying data-driven solution to engineering problems.Non-time series approach to predicting wind powerAlthough the wind power produced from a turbine or wind farm is a time series with some seasonality the forecast can be made without using lag features from the target variable. In adition this method is robust and simpler to implement than time series approach.Power production corresponding to the test set is not available. Hence a validation set is created from the training data to evaluate model performance. The new training set is 80 percent of the original training data and each observation in the data represents hourly report indexed by the feature named ID.XGBoost algorithm is used to forecast hourly wind power (MW) produced at six wind farms. Cumulative absolute percentage error (CAPE) is used to assess model performance.def cape_func(y_true y_pred):        return 100*np.sum(np.abs(y_pred-y_true))/np.sum(y_true)The plots of wind power (MW) versus ID (index of hourly wind power produced) are displayed below for six different wind farms considered.Image by authorImage by authorImage by authorImage by authorImage by authorImage by authorPerformance of the model varies tangibly across wind farms suggesting that there are some missing wind farm specific variables that could improve the forecast. Examples of such variables include the fraction of wind turbines in service at the time of forecast and total installed capacity.Best performance was obtained for wind farm 6 with CAPE value of 21.7 percent. The jupyter lab code for this forecast example including messy weather data cleaning can be found here.Next stepsArtificial neural network algorithm will be implemented using the non-time series approach described here and compared with the XGBoost results. In addition time series approach using long-short term memory (LSTM) algorithm will be investigated. Furthermore all models from time series and non-time series approach will be evaluated using the CAPE metric.ConclusionsVariability and predictability of wind energy and the basis for using multiple numerical weather predictions in wind power forecast were discussed.Massive missing data and mismatch in the height at which a variable is predicted needs to be handled carefully and some tips were provided for tackling the challenges.In addition knowledge of relevant physics is critical when applying data-driven solution to engineering problems.Finally wind power was forecasted using the numerical weather predictions and wind farm specific variables without using lag features based on the target.For more open source renewable energy related contributions support wind energy analytics.ResourcesBertrand F. Powerful EDA (Exploratory Data Analysis) in just two lines of code using SweetvizData source: https://challengedata.ens.fr/participants/challenges/34/Forecasting wind power from multiple numerical weather predictions was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,8,The Measure of Central Tendencies in Statistics -A Beginner’s Guide,https://www.analyticsvidhya.com/blog/2021/07/the-measure-of-central-tendencies-in-statistics-a-beginners-guide/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Statistics. Whenever I hear this term I imagine of below Image ... 
The post The Measure of Central Tendencies in Statistics -A Beginner&#8217;s Guide appeared first on Analytics Vidhya."
2021,7,8,Building a Classification Model and Making predictions Using Google Automl and Bigquery,https://www.analyticsvidhya.com/blog/2021/07/building-a-classification-model-and-making-predictions-using-google-automl-and-bigquery/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction For the analysis I have used a power quality analysis ... 
The post Building a Classification Model and Making predictions Using Google Automl and Bigquery appeared first on Analytics Vidhya."
2021,7,8,How to treat outliers in a data set?,https://www.analyticsvidhya.com/blog/2021/07/how-to-treat-outliers-in-a-data-set/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction When we started our data science journey and worked with ... 
The post How to treat outliers in a data set? appeared first on Analytics Vidhya."
2021,7,8,Indexing in Natural Language Processing for Information Retrieval,https://www.analyticsvidhya.com/blog/2021/07/indexing-in-natural-language-processing-for-information-retrieval/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Overview This blog covers GREP(Global-Regular-Expression-Print) and its drawbacks Then we move ... 
The post Indexing in Natural Language Processing for Information Retrieval appeared first on Analytics Vidhya."
2021,7,8,Building a Hand Tracking System using OpenCV,https://www.analyticsvidhya.com/blog/2021/07/building-a-hand-tracking-system-using-opencv/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon OpenCV is a library used for computer vision applications. With help ... 
The post Building a Hand Tracking System using OpenCV appeared first on Analytics Vidhya."
2021,7,8,Getting Started With Snowflake Data Platform,https://www.analyticsvidhya.com/blog/2021/07/getting-started-with-snowflake-data-platform/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Snowflake is a cloud data platform solution with unique features ... 
The post Getting Started With Snowflake Data Platform appeared first on Analytics Vidhya."
2021,7,8,AutoML using Pycaret with a Regression Use-Case,https://www.analyticsvidhya.com/blog/2021/07/automl-using-pycaret-with-a-regression-use-case/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Overview In simple words PyCaret is a machine learning library for ... 
The post AutoML using Pycaret with a Regression Use-Case appeared first on Analytics Vidhya."
2021,7,8,15 Python Built-in Functions which You Should Know while learning Data Science,https://www.analyticsvidhya.com/blog/2021/07/15-python-built-in-functions-which-you-should-know-while-learning-data-science/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction In my previous article I completely discuss all the basic ... 
The post 15 Python Built-in Functions which You Should Know while learning Data Science appeared first on Analytics Vidhya."
2021,7,8,Working with Modules in Python: Must Known Fundamentals for Data Scientists,https://www.analyticsvidhya.com/blog/2021/07/working-with-modules-in-python-must-known-fundamentals-for-data-scientists/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Nowadays the Python programming language becomes one of the most ... 
The post Working with Modules in Python: Must Known Fundamentals for Data Scientists appeared first on Analytics Vidhya."
2021,7,8,"Smarten Announces SmartenApps for Tally, Out-of-the-Box Augmented Analytics Mobile App and Web Portal!",https://www.smarten.com/blog/smarten-announces-smartenapps-for-tally-out-of-the-box-augmented-analytics-mobile-app-and-web-portal/,Smarten is pleased to announce its SmartenApps for Tally® augmented analytics solution. SmartenApps is designed to integrate with Tally ERP via cloud-based access for easy user access and on-the-road task completion. With SmartenApps for Tally business users can access Tally ERP Solutions data on iOS [&#8230;]
2021,7,8,Build Sketches from Photographs using OpenCV,https://www.analyticsvidhya.com/blog/2021/07/build-sketches-from-photographs-using-opencv/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Table of content What is OpenCV and its Uses Project Explanation ... 
The post Build Sketches from Photographs using OpenCV appeared first on Analytics Vidhya."
2021,7,8,Topic Modelling With LDA -A Hands-on Introduction,https://www.analyticsvidhya.com/blog/2021/07/topic-modelling-with-lda-a-hands-on-introduction/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Imagine walking into a bookstore to buy a book on ... 
The post Topic Modelling With LDA -A Hands-on Introduction appeared first on Analytics Vidhya."
2021,7,8,Data Visualization – A Useful tool to Explore Data,https://www.analyticsvidhya.com/blog/2021/07/data-visualization-a-useful-tool-to-explore-data/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Data visualization and its importance Let&#8217;s see what does technical definition ... 
The post Data Visualization &#8211; A Useful tool to Explore Data appeared first on Analytics Vidhya."
2021,7,8,IBM Product Master on IBM Cloud Pak for Data,https://www.ibm.com/blogs/journey-to-ai/2021/07/ibm-product-master-on-ibm-cloud-pak-for-data/,"I’m excited to announce that IBM® Product Master is available on IBM Cloud Pak® for Data 4.0 IBM&#8217;s flagship data platform for managing all of an organization’s data-related tasks from data cataloging to deriving meaningful insights from the data and operationalizing these insights at scale.  Today I’ll explain the new possibilities introduced by this platform [&#8230;]
The post IBM Product Master on IBM Cloud Pak for Data appeared first on Journey to AI Blog."
2021,7,8,A Lightning Fast Look at Single Line Exploratory Data Analysis,https://www.kdnuggets.com/2021/07/single-line-exploratory-data-analysis.html,Here's a very quick look at how you can perform EDA with a single line of code using D-Tale.
2021,7,8,Pandas not enough? Here are a few good alternatives to processing larger and faster data in Python,https://www.kdnuggets.com/2021/07/pandas-alternatives-processing-larger-faster-data-python.html,While the Pandas library remains a crucial workhorse in data processing and management for data science some limitations exist that can impact efficiencies especially with very large data sets. Here a few interesting alternatives to Pandas are introduced to improve your large data handling performance.
2021,7,8,MLOps is an Engineering Discipline: A Beginner’s Overview,https://www.kdnuggets.com/2021/07/mlops-engineering-discipline.html,MLOps = ML + DEV + OPS. MLOps is the idea of combining the long-established practice of DevOps with the emerging field of Machine Learning.
2021,7,8,Using different fonts with ggplot2,https://www.r-bloggers.com/2021/07/using-different-fonts-with-ggplot2/," I was recently asked to convert all the fonts in my ggplot2-generated figures for a paper to Times New Roman. It turns out that this is easy but it brought up a whole host of questions that I don’t have … Continue reading →


The post Using different fonts with ggplot2 first appeared on R-bloggers."
2021,7,8,Exploratory Functional PCA  with Sparse Data,https://www.r-bloggers.com/2021/07/exploratory-functional-pca-with-sparse-data/,"
I have written about the basics of Functional Data Analysis in three prior posts. In Post 1 I used the fda package to introduce the fundamental concept of using basis vectors to represent longitudinal or time series data as a curv...


The post Exploratory Functional PCA  with Sparse Data first appeared on R-bloggers."
2021,7,8,India Fish Stat 2020,https://www.r-bloggers.com/2021/07/india-fish-stat-2020/,"Dashboard based on India Fisheries statistics 2020.
The post India Fish Stat 2020 first appeared on R-bloggers."
2021,7,8,Datometry to Teradata and Oracle customers: Move to cloud without changing code,https://www.zdnet.com/article/datometry-to-teradata-and-oracle-customers-move-to-cloud-without-changing-code/#ftag=RSSbaffb68,Datometry introduces a runtime that enables customers to move off Teradata to cloud data warehouses without changing their applications and is now extending that to Oracle.
2021,7,8,"Building a flexible, intuitive, and fast forecasting library",http://practicalquant.blogspot.com/2021/07/building-flexible-intuitive-and-fast.html,The Data Exchange Podcast Rezan Hosseini and Albert Chen on the impressive new forecasting library Greykite and its flagship algorithm Silverkite.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Take the 2021 Data Engineering Survey and get a free copy of the results and be entered into a drawing for a free Data Teams book &amp; other prizes. Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,7,7,How Data Scientists Can Troubleshoot ETL Issues Like a Data Engineer,https://towardsdatascience.com/how-to-troubleshoot-etl-issues-like-a-data-engineer-d761f7d361d4?source=rss----7f60cf5620c9---4,Common causes and how to handle themContinue reading on Towards Data Science »
2021,7,7,Parameters in R Markdown,https://towardsdatascience.com/parameters-in-r-markdown-ade0cfac0c9b?source=rss----7f60cf5620c9---4,Using Parameters for Rapidly Reproducible AnalysisContinue reading on Towards Data Science »
2021,7,7,Why the CLI is Essential for Data Scientists,https://towardsdatascience.com/why-the-cli-is-essential-for-data-scientists-cd7016f86d49?source=rss----7f60cf5620c9---4,CLI examples and use cases to create more efficient Machine Learning workflowsContinue reading on Towards Data Science »
2021,7,7,5 Resources to Help You Learn and Master Git and Version Control,https://towardsdatascience.com/5-resources-to-help-you-learn-and-master-git-and-version-control-c9824caaca28?source=rss----7f60cf5620c9---4,Version control can be less of a hassle with these resources.Continue reading on Towards Data Science »
2021,7,7,FEDERATED LEARNING : Your data stays with you,https://towardsdatascience.com/federated-learning-your-data-stays-with-you-fc1d49b35ec4?source=rss----7f60cf5620c9---4,Federated Learning : Your Data Stays with YouWith federated learning we can improve centralized machine learning model performance in an alternative way without sharing user’s data.Photo by John Salvino on UnsplashThis is the 21st century and “Data is the new gold”. With the emergence of new technologies higher computational power and of course huge amount of data Artificial intelligence has gained a lot of momentum and the global artificial intelligence (AI) market is expected to grow from USD 58.3 billion in 2021 to USD 309.6 billion by 2026 at a Compound Annual Growth Rate (CAGR) of 39.7% during the forecast period as per MarketsandMarkets.While Artificial Intelligence brings a lot of comfort in our daily life without us even realizing it there are some challenges as well. And data privacy is one of them. Apps gather your data ( of course not to sell them to third party applications or do they? ) to give you more personalized recommendations and results ( along with ads 😐).SO THE QUESTION ARISES DOES THE DATA LEAVE YOUR PHONE ?The answer is yes. For example when you grant access of your location information to any application it collects your location data. Now it’s upon the application how they want their AI algorithm to use it. There are two options :On server : The machine learning/deep learning model is deployed in the server where it trains the model on the data it receives from billions of smartphones.On-device : The ML/DL model is deployed on the phone where the user data is used for training and improving the model for better recommendations.Both have their own advantages and disadvantages. Training on the server needs huge amount of storage to store the data and a world-class security to safeguard them from data breaches. Whereas on-device training is trained on limited amount of data and the model performance is compromised.Solution : Training a centralized model on decentralized data. Boom!!!!Okay let me explain it.For better user experience a company will want data from billions of smartphones to be trained on a centralized model present in the server. But for that the data has to leave the smartphones. But we don’t want that right? Instead if a copy of the centralized model is present in all the devices on which the training is happening then we have already solved the problem of performance. Now somehow we have to aggregate all the results from each smartphone into a single one. Now the the training results (as we Machine Learning engineers call it : weights) can obviously be sent to the server where they get merged. Now the weights are highly encrypted and the key lies with the model that is present on the device.Ohhhhh yeahh!!!And to further improve user privacy secure-aggregation protocol is used which enables the server to combine the encrypted results only to decrypt the aggregate by adding zero-sum masks. To read more on this please refer to this paper.Finally the aggregated weights are sent back to the model on the device and we now have a new improved model update.With every new start comes up new challenges. And trust the beauty of new challenges they are here to help us grow.So let’s discuss about the challenges.Some data is very specific to a particular user. And that will bring down the overall model performance. We don’t want the model to memorize the rare data from a particular user.Solution : a) By devising a mechanism to control the amount an individual user can contribute towards the overall result. b) By adding noise to more specific data. This is also referred to as Differential Privacy. I found this article quite intuitive.2. Now we have our new model formed from aggregated results. But how can we see how the model is performing on new data before rolling out the update?Solution: Simple! We can apply the same concept of train validation split that we all know of. Instead here we will have the users as our experiment!!!! Sounds interesting hunh? We will split users as training and validation. From a universal set of smartphone users we have a small proportion of them who will validate the result. And the rest will train the model. So the model is tested on real time data.3. Can simple averaging aggregation work for all the algorithms? Let’s explain this with two examples:a) Let’s take Normal bayes (openCV). The mean vector and the covariance matrix are highly influenced by the number of samples per class. Now let’s say we have two smartphones and a binary classification problem with class A and class B.User 1:Image by author : Contains three samples of class A and one of class BUser 2:Image by Author : contains 3 samples of class B and one of class A.Where x ki ( j ) represents the value of the i-th feature attribute of the j-th sample belonging to the class k in the training sample and the final n -dimensional (a total of n feature attributes) mean vector of class k ‘μk’ is estimated as:So the mean vector’s values are highly influenced by the number of samples per class. So the mean vector μA(1) of user 1 has 75% influence of class A and μA(2) 25%. So when we merge them by taking the average 1/2(μA(1) + μA(2)) are we able to retain the information specific to the classes?b) For algorithms like SVM whose weight is nothing but the support vectors that depend on the number of samples in the dataset can we have a weight matrix that is of constant size for all the results? We night need to device aggregating algorithms specific to Machine learning task at hand.4. Trade off between Privacy and Accuracy :Sometimes to increase the privacy of the user’s data some noise is added which results in the data being deviated from it’s actual behavior thus resulting in some accuracy drop.Conclusion :Federated learning can solve a lot of problems related to user’s privacy while improving the model performance for better recommendations. This is a fairly new domain and more research can solve a lot of challenges faced by what we call as Collaborative learning.References:Federated Learning: Collaborative Machine Learning without Centralized Training DataFederated Learning: Strategies for Improving Communication EfficiencyTensorFlow FederatedFEDERATED LEARNING : Your data stays with you was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,7,Spotiscience: A Tool for Data Scientists and Music Lovers,https://towardsdatascience.com/spotiscience-a-tool-for-data-scientists-and-music-lovers-a3e32bd82ed1?source=rss----7f60cf5620c9---4,Spotiscience seeks to make it easy to download and predict data on Spotify’s musicPhoto by Matheus Ferrero on UnsplashWho doesn’t like to work with music? Many of us work all day with Spotify running in the background processes of our computers while the random music from the playlists and artists that we like are generating a magical atmosphere with their melodies. they deliver that energy necessary to work with full productivity.In order to integrate my work as a Data Scientist and my passion for music I decided to create a tool called “Spotiscience” which allows downloading data of songs artists albums and playlists using the official Spotify API. In addition this data is modeled to generate new data such as knowing the mood of a song or the topics of the song lyrics and finding similar songs all this sounds very interesting right? If you want to know more keep reading this Article!In this article you will learn to:Download data of songs albums playlists and artists from the Spotify APIDownload lyrics of songs from API GeniusPredict the music mood of a songFind most relevant topics of a song lyricSearch similar songs in albums artist discography and playlists from SpotifyIndex1.1 SpotiscienceDownloader1.1.1 Initial Settings1.1.2 Extraction of Song Features1.1.3 Extraction of Albums1.1.4 Extraction of Playlists1.1.5 Extraction of Playlist and Artist Information1.2 SpotisciencePredicter1.2.1 Initial Settings1.2.2 Prediction of Song Mood1.2.3 Prediction of Topics from Song Lyrics1.2.4 Prediction of Similar Songs1. SpotiscienceSpotiscience is a project that I created on GitHub programmed in Python in which you can interact with the Spotify API and Genius API to extract data and features of songs albums artists and playlists. You can also analyze this data to generate new information such as mood prediction topic modeling and mathematical distances to find similar songs. To download Spotiscience you can access the Github repository.cristobalvch/spotiscienceTo understand the application and configuration of Spotiscience I will detail the 2 main classes of this tool:1.1 SpotiscienceDownloaderThis class extracts the data from the Spotify API and Genius API.1.1.1 Initial SettingsTo use it it must be set as follows:import spotiscience#create a dictionary with authorization keysCREDENTIALS = {}CREDENTIALS[&#39;client_id&#39;] = &quot;your_spotify_client_id&quot;CREDENTIALS[&#39;client_secret&#39;] = &quot;your_spotify_client_secret&quot;CREDENTIALS[&#39;redirect_url&#39;] = &quot;your_redirect_url&quot;CREDENTIALS[&#39;user_id&#39;] = &quot;your_spotify_user_id&quot;CREDENTIALS[&#39;genius_access_token&#39;] = &quot;your_genius_access_token&quot;&quot;&quot;&quot;You also can set your credentials id on credentials.py&quot;&quot;&quot;# returns &#39;downloader class&#39;sd = spotiscience.SpotiScienceDownloader(credentials=CREDENTIALS)To obtain the authorization credentials for the Spotify API and Genius API you can watch the following tutorials:Authentication Spotify API TutorialAuthentication Genius API TutorialTo obtain the “user_id” of your Spotify Account you need to open the Desktop Spotify Application go to “profile” and copy the link to profile as follows:Photo by AuthorYou will obtain this result your user_id is the part in bold all the other parts of the link can be deleted.“https://open.spotify.com/user/{USER_ID}?si=9f52cafadbf148b2”1.1.2 Extraction of Song FeaturesTo extract the features of a song you should search the song on Spotify and then copy the link of the song as follows:for this case I copied the link of the song “Blinding Lights” by The Weeknd:song_copy_link = &quot;https://open.spotify.com/track/0VjIjW4GlUZAMYd2vXMi3b?si=369f90167c9d48fb&quot;song = sd.get_song_features(song_id=song_copy_link)The result will be a dictionary with the following song features. To obtain more information about these features you can read the official documentation about Audio Features on Web API Spotify{&#39;id&#39;: &#39;0VjIjW4GlUZAMYd2vXMi3b&#39; &#39;name&#39;: &#39;Blinding Lights&#39; &#39;artist&#39;: &#39;The Weeknd&#39; &#39;album&#39;: &#39;After Hours&#39; &#39;release_date&#39;: &#39;2020-03-20&#39; &#39;popularity&#39;: 94 &#39;length&#39;: 200040 &#39;acousticness&#39;: 0.00146 &#39;danceability&#39;: 0.514 &#39;energy&#39;: 0.73 &#39;instrumentalness&#39;: 9.54e-05 &#39;liveness&#39;: 0.0897 &#39;valence&#39;: 0.334 &#39;loudness&#39;: -5.934 &#39;speechiness&#39;: 0.0598 &#39;tempo&#39;: 171.005 &#39;key&#39;: 1 &#39;time_signature&#39;: 4}You also can extract the music genre and the lyrics of the song as follows:# Returns song lyricsd.get_song_lyrics(songname=song[&#39;name&#39;]artistname=song[&#39;artist&#39;])#Returns song Genresd.get_song_music_genre(song_id=song[&#39;id&#39;])1.1.3 Extraction of AlbumsTo extract the features of the songs from a album you must search the album or albums on Spotify and copy the link of the album. The album extraction method has an id parameter that receives a string or list of strings of the albums links and it’s necessary to specify the parameter is_artist in “False”:#Returns songs features of album or albumsalbums =[‘https://open.spotify.com/album/4yP0hdKOZPNshxUOjY0cZj?si=p5ItRNgXRlarmq4cihAVmA&amp;dl_branch=1&#39;‘https://open.spotify.com/album/6Yf9kML4wT3opmXhTUIfk7?si=clKN-hzuTB236hINPATp-Q&amp;dl_branch=1&#39;]sd.get_albums_song_features(id=albumsis_artist=False)The result will be a dictionary where the keys are the album’s name and the content corresponds to a list with all the features of album’s songs.It’s also possible to download the discography of an artist for this case the parameter id just receives a string and It’s necessary to specify is_artist in “True” as follows:#Returns songs features of artist&#39;s discographyartist = &#39;https://open.spotify.com/artist/4fvA5FEHcdz97gscK90xZa?si=HNLNN7-dS5OR2W9TIUqQug&amp;dl_branch=1&#39;sd.get_albums_song_features(id=artistis_artist=True)1.1.4 Extraction of PlaylistsSong features can be extracted from a playlist as well. For this case the playlist_id parameter only receives a single string and the total number of songs to be extracted must be specified as follows:#Return song features of playlistplaylist = ‘https://open.spotify.com/playlist/37i9dQZF1DXcfZ6moR6J0G?si=da1435f1a0804933&#39;sd.get_playlist_song_features(playlist_id=playlistn_songs=50)The result will be a dictionary where the key is the name of the playlist and the content corresponds to a list with all the features of playlist songs.1.1.5 Extraction of Playlist and Artist InformationFinally the main information of a playlist and an artist can be extracted as follows:playlist = ‘https://open.spotify.com/playlist/37i9dQZF1DXcfZ6moR6J0G?si=da1435f1a0804933&#39;artist = &#39;metallica&#39;#Returns playlist informationsd.get_playlist_information(playlist_id=playlist)#Returns song informationsd.get_artist_information(artist=artist)The results will be 2 dictionaries with the information of the playlist and the artist.To have a better understanding of all the SpotiscienceDownloader methods you can take a look to the source code of the module downloader.py in the GitHub repo by clicking here.1.2 SpotiSciencePredicterThis class is for modeling song data using classification techniques for supervised learning topic modeling with natural language processing and song similarity with mathematical distances.1.2.1 Initial SettingsFor setting this class you only needs to call it as follows:import spotiscience# returns &#39;predicter class&#39;sp = spotiscience.SpotiSciencePredicter()1.2.2 Prediction of Song MoodTo perform song mood prediction I used a machine learning approach by tagging a group of songs from Mood Playlists created by Spotify then I trained a model with Random Forest Classifier algorithm to tag songs based on their features.For more information about this topic you can read my article of Music Mood Prediction by clicking hereTo predict the mood you just have to pass the data of the song extracted with SpotiscienceDownloader as follows:#returns the tag of mood sp.predict_song_mood(song=song)The result will be a string with the corresponding mood category these categories are; “sad calm energy and happy”1.2.3 Prediction of topics from song lyricsThe topic prediction of song lyrics uses any of the algorithms Latent Dirichlet Allocation Model (LDA) Non Negative Matrix Factorization Model (NMF) or Latent Semantic Indexing Model (LSI). To do this I based my code on the following article which you can read here.To predict the topic of lyrics you must configure the following parameters:lyric = the lyric of the songmodel = the model to use [options are “lsi””lda” or “nmf”]lang = language of the song lyric [options are “english” or “spanish”]n_grams = number of subsence of words to groupn_topics = number of returned topicstop_n = number of words per returned topicFor more information about the parameter n_grams you can read the official documentation about vectorization with sklearn by clicking herelyric = song_lyricsmodel = &#39;lda&#39; (available type &#39;lda&#39; &#39;lsi&#39; &#39;nmf&#39;)lang = &#39;english&#39; (available type &#39;english&#39;&#39;spanish&#39;)n_grams = (11)n_topics = 1top_n = 5#predict the topics of the song lyricsp.predict_topic_lyric(lyricmodellangn_gramsn_topicstop_n)1.2.4 Prediction of Similar SongsTo predict the similarity of songs I use the Manhattan Distance (l1) and Euclidean Distance (l2) to calculate the distance among song features and sorting the results in ascending.to predict song similarity you must to configure the following parameters:object = reference song to comparetarget = group of songs to evaluate in albums playlist or artistdistance = distance to use [options are “l1” and “l2”]n_features = number of song features to calculate distancetop_n = number of songs to return in tuple resultsFor more information about the parameter n_features you can read the source code of the method by clicking here.Example 1: Predicting which songs of “Nu Metal Generation” Playlist are most similars to song “Change (In the House of Flies)” by “Deftones”.playlist_link = &quot;https://open.spotify.com/playlist/37i9dQZF1DXcfZ6moR6J0G?si=452e104160384c8e&quot;song_link = &quot;https://open.spotify.com/track/51c94ac31swyDQj9B3Lzs3?si=5aca903582464acd&quot;target = sd.get_playlist_song_features(playlist_linkn_songs=70)object  = sd.get_song_features(song_link)distance = &#39;l2&#39;n_features = 6top_n = 10#returns most similar songs from playlistsp.predict_similar_songs(objecttargetdistancen_featurestop_n)Example 2: Predicting which songs of “Dua Lipa” Discography are most similars to the song “Blinding Lights” by “The Weeknd”artist_link = &quot;https://open.spotify.com/artist/6M2wZ9GZgrQXHCFfjv46we?si=VJ3J-isZRbSM5x2pNUnrhw&amp;dl_branch=1&quot;song_link = &quot;https://open.spotify.com/track/0VjIjW4GlUZAMYd2vXMi3b?si=369f90167c9d48fb&quot;target = sd.get_albums_song_features(id=artist_linkis_artist=True)object  = sd.get_song_features(song_link)distance = &#39;l2&#39;n_features = 6top_n = 10#returns most similar songs from playlistsp.predict_similar_songs(objecttargetdistancen_featurestop_n)The result in both examples is a dictionary where the key is the name of the song of reference (object) and the content is a list of tuples. Each tuple is a pair value of the name of the song and its distance with the reference song (object).Note: It’s also possible to predict similar songs in albums without having to download the entire discography of the artist. To do this you can use the album features on the target parameter.2. ConclusionMixing 2 different areas like data science and music can generate great options that serve to understand how music develops in a culture in constant change where the sound of an instrument a poetry lyric and vocal skills can be interpreted in so many different ways. With the help of technology we seek to achieve an approximation of these interpretations and meanings to study what is that invisible energy that makes music stay with us throughout the course of our lives. I hope Spotiscience can be one of those technologies that helps Data Scientists Developers and Music Lovers like me.3. Referenceshttps://www.kaggle.com/thebrownviking20/topic-modelling-with-spacy-and-scikit-learnhttps://towardsdatascience.com/predicting-the-music-mood-of-a-song-with-deep-learning-c3ac2b45229eMy Other Articles¡Data Science Para Todos!Artificial Intelligence: Extracting Qualifying Adjectives from Comments on Donald Trump Facebook…How to Analyze Emotions and Words of the Lyrics From your Favorite Music ArtistAutomate the Process of Adding Your User’s Saved Tracks by Genre to Playlists on SpotifyAutomating Playlists on Spotify From the Music Folders of your Computer.Clustering Music to Create your Personal Playlists on Spotify Using Python and K-Means.Una Forma Interactiva para Buscar y Analizar Ofertas de Trabajo en la Web.Una Forma Genial para Buscar y Analizar Arriendos de Departamentos en la Web.Cómo Hacer Reflexionar a Tu Computador con las Noticias del PaísSpotiscience: A Tool for Data Scientists and Music Lovers was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,7,The Hard Truth: Data Science Isn’t for Everyone,https://towardsdatascience.com/the-hard-truth-data-science-isnt-for-everyone-1689b7c05e62?source=rss----7f60cf5620c9---4,Hard work and effort sometimes aren&#x2019;t enough.Continue reading on Towards Data Science »
2021,7,7,Introducing Packed BERT for 2x faster Training in Natural Language Processing,https://towardsdatascience.com/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing-eadb749962b1?source=rss----7f60cf5620c9---4,New BERT packing algorithm for more efficient trainingAuthors: Dr. Mario Michael Krell and Matej KosecImage by author.By using a new packing algorithm we have sped up Natural Language Processing by more than 2 times while training BERT-Large. Our new packing technique removes padding enabling significantly more efficient computation.We suspect this could also be applied to genomics and protein folding models and other models with skewed length distributions to make a much broader impact in different industries and applications.We introduced Graphcore’s highly efficient Non-Negative Least Squares Histogram-Packing algorithm (or NNLSHP) as well as our BERT algorithm applied to packed sequences in a new paper [1].Computational Waste in NLP due to sequence paddingWe began to investigate new ways of optimising BERT training while working on our recent benchmark submissions to MLPerf™. The goal was to develop useful optimisations that could easily be adopted in real-world applications. BERT was a natural choice as one of the models to focus on for these optimisations as it is widely used in industry and by many of our customers.It really surprised us to learn that in our own BERT-Large training application using the Wikipedia dataset 50% of the tokens in the dataset were padding — resulting in a lot of wasted compute.Padding sequences to align them all to equal length is a common approach used with GPUs but we thought it would be worth trying a different approach.Sequences have a large variation of length for two reasons:The underlying Wikipedia data shows a large variation in document lengthThe BERT-pre-processing itself randomly reduces the size of extracted documents that are combined to generate a training sequenceFilling up the length to the maximum length of 512 results in 50% of all tokens being padding tokens. Replacing the 50% of padding by real data could result in 50% more data being processed with the same computational effort and thus 2x speed-up under optimal conditions.Figure 1: Wikipedia dataset distributions. Image by author.Is this specific to Wikipedia? No.Well then is it specific to language? No.In fact skewed length distributions are found everywhere: in language genomics and protein folding. Figures 2 and 3 show distributions for the SQuAD 1.1 dataset and GLUE datasets.Figure 2: SQuAD 1.1 BERT pre-training dataset sequence length histogram for maximum sequence length of 384. Image by author.Figure 3: GLUE dataset sequence length histograms for maximum sequence length of 128. Image by author.How can we handle the different lengths while avoiding the computational waste?Current approaches require different computational kernels for different lengths or for the engineer to programmatically remove the padding and then add it back in repeatedly for each attention block and loss calculation. Saving compute by blowing up the code and making it more complex was not appealing so we searched for something better. Can’t we just put multiple sequences together in a pack with a maximum length and process it all together? It turns out we can!This approach requires three key ingredients:An efficient algorithm to decide which samples to put together to have as little remaining padding as possibleAdjusting the BERT model to process packs instead of sequencesAnd adjusting the hyperparametersPackingAt first it seemed unlikely that you would be able to pack a large dataset such as Wikipedia very efficiently. This problem is commonly known as bin-packing. Even when packing is limited to three sequences or less the resulting problem would be still strongly NP-complete lacking an efficient algorithmic solution. Existing heuristics packing algorithms were not promising because they had a complexity of at least O(n log(n)) where n is the number of sequences (~16M for Wikipedia). We were interested in approaches that would scale well to millions of sequences.Two tricks helped us to reduce the complexity drastically:Limiting the number of sequences in a pack to three (for our first solution approach)Operating solely on the histogram of sequence length with one bin for each occurring lengthOur maximum sequence length was 512. So moving to the histogram reduced the dimension and complexity from 16 million sequences to 512 length counts. Allowing a maximum of three sequences in a pack reduced the number of allowed length combinations to 22K. This already included the trick to require the sequences to be sorted by length in the pack. So why not try 4 sequences? This increased the number of combinations from 22K to 940K which was too much for our first modelling approach. Additionally depth 3 already achieved remarkably high packing efficiency.Originally we thought that using more than three sequences in a pack would increase the computational overhead and impact the convergence behaviour during training. However to support applications such as inference which require even faster real-time packing we developed the highly efficient Non-Negative Least Squares Histogram-Packing (NNLSHP) algorithm.Non-Negative Least Squares Histogram-Packing (NNLSHP)Bin packing is quite often formulated as a mathematical optimization problem. However with 16 million sequences (or more) this is not practical. The problem variables alone would exceed most machines’ memory. The mathematical program for a histogram-based approach is quite neat. For simplicity we decided for a least squares approach (Ax=b) with histogram vector b. We extended it by requesting the strategy vector x to be non-negative and adding weights to allow for minor padding.The tricky part was the strategy matrix. Each column has a maximum sum of three and encodes which sequences get packed together to exactly match the desired total length; 512 in our case. The rows encode each of the potential combinations to reach a length the total length. The strategy vector x is what we were looking for which describes how often we choose whichever of the 20k combinations. Interestingly only around 600 combinations were selected at the end. To get an exact solution the strategy counts in x would have to be positive integers but we realised that an approximate rounded solution with just non-negative x was sufficient. For an approximate solution a simple out-of-the-box solver could be used to get a result within 30 seconds.Figure 4: Example of a strategy matrix for sequence length 8 and packing depth 3. The rows stand for the sequences of length 1–8 that get packed together and the columns stand for all possible length combinations in a pack with no particular ordering. Image by author.At the end we had to fix some samples that did not get assigned a strategy but those were minimal. We also developed a variant solver that enforces that each sequence gets packed potentially with padding and has a weighting dependent on the padding. It took much longer and the solution was not much better.Shortest-Pack-First Histogram PackingNNLSHP delivered a sufficient packing approach for us. However we were wondering if we could theoretically get a faster online capable approach and remove the limitation of putting only 3 sequences together.Therefore we took some inspiration from existing packing algorithms but still focused on the histograms.There are four ingredients for our first algorithm Shortest-pack-first histogram-packing (SPFHP):Operate on the counts of the histogram from longest sequences to shortestIf the current sequence length does not fit into any pack start a new set of packsIf there are multiple fits take the pack where the sum of sequence length is the shortest and modify the counts respectivelyCheck again for fits of the remaining countsThis approach was the most straightforward to implement and took only 0.02 seconds.A variant was to take the largest sum of sequence length instead of the shortest and split counts to get more perfect fits. Overall this did not change efficiency much but increased code complexity a lot.https://medium.com/media/56d00e2f0c3aa972406a6d7ff8aad22a/hrefWikipedia SQuAD 1.1 GLUE packing resultsTable 1 2 and 3 show the packing results of our two proposed algorithms. Packing depth describes the maximum number of packed sequences. Packing depth 1 is the baseline BERT implementation. The maximum occurring packing depth wen no limit is set is denoted with an additional “max”. The number of packs describes the length of the new packed dataset. Efficiency is the percentage of real tokens in the packed dataset. The packing factor describes the resulting potential speed-up compared to packing depth 1.We had four main observations:The more skewed a distribution the higher the benefits of packing.All datasets benefit from packing. Some even by more than a factor of 2.SPFHP gets more efficient when the packing depth is not limited.For a maximum number of 3 packed sequences the more complex NNLSHP is the more efficient it is (99.75 vs. 89.44).Table 1: Key performance results of proposed packing algorithms (SPFHP and NNLSHP) on Wikipedia. Image by author.Table 2: Performance results of proposed packing algorithms for SQUaD 1.1 BERT pre-training. Image by author.Table 3: Performance results of proposed packing algorithms for the GLUE dataset. Only the baseline and the SPFHP packing results without limiting the packing depth are displayed. Image by author.BERT processing adjustmentSomething interesting about the BERT architecture is that most processing happens on a token level which means it does not interfere with our packing. There are only four components that need an adjustment: the attention mask the MLM loss the NSP loss and the accuracy.The key for all four approaches to handle different numbers of sequences was vectorization and using a maximum number of sequences that can be concatenated. For attention we already had a mask to address the padding. Extending this to multiple sequences was straightforward as can be seen in the following TensorFlow pseudo code. The concept is that we made sure that attention is limited to the separate sequences and cannot extend beyond that.https://medium.com/media/77b7bf432a0644566575a7a31cfec6eb/hrefFigure 5: Example zero-one maskFor the loss calculation in principle we unpack the sequences and calculate the separate losses eventually obtaining the average of the losses over the sequences (instead of packs).For the MLM loss the code looks like:https://medium.com/media/25253b6aea63d08761802509d7328ad4/hrefFor the NSP loss and the accuracy the principle is the same. In our public examples you can find the respective code with our in-house PopART framework.Wikipedia overhead and speedup estimateWith our modification of BERT we had two questions:How much overhead does it bring with it?How much does the overhead depend on the maximum number of sequences that are put together in a pack?Since data preparation in BERT can be cumbersome we used a shortcut and compiled the code for multiple different packing depths and compared the respective (measured) cycles. The results are displayed in Table 4. With overhead we denote the percentage decrease in throughput due to changes to the model to enable packing (such as the masking scheme for attention and the changed loss calculation). The realized speed-up is the combination of the speed-up due to packing (the packing factor) and the decrease in throughput due to the overhead.Table 4: Estimated speed-up comparison of proposed packing algorithms (SPFHP and NNLSHP) on Wikipedia. Image by author.Thanks to the vectorization technique the overhead is surprisingly small and there is no disadvantage from packing many sequences together.Hyperparameter-adjustmentsWith packing we are doubling the effective batch size (on average). This means we need to adjust the training hyperparameters. A simple trick is to cut the gradient accumulation count in half to keep the same effective average batch size as before training. By using a benchmark setting with pretrained checkpoints we can see that the accuracy curves perfectly match.Figure 6: Comparison of learning curves for packed and unpacked processing with reduced batch size for the packed approach. Images by author.The accuracy matches: the MLM training loss may be slightly different at the beginning but quickly catches up. This initial difference could come from slight adjustments of the attention layers which might have been biased towards short sequences in the previous training.To avoid a slowdown it sometimes helps to keep the original batch size the same and adjust the hyperparameters to the increased effective batch size (doubled). The main hyperparameters to consider are the beta parameters and the learning rates. One common approach is to double the batch size which in our case decreased performance. Looking at the statistics of the LAMB optimizer we could prove that raising the beta parameter to the power of the packing factor corresponds to training multiple batches consecutively to keep momentum and velocity comparable.Figure 7: Comparison of learning curves for packed and unpacked processing with heuristics applied. Images by author.Our experiments showed that taking beta to the power of two is a good heuristic. In this scenario the curves are not expected to match because increasing the batch size usually decreases convergence speed in the sense of samples/epochs until a target accuracy is reached.Now the question is if in the practical scenario do we really get the anticipated speed-up?Figure 8: Comparison of learning curves for packed and unpacked processing in the optimized setup. Images by author.Yes we do! We gained an additional speedup because we compressed the data transfer.ConclusionPacking sentences together can save compute effort and the environment. This technique can be implemented in any framework including PyTorch and TensorFlow. We obtained a clear 2x speed-up and along the way we extended the state of the art in packing algorithms.Other applications that we are curious about are genomics and protein folding where similar data distributions can be observed. Vision transformers could also be an interesting area to apply differently sized packed images. Which applications do you think would work well? We would love to hear from you!Read the PaperAccess the Code on GitHubThank youThanks to our colleagues in Graphcore’s Applications Engineering team Sheng Fu and Mrinal Iyer for their contributions to this work and thank you to Douglas Orr from Graphcore’s Research team for his valuable feedback.References[1] M. Kosec S. Fu M. M. Krell Packing: Towards 2x NLP BERT Acceleration (2021) arXivIntroducing Packed BERT for 2x faster Training in Natural Language Processing was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,7,3 Visualization Layers for Information-Rich Charts with Altair and Python,https://towardsdatascience.com/3-visualization-layers-for-information-rich-charts-with-altair-and-python-7a66c0cbdd39?source=rss----7f60cf5620c9---4,How to Improve your Storytelling by Adding Useful Information on Your ChartContinue reading on Towards Data Science »
2021,7,7,What is a Variational Autoencoder?,https://towardsdatascience.com/what-is-a-variational-autoencoder-9b41bd63f65e?source=rss----7f60cf5620c9---4,A Quickstart Guide to Generative Machine Learning with CodeContinue reading on Towards Data Science »
2021,7,7,Wind energy analytics toolbox: Iterative power curve filter,https://towardsdatascience.com/wind-energy-analytics-toolbox-iterative-power-curve-filter-fec258fdb997?source=rss----7f60cf5620c9---4,An open source module for filtering SCADA data from operating wind turbines.Photo by Thomas Reaubourg on UnsplashIntroductionIn this article I will introduce the first of a collection of modules to be developed for analyzing data from operating wind turbines- The Iterative Power Curve Filter.Theoretically the power output from a wind turbine (WT) is proportional to the cube of wind speed. A plot of this relationship is known as the power curve and it is perhaps the most important plot in wind energy analysis.Original equipment manufacturers (OEMs) provide a theoretical power curve which maps the input wind speed to output power given ideal conditions. However this relationship is rarely the case in operational turbines for many reasons such as the wind farm terrain placement of the anemometer efficiency issues and wake effects because of closeness to other turbines.Therefore it is of interest to understand the actual relationship between wind speed and power for operating turbines and this is rightly named the operational power curve which can be quite different from the theoretical power curve.An operational power curve gives the relationship between wind speed and power output for a given turbine during normal operating conditions. Normal operating conditions is often defined as non-downtime no-fault and no-event data points.Operational power curves are created by cleaning time series data collected from WTs through hundreds of sensors. The typical frequency at which the data is logged is 0.0017s^-1 which translates to every 10 minute.WT sensors are programmed with thresholds to raise alarms when abnormal operating conditions are detected. This is known as the Supervisory Control and Data Acquisition (SCADA) system.However simply filtering the SCADA data based on triggered alarms is often insufficient to obtain data points corresponding to normal operating conditions. As a result statistical filtering is required.SCADA data filtering is a fundamental part of many wind energy analytics projects including turbine underperformance analysis side-by-side comaprison of units anomaly detection and data-driven automation of processes using machine learning techniques.Hence developing off-the-shelf modules for pre-processing scada data will benefit engineers and analyts by significantly reducing time and effort typically used for data cleaning while lowering entry barrier for leveraging advanced analytics in the industry.Filtering procedurePower curve filtering procedureThe Iterative Power Curve Filter can handle multiple turbines across several wind farms if there is a unique identifier for each unit. The procedure consists of two main steps as outlined below:Primary FilteringDowntime data points are removed.Likely faults are excluded. This step is empirical and the idea is to remove unreasonable production data at moderate to high wind speeds.Secondary FilteringCompute statistical parameters (mean and standard deviation) of partially filtered power curve from primary filtering process.Exclude data points outside of +/- x std as specified by user.The two steps above are repeated over a few cycles selected by user.Related procedure was described in the M.S. thesis of Abdul Mouez Khatab — “Performance analysis of operating wind turbines” 2017.Module usageThe scada-data-analysis library hosted on PyPi contains the filtering module and details of code implementation can be found on Github. The library can be installed by a simple pip command as shown here.# Pip install librarypip install scada-data-analysisIn addition the project github repo may be cloned as follows:# Clone github repogit clone https://github.com/abbey2017/wind-energy-analytics.gitUsing this library you can filter messy SCADA data in 4 steps as shown below:# Import relevant librariesimport pandas as pdfrom scada_data_analysis.modules.power_curve_preprocessing import PowerCurveFiltering# Load turbine scada datadf = pd.read_csv(&#39;path\to\data&#39;)# Instantiate power curve filtering classpc_filter = PowerCurveFiltering(turbine_label=&#39;Wind_turbine_name&#39;                                            windspeed_label=&#39;Ws_avg&#39; power_label=&#39;P_avg&#39; data=df cut_in_speed=3 bin_interval=0.5 z_coeff=2.5 filter_cycle=5 return_fig=True image_path=&#39;..\images&#39;)# Process raw scada datanormal_df abnormal_df = pc_filter.process()ResultsThe github repo for this project has example datasets and Jupyter lab notebook usage of the power curve filter. Sample results are shown here based on publicly available SCADA data from the La Haute Borne wind farm in France operated by Engie.A peek into the futureResults from the power curve filtering module may be used as ground truth for training machine learning models that can be deployed for near real-time monitoring of WT performance or other advanced control technologies.New modules aimed at enabling engineers and analyts to leverage advanced analytics will be added to the wind energy analytics toolbox. Some current ideas include modules for generating models that can estimate/predict expected power from WTs based on historical SCADA data faulty equipment detection and underperformance categorization modules.I would like to hear your feedback on this article and the wind energy analytics toolkit. Module suggestions including likely applications are welcome. Please send an email to the project maintainer at windenergyanalytics@gmail.com or open an issue on project’s github page.Wind energy analytics toolbox: Iterative power curve filter was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,7,Every Data Scientist Should Use PyCaret,https://towardsdatascience.com/every-data-scientist-should-use-pycaret-45a1e8e984be?source=rss----7f60cf5620c9---4,Here&#x2019;s why&#x2026; the one-stop-shop for Machine LearningContinue reading on Towards Data Science »
2021,7,7,How to choose an appropriate Machine Learning Algorithm for Data Science Projects?,https://www.analyticsvidhya.com/blog/2021/07/how-to-choose-an-appropriate-ml-algorithm-data-science-projects/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction In this article I am going to explain the steps ... 
The post How to choose an appropriate Machine Learning Algorithm for Data Science Projects? appeared first on Analytics Vidhya."
2021,7,7,eBook: How to use third-party data to make smarter decisions,https://www.kdnuggets.com/2021/07/roidnab-ebook-data-smarter-decisions.html,Get yourself a copy of this eBook and learn how to use third-party data to make smarter decisions.
2021,7,7,"Relax! Data Scientists will not go extinct in 10 years, but the role will change",https://www.kdnuggets.com/2021/07/poll-data-scientists-not-extinct-10-years.html,About 70% of KDnuggets readers think that the demand for Data Scientists will increase and 50% think it will increase significantly. At the same time over 90% think the role of Data Scientist will change. What will the Data Scientist role be in 10 years?
2021,7,7,How to Get Practical Data Science Experience to be Career-Ready,https://www.kdnuggets.com/2021/07/practical-data-science-experience-career-ready.html,Becoming a professional in the field of data science takes more than just book-smarts. You need to have experience with real-world data sets frequently-used tools and an intuition for solutions that you can only gain from hands-on experience. These resources will jump start developing your practical skills.
2021,7,7,How to Build An Image Classifier in Few Lines of Code with Flash,https://www.kdnuggets.com/2021/07/build-image-classifier-in-few-lines-of-code-with-flash.html,Introducing Flash: The high-level deep learning framework for beginners.
2021,7,7,"KDnuggets™ News 21:n25, Jul 7: Data Scientists and ML Engineers Are Luxury Employees; 5 Lessons from McKinsey That Will Make You a Better Data Scientist",https://www.kdnuggets.com/2021/n25.html,Are Data Scientists and ML Engineers Luxury Employees? 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist; Managing Your Reusable Python Code as a Data Scientist; GitHub Copilot: Your AI pair programmer - what is all the fuss about? and more.
2021,7,7,Free checklist: 30 Days to Data Analyst,https://www.r-bloggers.com/2021/07/free-checklist-30-days-to-data-analyst/," “How do I get better at data?” is a question I get a lot. It’s an interesting one to answer — plenty of great resources are out there it’s just a matter of finding the right sequence for the right objective and circumstances. I assume “get better at data” ...


The post Free checklist: 30 Days to Data Analyst first appeared on R-bloggers."
2021,7,7,R-bloggers.com is moving to send update e-mails from follow.it (instead of feedburner),https://www.r-bloggers.com/2021/07/r-bloggers-com-is-moving-to-send-update-e-mails-from-feedburner-to-follow-it/,"tl;dr From this week moving forward e-mail updates from R-bloggers will be sent via the follow.it service. If you&#8217;d like to start getting your email updates on new R news and tutorials please click here to subscribe. You can now also have your updates be sent via telegram or filter to see updates only from posts including/excluding some keywords.   Details Dear R-bloggers readers For over 10 years R-bloggers has been sending e-mail updates using feedburner. Sadly google has decided to shut-down their (free) e-mail delivery service for feedburner. In order to mitigate this change all confirmed e-mail subscribers have been moved to getting their email updates via the follow.it service. If you&#8217;d like to start getting your email updates on new R news and tutorials please click here to subscribe. The new emails will be sent from follow.it and have the title &#8220;R-bloggers &#8211; new message&#8221;. By the end of this week email will no longer be sent from feedburner (so to avoid duplication). As a bonus from now on you&#8217;ll be able to set various delivery settings such as getting the updates from telegram or filter the posts you see based on the inclusion/exclusion of some keywords.
The post R-bloggers.com is moving to send update e-mails from follow.it (instead of feedburner) first appeared on R-bloggers."
2021,7,7,Snecko eye lets you play more cards,https://www.r-bloggers.com/2021/07/snecko-eye-lets-you-play-more-cards/," In a previous post I used simulations
to estimate how long it would take to collect the unique Unowns in
Pokemon Go! The message of the post was that we can use simulations
to solve problems when the analytic solution is not clear or obvious.
The curr...


The post Snecko eye lets you play more cards first appeared on R-bloggers."
2021,7,7,"Open-source growth and venture capital investment: Data, databases, challenges, and opportunities",https://www.zdnet.com/article/open-source-growth-and-venture-capital-investment-data-databases-challenges-and-opportunities/#ftag=RSSbaffb68,Open-source software used to be poorly understood by commercial forces and it's often approached in a biased way. A new generation of investment funds goes to show that things are changing.
2021,7,7,Fauna adds geo-isolation to globally distributed database cloud,https://www.zdnet.com/article/fauna-adds-geo-isolation-to-globally-distributed-database-cloud/#ftag=RSSbaffb68,While globally distributed databases are no longer rare few of them offer a key capability for addressing data residency laws. Fauna is adding region groups that allow organizations to keep within a region or node.
2021,7,7,"Beautiful Soup Tutorial 3. – Scraping 1,000+ product pages and various data points",https://data36.com/beautiful-soup-tutorial-scraping-multiple-product-pages-data-points/,"Do you remember when I told you in the first tutorial that “sooner or later you’ll come to a point where you have to collect large amounts of...
The post Beautiful Soup Tutorial 3. – Scraping 1000+ product pages and various data points appeared first on Data36."
2021,7,7,Can I Improve My Shopify Store Competitive Position?,https://www.smarten.com/blog/can-i-improve-my-shopify-store-competitive-position/,Analytics Can Take Shopify (and your business) to the Next Level! The Shopify management team has spent a lot of time and effort building its brand and its popularity around the world. Shopify has many features that appeal to eCommerce and online business owners. In [&#8230;]
2021,7,7,PondBase – Visualize fisheries resource in India,https://www.r-bloggers.com/2021/07/pondbase-visualize-fisheries-resource-in-india/,"A dashboard to support the rapid pond mapping initiative by Center for aquatic livelihood Jaljeevika. Please check https://github.com/asitav-sen/PondBase for details.
The post PondBase – Visualize fisheries resource in India first appeared on R-bloggers."
2021,7,6,Create a privacy filter webservice with FastAPI and Heroku,https://towardsdatascience.com/create-a-privacy-filter-webservice-with-fastapi-and-heroku-4755ef1ccb25?source=rss----7f60cf5620c9---4,How to make your PII removal service easy to use increasing the usage and thus improving privacy protection.Continue reading on Towards Data Science »
2021,7,6,How to Use Elastic Net Regularization with any GLM,https://towardsdatascience.com/how-to-use-elastic-net-regularization-with-any-glm-6eab524bbcc6?source=rss----7f60cf5620c9---4,A new algorithm developed by Stanford researchers and its application in R.Generalized Linear Models (GLMs) are one of the most widely used inferential modeling techniques. Their simplicity makes them easy to interpret so when communicating causal inference to stakeholders they’re a very effective tool.Elastic net regularization a widely used regularization method is a logical pairing with GLMs — it removes unimportant and highly correlated features which can hurt both accuracy and inference. These two methods are a useful part of any data science toolkit.Photo by JESHOOTS.COM on UnsplashPrior to March of 2021 the combination of GLMs and elastic net regularization was fairly complex. However researchers at Stanford released a paper that leverages cyclic coordinate descent to allow efficient computation of a model’s coefficients for any link function not just the simple ones. They also implemented this model in the famous R package called glmnet.In this post we’ll discuss the fundamentals of GLMs elastic net regularization and understand how the two can work together. Let’s dive right in…Technical TLDRIn R the glmnet package now supports all link function families. If your link is canonical pass a string to the family parameter. If not pass a family function. Note that you can check the documentation for a list of the supported canonical functions.On the backend the package has two major changes…Substitutes the OLS solver for maximum likelihood. Note that this is only done for non-canonical link functions — canonical links are still solved using OLS.Use Iteratively Reweighted Least Squares (IRLS) to solve. IRLS (also called Fisher Scoring) is needed because the maximum likelihood is not guaranteed to be concave.Ok I think I got it. But how does this new method actually work?Let’s slow down a bit and start with how GLM’s actually work.What’s a Generalized Linear Model (GLM)?GLM’s are a fairly simple concept that has been around since the 1970s. With the rise of big data they have become invaluable tools in developing inferential models i.e. models that give us statistically valid conclusions about our data.Why you might ask?Well GLM’s are just linear regressions with an extra trick: a link function. The purpose of a link function is to transform our dependent variable so that the linear model can fit better.Let’s take a look at a simple example.Figure 1: OLS vs. GLM with a log link function. Image by author.On the top in figure 1 we have an exponential curve that is being fit by a GLM without a link function. Yes this is just plain old linear regression. Not a great fit right?Since we know that the data roughly follow an exponential distribution we use a GLM with a log link function and observe a much better fit (bottom).From an inferential perspective the main benefit of using a GLM to fit our data is we can interpret the coefficients just like with linear regression. So in our case a one-unit change in X leads to a 1.007 change in the natural log of y. If you want to take it a step further and look at the raw units of y for a given x we can take the inverse of our log exp(1.007 * x) to get the fitted y for any value of x.Pretty useful right?Now GLM link functions are chosen depending upon the structure of your dependent variable. In our case above our data were continuous and looked exponential so we used the log distribution. If it’s integer data we’d use the Poisson distribution. If it’s continuous and roughly normally distributed we’d use the Gaussian (aka normal) distribution. And a personal favorite is the Tweedie distribution which is essentially a Poisson distribution that allows for many 0 values.What is Elastic Net Regularization?Now that we have an understanding of GLM’s let’s take a look at the hero of this post Elastic Net Regularization.The purpose of any type of regularization is to remove the influence of predictors that are not useful. And elastic net regularization is no different — it removes predictors that are highly correlated and don’t improve our model’s accuracy.So let’s say we’re looking to model stock prices with a GLM (probably not a good idea by the way). Our dependent variable is the price of XLE an energy-sector ETF. Our independent predictor variables are:Day of the weekXLE’s price one day priorXLE’s price two days priorThe color of your shirt two days priorIf you fit price with those predictors we’d expect day of the week yesterday’s XLE price and XLE’s price two days ago to be highly predictive of today’s price. Your shirt color sadly has zero influence on price so elastic net would shrink our coefficient to 0.But we have a problem. If we look at the correlation between yesterday’s and two-days-ago’s price we will notice high colinearity. Colinearity can prevent models to converge and reduce the interpretability of our coefficients so elasitc net identifies this and reduces the coefficient for price two days ago.The first procedure where we remove unimportant coefficients by shrinking its coefficient to zero is called LASSO (or L1) regularization. The second where we reduce the coefficient size of predictors correlated with other predictors is called Ridge regression (or L2 regularization).These two components working in tandem are called Elastic Net Regression.The Math of Elastic Net RegularizationWith some intuition developed let’s take a look at the mathematical definition of elastic net for OLS linear regression.Figure 2: formula for elastic net regularization. Image by author.In figure 2 we can see that we are looking to find model coefficients that best minimize the value in the square brackets. On the left side we have the OLS term which calculates the sum of our squared error. On the right side we have our elastic net regularization term. Let’s quickly define each of those variables…After studying the equation a bit we’ll notice that alpha is a very important parameter. In practice we only allow alpha to take on values between 0 and 1. So if alpha = 0 the L1 term cancels out and we end up with Ridge regression. Conversely if alpha = 1 the L2 term cancels out and we end up with LASSO.Often we want to reap the benefits of both ridge and LASSO so an alpha — somewhere around 0.5 can both shrink coefficients to zero while also leveraging ridge regression’s coefficient reduction abilities.Pretty cool right?One final point: lambda is not a parameter specified by the user. Instead the user specifies a range of lambdas and elastic net tries to determine the best one. Here lambda completely controls the bias-variance tradeoff. If lambda is large we regularize more and thereby create a low-bias/high-variance model. If lambda is 0 we regularize less and focus on accuracy thereby creating a high-bias/low-variance model.And there you have it — elastic net regularization in a nutshell.Why doesn’t elastic net work for all link functions?As you might imagine it’s quite useful to be able to apply elastic net regularization to many different types of link functions. However up until recently there have been complicated workarounds to get elastic net to work with certain link functions. But a breakthrough by researchers at Stanford who have been pioneering the use of GLMs in R (the programming language) developed a new method that allows elastic net to work for all link functions.In short OLS minimization works for link functions that are canonical. In this context cononical means that the link function has certain properties which guarantee that it’s shape is concave (think parabola such as x²). Other more complex link functions don’t necessarily have a constant and positive second derivative so their shape can be bumpy which makes finding the global minimum more difficult.The SolutionThe new method intelligently moves away from OLS and instead leverages Maximum Likelihood Estimation (MLE) another popular fitting criteria. The downside with maximum likelihood is that it requires a step solver — we have to iteratively test values until we reach a sufficiently optimal solution. On top of that MLE doesn’t always guarantee a global minimum so we technically could be missing out on a better set of coefficients.But in practice MLE produces robust results.The Solution in RAs noted earlier the researchers focused their effort on improving R’s ability to handle all types of link functions. The famous glmnet package has been updated to support both canonical and non-canonical link functions.But how can you tell whether your link function is canonical?Well if your link function is gaussian binomial poisson multinomial cox or mgaussian it’s canonical. Now this is not a complete list but these are the canonical links currently supported by glmnet.On the coding side if your function is in that list you can pass the link function family as a string. And if not you’ll need to pass a family function. See the examples below:library(glmnet)# canonical exmaple - pass gaussian stringfit &lt;- glm(y ~ x family = &quot;gausian&quot;)# non-canonical exmaple - pass quasi-poisson functionfit &lt;- glm(y ~ x family = quasipoisson())With this update we can now pick any distribution that best represents our data regardless of its complexity. We could even make up some new link functions if we’re feeling creative.But how does the solution actually work?The new optimization algorithm uses a method called cyclic coordinate descent. Unlike gradient-based optimizations that calculate a “hill” through the model’s derivatives such as stochastic gradient descent coordinate descent uses the raw data. From there it moves along those coordinates in search of a global minimum.A way to conceptualize the difference between the two is by thinking about the city of San Fransisco. It has lots of hills which represent our feature space. The height of the hill is an estimate of our model’s fit and the latitude and longitude are two features that were optimizing.Cyclic coordinate descent uses San Fransisco’s roads to find the lowest point where each road represents different combinations of our variables. Gradient descent on the other hand completely ignores the roads and just moves down the hill with the steepest slope.Figure 3: cyclical coordinate descent vs. stochastic gradient descent. Image by author.The nice thing about cyclic coordinate descent is you don’t have to calculate a full gradient based on the model’s derivatives which is impossible for certain link functions.For completeness we’ll cover the optimization algorithm here. It’s going to get technical so feel free to skip to the Implementation Notes and comments.The main algorithm labeled algorithm 1 in the paper is the following:There’s a lot going on but let’s quickly summarize. After initializing a bunch of possible values of lambda we iterate through each and try to minimize our loss for that value searching for the best lambda. However because we’re working with MLE instead of OLS we need to use cyclic coordinate descent. Then we look to minimize the weighted least squares (WLS) expression in the last step which involves another similar algorithm.I found it helpful to just stare at the algorithm but if you’re curious about the details check out section 2.3 of the paper.Implementation NotesWhen running elastic net regularization often we want to use both LASSO and Ridge. So it’s good to try to different values of alpha and see what variables are kept. If you’re looking to optimize it’s a good idea to use cross validation to find the best value of alpha.Choosing a good link function is essential for model accuracy and stability.Choosing a more “advanced” link function can save time. One example is using the Tweedie distribution instead of a zero-inflated Poisson — I could have saved a lot of time on my thesis if I knew about the Tweedie distribution.It’s often good to standardize your predictors so their scale does not impact your variable selection. Z-scores are also very easy to interpret and can be converted back to the original units if needed.The glmnet package in R has paved the way towards making GLMs easily accessible. Most other languages lag behind this package by several release versions but hopefully there will be python library that supports all link functions soon.Thanks for reading! I’ll be writing 44 more posts that bring “academic” research to the DS industry. Check out my comments for links/ideas on using GLMs for inferential modeling.How to Use Elastic Net Regularization with any GLM was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,6,What Mainstream AI is (Not) Doing,https://towardsdatascience.com/what-mainstream-ai-is-not-doing-9a2f7236a4f7?source=rss----7f60cf5620c9---4,The pandemic accelerated AI adoption — and made Big Tech richer — but did AI adoption happen in the places where it was needed?After dabbling in machine learning during my undergrad I scored a job as a Business Analyst catering to the Public Sector and Education business of one of the leading AI service providers in India. Having landed the job in the pandemic — which I must say I consider myself fortunate to — and onboarded onto the role fully online I have been at the forefront of the transition of organizations from real-world to digital world. And I say this not only for the company I work for but for the numerous governmental agencies and educational institutions that have approached us seeking to undergo a digital transformation in order to enhance their processes in the current situation as well as for the future.I genuinely believe that these past one-and-a-half years have been a blessing for the entire IT industry and in association to the AI industry — since AI is now nearly ubiquitous when it comes to IT — as evidenced by the incredulous growth the Big Tech has achieved by a two-prong strategy of undergoing digital transformation themselves (thereby cutting expenditures) and providing the same services to other industries (and generating immense revenue).Mainstream AI needs to bridge the gap between commercial and social applications. (Photo by Indira Tjokorda on Unsplash)Increasing AI AdoptionAs previously mentioned Big Tech companies while initially apprehensive of the impact of the ‘new normal’ soon realized its potential and took full advantage of it. Everything increased — from ad revenues due to digital ads becoming the major avenue to reach consumers online shopping sales of laptops and mobiles as everything went online social media engagement to increased cloud consumption as businesses went digital. Inadvertently this forced shift to online along with the fear of missing out acted as a catalyst for stakeholders to adopt not only traditional digital transformation but also to jump onto the bandwagon of newer technologies like cloud artificial intelligence and blockchain.As per a KPMG survey there has been a 37% increase in AI adoption in at least one function in FinTech 20% increase in Tech and 29% increase in Retail compared to last year. Couple this with the $67.9 billion in investment AI saw in 2020 — including Microsoft’s $1 billion investment in Open AI previously a not-for-profit organization — and the representative mapping of use cases this money was spent on based on a report by McKinsey. About 50% of use cases in AI in 2020 were related to Natural Language Processing and 20% to Computer Vision across industries like IT FMCG Healthcare FinTech Legal and Automotive. In comparison to this mainstream Google via their AI for Social Good initiative offered just $25 million for research of AI applications that improve society.This renewed push for digital transformation the change in the adoption mindset and the amount of money being poured into the space (this time even from the public sector) was the perfect opportunity for the AI community to capitalize and enhance their efforts to tackle the problems facing AI and to build applications for the social good especially in these times. Unfortunately examples of that happening are few and far between.Use Cases that need AI (based on UN SDGs)International organizations like International Telecommunication Unit (ITU) and McKinsey use the United Nation’s Sustainable Development Goals (UN SDG) adopted in 2015 as a guideline for the world’s major problems today and try to map how AI can be used to these problems. Serendipitously a research paper published in Nature estimates that 79% of the targets within the 17 SDGs can be reached using AI in some manner. Let’s look at the 17 SDGs and how AI can achieve these targets as set forth by the UN:No Poverty: To achieve UN’s goal to eradicate extreme poverty by 2030 AI can be used to analyze satellite data or mobile usage data and detect areas of poverty followed by complementing policies to aid the residents in these areas. Better weather prediction leveraging AI can help in evacuating these people to safety. Economically in a bid to bridge the gap between the rich and poor AI can be give credit ratings for loans to those below the poverty line (assuming the underlying data is not biased)Zero Hunger: Ancillary AI applications that indirectly contribute to this goal have already been gaining traction in the agriculture sector while last mile routing solutions were developed to provide for food during the pandemic. Examples in agriculture include using Computer Vision to detect crop diseases analyzing and modeling historical data to support farmer decisions harvest forecasting weather forecasting and IoT-enabled AI-powered farm equipment. However agricultural AI research will count for nothing if the end-product cannot be put into the hands of the poor farmers of third-world countries.Good Health and Well Being: This goal falls squarely into the healthcare sector which has received focus and funds to battle the pandemic. Intel research shows that there has been a 39 percentage point increase in healthcare leaders adopting or wanting to adopt AI post the pandemic. Examples of AI use cases in the healthcare sector include AI and data powered support systems in hospitals automation of drug discovery tuberculosis and cancer detection increasing AIDS awareness solutions to reduce traffic accidents (cue autonomous vehicles) suicide prevention and reduction of distressing posts on social media (credit where its due: this last one is Facebook). However many of these solutions are borderline ethical and with the dangers of datasets not representing the real world and lives being at stake it is a big gamble for now.Quality Education: Another sector that has seen tremendous increase in AI adoption due to the pandemic. I can say from personal experience that AI is increasingly being used to monitor students’ attention or to carry out emotional surveillance to determine how comfortable children are learning certain subjects identifying students who are struggling before their test results become available. Attempts at using AI to deliver personalized and adaptive teaching are being made especially on online learning platforms. However AI in education has a long way to go to provide truly personalized education.AI Investment Increased in Education and Healthcare Roughly During the Pandemic (Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)Gender Equality: There has been very little direct involvement of AI in achieving gender equality mainly because it is a socio-cultural problem. That being said the AI community needs to make sure that it does not promote gender bias by ensuring the underlying datasets and deployed systems do not have bias inherent in them.Clean Water and Sanitation: The only properly researched use case pertaining to this goal is using AI to predict and suggest steps to improve the quality of water in a water treatment plant. Potential use cases include catchment area management and water pipeline / flow managementAffordable and Clean Energy: Again a not frequently researched field most of the efforts have been towards how AI can optimize energy production and consumption and better predict demand. More research is needed into how AI can help setup Smart Grids.Decent Work and Economic Growth: This is again a more socio-cultural problem then a technological problem. However generally speaking the rise of the AI industry and its share in the global economy can spur economic growth but at the risk of replacing low-skilled workers which is why the AI industry needs to be self-aware and balance the advantages and disadvantages it brings to society as well as to the environment. In addition with the automation that AI provides productivity will also increase.Industry Innovation and Infrastructure: Though research into many applications of AI in infrastructure is ongoing and the underlying concepts are sound there is not much adoption in this area. There is a need for an adoption push in use cases such as air and water quality management energy management asset and construction management predictive maintenance enhancing productivity automation and smart cities and grids.Reduced Inequalities: This social problem can be indirectly solved in part by using AI. For financial equality applications can be developed that predict credit ratings and provide loans for the low-income group as well as help them understand the personal investment world through data. For cultural racial and caste equality I believe it would be more beneficial if all AI applications managed to eradicate the underlying bias in data itself rather than coming up with specific applications targeted to promoting equality. A representative example here would be tweaking recommendation engines to not promote hate speech. On a larger scale data related to demographics and social patterns in a particular community can be analyzed to conclude how inclusive the community is.Sustainable Cities and Communities: Many points pertaining to this goal have been covered above. Large-scale AI can help achieve sustainable and smart cities through solutions geared towards better transportation infrastructure and reduced accidents water and energy management predicting earthquakes spread of wildfires and oil spills ecology analysis air quality analysis etc. Again the challenge here is large-scale adoption and policy support.Responsible Consumption and Production: AI-enabled systems can leverage historical data and patterns to optimize production and consumption schedules in various sectors like manufacturing energy etc. while the automation that AI brings will improve productivity. From a policy standpoint the process of government procurement of services and goods which is very slow and corrupt in many places can thank AI for systems that automate the filtering out of unqualified vendors.Climate Action: AI cuts both ways when it comes to climate action with the humongous energy requirements for developing and running AI models indirectly adding to the CO2 in the air while the possibility of automating energy management climate modeling and predicting natural disasters being its positive side.Life Below Water: Research has shown oceans and sea-life can be conserved by leveraging AI in the detection and mitigation planning automation of oil spills and catchment area management. Satellite data analysis can be useful in ecological forecasting (which includes coral bleaching and algal blooms among other events) tracking and regulating trawler activities.Life on Land: Research shows AI can be used to identify forest animals through their footprints which can play a non-invasive part in wildlife conservation efforts. Remote sensing leveraging AI can be used for assessing predicting and mapping forest structural features which serve as indicators of forest condition and help in conserving forests. Neural networks and objective-oriented techniques can be used to better classify vegetation cover types and hence detect desertification and droughts.Peace Justice and Strong Institutions: AI can be leveraged in surveillance systems and social media to detect notify and filter violent people and/or content bullying and child pornography. Facial recognition software can also be used in predictive policing. However this is one section for which the required technology is ready but its adoption is controversial due to ethical privacy and discrimination concerns.Partnerships for the Goals: This particular section in the UN’s SDG list lists down targets towards collaboration for achieving the other 16 goals and as such is not related to technology. So we will skip this.How Big Tech and Government can Control AITo pursue research of any kind in any sector or industry requires funding. Research will be focused where the funds are. Keeping this in mind let’s take a look at how Big Tech and international governments have the potential to centralize AI research and determine how the industry moves forward.Corporate Participation in Academic Research Across Conferences (Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)Big Tech’s Acquisition of AI Startups (Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)From 2012 when Big Tech started entering the AI field after a breakthrough in machine learning techniques to 2019 there has been a 550% increase in footfall at NeurIPS the largest machine learning conference that is held. And Big Tech reps attend it with the aim of luring PhDs into their companies. These companies also hire tenure-track professors to help them in their research. Big Tech’s talent grab is not only limited to academia but has even expanded to startups in recent years as this article diagrammatically presents.The two graphs below reinforce the belief that corporate has a strong interest in academic AI research especially in America. The position of corporates in peer-reviewed research is replicated in research papers published in journals or showcased in conferences as well.Private Investment in AI Across Countries. USA has the highest (Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)Peer-Reviewed AI Publications in the USA. Corporate has the highest involvement(Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)Peer-Reviewed AI Publications in China. Government has the highest involvement(Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)It is instantly noticeable from the graphs above: USA China and the European Union are the leaders in AI research. In America corporate has a much larger interest than the government or academia. However in China the government — and not the corporates — is the one that centralizes AI which is obvious if we come to think of China’s political structure. The image below reinforces this as we can see that the corporate-academic partnership for publishing papers is much less in China (in spite of the large number of papers that China publishes) than in the USA.Academic-Corporate Partnership in AI Publications. USA leads the others by a large margin (Image via Stanford AI Index 2021 under the Attribution-NoDerivatives 4.0 International license)China is a case of more than average AI centralization by the government. Nonetheless the reason that governments have a stake in AI is that they have the power to roll out legislation concerning the proper use of publicly and privately available data for AI applications (case in point: EU’s GDPR). And specifically for projects in the social realm—and not to mention the large scale sectors of infra smart cities water management etc over which the government has a larger control — the government is the one that can announce policies development schemes and scope out and fund large-scale projects which leverage AI to achieve the project objective. To that extent ITU’s research shows that 131 countries have already passed or are in the process of passing a data policy with 18 countries having a specific AI policy in place as of 2020.Challenges in the IndustryHowever the AI industry moves forward as a result of corporate and/or government support the things that need to be addressed foremost to ensure that it isn’t ‘two steps forward one step back’ are the problems facing the industry and community right now.Explainability of AI: Machine learning models — neural networks especially — frequently behave as a black box and it is often unclear how they arrive at their conclusion. Sometimes it is preferable to have a model that provides a result that can be backtracked so as to be able to modify the proper parameters in the model whenever required. This aspect gains increased focus in the healthcare industry because of the stakes involved and also recently in facial recognition systems because of erroneous outputs leading to disastrous results.Unavailability of Data: In many use cases collecting the right data and creating an actionable dataset is a time-consuming task. This because firstly what are the avenues from where data can be collected has to be figured out; secondly gathering this data from disparate sources takes time; thirdly organizations have to modify the collected data to comply with data and privacy regulations while still keeping the data actionable. For academia though getting past the first stage itself is tough because Big Tech and government has much of the data (collected from their various services) which it keeps private or classified as the case may be.Bias and Misrepresented Datasets: The data that organizations collect and the method with which they do so represent the reality of the place from where the data has been collected. This results in cultural or social prejudices making its way into the datasets and it becomes the organization’s responsibility to weed the bias out. Taking it a step further it is also the organization’s responsibility to make sure that the application is deployed in a place that has an identical representative population to the one from which the dataset was prepared to ensure that controversies and failed deployments do not occur. Bias creeping into the models is a real threat for facial recognition systems and large language models.Generalizability: OpenAI is the torchbearer for the AI industry’s quest towards a Artificial General Intelligence — a single AI that can do everything a human can do. At present this is a distant possibility and so for every use case discussed above chances are the whole process from data collection to model deployment will be redone specifically for that use case. This seems like a waste of time and effort which it is. However generalizability in itself isn’t as bad as the next point that it brings us to.Energy Consumption: OpenAI itself chasing generalizability in natural language processing has kept outdoing itself and building bigger neural language models (cue GPT-3). The energy that is required to train (even once) and deploy these models is really really huge — so much so that it gets you thinking that if the current trend of building ever-bigger models continues then whatever advantages AI brings for the environment won’t actually matter. Add to this the fact that GPT-3 doesn’t even understand human language; it is just a master in manipulating it.Corporate-Academia Gap: The gap between corporate and academic resources and research is there for all to see. Which is why the AI community needs to refocus itself and build more applications that do actual good irrespective of corporate or government interests.ConclusionWe can clearly see that sectors such as healthcare and education that had some presence of AI saw a wild rise in AI adoption during the pandemic. Other sectors have had research being conducted but lack large-scale visibility interest investment implementation and adoption. This can be partly attributed to the problems that the AI industry is facing majorly in relation to availability of data and the control that Big Tech and government policies wield on the industry. That being said the presence of Big Tech and government interest in AI is a good thing because it keeps the technology from becoming a thing of the past through continuous funding. The only thing that is needed is a shift in focus by all the stakeholders in the industry.However fear not because not all hope is lost! AI clearly has the technical capability to help society as evidenced by the mapping of capability to social goals. And as the title of the article implies AI is being used for much social good; it has just not reached a tipping point towards mainstream interest and investment. People think tanks and organizations are actively involved in research and development of applications that aim for a better society. Here is to hoping they get the support they deserve!ReferencesNew York Times: ‘A Perfect Positive Storm’: Bonkers Dollars for Big TechForbes: How The Pandemic Has Accelerated Cloud AdoptionStanford AI Index 2021McKinsey: The State of AI in 2020International Telecommunication Unit: Artificial Intelligence for goodUnited Nations Sustainable Development GoalsMcKinsey: Applying AI for Social GoodNature: The Role of Artificial Intelligence in Achieving the Sustainable Development Goals17 ways to reach the UN’s sustainable goals with AIWired: The Dark Side of Big Tech’s Funding for AI ResearchWhat Mainstream AI is (Not) Doing was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,6,The Most Important Tool for Data Engineers,https://towardsdatascience.com/the-most-important-tool-for-data-engineers-f06a05f19ee1?source=rss----7f60cf5620c9---4,And it has nothing to do with Python or SQLContinue reading on Towards Data Science »
2021,7,6,All Fundamentals of Python Functions that You Should Know – A Quick Brush Up!,https://www.analyticsvidhya.com/blog/2021/07/all-fundamentals-of-python-functions-that-you-should-know-a-quick-brush-up/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction In this article you’ll learn about Python functions and their applications ... 
The post All Fundamentals of Python Functions that You Should Know &#8211; A Quick Brush Up! appeared first on Analytics Vidhya."
2021,7,6,Everything You Should Know about Iterables and Iterators in Python as a Data Scientist!,https://www.analyticsvidhya.com/blog/2021/07/everything-you-should-know-about-iterables-and-iterators-in-python-as-a-data-scientist/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Python is a beautiful Programming Language. Because of its flexibility ... 
The post Everything You Should Know about Iterables and Iterators in Python as a Data Scientist! appeared first on Analytics Vidhya."
2021,7,6,Start Learning SVM (Support Vector Machine) Algorithm Here!,https://www.analyticsvidhya.com/blog/2021/07/svm-support-vector-machine-algorithm/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Source Overview In this article we will learn the working of ... 
The post Start Learning SVM (Support Vector Machine) Algorithm Here! appeared first on Analytics Vidhya."
2021,7,6,We are e-going to useR!2021,https://www.r-bloggers.com/2021/07/we-are-e-going-to-user2021/,"Check out Mirai’s contributions to useR!2021.
This year’s useR! conference will be held as an online event designed to be global and give space to the amazing ecosystem that is the R Practitioners Community.
The program is very interesting and inc...
The post We are e-going to useR!2021 first appeared on R-bloggers."
2021,7,6,Detecting fiducial markers from a PCB with pattern matching,https://towardsdatascience.com/detecting-fiducial-markers-from-a-pcb-with-pattern-matching-4ed7f3bfb644?source=rss----7f60cf5620c9---4,Location location location.It may be easy to forget but whatever is going on inside our electronic gadgets is not magic. I am amazed by the thought that some products are so complex that no single person can comprehend how every part interacts precisely yet a manufacturing process exists such that the thing that is shipped out of the plant works. Anything that involves integrated circuits certainly falls into that category.If you’ve looked attentively at a printed circuit board (PCB) you might have noticed small golden disks that are not electrically connected to any other structures. Those are fiducial markers (or fiducials): their purpose is to make it as easy as possible for a vision system to find them to locate the PCB in the image.Fiducial markers of a PCB. Image by the author.In this article we’ll go through the steps to locate the fiducial markers with OpenCV’s matchTemplate() function. You can find the code and the image in this repository.Why do PCBs need fiducial markers?During the PCB assembly automated systems will place the discrete components (the chips the capacitors the connectors …) at their respective location on the PCB before being soldered on the soldering pads. The detection of the fiducial markers allows precise placement of the components on the PCB. After placement an automated inspection system will check if everything is present where it should be and placed within tolerances. Once more the fiducial markers will provide the reference points needed to translate the physical points from the PCB drawing (in millimeters) to image points (in pixels).PreprocessingWe begin by converting the image to grayscale since we rely on the distinct shape of the fiducial markers not their specific color. This is done with OpenCV’s cvtColor() function.Image converted to grayscale. Image by the author.The fiducial markers on this PCB are composed of two concentric disks whose respective diameters are 26 and 68 pixels.Manual measurement of the inner disk’s diameter. Image by the author.OpenCV’s matchTemplate() function requires a template image of the target object. In this case it would be an image of a fiducial marker. We could crop one of the three fiducial markers in our image and use it as our template but this approach is prone to missing true fiducial markers that have minor appearance differences with the arbitrarily chosen template image. Instead we’ll create a synthetic image of an ideal fiducial marker:https://medium.com/media/ccbed76e0ad1e699d39d8a03c02e1702/hrefA synthetic fiducial marker. Image by the author.The fiducial marker image is standardized to zero mean and unity standard deviation because we want the result of matchTemplate() to be close to zero in areas of uniform values in the searched image and the intensity values of the positive matches to be close to 1.Pattern matchingWe now have all the ingredients needed to call matchTemplate():https://medium.com/media/c1ef4a4b1c7266b95fe26452ff50c723/hrefIt is important to note that we padded the resulting matched image with zeros to have an image that has the same dimensions as the original image. It is necessary because matchTemplate() being based on convolution the output image has dimensions (Wₒᵣᵢ-Wₚₐₜₜₑᵣₙ+1 Hₒᵣᵢ-Hₚₐₜₜₑᵣₙ+1). In our case the pattern image having dimensions (69 69) the output of matchTemplate() has dimensions 68 pixels narrower and 68 pixels shorter than the original image. To compensate for this effect we zero-padded 34 pixels in the image periphery. We also rescaled the gray levels of the matched image from [-1 1] to [0 255] to visualize it easily.Zero-padded and rescaled matched image. Image by the author.We are interested in the peaks in the intensity of the matched image represented by bright spots in the above image. Deciding which bright spots are the fiducial markers we are searching for is a matter of setting the right threshold.Instead of setting the threshold manually we’ll use the fact that we know there should be three fiducial markers in this match image and therefore applying the right threshold should result in a binary image with exactly three blobs¹. To do that we’ll gradually lower the threshold starting from 255 and counting the blobs in the resulting thresholded images. When we reach three blobs we stop.https://medium.com/media/669aafbe34c32652313dd741d73b5e0d/hrefLocation of one of the three found fiducial markers. Image by the author.Computation of the transformation matrixThree correspondences between reference points on the PCB plane (in millimeters) and their respective locations in the image (in pixels) is the minimum needed to compute a homography i.e. the transformation matrix that allows us to convert coordinates in millimeters to coordinates in pixels. Using this transformation matrix we can annotate the image with the PCB edges.Annotation of the image showing the three found fiducial markers and the PCB edges. Image by the author.The annotated image above confirms that the three fiducial markers were correctly found and the computed transformation matrix is accurate. It is now possible to crop any area of the image where there should be a given component that we want to inspect assuming we have its physical coordinates on the PCB.In a manufacturing environment it is common that the objects to be inspected are moving on a conveyor. We cannot assume that the object to be inspected will always be precisely located at the same place relative to the camera. The transformation matrix computed from the detection of the fiducial markers allows us to retrieve a given area of interest on the PCB even if there is a translation and a rotation from one object to the next.What we just did is the first part of an automated inspection process. It is probably not the most difficult part but it certainly is the most critical: if the object location fails nothing else will make sense.Next time you look at a PCB — reminding yourself that it is not a magical artifact but rather an engineering marvel — try to spot the fiducial markers that facilitate its assembly and automated inspection.If you have an application in mind that requires fiducial markers location let me know. I would be pleased to hear about it.[1] In computer vision a blob is a cluster of connected pixels.Detecting fiducial markers from a PCB with pattern matching was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,6,10 challenges internal data leaders will face creating a revenue-generating data product,https://towardsdatascience.com/10-challenges-internal-data-leaders-will-face-creating-a-revenue-generating-data-product-e954ae79308a?source=rss----7f60cf5620c9---4,Continue reading on Towards Data Science »
2021,7,6,The Only Book You Need to Ace Advanced Machine Learning Strategy,https://towardsdatascience.com/the-only-book-you-need-to-ace-advanced-machine-learning-strategy-8d48ddbbe73d?source=rss----7f60cf5620c9---4,Spoiler: It&#x2019;s an Andrew Ng masterpiece and is free.Continue reading on Towards Data Science »
2021,7,6,Facial Landmark Detection Simplified With Opencv,https://www.analyticsvidhya.com/blog/2021/07/facial-landmark-detection-simplified-with-opencv/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Today we are going to use OpenCV and MediaPipe to ... 
The post Facial Landmark Detection Simplified With Opencv appeared first on Analytics Vidhya."
2021,7,6,A Brief Introduction to the Concept of Data Warehouse,https://www.analyticsvidhya.com/blog/2021/07/a-brief-introduction-to-data-warehouse/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction A Data Warehouse is Built by combining data from multiple ... 
The post A Brief Introduction to the Concept of Data Warehouse appeared first on Analytics Vidhya."
2021,7,6,Learn How To Do Real-Time Background Replacement using OpenCV and CVzone,https://www.analyticsvidhya.com/blog/2021/07/learn-how-to-do-real-time-background-replacement-using-opencv-and-cvzone/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction OpenCV is an open-source computer vision library that provides privileges ... 
The post Learn How To Do Real-Time Background Replacement using OpenCV and CVzone appeared first on Analytics Vidhya."
2021,7,6,How do banks deliver governed data access across their organization?,https://www.ibm.com/blogs/journey-to-ai/2021/07/how-do-banks-deliver-governed-data-access-across-their-organization/,"In the banking world how do you describe the corpus of a visionary? They come in all shapes and sizes. Their inspiration originates from personal backgrounds professional experiences and industries. The distinction is typically earned through invention business acumen and profit or change the human condition. Their ingenuity passion or grit transform visions into reality. [&#8230;]
The post How do banks deliver governed data access across their organization? appeared first on Journey to AI Blog."
2021,7,6,ROC Curve Explained,https://www.kdnuggets.com/2021/07/roc-curve-explained.html,Learn to visualise a ROC curve in Python.
2021,7,6,A Learning Path To Becoming a Data Scientist,https://www.kdnuggets.com/2021/07/learning-path-data-scientist.html,"Becoming a professional data scientist may not be as easy as ""1... 2... 3..."" but these 10 steps can be your self-learning roadmap to kickstarting your future in the exciting and ever-expanding field of data science."
2021,7,6,How To Transition From Data Freelancer to Data Entrepreneur (Almost Overnight),https://www.kdnuggets.com/2021/07/transition-data-freelancer-data-entrepreneur-overnight.html,Data freelancers trade hours for dollars while data entrepreneurs have found a way to make money while they sleep. Ready to make the transition? Keep reading to learn how to do it as SEAMLESSLY and PROFITABLY as possible.
2021,7,6,almost reversed 2-lag Markov chain,https://www.r-bloggers.com/2021/07/almost-reversed-2-lag-markov-chain/,"Another simple riddle from the Riddler: take a binary sequence and associate to this sequence a score vector made of the numbers of consecutive ones from each position. If the sequence is ten step long and there are 3 ones located at random what is the expected total score? (The original ...
The post almost reversed 2-lag Markov chain first appeared on R-bloggers."
2021,7,6,Euro Semi-Finals: England is the Favorite!,https://www.r-bloggers.com/2021/07/euro-semi-finals-england-is-the-favorite/," Using the  FIFA World Ranking and the Elo rating system we will try to estimate the probability of England winning ... Read moreEuro Semi-Finals: England is the Favorite!


The post Euro Semi-Finals: England is the Favorite! first appeared on R-bloggers."
2021,7,6,shiny.fluent Video Tutorial: Building Beautiful Shiny Apps,https://www.r-bloggers.com/2021/07/shiny-fluent-video-tutorial-building-beautiful-shiny-apps/,"
Build Beautiful Shiny Apps – A Live Coding Session at BostonR Thank you to all the attendees who joined Kamil Żyła’s live coding session at the BostonR June meetup! If you missed out on the chance to participate don’t worry! Check out his recorded presentation below. If you ...


The post shiny.fluent Video Tutorial: Building Beautiful Shiny Apps first appeared on R-bloggers."
2021,7,6,Point Biserial Correlation in R-Quick Guide,https://www.r-bloggers.com/2021/07/point-biserial-correlation-in-r-quick-guide/,"Point Biserial correlation in R What do you understand by biserial correlation? In some situations in which one variable is dichotomous according to some...
The post Point Biserial Correlation in R-Quick Guide appeared first on finnstats.
The post Point Biserial Correlation in R-Quick Guide first appeared on R-bloggers."
2021,7,6,Hey Data Scientists from Germany – eoda is hiring,https://www.r-bloggers.com/2021/07/hey-data-scientists-from-germany-eoda-is-hiring/," We are hiring: Data Scientist and Senior Data Scientist Machine Learning Artificial Intelligence and Big Data – these are the buzzwords you hear every day. But for you they are more than just buzzwords? You are familiar with the methods technologies and working methods behind them know how to use them ...


The post Hey Data Scientists from Germany – eoda is hiring first appeared on R-bloggers."
2021,7,6,Walkthrough UbiOps and Tidymodels,https://www.r-bloggers.com/2021/07/walkthrough-ubiops-and-tidymodels/,"
In this walkthrough I modified a tutorial from the UbiOps cookbook ‘Python Scikit learn and UbiOps’
but I replaced everything python with R. So in stead of scikitlearn I’m using {tidymodels} and where python uses a requirement.txt I will use {renv}.
So in a way I’m going ...


The post Walkthrough UbiOps and Tidymodels first appeared on R-bloggers."
2021,7,6,Taking Outlier Treatment to the Next Level,https://www.r-bloggers.com/2021/07/taking-outlier-treatment-to-the-next-level/,"
By Joachim Gassen (Humboldt University Berlin TRR 266 “Accounting for Transparency”) and David Veenman (University of Amsterdam)
“To reduce the impact of outliers on our findings we winsorize the dependent and independent variables at the top and...


The post Taking Outlier Treatment to the Next Level first appeared on R-bloggers."
2021,7,6,How do banks deliver governed data access across their organization?,https://www.ibm.com/blogs/journey-to-ai/2021/07/how-do-banks-deliver-governed-data-access-across-their-organization/,"In the banking world how do you describe the corpus of a visionary? They come in all shapes and sizes. Their inspiration originates from personal backgrounds professional experiences and industries. The distinction is typically earned through invention business acumen and profit or change in the human condition. Their ingenuity passion or grit transform visions into [&#8230;]
The post How do banks deliver governed data access across their organization? appeared first on Journey to AI Blog."
2021,7,5,A New Approach to Film Making,https://towardsdatascience.com/a-new-approach-to-film-making-80cec94284f0?source=rss----7f60cf5620c9---4,An analysis of movie statistics reveals a new trend has taken hold of the industry and is already generating huge profits.Continue reading on Towards Data Science »
2021,7,5,How Hacking and AI Research are Related,https://towardsdatascience.com/how-hacking-and-ai-research-are-related-55427bab6543?source=rss----7f60cf5620c9---4,Why being a great hackathon contestant can help with a research careerContinue reading on Towards Data Science »
2021,7,5,5 things that make my job as a Data Scientist easier,https://towardsdatascience.com/5-things-that-make-my-job-as-a-data-scientist-easier-dc0820f0f136?source=rss----7f60cf5620c9---4,After working as a Data Scientist for a year I am here to share some things I learnt along the way that I feel are helpful and have increased my efficiency. Hopefully some of these tips can help you in your journey :)Photo by Boitumelo Phetla on UnsplashTime Series Data Processing with PandasIf you work with time series data chances are you have spent a significant amount of time accounting for missing records or aggregating data at a particular temporal granularity either via SQL queries or writing custom functions. Pandas has a very efficient resample function that can help you process the data at a particular frequency by simply setting the DataFrame index to be the timestamp column.I am going to use the room occupancy dataset to give an example of this function. You can find the dataset here. This dataset records observations at a minute level.import pandas as pddata = pd.read_csv(&#39;occupancy_data/datatest.txt&#39;).reset_index(drop = True)data.head(5)First I show a simple aggregation one can do to get metrics at an hourly level.data.index = pd.to_datetime(data[&#39;date&#39;])pd.DataFrame(data.resample(&#39;H&#39;).agg({&#39;Temperature&#39;:&#39;mean&#39;                                     &#39;Humidity&#39;:&#39;mean&#39;                                     &#39;Light&#39;:&#39;last&#39;                                     &#39;CO2&#39;:&#39;last&#39;                                     &#39;HumidityRatio&#39; : &#39;mean&#39;                                     &#39;Occupancy&#39; : &#39;mean&#39;})).head(5)Though this dataset is not sparse in the real world one often encounters data which has missing records. It is important to account for those records since you might want to put in 0 values if there were no records or use the previous or next time steps for imputation. Below I removed records for hour 15 to show how you can use the hour 14 timestamp to impute the missing value:data = pd.read_csv(&#39;occupancy_data/datatest.txt&#39;).reset_index(drop = True)data_missing_records = data[~(pd.to_datetime(data.date).dt.hour == 15)].reset_index(drop = True)data_missing_records.index = pd.to_datetime(data_missing_records[&#39;date&#39;])data_missing_records.resample(&#39;H&#39; base = 1).agg({&#39;Temperature&#39;:&#39;mean&#39;        &#39;Humidity&#39;:&#39;mean&#39;        &#39;Light&#39;:&#39;last&#39;        &#39;CO2&#39;:&#39;last&#39;        &#39;HumidityRatio&#39; : &#39;mean&#39;         &#39;Occupancy&#39; : &#39;mean&#39;}).fillna(method  = &#39;ffill&#39;).head(5)2. Fast Visualization via Plotly ExpressFrom analysis to model training to model reporting visualizations are often required. Especially with time series graphs I noticed I was spending a lot of time trying to customize the size and angle of my x-axis ticks in matplotlib. After I switched to using Plotly Express I cut down the time I spent in making graphs looking cleaner/crisper by around 70%. And if I want to implement specific details in my visuals I can still do that by using Plotly Graph Objects. Additionally Plotly offers a lot of easy options via Express like setting group colors in plots which results in more powerful visualizations.import plotly.express as pxdata[&#39;Temp_Bands&#39;] = np.round(data[&#39;Temperature&#39;])fig = px.line(data x = &#39;date&#39;              y = &#39;HumidityRatio&#39;              color = &#39;Temp_Bands&#39;             title = &#39;Humidity Ratio across dates as a function of             Temperature Bands&#39;             labels = {&#39;date&#39; : &#39;Time Stamp&#39;                      &#39;HumidityRatio&#39; : &#39;Humidity Ratio&#39;                      &#39;Temp_Bands&#39; : &#39;Temperature Band&#39;})fig.show()Using the occupancy dataset mentioned above I used Plotly Express to create line plots with color grouping. We can see how easy it to create these plots with just two functions.3. Speed up pandas apply() via SwifterI sometimes run into long wait times for processing pandas columns even with running code on a notebook with a large instance. Instead there is an easy one word addition that can be used to speed up the apply functionality in a pandas DataFrame. One only has to import the library swifter.def custom(num1 num2):        if num1 &gt; num2:        if num1 &lt; 0:            return &quot;Greater Negative&quot;        else:            return &quot;Greater Positive&quot;    elif num2 &gt; num1:        if num2 &lt; 0:            return &quot;Less Negative&quot;        else:            return &quot;Less Positive&quot;    else:        return &quot;Rare Equal&quot;import swifter import pandas as pdimport numpy as npdata_sample = pd.DataFrame(np.random.randint(-10000 10000 size = (50000000 2)) columns = list(&#39;XY&#39;))I created a 50 million rows DataFrame and compared the time taken to process it via swifter apply() vs the vanilla apply(). I also created a dummy function with simple if else conditions to test the two approaches on.%%timeresults_arr = data_sample.apply(lambda x : custom(x[&#39;X&#39;] x[&#39;Y&#39;]) axis = 1)%%timeresults_arr = data_sample.swifter.apply(lambda x : custom(x[&#39;X&#39;] x[&#39;Y&#39;]) axis = 1)We are able to reduce the processing time by 64.4% from 7 minutes 53 seconds to 2 minutes 38 seconds.4. Multiprocessing in PythonWhile we are on the topic of decreasing time complexity I often end up dealing with datasets that I wish to process at multiple granularities. Using multiprocessing in python helps me save that time by utilizing multiple workers.I demonstrate the effectiveness of multiprocessing using the same 50 million rows data frame I created above. Except this time I add a categorical variable which is a random value selected out of a set of vowels.import pandas as pdimport numpy as npimport randomstring =  &#39;AEIOU&#39;data_sample = pd.DataFrame(np.random.randint(-10000 10000 size = (50000000 2)) columns = list(&#39;XY&#39;))data_sample[&#39;random_char&#39;] = random.choices(string k = data_sample.shape[0])unique_char = data_sample[&#39;random_char&#39;].unique()I used a for loop vs the Process Pool executor from concurrent.futures to demonstrate the runtime reduction we can achieve.%%timearr = []for i in range(len(data_sample)):        num1 = data_sample.X.iloc[i]    num2 = data_sample.Y.iloc[i]        if num1 &gt; num2:        if num1 &lt; 0:            arr.append(&quot;Greater Negative&quot;)        else:            arr.append(&quot;Greater Positive&quot;)    elif num2 &gt; num1:        if num2 &lt; 0:            arr.append(&quot;Less Negative&quot;)        else:            arr.append(&quot;Less Positive&quot;)    else:        arr.append(&quot;Rare Equal&quot;)def custom_multiprocessing(i):        sample = data_sample[data_sample[&#39;random_char&#39;] == \    unique_char[i]]        arr = []        for j in range(len(sample)):        if num1 &gt; num2:            if num1 &lt; 0:                arr.append(&quot;Greater Negative&quot;)            else:                arr.append(&quot;Greater Positive&quot;)        elif num2 &gt; num1:            if num2 &lt; 0:                arr.append(&quot;Less Negative&quot;)            else:                arr.append(&quot;Less Positive&quot;)        else:            arr.append(&quot;Rare Equal&quot;)                sample[&#39;values&#39;] = arr        return sampleI created a function that allows me to process each vowel grouping separately:%%time import concurrentdef main():    aggregated = pd.DataFrame()        with concurrent.futures.ProcessPoolExecutor(max_workers = 5) as executor:        results = executor.map(custom_multiprocessing range(len(unique_char)))if __name__ == &#39;__main__&#39;:    main()We see a reduction of CPU time by 99.3%. Though one must remember to use these methods carefully since they will not serialize the output therefore using them via grouping can be a good means to leverage this capability.5. MASE as a metricWith the rise of using Machine Learning and Deep Learning approaches for time series forecasting it is essential to use a metric NOT just based on the distance between predicted and actual value. A metric for a forecasting model should use errors from the temporal trend as well to evaluate how well a model is performing instead of just point in time error estimates. Enter Mean Absolute Scaled Error! This metric that takes into account the error we would get if we used a random walk approach where last timestamp’s value would be the forecast for the next timestamp. It compares the error from the model to the error from the naive forecast.def MASE(y_train y_test pred):       naive_error = np.sum(np.abs(np.diff(y_train)))/(len(y_train)-1)            model_error = np.mean(np.abs(y_test - pred))return model_error/naive_errorIf MASE &gt; 1 then the model is performing worse than a random walk. The closer the MASE is to 0 the better the forecasting model.In this article we went through some of the tricks I often use to make my life easier as a Data Scientist. Comment to share some of your tips! I would love to learn more about tricks that other Data Scientists use in their work.This is also my first Medium article and I feel like I am talking to nothingness so if you have any feedback to share then please feel free to critique and reach out :)5 things that make my job as a Data Scientist easier was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,5,Handling data bias,https://towardsdatascience.com/handling-data-bias-9775d07991d4?source=rss----7f60cf5620c9---4,A journey towards ethical AIPhoto by Louis Reed on Unsplash with edits from authorIf you also carry a vision of ensuring that the product you are working on follows all the written rules of “AI for good” then you would have definitely encountered a situation where your data is biased.Biased models biased data or biased implementation — are typical woes of a Data Scientist’s life. So first we need to understand and acknowledge that bias exists and can take any shape and form.Yes bias is a broad term and it can be present in the data collection algorithm or even at the ML output interpretation stage.Created by the author using PowerPointWhy does bias hurt?bias can lead to disparate access to opportunities on the grounds of several human characteristics such as race age or gender and should be discouragedAs per the AI index report by Stanford University AI/ML organizations construe the following risks as prevalent to the industry and are trying hard to mitigate such risks as they are detrimental to their business and humanity in general.AI Index ReportData bias can be of many forms:Structural Bias: The data can be biased purely because it is at the disposal of structural differences. The representation of women synonymous with the nurse cook the teacher is apparently emanating from societal construct. An e-commerce giant tried to build a recruitment tool that picked up the nuances of their existing staff which was needless to say biased. A lot of attributes such as sports social activities achievements etc were picked by machines that led to a biased tool with a preference towards men.Data Collection: Possible reasons for the bias in data collection could be based on time of the day age group of people country of origin the strata of class etc. Data fed to the algorithms should be continuously updated to reflect the true picture of the world we live in and in turn of the future state of the world that we want to make predictions on.Data Manipulation: It is easier to drop the instances with no label attached or the ones with missing values. But it is important to check whether the observations being eliminated are leading to misrepresented data specific to gender race nationality and related attributes.Algorithm bias: The algorithm will learn what the data pattern suggests it learn. The algorithm either mirrors the prevalent biases or to our worst fear amplifies them. If the judgments have been biased towards a particular group of people so does the machine learn from the training data. The bias in algorithms stems from the data which is either not a correct representative or is sprouting from the existential prejudices. If the input data is imbalanced then we need to ensure the algorithm still sees sufficient instances of minority class to perform well on it. There are multiple ways to achieve data rebalancing primary ones include synthetic data creation or assigning class weights so that the algorithm puts a higher penalty on each wrong prediction made on minority class.Implementation bias: All ML models are built on the fundamental assumptions that the train and test dataset should belong to similar distribution. A model trained on summer season data might have different feature distribution and hence will not be an appropriate fit to predict consumer behavior in the winter season. The model will only do good if the new data is similar to the data observed in the past on which the model was trained. Not just implementation but the interpretation can also be biased. What if we in our pursuit to analyze the algorithm output try to superimpose our beliefs and support our (biased) view.While bias is one of the factors to be mended in our pursuit of an Ethical AI framework it is certainly not trivial to mitigate.Some of the important aspects to build an “AI for good” ecosystem is:The data collector developer and product manager are generally the people who work in the field and are closer to the data. It is important for organizations to sensitize their employees and spread awareness about the possible causes of bias and how to mitigate themHaving an expert (AI Ethicist) who is adept at identifying the sources of bias can help businesses align their vision with the Ethical frameworkA governance team comprised of people from different teams like privacy ethics and compliance product and engineering will help provide a fresh perspective at identifying the conceivably ignored biases.There is no one rulebook that can be read and implemented at once it is an ever-evolving framework.Further it is commendable that the efforts in maintaining an unbiased fair and trustworthy AI framework are not seen as esoteric anymore and are garnering the right attention across the world.Handling data bias was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,5,"How to detect constant, quasi-constant features in your dataset",https://towardsdatascience.com/how-to-detect-constant-quasi-constant-features-in-your-dataset-a1ab7aea34b4?source=rss----7f60cf5620c9---4,Feature Selection using fast_mlContinue reading on Towards Data Science »
2021,7,5,Unit 4) Genetic Programming,https://towardsdatascience.com/unit-4-genetic-programming-d80cd12c454f?source=rss----7f60cf5620c9---4,Cover the Main Topics of Genetic Programming and Apply Them to a Time Series Analysis ProblemContinue reading on Towards Data Science »
2021,7,5,Understanding Taming Transformers for High-Resolution Image Synthesis,https://www.analyticsvidhya.com/blog/2021/07/understanding-taming-transformers-for-high-resolution-image-synthesis-vqgan/,"ArticleVideo Book Overview Introducing a convolutional VQGAN which learns a codebook of context-rich visual parts This approach is readily applied to conditional synthesis tasks where ... 
The post Understanding Taming Transformers for High-Resolution Image Synthesis appeared first on Analytics Vidhya."
2021,7,5,"Top Stories, Jun 28 – Jul 4: 5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist",https://www.kdnuggets.com/2021/07/top-news-week-0628-0704.html,Also: What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?; Add A New Dimension To Your Photos Using Python; Managing Your Reusable Python Code as a Data Scientist; Data Scientists are from Mars and Software Developers are from Venus
2021,7,5,GitHub Copilot: Your AI pair programmer – what is all the fuss about?,https://www.kdnuggets.com/2021/07/github-copilot-ai-pair-programmer.html,"GitHub just released Copilot a code completion tool on steroids dubbed your ""AI pair programmer."" Read more about it and see what all the fuss is about."
2021,7,5,Data Scientists and ML Engineers Are Luxury Employees,https://www.kdnuggets.com/2021/07/data-scientists-machine-learning-engineers-luxury-employees.html,Maybe it seems that everyone wants to become a data scientist and every organization wants to hire one as quickly as possible. However a mismatch often exists between what companies tend to need and what ML practitioners want to do. So it's time for the field to take another step toward maturity through an enhanced appreciation of the broad range of technical foundations for an organization to become data-driven.
2021,7,5,Predict Customer Churn (the right way) using PyCaret,https://www.kdnuggets.com/2021/07/pycaret-predict-customer-churn-right-way.html,A step-by-step guide on how to predict customer churn the right way using PyCaret that actually optimizes the business objective and improves ROI.
2021,7,5,breaking sticks of various length,https://www.r-bloggers.com/2021/07/breaking-sticks-of-various-length/," A riddle from the Riddler with a variation on the theme of breaking sticks: Given a stick of length L what is the optimal manner to break said stick to achieve a maximal product of the individual lengths? While the pen &#038; paper resolution is a one-line back-of-the-envelope calculation with an ...


The post breaking sticks of various length first appeared on R-bloggers."
2021,7,5,Full Workspace Automation through a Programmatic Interface (API) Available Now,https://www.r-bloggers.com/2021/07/full-workspace-automation-through-a-programmatic-interface-api-available-now/," Full Workspace Automation through a Programmatic Interface (API) Available Now
Each workspace already is an API
QBit Workspace is a new service to immediately deploy data science results at scale. You can think of it as an online data science editor (like RStudio) which can also be controlled and automated ...


The post Full Workspace Automation through a Programmatic Interface (API) Available Now first appeared on R-bloggers."
2021,7,5,Path of Least Resistance: Hosting Shiny Apps on Shinyapps.io,https://www.r-bloggers.com/2021/07/path-of-least-resistance-hosting-shiny-apps-on-shinyapps-io/," Shinyapps.io is the easiest and quickest way for sharing Shiny apps with the world.


The post Path of Least Resistance: Hosting Shiny Apps on Shinyapps.io first appeared on R-bloggers."
2021,7,5,The Important Components of Augmented Analytics!,https://www.smarten.com/blog/the-important-components-of-augmented-analytics/,AutoML NLP and Clickless Analytics Come Together to Produce Auto Insights! When it comes to the new world of analytics the augmented analytics approach allows business users with no data science background to readily access and use analytics in an intuitive way. There are some [&#8230;]
2021,7,4,10 Tips and Tricks for Data Scientists Vol.10,https://www.r-bloggers.com/2021/07/10-tips-and-tricks-for-data-scientists-vol-10/," We have started a series of articles on tips and tricks for data scientists (mainly in Python and R). In case you have ... Read more10 Tips and Tricks for Data Scientists Vol.10


The post 10 Tips and Tricks for Data Scientists Vol.10 first appeared on R-bloggers."
2021,7,4,Working with web data in R part II – APIs,https://www.r-bloggers.com/2021/07/working-with-web-data-in-r-part-ii-apis/,"
(If you haven’t read part I you can find it here.)
Alright this is a long overdue post: back in October I promised a part II to show how to pull data from the web via an API. Well better late than never!
Web APIs
There is so much ...


The post Working with web data in R part II – APIs first appeared on R-bloggers."
2021,7,4,simplevis: interactive plots with plotly::ggplotly,https://www.r-bloggers.com/2021/07/simplevis-interactive-plots-with-plotlyggplotly/,"
library(simplevis)
library(dplyr)
library(palmerpenguins)
Introduction
simplevis provides gglot2 (and leaflet) wrapper functions to make it easier to make beautiful visualisation with less brainpower required.
In the first simplevis blog po...


The post simplevis: interactive plots with plotly::ggplotly first appeared on R-bloggers."
2021,7,3,Note (2) for DESeq2 time series data analysis,https://www.r-bloggers.com/2021/07/note-2-for-deseq2-time-series-data-analysis/,"More notes on using LRT to test time-series data. Thanks for the discussion with Jie. swapping the levels of time factor won't change the LRT results as if the time variable is a factor LRT won't see it as a trajectory analysis but rather a fact...
The post Note (2) for DESeq2 time series data analysis first appeared on R-bloggers."
2021,7,3,ETF Tracking Error Minimization using R code,https://www.r-bloggers.com/2021/07/etf-tracking-error-minimization-using-r-code/,"    This post explains how to construct ETF tracking error (TE) minimization and introduce R packages which perform (sparse) index tracking. ETF (Exchange Traded Fund) is a traded fund listed on the exchange. ETF tries to mimic or follow a target benchm...


The post ETF Tracking Error Minimization using R code first appeared on R-bloggers."
2021,7,3,How open source packages are like smartphone apps,https://www.r-bloggers.com/2021/07/how-open-source-packages-are-like-smartphone-apps/," I remember my first days of coding R: I would search around for how to do X task and find out that Y package would be really helpful. But that led to more questions: What are packages in the first place and how do you use them? In this post ...


The post How open source packages are like smartphone apps first appeared on R-bloggers."
2021,7,3,Hosting Data Apps: 3 Months and 20 Posts Later,https://www.r-bloggers.com/2021/07/hosting-data-apps-3-months-and-20-posts-later/," The Hosting Data Apps website was launched 3 months ago. We review how the content was received by our readers and where the site is going next.


The post Hosting Data Apps: 3 Months and 20 Posts Later first appeared on R-bloggers."
2021,7,3,"Data Storytelling with Beginners, Valorant Edition",https://www.juiceanalytics.com/writing/data-storytelling-with-beginners-valorant-edition,"We had a couple of rising High School Sophomores as summer interns at Juice. One afternoon I asked them to make a Juicebox app on a topic of their choice. They chose to look at their gaming performance in Valorant a team-based first-person shooter PC game. Then they documented the app building experience. Here’s what they had to say…It is a fierce rivalry: two friends Owen and Marko wanted to compare their Valorant statistics with each other to see who is truly the better player.We needed to get to the facts. To start we looked up their statistics on Tracker.gg/Valorant. There was a plethora of data on the website making it a struggle to convert the statistics we saw on the website to data that we could use for comparison.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


After a few minutes of converting the statistics to usable data (Ed. Note: They got some professional help here) we created our Juicebox app using our newly found data.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We started out by inserting a few images of Valorant for our app. A picture can be worth a thousand data points.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We wanted to use Juicebox to visualize and compare Owen and Marko’s Valorant statistics. We used the scatterplot feature on our app to insert our data. We created multiple filters so we could select which players data or which game-mode we wanted to look at.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Next we added a leaderboard so we could compare Owen and Marko to see who the better player is. The leaderboard lets you compare by a bunch of different performance measures.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We added a bar chart to show the kills and deaths for every agent. Through this visualization we were able to see how much more Owen plays Valorant as his kills and deaths were higher for every agent in the game. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Finally we also added a Table which showed the last set of values for every agent in every game mode incase to finalizing the show of the data.Using all these Juicebox features we figured out that Owen dominated Marko in just about every single category proving his superiority. Yet compared to many other Valorant players Owen and Marko are both pretty average.Here's the final app if you want to take a look.Ready to demonstrate how you dominate your gaming friends?


	Try Juicebox Free"
2021,7,2,POI Classification Using Visit and Popularity Metrics — Part 1,https://towardsdatascience.com/poi-classification-using-visit-and-popularity-metrics-part-1-ae5e94f92077?source=rss----7f60cf5620c9---4,Train Bus or Plane? Predictive Classification of Points of Interest Using Visit and Popularity MetricsUsing geospatial features for classification of locations as Bus Stops Train Stations or AirportsImage by Annie Spratt and UnsplashThis article looks to demonstrate how to do basic classification with supervised machine learning using an applicable real-world dataset from SafeGraph. SafeGraph is a data provider that provides POI data for hundreds of businesses and categories. It provides data for free to academics. For this project I have chosen to use SafeGraph patterns data in order to classify records as various POI’s. The schema for the patterns data can be found here: Schema InfoFrom the numerous categories that can be chosen in the shop this project focuses on the classification of different transportation services. The categories chosen for this project are:Bus and Other Vehicle Transit SystemsRail TransportationOther Airport OperationsEach of these categories correlates to POI data for train stations bus stops and airport data.This article is the first of a three-part series using this data:POI Classification using safegraph data and sklearn classifiersModel tuning using PCA and cross-validationPOI Classification using spark Deep Learning classifiersPOI Classification within single NAICS code (fast-food restaurant classification)Image from Markus Spiske and UnsplashBelow are snapshots of the code that was written for this project and brief descriptions of the process of the code. The link to the code for this project can be found here: Notebook LinkSection 1: DependenciesThe following are the dependencies for this project. The packages that we need for this project are Pandas NumPy Seaborn Matplotlib and Pysparkimport pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltxfrom sklearn.metrics import plot_confusion_matrix— —!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null!wget -q https://www-us.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz!tar xf spark-3.1.2-bin-hadoop2.7.tgz!pip install -q findspark!pip install pyspark— —import findsparkfindspark.init(“/content/spark-3.1.2-bin-hadoop2.7”)from pyspark.sql import SparkSessionspark = SparkSession.builder.master(“local[*]”).getOrCreate()Section 2: Data Loadfrom pydrive.auth import GoogleAuthfrom pydrive.drive import GoogleDrivefrom google.colab import authfrom oauth2client.client import GoogleCredentialsauth.authenticate_user()gauth = GoogleAuth()gauth.credentials = GoogleCredentials.get_application_default()drive = GoogleDrive(gauth)def pd_read_csv_drive(id drive dtype=None):    downloaded = drive.CreateFile({‘id’:id})    downloaded.GetContentFile(‘Filename.csv’)    return(pd.read_csv(‘Filename.csv’dtype=dtype))def get_drive_id(filename):    drive_ids = {‘patterns’ : ‘1ReqpLgv50_3mCvZuKLlMdHxwfCPIHmqz’}    return(drive_ids[filename])transportation_df = pd_read_csv_drive(get_drive_id(‘patterns’) drive=drive)transportation_df.head(3)The output looks something like this (click on image for a better view):The table generated below is the patterns data for the three POI categories listed above for the month of December 2018. This particular month was chosen because public transportation is more likely to be utilized during the holiday season — thus showing more clear popularity and visitor metrics for each record.some of the columns that we will be using as features for our classification models are:raw_visit_counts — Number of visits in our panel to this POI during the date range.raw_visitor_counts — Number of unique visitors from our panel to this POI during the date range.visits_by_day — The number of visits to the POI each day (local time) over the covered time period.distance_from_home — Median distance from home traveled by visitors (of visitors whose home we have identified) in meters.median_dwell — Median minimum dwell time in minutes.bucketed_dwell_times — Key is range of minutes and value is number of visits that were within that durationpopularity_by_hour — A mapping of hour of day to the number of visits in each hour over the course of the date range in local time. First element in the array corresponds to the hour of midnight to 1 ampopularity_by_day — A mapping of day of week to the number of visits on each day (local time) in the course of the date rangedevice_type — The number of visitors to the POI that are using android vs. ios. Only device_type with at least 2 devices are shown and any category with less than 5 devices are reported as 4Section 3: Data CleansingThe first course of action would be to drop unnecessary columns.transportation_df = transportation_df.drop([‘parent_safegraph_place_id’’placekey’’safegraph_place_id’’parent_placekey’’parent_placekey’’safegraph_brand_ids’’brands’ ‘poi_cbg’] axis = 1)transportation_df.head(3)Here we are dropping the parent_safegraph_place_id placekey parent_placekey safegraph_brand_ids brands poi_cbg. These columns are in correlation to identifiers brands census block groups and placekeys. These columns are unrelated to the scope of this particular project and thus we are removing them.#Creation of ground truth Class column (Helper function)def class_definer(record):    fixed_record = record.lower()    if(‘bus’ in fixed_record):        return ‘Bus’    elif(‘airport’ in fixed_record):        return ‘Airport’    elif(‘train’ in fixed_record):        return ‘Train’    elif(‘metro’ in fixed_record):        return ‘Train’    elif(‘transport’ in fixed_record):        return ‘Bus’    elif(‘amtrak’ in fixed_record):        return ‘Train’    elif(‘bart’ in fixed_record):        return ‘Train’    elif(‘cta’ in fixed_record):        return ‘Train’    elif(‘mta’ in fixed_record):        return ‘Train’    elif(‘transit’ in fixed_record):        return ‘Bus’    elif(‘mbta’ in fixed_record):        return ‘Train’    elif(‘station’ in fixed_record):        return ‘Train’    elif(‘railway’ in fixed_record):        return ‘Train’    else:        return ‘Unknown’transportation_df[&#39;Class&#39;] = transportation_df[&#39;location_name&#39;].transform(lambda x: class_definer(x))transportation_df.head(3)These code snippets are attempting to create a Class column to establish the ground truth that is required for supervised learning algorithms such as classification. The data that is used for this project comes with some caveats especially with the bus stop data. The bus stop NAICS category consists of ‘Bus Stop and Other Transit Services’. This category in particular has multiple items such as bus stops as well as truck rentals and yacht services. Thus in order to remove the records that are irrelevant the above function was used.transportation_df = transportation_df[transportation_df[‘Class’] != ‘Unknown’].dropna()transportation_df.head(3)These unknown columns are representative of the Motor Transit records that come with the Bus data in the patterns data. We can drop these records because they are not exactly aligned with the notions of bus services — more tending towards ideas such as Truck services and Yacht rentals and other transit services that are away from the concept of a bus service.Now the data is formatted properly and has a Class column that can be used for classification algorithms. The next course of action is to format the dataframe in a way where all of the features can be utilized for the classification algorithm. This process requires the horizontal expansion of columns such as the visits_by_day column and the popularity_by_hour column. Some of these columns that need to be horizontally exploded are columns of arrays and some of them are columns of JSON. We will first look into the horizontal expansion of the JSON columns:In order to do this we will use pyspark specifically the from_json expression to horizontally explode the columns. In order to do this we must first convert the pandas dataframe to a Spark dataframe.transportation_df = spark.createDataFrame(transportation_df)transportation_df.show(2)Now that the data is in a Spark dataframe we need to create a schema for the JSON string columns that need to be exploded. These schemas will show the unique columns that would be produced when the JSON string is expanded along with the data type associated with the column and using this schema we can explode the JSON string columns.#Horizontal Explosion of JSON columns using Pysparkfrom pyspark.sql.functions import from_jsonexprfrom pyspark.sql.types import StructType StructField StringType ArrayType IntegerTypeday_schema = StructType(  [     StructField(‘Monday’ IntegerType()True)     StructField(‘Tuesday’ IntegerType()True)     StructField(‘Wednesday’ IntegerType()True)     StructField(‘Thursday’ IntegerType()True)     StructField(‘Friday’ IntegerType()True)     StructField(‘Saturday’ IntegerType()True)     StructField(‘Sunday’ IntegerType()True)  ])device_schema = StructType(  [     StructField(‘android’ IntegerType()True)     StructField(‘ios’ IntegerType()True)  ])bucketedDT_schema = StructType(  [    StructField(‘&lt;5’IntegerType()True)    StructField(‘5–10’IntegerType()True)    StructField(‘11–20’IntegerType()True)    StructField(‘21–60’IntegerType()True)    StructField(‘61–120’IntegerType()True)    StructField(‘121–240’IntegerType()True)    StructField(‘&gt;240’IntegerType()True)  ])transportation_df = transportation_df.withColumn(‘popularity_by_day’ from_json(‘popularity_by_day’ day_schema)).withColumn(‘device_type’ from_json(‘device_type’ device_schema)).withColumn(‘bucketed_dwell_times’from_json(‘bucketed_dwell_times’bucketedDT_schema)).select(‘location_name’’raw_visit_counts’’raw_visitor_counts’’visits_by_day’‘distance_from_home’’median_dwell’‘bucketed_dwell_times.*’’popularity_by_hour’’popularity_by_day.*’‘device_type.*’’Class’)transportation_df = transportation_df.toPandas()transportation_df.head(3)Now that the JSON strings have been exploded we can explode the array columns. Since this can be done fairly easily in Pandas we can convert the dataframe back to a pandas dataframe. The expansion of the array columns requires the column to be filled with arrays rather than a column of strings formatted like an array. Safegraph patterns data makes these columns strings rather than arrays so the first course of action is to convert the string to an array. This can be done using the literal_eval function from the ast package. From here we can take the individual values located in each index and explode them into separate columns.from ast import literal_evaltransportation_df[‘popularity_by_hour’] = transportation_df[‘popularity_by_hour’].transform(lambda x: literal_eval(x))pops = [‘popularity_’ + str(i) for i in range(125)]transportation_df[pops] = pd.DataFrame(transportation_df.popularity_by_hour.to_list() index=transportation_df.index)transportation_df = transportation_df.drop([‘popularity_by_hour’] axis = 1)transportation_df = transportation_df.reindex()transportation_df.drop(‘visits_by_day’ axis=1 inplace=True)transportation_df.head(3)The final part of data cleansing that we need to do is using the Sklearn LabelEncoder function to convert the Class function to numeric values for the classification models.from sklearn import preprocessingle = preprocessing.LabelEncoder()le.fit(transportation_df[‘Class’])transportation_df[‘Class’] = le.transform(transportation_df[‘Class’])transportation_df.head(3)Section 4: ClassificationImage from Umberto and UnsplashSince there are 3 unique classes we need to use a multiclass classifier. We will try Gaussian Naive Bayes Decision Trees and the K-Nearest Neighbors Classifier. First we must split our data into test and train sets.from sklearn.metrics import confusion_matrixfrom sklearn.model_selection import train_test_splitx_cols = []for item in list(transportation_df.columns):if(item != ‘Class’ and item != ‘location_name’):x_cols.append(item)X = transportation_df[x_cols]y = transportation_df[‘Class’]X_train X_test y_train y_test = train_test_split(X y random_state = 0)Gaussian Naive Bayes Classifier:Here is a little background regarding the functionality of the Naive Bayes Classifier: Gaussian Naive Bayesfrom sklearn.naive_bayes import GaussianNBgnb = GaussianNB().fit(X_train y_train)gnb_predictions = gnb.predict(X_test)gnb_predictionsaccuracy = gnb.score(X_test y_test)print(accuracy)confusion_matrix(y_test gnb_predictions)Now let&#39;s visualize the results using a heatmapplot_confusion_matrix(gnb X_test y_test normalize=’true’ values_format = ‘.3f’ display_labels=[‘Airport’’Bus’’Train’])yellow indicates high incidence and purple indicates low incidence. Our “correct” predictions are shown in the diagonal from the top-left corner to the bottom-right corner.Ideally we would see yellow in the diagonal and purple everywhere else.This shows that the Gaussian Naive Bayes does not do too well with classifying this particular dataset. This could be attributed to the classifier considering each of the features individually without the considerations of their correlations (thus the classifier being ‘naive’). This would make it difficult for the classifier to do well when features have high correlation coefficients such as this particular dataset where many of the features are exploded from one column and are directly correlated with one another.The accuracy of the model comes out to about 26%. The heatmap shows that a lot of values that are actually an airport are misclassified as a train station. The same can be said of true bus stations being misclassified as train stations. This shows that the majority of the records are actually classified as train stations.Decision Trees:Here is a little background regarding the functionality of the Decision Tree classifier: Decision Treefrom sklearn.tree import DecisionTreeClassifierdtree_model = DecisionTreeClassifier(max_depth = 3).fit(X_train y_train)dtree_predictions = dtree_model.predict(X_test)dtree_model.score(X_testy_test)confusion_matrix(y_test dtree_predictions)Visualizing results using a heatmapplot_confusion_matrix(dtree_model X_test y_test normalize=’true’ values_format = ‘.3f’ display_labels=[‘Airport’’Bus’’Train’])From this we can see that the Decision Tree model performs much better than the Naive Bayes with an accuracy of 75%. Considering that this is a multiclass classifier this accuracy is fairly decent.The classifier seems to do a very good job at correctly classifying Airports doing so at an accuracy rate of ~91.7% indicating that a model that takes into account the correlations between the various features of the data can perform very well in classifying the airports of this data correctly. The same cannot be said of the bus stop records for none of the true Bus stop records were classified as bus stops. The Train station data seems to perform slightly better classifying ~55.3% of true Train station data as Train stations. An interesting observation to make regarding this particular model’s predictions is that none of the records are classified as a Bus stop — -either correctly or incorrectly. This could be attributed to the smaller number of bus station records in comparison to the Train station and Airport records and the nature of the Decision Tree algorithm. The Decision Tree algorithm performs extremely well when using very balanced data but doesn’t perform as well on imbalanced data as in the case we have here.K-Nearest Neighbors:Here is a little background regarding the functionality of the KNN classifier: KNNfrom sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors = 22).fit(X_train y_train)accuracy = knn.score(X_test y_test)knn_predictions = knn.predict(X_test)confusion_matrix(y_test knn_predictions)Visualizing results using a heatmapplot_confusion_matrix(knn X_test y_test normalize=’true’ values_format = ‘.3f’ display_labels=[‘Airport’’Bus’’Train’])This classifier has an accuracy of about ~ 68.0%. The classifier does extremely well with classifying the Airport records properly having an accuracy of ~93.7%. The number of correctly classified bus stops is no longer 0 but it&#39;s still very low this can again be attributed to the imbalanced data. This classifier does slightly worse with train classification than the decision tree model predicting only ~18.3% of the true train stations as train stations.Conclusion:This project gave us an introduction to the classification of POI categories based on SafeGraph Patterns visit data. Our algorithms saw some success in classifying airports vs train stations vs bus stops all based on things like dwell time visitor distance from home popularity by the hour of the day and popularity by day of the week.The next post in the series (link coming soon!) will add complexity and power to our classifiers by tuning our models and conducting principal component analysis.Questions?I invite you to ask them in the #safegraphdata channel of the SafeGraph Community a free Slack community for data enthusiasts. Receive support share your work or connect with others in the GIS community. Through the SafeGraph Community academics have free access to data on over 7 million businesses in the USA UK and Canada.POI Classification Using Visit and Popularity Metrics — Part 1 was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,2,Custom Estimator With PyCaret | Part 2 | by Fahad Akbar,https://towardsdatascience.com/custom-estimator-with-pycaret-part-2-by-fahad-akbar-aee4dbdacbf?source=rss----7f60cf5620c9---4,A hands-on guide to building and deploying a Sklearn compatible estimator from scratch in Scipy through PyCaretContinue reading on Towards Data Science »
2021,7,2,You DO need math for Machine Learning,https://towardsdatascience.com/you-do-need-math-for-machine-learning-cf934a607960?source=rss----7f60cf5620c9---4,Don&#x2019;t skip the most unappreciated skill in ML. Write code AND learn the theory.Continue reading on Towards Data Science »
2021,7,2,Machine Translation on low resource languages,https://towardsdatascience.com/machine-translation-on-low-resource-languages-65cb268fcab1?source=rss----7f60cf5620c9---4,MT using simple transformers | Image by AuthorUsing Simple transformer to translate French into Ewe and Fongbe on Deepnote platform.IntroductionEwe and Fongbe are Niger–Congo languages part of a cluster of related languages commonly called Gbe. Fongbe is the major Gbe language of Benin (with approximately 4.1 million speakers) while Ewe is spoken in Togo and southeastern Ghana by approximately 4.5 million people as a first language and by a million others as a second language. They are closely related tonal languages and both contain diacritics that can make them difficult to study understand and translate. For more information go to Zindi.Zindi MT ChallengeObjectivesThe objective of this challenge is to create a machine translation system capable of converting text from French into Fongbe or Ewe. I will be using same model to train and translate both datasets to ease the processing power and memory issues.Simple TransformersThis library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize a model train the model and evaluate a model. For more information visit GitHub Repo.SupportsSequence ClassificationToken Classification (NER)Question AnsweringLanguage Model Fine-TuningLanguage Model TrainingLanguage GenerationT5 ModelSeq2Seq TasksMulti-Modal ClassificationConversational AI.Text Representation Generation.Installing and loading librarieshttps://medium.com/media/85fa5a172cd75f396990c3fc9724ac7b/hrefDataI have used only 35k samples so that my GPU doesn&#39;t run out of memory and I have used original data without preprocessing.https://medium.com/media/da2e57dfa78f95abe2db249e95e86188/hrefCleaning function was useful in English translation but in this case we get poor results if we clean the data. So we are going to turn it off.https://medium.com/media/6ad8e37df73a3169c1eb82d996a43572/hrefDividing train and test data into two data frames based on languages.https://medium.com/media/2416326f69a78651453d5ee067d29b55/hrefTraining Fongbe ModelUsing simple transformer seq2seq I have downloaded Helsinki-NLP/opus-mt-en-mul which work best in our case and using specific Seq2SeqArgs to set arguments of model.Arguments:num_train_epochs = 30batch_size = 32max_length = 120src_lang =”fr”tgt_lang =”fon”overwrite_output_dir = TrueTrain / Eval splithttps://medium.com/media/4dde8348dd29588429f1180b4201e51c/hrefArgumentsI experimented with multiple model argument and produce the best possible to give us better result.https://medium.com/media/61a3a91ebad05a86a24ef5bdec421c93/hrefInitializing Modelhttps://medium.com/media/ea2024c68952e7ec6883975bb8b69ddb/hrefEvaluation Metrichttps://medium.com/media/6d101ece95d68ab2a7e83e3f50de43b5/hrefTraining Modelhttps://medium.com/media/334324c8c55d530de2097850077806f7/hrefSingle PredictionThe model is performing accuratly.https://medium.com/media/e6994a14b48ff39aa4c9e0bde1c74bf1/hrefPredicting Fongbe test data and savinghttps://medium.com/media/09974980422c0130cb9cd15332086e5f/hrefSaving the model Fon modelhttps://medium.com/media/0789de16981e0b42f369d686f5eab840/hrefTraining Ewe ModelUsing simple transformer seq2seq I have downloaded Helsinki-NLP/opus-mt-en-mul which work best in our case and using specific Seq2SeqArgs to set arguments of model.Arguments:num_train_epochs = 30batch_size = 32max_length = 120src_lang =”fr”tgt_lang =”ewe”overwrite_output_dir = Truehttps://medium.com/media/a3dbf479e126a71d229b38c599815baf/hrefModel ArgumentsI experimented with multiple model argument and produce the best possible to give us better result.https://medium.com/media/ab7c8f2af5f817b59c55dbdcc21d1a98/hrefInitializing Ewe modelhttps://medium.com/media/6acf5abd2d2ab8c21e792b1e4203c647/hrefTraining modelhttps://medium.com/media/e3142281a9e44248b902218000ba4aa3/hrefPredicting and saving CVShttps://medium.com/media/f18705ea7bfbb99129f1686cdb0bfc43/hrefSaving modelhttps://medium.com/media/ce6d360df6ffd8c9e5c4c570a5fe6e18/hrefJoining both MT predicted translationhttps://medium.com/media/7608ec2101155947d7066e66d274d4d3/hrefCreating Submission filehttps://medium.com/media/030e9227196697a823778cadb051b0a4/hrefLeaderboardThe error metric for this competition is Rouge Score ROUGE-N (N-gram) scoring (Rouge1) reporting the F-measure.Zindi LeaderboardFinal thoughtsMachine translation is underrated in the world of NLP due to Google translation and other giants making translation perfect but they don’t offer all the languages some of the low resource languages don’t even make it. This was an interesting journey as I started with the seq2seq model using attention to transformers and then stumble upon Helsinki NLP language models. I had issues with memory and GPU and even then I was not improving my score as there were many limitations while processing huge data. After spending a month I stumble upon simple transformers which were created to make fine-tuning simple and easy for everyone. In the end I was happy with the result while using effective and fast ways to effectively train and predict low resource languages from Africa.Code available on GitHub and Deepnote .Machine Translation on low resource languages was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,2,Automating Emails in Apache Airflow: A How-To Guide,https://towardsdatascience.com/automating-emails-in-apache-airflow-a-how-to-guide-1d2330a29d1e?source=rss----7f60cf5620c9---4,A simple walkthrough of writing a one-step DAG to automate an e-mail with Python&#x2019;s SMTP library Gmail and Apache AirflowContinue reading on Towards Data Science »
2021,7,2,Images Generation with Neural Style Transfer and Tensorflow,https://towardsdatascience.com/images-generation-with-neural-style-transfer-and-tensorflow-a823b0353b06?source=rss----7f60cf5620c9---4,Creation of unique images using machine learning algorithmsContinue reading on Towards Data Science »
2021,7,2,Unsupervised Representation Learning on Distributed Regions in the Human Brain (Part-IV),https://towardsdatascience.com/unsupervised-representation-learning-on-distributed-regions-in-the-human-brain-part-iv-55fecf4e1b6f?source=rss----7f60cf5620c9---4,The Intersection of the Unsupervised Learning and the Human BrainLatent Space of ROI’s of Ventral Temporal Cortex (Image By Author)This is the fourth article of the series namely “Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal Cortex”. If you want to check out the whole series go to the following link.Cognitive Computational Modelling for Spatio-Temporal fMRI in Ventral Temporal CortexI will introduce the topic of unsupervised representation learning on distributed regions in the human brain and its use case in the research of brain decoding. Let’s get started.All related materials are hosted on my GitHub page. Don’t forget to check it out. If you are a paper lover you can read the paper version of this series of articles that can also be found in my repo.cankocagil/Cognitive-Computational-Modelling-for-Spatio-Temporal-fMRI-in-Ventral-Temporal-CortexUnsupervised &amp; Manifold Learning in Human BrainFunctional MRI data are very high-dimensional if one considers all the voxels or surface coordinates acquired with standard imaging parameters. As in our dataset with the structure of 4D time-series image data we have a curve of dimensionality problem. Hence dimension reduction and manifold learning algorithms can reduce the dimensionality of fMRI space by preserving geodesic relations in the lower space representations. We performed PCA LDA ICA NNMF and MDS as dimension reduction algorithms. Besides t- SNE UMAP ISOMAP LLE and Spectral Embedding are performed to generate lower-dimensional manifolds of the fMRI space. Let’s start discovering the intersection of unsupervised learning and the human brain. I performed many unsupervised learning algorithms as it further helps to understand geodesic information underlying the human brain and gives prior information on whether the neural activities in distributed regions are decodable or not.Let’s install and import all necessary packages. Please refer to previous articles (part-I) for the dataset understanding. Note that this article is not for answering the following questions (which are previously answered)How fMRI data is structured? (part-I)Why spatio-temporal masking is performed on the human brain? (Part-II)Here are the pip commands for installing everything we need.https://medium.com/media/4e1d1900606bc69f98d184d0de16035e/hrefWe installed everything (even more) we need. Let’s import them as follows.https://medium.com/media/ffb22af01a5f9bfcf387df13acd0d9c3/hrefWe are ready to go! Let’s fetch the Haxby dataset.https://medium.com/media/ca18e4b261e0c0b41c3eee1ef29fb506/hrefNext thing is to prepare mask standardize and convert fMRI data to NumPy matrix form as follows.https://medium.com/media/d95e9c9daf5dc968e8624a87c74c2a34/hrefhttps://medium.com/media/d7357d2aeedbceed7f22e80dbb8c4537/hrefPreprocessing part is done. So we can move to the actual processes we want to perform. In this code snippet “masks” are the masked regions in the human brain that we want to perform cognitive tasks on it and categories are just their labels. We do not use whole voxels for representation learning purposes. Our main idea is to extract and visualize the latent variables in the human brain where the distributed and overlapping patterns of neural activity are happening.Finally we can start the actual business as follows. In the following figures different colors represent different categories. (i.e. blues belong to class 1 oranges belong to class 2 etc.)To properly visualize the latent space I utilized the plotly python package. The following code performs 2D and 3D visualization in an interactive fashion.https://medium.com/media/dac56bb608b3aa68b8f39d222dce9a7b/hrefDimension Reduction: PCAPCA is a linear unsupervised dimension reduction algorithm and it computes principal vectors to change the basis of the representation [26]. PCA is a used algorithm in a broad range of topics from image compression to decorrelation of texts. Here we performed PCA on RoI’s of subject 5 and visualized it as follows.https://medium.com/media/c0810b4349e35bf14ba73f4d4a41855a/hrefPCA on Masked Regions in the Human Brain (Image By Author)https://medium.com/media/6b237ad9db5fab595b4c73f3cd0fac93/hrefPCA on Masked Regions in the Human Brain (Image By Author)Dimension Reduction: LDALDA is a supervised dimensionality reduction algorithm and it is a generalization of Fisher’s linear discriminant aims to find linear subspace that characterizes the original data space. Since it is supervised it is a powerful paradigm in representation learning. Here we performed LDA on RoI’s of subject 5 and visualized it. From the figures we can see that LDA is outperforming other methods by uniquely separating geodesic distances in the manifolds.https://medium.com/media/2be236256b52e06e7e3af4d044836661/hrefLDA on Masked Regions in the Human Brain (Image By Author)Dimension Reduction: ICAICA is a computational approach for separating multi-variate signals into their additive components. It is a natural paradigm for unsupervised dimensionality reduction. Here we performed ICA on RoI’s of subject 5 and visualized it as follows.https://medium.com/media/1ead80071709c9a93fa53e7832bf7f23/hrefICA on Masked Regions in the Human Brain (Image By Author)Dimension Reduction: NNMFNNMF is an iterative non-negative factor analysis to decompose a non-negative matrix into its linear subspaces. It is useful in extracting natural linear subspaces of original data samples. Here we performed NNMF on RoI’s of subject 5 and visualized it.https://medium.com/media/b3d90680dff3df40e6bea1a2b83dce33/hrefNNMF on Masked Regions in the Human Brain (Image By Author)Manifold Learning: MDSMDS is a classical approach for extracting non-linear sub-spaces of the original data space by preserving geodesic distance in the manifold. Lower dimensional embedding is obtained to represent original data in the manifold. Here we performed MDS on RoI’s of subject 5 and visualized it.https://medium.com/media/c205c2db2082957f6151918c4e1f1680/hrefMDS on Masked Regions in the Human Brain (Image By Author)Manifold Learning: t-SNET-SNE is an iterative statistical approach for producing non-linear embedding of the original data space by preserving small pairwise distances or localized similarities. It minimizes the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. Here we performed t-SNE on RoI’s of subject 5 and visualized it.https://medium.com/media/78a498c0dd9ece4ac46fdd6fe66a150e/hreft-SNE on Masked Regions in the Human Brain (Image By Author)Manifold Learning: UMAPUMAP is a recent approach for non-linear embedding and it generally outperforms t-SNE by a significant margin. It is very similar to t-SNE but it also preserves the global geodesic structure of the data. Here we performed UMAP on RoI’s of subject 5 and visualized it.https://medium.com/media/2e57c06877da078da75624efbbfe6a9f/hrefUMAP on Masked Regions in the Human Brain (Image By Author)Manifold Learning: ISOMAPISOMAP map is also a non-linear embedding algorithm through isometric mapping for accurately estimating the intrinsic geometry of manifold by preserving geodesic distances in the manifold. Here we performed ISOMAP on RoI’s of subject 5 and visualized it.https://medium.com/media/d65874f5b5eac16480f32156078f770d/hrefISOMAP on Masked Regions in the Human Brain (Image By Author)Manifold Learning: LLELLE is a topology-preserving non-linear dimension reduction algorithm trying to preserve neighbor structure in the manifold and it is generally outperforming ISOMAP in terms of optimization and speed thus it has very practical uses in literature. Here we performed LLE on RoI’s of subject 5 and visualized it.https://medium.com/media/ccafd6d0096fa5169e1dfcd916ffa84b/hrefLLE on Masked Regions in the Human Brain (Image By Author)Manifold Learning: Spectral EmbeddingSpectral embedding is also a non-linear embedding algorithm that forms an affinity matrix and applies spectral decomposition to the laplacian graph. Here we performed Spectral embedding on RoI’s of subject 5 and visualized it.https://medium.com/media/47dae4d43a993ff1ec3d7f3257d73ac0/hrefSpectral Embedding on Masked Regions in the Human Brain (Image By Author)That’s it for this article. We covered unsupervised representation learning on distributed regions in the human brain and its use case in the research of brain decoding. Congratulations! You completed the fourth article and took a step through cognitive computational approaches for decoding the human brain.In the next article we’ll perform comprehensive decoding algorithms from classical ML algorithms to neural networks.Links of ArticlesPublished ArticlesIntroduction to Cognitive Computational Modelling of Human Brain (Part-I)2.Discovery Neuroimaging Analysis (Part-II)3.Functional Connectivity and Similarity Analysis of Human Brain (Part-III)4.Unsupervised Representation Learning on Distributed Regions in the Human Brain (Part-IV)2. On the Way (Coming soon…)Placeholder for Part-VFurther Readinghttps://www.hindawi.com/journals/cmmm/2012/961257/The following list of references is utilized in my research for both machine learning and neuroscience sides. I highly recommend copy-and-paste the references and review them in brief.References[1] J. L. Ba J. R. Kiros and G. E. Hinton. Layer normalization 2016.[2] L. Buitinck G. Louppe M. Blondel F. Pedregosa A. Mueller O. Grisel V. Niculae P. Prettenhofer A. Gramfort J. Grobler R. Layton J. VanderPlas A. Joly B. Holt 10 and G. Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning pages 108–122 2013.[3] X. Chu Z. Tian Y. Wang B. Zhang H. Ren X. Wei H. Xia and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers 2021.[4] K. Crammer O. Dekel J. Keshet S. Shalev-Shwartz and Y. Singer. Online passive aggressive algorithms. 2006.[5] K. J. Friston. Statistical parametric mapping. 1994.[6] C. G. Gross C. d. Rocha-Miranda and D. Bender. Visual properties of neurons in inferotemporal cortex of the macaque. Journal of neurophysiology 35(1):96–111 1972.[7] S. J. Hanson T. Matsuka and J. V. Haxby. Combinatorial codes in ventral temporal lobe for object recognition.[8] J. Haxby M. Gobbini M. Furey A. Ishai J. Schouten and P. Pietrini. ”visual object recognition” 2018.[9] R. A. Heckemann J. V. Hajnal P. Aljabar D. Rueckert and A. Hammers. Automatic anatomical brain mri segmentation combining label propagation and decision fusion. NeuroImage 33(1):115–126 2006.[10] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus) 2020.[11] S. Huang W. Shao M.-L. Wang and D.-Q. Zhang. fmribased decoding of visual information from human brain activity: A brief review. International Journal of Automation and Computing pages 1–15 2021.[12] R. Koster M. J. Chadwick Y. Chen D. Berron A. Banino E. Duzel D. Hassabis and D. Kumaran. Big-loop recurrence ¨ within the hippocampal system supports integration of information across episodes. Neuron 99(6):1342–1354 2018.[13] E. Maor. The Pythagorean theorem: a 4000-year history. Princeton University Press 2019.[14] K. A. Norman S. M. Polyn G. J. Detre and J. V. Haxby. Beyond mind-reading: multi-voxel pattern analysis of fmri data. Trends in cognitive sciences 10(9):424–430 2006.[15] A. J. O’toole F. Jiang H. Abdi and J. V. Haxby. Partially distributed representations of objects and faces in ventral temporal cortex. Journal of cognitive neuroscience 17(4):580–590 2005.[16] F. Pedregosa G. Varoquaux A. Gramfort V. Michel B. Thirion O. Grisel M. Blondel P. Prettenhofer R. Weiss V. Dubourg J. Vanderplas A. Passos D. Cournapeau M. Brucher M. Perrot and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830 2011.[17] R. A. Poldrack. Region of interest analysis for fmri. Social cognitive and affective neuroscience 2(1):67–70 2007.[18] M. Poustchi-Amin S. A. Mirowitz J. J. Brown R. C. McKinstry and T. Li. Principles and applications of echo-planar imaging: a review for the general radiologist. Radiographics 21(3):767–779 2001.[19] R. P. Reddy A. R. Mathulla and J. Rajeswaran. A pilot study of perspective taking and emotional contagion in mental health professionals: Glass brain view of empathy. Indian Journal of Psychological Medicine page 0253717620973380 2021.[20] S. M. Smith K. L. Miller G. Salimi-Khorshidi M. Webster C. F. Beckmann T. E. Nichols J. D. Ramsey and M. W. Woolrich. Network modelling methods for fmri. Neuroimage 54(2):875–891 2011.[21] K. Tanaka. Inferotemporal cortex and object vision. Annual review of neuroscience 19(1):109–139 1996.[22] M. S. Treder. Mvpa-light: a classification and regression toolbox for multi-dimensional data. Frontiers in Neuroscience 14:289 2020.[23] M. P. Van Den Heuvel and H. E. H. Pol. Exploring the brain network: a review on resting-state fmri functional connectivity. European neuropsychopharmacology 20(8):519–534 2010.[24] G. Varoquaux A. Gramfort J. B. Poline and B. Thirion. Brain covariance selection: better individual functional connectivity models using population prior. arXiv preprint arXiv:1008.5071 2010.[25] Y. Wang J. Kang P. B. Kemmer and Y. Guo. An efficient and reliable statistical method for estimating functional connectivity in large scale brain networks using partial correlation. Frontiers in neuroscience 10:123 2016.[26] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.[27] S. Wold K. Esbensen and P. Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems 2(1–3):37–52 1987.Unsupervised Representation Learning on Distributed Regions in the Human Brain (Part-IV) was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,2,Clustering in Geospatial Applications — which model should you use?,https://towardsdatascience.com/clustering-in-geospatial-applications-which-model-should-you-use-59a039332c45?source=rss----7f60cf5620c9---4,A novel comparison between KMeans DBSCAN Hierarchical clustering models in machine learning applied to urban networksContinue reading on Towards Data Science »
2021,7,2,Learn You Some Kedro*,https://towardsdatascience.com/learn-you-some-kedro-be67d4fc0ce7?source=rss----7f60cf5620c9---4,For reproducible maintainable and modular data science codeContinue reading on Towards Data Science »
2021,7,2,How to spend your time when you are waiting for a Data Analysis Output,https://towardsdatascience.com/how-to-spend-your-time-when-you-are-waiting-for-a-data-analysis-output-e71b383f43cb?source=rss----7f60cf5620c9---4,Some suggestions to not waste your time when your computer is running your preferred algorithms and you are waiting for results.Continue reading on Towards Data Science »
2021,7,2,PyTorch 1.9 – Towards Distributed Training and Scientific Computing,https://www.analyticsvidhya.com/blog/2021/07/pytorch-1-9-towards-distributed-training-and-scientific-computing/,"ArticleVideo Book Overview So what does the newest release of PyTorch i.e 1.9 have to offer? Facebook’s PyTorch team has vastly amped up its ... 
The post PyTorch 1.9 &#8211; Towards Distributed Training and Scientific Computing appeared first on Analytics Vidhya."
2021,7,2,Accelerate efficiency gains with optimization and AI,https://www.ibm.com/blogs/journey-to-ai/2021/07/accelerate-efficiency-gains-with-optimization-and-ai/,"Today machine learning (ML) artificial intelligence (AI) and decision optimization (DO) are not just buzzwords you read in the press but urgent requirements for any company that fears disruption and wants to do pragmatic analysis in order to make better decisions with their data. Data is the next natural resource but as with any resource [&#8230;]
The post Accelerate efficiency gains with optimization and AI appeared first on Journey to AI Blog."
2021,7,2,Governed ModelOps with Anaconda and IBM Cloud Pak® for Data,https://www.ibm.com/blogs/journey-to-ai/2021/07/governed-modelops-with-anaconda-and-ibm-cloud-pak-for-data/,"Using open source packages and libraries during the development stage for artificial intelligence and machine learning (AI/ML) models can enable data scientists to capitalize on the latest innovations. But these packages and libraries also pose security and governance challenges for enterprises. Given the excitement and growth in data science and AI companies around the world [&#8230;]
The post Governed ModelOps with Anaconda and IBM Cloud Pak® for Data appeared first on Journey to AI Blog."
2021,7,2,Semantic Search: Measuring Meaning From Jaccard to Bert,https://www.kdnuggets.com/2021/07/semantic-search-measuring-meaning-jaccard-bert.html,In this article we’ll cover a few of the most interesting — and powerful — of these techniques — focusing specifically on semantic search. We’ll learn how they work what they’re good at and how we can implement them ourselves.
2021,7,2,"High-Performance Deep Learning: How to train smaller, faster, and better models – Part 3",https://www.kdnuggets.com/2021/07/high-performance-deep-learning-part3.html,Now that you are ready to efficiently build advanced deep learning models with the right software and hardware tools the techniques involved in implementing such efforts must be explored to improve model quality and obtain the performance that your organization desires.
2021,7,2,Prepare Behavioral Questions for Data Science Interviews,https://www.kdnuggets.com/2021/07/prepare-behavioral-questions-data-science-interviews.html,This is part 5 of a series by the author which helps readers nail the data science interviews with confidence.
2021,7,2,How to Calculate Partial Correlation coefficient in R-Quick Guide,https://www.r-bloggers.com/2021/07/how-to-calculate-partial-correlation-coefficient-in-r-quick-guide/," partial correlation coefficient r When we want to find the linear relationship between two variables we often choose the Pearson correlation coefficient. But some...
The post How to Calculate Partial Correlation coefficient in R-Quick Guide appeared first on finnstats.


The post How to Calculate Partial Correlation coefficient in R-Quick Guide first appeared on R-bloggers."
2021,7,2,rOpenSci at useR!2021 – Presentations from Staff and Community,https://www.r-bloggers.com/2021/07/ropensci-at-user2021-presentations-from-staff-and-community/,"Are you putting together your useR!2021 conference schedule this weekend? Four rOpenSci staff and lots of community members are giving presentations and there’s something for everyone!
🔗
Talks by rOpenSci staff
Jeroen Ooms Lead Infra...
The post rOpenSci at useR!2021 – Presentations from Staff and Community first appeared on R-bloggers."
2021,7,2,Improving a Visualization,https://www.r-bloggers.com/2021/07/improving-a-visualization/," I saw this post
on Reddit’s r/dataisbeautiful showing this plot of streaming services market
share comparing 2020 to 2021
US Streaming Services Market Share 2020 vs 2021
and thought it looked like a good candidate for trying out some plot improvem...


The post Improving a Visualization first appeared on R-bloggers."
2021,7,2,Free Online Calculating Inventory Safety Stock Calculator,https://www.deep-data-mining.com/2021/07/introduce-free-online-tool-for.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I made a youtube video at https://lnkd.in/d3hECWU to introduce a free online tool for calculating inventory safety stock. A spreadsheet file that implements the calculator is downloadable. Here is the link to the calculator. Enjoy!      
2021,7,2,"Gradient Flow Snapshot #61: Working with data in GitHub using Flat Data, Data Engineering Survey",http://practicalquant.blogspot.com/2021/07/gradient-flow-snapshot-61-working-with.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,7,1,"Word, Subword, and Character-Based Tokenization: Know the Difference",https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17?source=rss----7f60cf5620c9---4,The differences that anyone working on an NLP project should knowImage by Sincerely Media on UnsplashNatural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that provides machines (computers) the ability to understand written and spoken human language in the same way as human beings. NLP is almost everywhere and helping people in their daily tasks. 😍 It is such a common technology now that we often take it for granted. A few examples are spell check autocomplete spam detection Alexa or Google assistant. NLP can be taken for granted but one can never forget that machines work with numbers and not letters/words/sentences. So to work with a large amount of text data readily available on the internet we need manipulation and cleaning of text which we commonly call text pre-processing in NLP.Pre-processing is the first step in working with text and in building a model to solve our business problem. Pre-processing in itself is a multi-stage process. In this article we will be only talking about tokenization and tokenizers. So let’s get started. 🏄🏼Note: We are mainly focusing on the English language.TokenizationTokenization is one of the most important steps in text pre-processing. Whether you are working with traditional NLP techniques or using advanced deep-learning techniques you cannot skip this step. 🙅🏻Tokenization in simple words is the process of splitting a phrase sentence paragraph one or multiple text documents into smaller units. 🔪 Each of these smaller units is called a token. Now these tokens can be anything — a word a subword or even a character. Different algorithms follow different processes in performing tokenization but the below given example will give you a basic idea about the difference between these three.Consider the following sentence/raw text.“Let us learn tokenization.”A word-based tokenization algorithm will break the sentence into words. The most common one is splitting based on space.[“Let” “us” “learn” “tokenization.”]A subword-based tokenization algorithm will break the sentence into subwords.[“Let” “us” “learn” “token” “ization.”]A character-based tokenization algorithm will break the sentence into characters.[“L” “e” “t” “u” “s” “l” “e” “a” “r” “n” “t” “o” “k” “e” “n” “i” “z” “a” “t” “i” “o” “n” “.”]Tokens are actually the building blocks of NLP and all the NLP models process raw text at the token level. These tokens are used to form the vocabulary which is a set of unique tokens in a corpus (a dataset in NLP). This vocabulary is then converted into numbers (IDs) and helps us in modeling. 😎We mentioned three different tokenization techniques here. Each of these techniques works differently and have their own advantages and disadvantages. Let us go into the details of each of these techniques to know more. 🏇🏻Word-based tokenizationThis is the most commonly used tokenization technique. It splits a piece of text into words based on a delimiter. The most commonly used delimiter is space. You can also split your text using more than one delimiter like space and punctuation marks. Depending on the delimiter you used you will get different word-level tokens.Word-based tokenization can be easily done using custom RegEx or Python’s split() method. Apart from that there are plenty of libraries in Python — NLTK spaCy Keras Gensim which can help you perform tokenization easily.Example:“Is it weird I don’t like coffee?”By performing word-based tokenization with space as a delimiter we get:[“Is” “it” “weird” “I” “don’t” “like” “coffee?”]If we look at the tokens “don’t” and “coffee?” we will notice that these words have punctuation attached to them. What if there is another raw text (sentence) in our corpora like this — “I love coffee.” This time there will be a token “coffee.” which can lead the model to learn different representations of the word coffee (“coffee?” and “coffee.”) and will make the representation of words (tokens) suboptimal. 🙆🏻The reason that we should take punctuation into account while performing tokenization is that we do not want our model to learn different representations of the same word with every possible punctuation (of course the ones that can follow a word). If we allow our model to do so we will be exploded with the number of representations a model will learn (each word × number of punctuations used in a language). 😳 So let’s take punctation into account.[“Is” “it” “wierd” “I” “don” “’” “t” “like” “coffee” “?”]This is better than what we had earlier. However if we notice tokenization has made three tokens for the word “don’t” — “don” “’” “t”. Better tokenization of “don’t” would have been “do” and “n’t” and this way if the model would have seen a word “doesn’t” in the future it would have tokenized it into “does” and “n’t” and since the model would have already learned about “n’t” in the past it would have applied its knowledge here. The problem sounds complicated but can be dealt with using some rules. 🤓You must have noticed that the state-of-the-art NLP models have their own tokenizers because each model uses different rules to perform tokenization along with tokenizing using spaces. Thus tokenizers of different NLP models can create different tokens for the same text. Space and punctuation and rule-based tokenization are all examples of word-based tokenization.Each word is then represented using an ID and each ID contains a lot of information as a word in a sentence usually has a lot of contextual and semantic information. 😲The technique sounds impressive but this type of tokenization leads to a massive corpus which leads to a big vocabulary. 😏 The state-of-the-art model Transformer XL uses space and punctuation tokenization and has a vocabulary size of 267735. That’s huge! This huge vocabulary size leads to a huge embedding matrix for the input as well as the output layers causing the model to be heavier and requiring more computational resources.This tokenization also gives different IDs to the words like “boy” and “boys” which are almost similar words in the English language (one is singular and the other is plural). We actually want our model to know that words like these are similar.To solve this huge vocabulary problem we can limit the number of words that can be added to the vocabulary. For example we can save only the most common (based on the frequency of occurrence of words in the corpora) 5000 words in our vocabulary. The model will then create IDs for those 5000 common words and mark the rest of the words as OOV (Out Of Vocabulary). But this leads to loss of information as the model will not learn anything about the OOV words. This can be a big compromise for the model as it will learn the same OOV representation for all the unknowns words. 🙄One more drawback is regarding the misspelled words. If the corpora have “knowledge” misspelled as “knowldge” the model will assign OOV token to the later word.Thus to solve all these issues researchers came up with character-based tokenization.Character-based tokenizationCharacter-based tokenizers split the raw text into individual characters. The logic behind this tokenization is that a language has many different words but has a fixed number of characters. This results in a very small vocabulary. 😍For example in the English language we use 256 different characters (letters numbers special characters) whereas it has close to 170000 words in its vocabulary. Thus character-based tokenization will use fewer tokens compared to word-based tokenization.One of the major advantages of character-based tokenization is that there will be no or very few unknown or OOV words. Thus it can create a representation of the unknown words (words not seen during training) using the representation for each character. Another advantage is that misspelled words can be spelled correctly rather can marking them as OOV tokens and losing information.This type of tokenization is quite simple and can greatly reduce memory and time complexity. So is it the best or perfect algorithm for tokenization? 🤔 The answer is no (at least for the English Language)! A character usually doesn’t carry any meaning or information as a word does. 😕Note: A few languages carry a lot of information in each character. So character-based tokenization can be useful there.Also reducing the vocabulary size has a trade-off with the sequence length in character-based tokenization. Each word is split into each character and thus the tokenized sequence is much longer than the initial raw text. For example the word “knowledge” will have 9 different tokens. 🙄Note: Researchers Karparthy Radford et al. Kalchbrenner et al. and Lee et al. have demonstrated the use of character-based tokenization and came up with some impressive results. Read these papers to know more!Character-based tokenization despite having some issues have solved a lot of problems faced by word-based tokenization. Let us see if we can solve the issues faced by character-based tokenization too.Subword-based tokenizationAnother popular tokenization is subword-based tokenization which is a solution between word and character-based tokenization. The main idea is to solve the issues faced by word-based tokenization (very large vocabulary size large number of OOV tokens and different meaning of very similar words) and character-based tokenization (very long sequences and less meaningful individual tokens).The subword-based tokenization algorithms using the following principles.Do not split the frequently used words into smaller subwords.Split the rare words into smaller meaningful subwords.For example “boy” should not be split but “boys” should be split into “boy” and “s”. This will help the model learn that the word “boys” is formed using the word “boy” with slightly different meanings but the same root word.At the start of this article we split the word “tokenization” into “token” and “ization” where “token” is the root word and “ization” is the second subword labeled as additional information for the root word. The subword splitting will help the model learn that the words with the same root word as “token” like “tokens” and “tokenizing” are similar in meaning. It will also help the model learn that “tokenization” and “modernization” are made up of different root words but have the same suffix “ization” and are used in the same syntactic situations. Another example can be the word “surprisingly”. Subword-based tokenization will split it into “surprising” and “ly” as these stand-alone subwords would appear more frequently.The subword-based tokenization algorithms generally use a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token. For example “tokenization” can be split into “token” and “##ization” which indicates that “token” is the start of the word and “##ization” is the completion of the word.Different NLP models use different special symbols to denote the subwords. “##” is used by the BERT model for the second subword. Kindly note that special symbols can be added to the start of the word as well.Most models which have obtained state-of-the-art results in the English language use some kind of subword-tokenization algorithms. A few common subword-based tokenization algorithms are WordPiece used by BERT and DistilBERT Unigram by XLNet and ALBERT and Bye-Pair Encoding by GPT-2 and RoBERTa. 😊Subword-based tokenization allows the model to have a decent vocabulary size and also be able to learn meaningful context-independent representations. It is even possible for a model to process a word which it has never seen before as the decomposition can lead to known subwords. 😇 🙌🏻Thus we saw how tokenization methods evolved from time to time to accommodate the ever-growing needs of the NLP domain and to come up with better solutions to the problems.References:https://huggingface.co/docs/tokenizers/python/latest/The links to the related research papers are provided in the article.Thank you everyone for reading this article. Do share your valuable feedback or suggestion. Happy reading! 📗 🖌Word Subword and Character-Based Tokenization: Know the Difference was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,1,Looking for the perfect words? Generate them.,https://towardsdatascience.com/looking-for-the-perfect-words-generate-them-b28b818c2f24?source=rss----7f60cf5620c9---4,Using an LSTM model to generate poems.Continue reading on Towards Data Science »
2021,7,1,Analysis of New York City Motor Vehicles Collisions,https://towardsdatascience.com/analysis-of-new-york-city-motor-vehicles-collisions-927da110dfc7?source=rss----7f60cf5620c9---4,A data analyst interview case study using Google BigQuery and TableauIn my last article I spoke about my transition into Data Analytics and how I recently landed a full-time Data Analyst position. Throughout the month of April ’21 I was breezing in and out of interviews with various North American companies. For some of these companies I had to partake in Excel SQL or Python tests while a few others had me work on case studies. In this article I will walk you through one of such case studies which I passed and my approach in tackling the problem.Photo by Luke Stackpoole on UnsplashThe TaskFirst case studies are a way for companies to test core skills before considering you for advanced interview stages. For this case study I was tasked with analysing the New York City Motor Vehicles Collision dataset in Google BigQuery from Jan 2014 to Dec 2017 and provide recommendations to reduce occurrence of accidents in Brooklyn a borough in New York. The entire dataset currently has over 1.7 million records from 2012 to date and can be accessed here.Side note: Google BigQuery has several public datasets that are updated periodically and can be used to build projects for your portfolio.My ApproachMy first instinct was to use google to solve the problem because “there’s nothing new under the sun”. I found previous articles which I found useful in developing my approach. A summary of my approach is shown in the image below.First StepsHere are a few tricks and steps you should use to approach future case studies.Understanding the task: This is relevant for any case study to ensure that your analysis does not go off-point. It is important to follow the instructions first before going the extra mile. In this case study I almost missed where I was asked to analyse only 2014–2017 data in the brief.Prepping the Data: Identifying the primary key and checking for duplicates and null values should be a no-brainer when exploring your dataset. Also look out for fields that might be relevant to your analysis so you do not end up importing irrelevant fields into your Business Intelligence tool. This is where SQL came in handy.Checking for Duplicates in Google BigQuery (P.S. Query returned no results is a good thing in this case.)Deep DiveTo analyse the dataset I made use of Tableau Public for two reasons: I wanted to create an interactive dashboard and Tableau was one of the skill sets mentioned in the job description. From exploring the dataset I got ideas of key features to do an in-depth analysis on. Some are highlighted below while others can be explored in the final dashboard.Collision Analysis: This was done to reveal top causes that led to collisions and fatalities. We can see here that most fatalities were caused by Driver Inattention/Distraction.Time Series Analysis: Reveal what time of day or day of week have most collisions. We can see from the chart below that most collisions occurred during rush hour (4PM–5PM). We also see significant numbers at early hours of the day.Fatality Analysis: This revealed that pedestrians were killed more often than other road users whenever collisions occurred.Bringing it all togetherUsing the insights gathered from my analysis I prepared a slide deck to provide recommendations. An additional tip is to ensure any recommendation you provide is backed up by your analysis — not prior knowledge. Also most companies would give a few hours to 5 business days to complete a case study. If you see you have more time please try not to rush through it.Recommendations provided based on my analysis (Slide created using Canva)The final submission for this case study was a slide deck and dashboard. The latter was an add-on because this was a major tech company and they loved it :). A preview of the interactive dashboard is shown below. I designed the background in Figma and the rest of the magic happened in Tableau.Relevant LinksTableau DashboardFinal Slide deckLinkedIn ProfileAnalysis of New York City Motor Vehicles Collisions was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,1,Access Google Drive Using Google Colab Running an R Kernel,https://towardsdatascience.com/access-google-drive-using-google-colab-running-an-r-kernel-3736db7835?source=rss----7f60cf5620c9---4,Import data from Google Drive files or directly from web URLsContinue reading on Towards Data Science »
2021,7,1,Analysis of the Indian education system,https://towardsdatascience.com/analysis-of-the-indian-education-system-388f7ad8c68f?source=rss----7f60cf5620c9---4,Comparison among different states on several factors in the education spaceSourceBackgroundIndia is a melting pot of cultures traditions and values. The subcontinent of 1+ billion people houses its citizens in 36 States and Union Territories. The quality of education is affected not only by the presence of different educational boards but also by different prevalent socio-economic factors.I grew up in a beautiful Christian brothers’ school in a quiet town. After completing school I stayed in a couple of different places in India which exposed me to diverse backgrounds and diverse schooling practices.Just like any problem can be solved most effectively by working at the grass-root level I believe that a nation can develop sustainably and steadily by improving the schools in which the leaders of tomorrow are studying.IntroductionIn this project I have compared the following factors among states to understand prevalent gaps in the Indian education system:Dropout rates between girls and boysDropout rates in different sections of the schoolEnrollment rates between girls and boysEnrollment rates in different sections of the schoolStudent to teacher ratioPresence of different factors like electricity toilet for boys and girls etc.Primary school: I-VUpper primary: VI-VIIISecondary: IX-XHigher secondary: XI-XIIAnalysisDropout and enrollment ratesImage by authorObservations:Enrollment rates reduce as we go to higher sections of the school and it drops drastically at higher secondary to 60% from above 100% in the primary school.Lakshadweep and Jammu &amp; Kashmir two union territories have exceptionally low enrollment compared to other states in earlier schooling sections.Maximum dropout rates are in secondary school and not higher secondary because the enrollment is already low.About 15% of 85% of students who enroll in secondary school drop out. This results in only 72% of students being retained in class X. That becomes their highest educational qualification.However it is wrong to look at it as a point estimate. Instead we should look at it as a range estimate: it mostly ranges between 60% and 83%. It varies a lot depending on the state.https://medium.com/media/72ce85ea1e10180143ca318d8d54ba6a/hrefWe also need to check if sex is a factor in enrollments and dropouts.Image by authorObservations:Boys have lower enrollments and higher dropoutsIn rural India parents send their boys to work as soon as they are of the age that they can earn and bring some money home. This may be the reason for slightly higher dropouts among boys. Also maybe parents are reluctant to educate girls and instead marry them off. It is important to note that the upper range of dropout rates for girls is more than for boys. So even though boys have a higher dropout rate girls’ dropout rates are affected by outliers.Student to teacher ratio in different sections of the schoolhttps://medium.com/media/d460597536742ab184df183e202d0eeb/hrefObservations:The highest student: teacher ratio is in higher secondary school.There is also a lack of teachers in secondary school. This dearth of qualified teachers may be due to schools being unable to attract qualified candidates with a meager salary.States like UP Bihar Jharkhand and WB are doing poorly in this aspect.Students receive less attention in secondary school. The situation only worsens in higher secondary school. These are important factors leading to higher dropouts in secondary and higher secondary school.Overall picturehttps://medium.com/media/a5f6ee06be7b58b1880fabff782e37eb/hrefDifferent factors in schoolinghttps://medium.com/media/1fcfedefdaddff4fd6dfbadb8e94d561/hrefObservations:More than 50% of states have at least 92% of schools with drinking waterMuch more schools have drinking water compared to electricityA very low percentage of schools have computer facilityA higher percentage of schools have girls toilets as compared to boys toiletsComparison of different factors among sections of the school:The mean % of features present in different sections of school:Observations:There is a sharp reduction in the no. of toilets from upper primary to secondary school but not so much in the availability of drinking water. Maybe the schools do not have the infrastructure to set up hygienic toilets. Unavailability of water in the toilets may be a reason.Also surprisingly secondary schools have the lowest availability of electricity. Also note that the percentage of computers drops between upper primary and higher secondary school.Most schools in rural areas have the facilities to teach only till secondary school(class X). In these upper classes there is a dearth of good facilities at school. The situation is aggravated by students dropping out to pursue work. As a result there isn’t enough demand to improve the conditions in the secondary section of these schools.Hypothesis testingWe have done an exploratory analysis of the data. Through hypothesis testing we can obtain conclusive results.DropoutSexWe do a t-test to obtain:t p = (-0.9810354738258853 0.3299538292807268)Therefore we fail to reject the Ho.The dropout rates between girls and boys are not different.Sections of schoolWe do an ANOVA test to obtain:F-Statistic=45.282 p=0.000Therefore we reject the Ho.There is a significant difference in dropout rates among different sections of the school.EnrollmentSexWe do a t-test to obtain:t p = (0.8775733057976992 0.38317741633598124)Therefore we fail to reject the Ho.The enrollment rates between girls and boys are not different.Sections of schoolWe do an ANOVA test to obtain:F-Statistic=58.335 p=0.000Therefore we reject the Ho.There is a significant difference in enrollment rates among different sections of the school.Student to teacher ratioWe do an ANOVA test to obtain:F-Statistic=12.850 p=0.000Therefore we reject the Ho.There is a significant difference in student: teacher ratio among different sections of the school.Other factorsWe do χ2 test to assess independence between the section of the school and the presence of different factors.The results of the χ2 test:p = 0.0054 at 12 degrees of freedom.Therefore we reject the Ho.The presence of different factors is dependent on the section of the school.Summary of hypothesis testsDropout and enrollment rates do not differ between boys and girls.Dropout and enrollment rates differ in different sections of the school.Student: teacher ratio is different in different sections of the school.The presence of different factors is dependent on the sections of the school.Visualizing performance of statesWe can visualize data up to 3D. I did PCA to find out the components which explain the maximum variance. This way we can shrink the dimensions while incorporating data from higher dimensions.Let us find these broader features. The factor loadings for the three principal components:Observations:PC1 explains the presence of different factors in schoolPC2 explains the student: teacher ratio in different sections of the schoolPC3 explains enrollments(and dropouts indirectly) in different sections of the schoolThese three factors were used to create non-hierarchical clusters. Maximum silhouette score was obtained with 7 clusters.Finding the knee pointVisualizing the clusters:https://medium.com/media/d0ed52f2f9c8f46aa4e78d18e74433df/hrefConclusionThe major factors accounting for the difference among different states’ education sector performance:Dropouts in secondaryEnrollments in higher secondaryPresence of electricity and computers in the schoolsFurther researchThe next big startup is apprehended to be an education company. I believe that instead of marketing a new product they are going to improve upon the existing infrastructure. According to the Pareto principle 80% of benefits will come from focusing on 20% of issues. The gaps in secondary school need to be covered. We need to direct efforts towards reducing dropouts in secondary school.Some actions in secondary school might include:Supporting the schools by providing qualified teachers or supporting the current teachers with quality teaching materialImproving the school facilities like ensuring availability of electricityThese tasks are no small feat and will require a lot of work at the grassroots level in villages and small rural towns.DataSchool Education in IndiaMHRD data: https://www.education.gov.in/sites/upload_files/mhrd/files/statistics-new/ESAG-2018.pdfAnalysis of the Indian education system was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,1,How to Segment a Pancreas CT,https://towardsdatascience.com/how-to-segment-ct-pancreas-3a390acb3c70?source=rss----7f60cf5620c9---4,A guide for finding and tracing pancreas on contrast-enhanced abdominal CTSpecial thanks to my good friend Dr. Megan Engels for helping me with this post.Introduction — what is segmentation?CT scans contain a wealth of information that can help us understand a patient’s health. As data scientists our role is to extract the information so it can be measured or quantified.The first step to analyzing CT or MRI scans is usually segmentation. By this I mean tracing — segmenting — important structures from background. From segmentations we extract important features like organ volume surface area brightness and texture patterns that tell us about various aspects of the disease.Segmentation can be a time-consuming process and is one of the first tasks we are trying to automate using deep-learning. For example our lab has developed a U-Net model to segment thirty-four unique organs from abdominal CT (paper).In this tutorial I’ll demonstrate how to segment the pancreas from abdominal CT. I’ll focus on using ITK-Snap which is a simple free tool that you can use to get started. I’m including some basics of anatomy but if you intend to embark on your own research project I highly recommend that you connect with clinical expert such as a radiologist to understand what the specific needs are for your task.TCIA DatasetFor this demo I’m using The Cancer Imaging Archive (TCIA) dataset which is a publicly available dataset of medical images provided by the NIH. The full dataset contains 85 scans taken from healthy patients. The dataset also provides segmentations although I’ll be showing you how to trace from scratch.ITK-SnapThere are many tools available that can view and annotate DICOM images (for a background on the DICOM file format see my other article). A quick way to get started is with ITK-Snap which is easy to download and use. It’s perfect for smaller datasets.A quick plug — for more complicated projects (for example projects that involve multiple tracers or repeated scans) our group at Mayo Clinic developed our own open-source software tool RIL-Contour. RIL-Contour has advanced features to make segmentation faster and more accurate and even data versioning which can help with quality control when you have multiple individuals working on the same project.Viewing ImagesITK-Snap supports images in both the NIFTI and DICOM formats. Figure 1 shows what the ITK-Snap interface looks like after you import a file.Figure 1. Importing a file into ITK-SnapThis is a 3D scan which is shown in 3 views — axial (also called “transverse”) (upper left) sagittal (upper right) and coronal (lower right). You’ll notice each view is labeled anterior (A) posterior (P) superior (S) inferior (I) right (R) and left (L).Right and left may appear backward to the reader —it’s given from the perspective of the patient who would be facing us.You can segment in any plane but it’s common to do most of the segmentation in the axial plane. This is because slice thickness in CT is usually larger in the axial plane (3–5mm vs. 0.5–0.8mm) so you end up having to trace fewer slices.WindowingBefore we start we need to adjust the contrast of the scan. This is called “windowing”. CT contrast is measured in Hounsfield Units which are normalized to air (HU = -1000) and water (HU = 0). To adjust the windowing in ITK-Snap go to Tools &gt; Image Contrast &gt; Contrast Adjustment. There’s a slider bar but it’s more accurate just to type in the numbers. For abdominal soft tissue the level is 50 and the window is 400 which means that the image pixels are clipped between -350 and +450.Figure 2. CT Abdomen before (left) and after windowing (right). Settings are Level=50 Window=400.These CTs are contrast-enhanced which means that iodine contrast agent was injected into the bloodstream. This will make the blood vessels appear brighter than the surrounding tissue. Additionally the patient has digestive contrast in the form of a barium swallow which makes the contents of the stomach and small intestine also appear bright.Abdominal AnatomyBefore we find the pancreas it’s helpful to understand a bit of abdominal anatomy. I’ve labeled a few landmarks in the scans below. If you consult an anatomy textbook you’ll see that the pancreas is tucked under the liver and behind the stomach usually adjacent to the spine.Figure 3. Anatomy of the abdomen in the coronal (left) and axial plane (right)Pancreas AnatomyJust like other organs there’s considerable variation in the location size and shape of the pancreas which can make it difficult to segment. The pancreas can be especially tricky to segment for several reasons —It’s less attached than other solid organs and tends to shift aroundIt directly interfaces other surrounding organs (especially the bowel wall)It has a heterogenous appearance even in healthy patients.Below I’ve copied a cartoon from Wikipedia. The pancreas organ is differentiated into three regions the head the body and the tail. The pancreas has a soft lobed appearance and is organized around a central pancreatic duct which carries the digestive juices excreted by the pancreas into the small intestine.Figure 4. Diagram of the pancreas from WikipediaOn CT this lobed appearance manifests as a soft lumpy texture. The pancreatic duct may or may not be visible (it tends to be less visible in younger patients). Be aware that the pancreas may contain cysts — some estimations put this number as high as 50%. Obesity also impacts the pancreas’ appearance causing fatty deposits that may make the pancreas appear darker. Finally pancreatic atrophy is also common in the elderly; it can be a sign of disease or just a normal variation in older adults.Figure 5. Closeup of pancreas in the axial plane. Key landmarks include the soft lobed texture and the dark pancreatic duct.Tracing the pancreas step by stepI usually think it’s easiest to find the pancreas body because of the distinctive texture and location. I’ll trace this structure first and then follow it up and down.The pancreas tends to be wrapped around several blood vessels including the aorta vena cava and renal arteries/veins. These vessels typically show up as bright circular structures on contrast-enhanced CT (non-contrast CT these distinctions are barely visible — and much harder to segment). Be sure to avoid accidentally including vessels in your segmentation which can introduce bias into your measurements.Finally it can be hard to separate the head and tail of the pancreas when they are touching nearby organs. One trick I like to use is to segment 4–5 obvious slices on the coronal plane and then use this as a guide for axial segmentation. This makes it easy to find the “corners” of the pancreas organ.After you’ve finished be sure to double-check your work. The most common mistake I tend to make is accidentally skipping over a slice or two.Figure 6. Finished pancreas segmentation in the axial (left) sagittal (middle) and coronal (right) planes. This segmentation took me about 20 minutes using ITK-SnapConclusionLike other things segmentation is a learning process (and a bit of an art form). In our paper we found that when multiple tracers outlined the same pancreas their agreement was only about 80% so it’s expected that your work might not be perfect.One thing that I like to tell students is that when you’re uncertain just take your best guess. You’re probably more accurate than you realize!How to Segment a Pancreas CT was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,7,1,An Ultimate Guide to Opencv-Learning Libraries 1.0,https://www.analyticsvidhya.com/blog/2021/07/an-ultimate-guide-to-opencv-learning-libraries-1-0/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction &#8220;Vision is an Art of Seeing what is invisible to ... 
The post An Ultimate Guide to Opencv-Learning Libraries 1.0 appeared first on Analytics Vidhya."
2021,7,1,Perform Logistic Regression with Pytorch Seamlessly,https://www.analyticsvidhya.com/blog/2021/07/perform-logistic-regression-with-pytorch-seamlessly/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Regression has numerous applications in real life. Linear regression is used ... 
The post Perform Logistic Regression with Pytorch Seamlessly appeared first on Analytics Vidhya."
2021,7,1,Build a simple Chatbot using NLTK Library in Python,https://www.analyticsvidhya.com/blog/2021/07/build-a-simple-chatbot-using-python-and-nltk/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon How amazing it is to talk to someone by asking and ... 
The post Build a simple Chatbot using NLTK Library in Python appeared first on Analytics Vidhya."
2021,7,1,How to Use NVIDIA GPU Accelerated Libraries,https://www.kdnuggets.com/2021/07/nvidia-gpu-accelerated-libraries.html,If you are wondering how you can take advantage of NVIDIA GPU accelerated libraries for your AI projects this guide will help answer questions and get you started on the right path.
2021,7,1,Learning Data Science Through Social Media,https://www.kdnuggets.com/2021/07/learning-data-science-through-social-media.html,Want your social media algorithms to show you actual algorithms? Spare a moment during your social media scrolling to learn a bit of data science. Here are suggestions for at-a-glance access to good ideas and tips on your favorite platforms.
2021,7,1,5 Lessons McKinsey Taught Me That Will Make You a Better Data Scientist,https://www.kdnuggets.com/2021/07/5-lessons-mckinsey-taught-better-data-scientist.html,How to stand out from your peers in the data world.
2021,7,1,Feature Importance in Random Forest,https://www.r-bloggers.com/2021/07/feature-importance-in-random-forest/," The Turkish president thinks that high interest rates cause inflation contrary to the traditional economic approach. For this reason he dismissed two central bank chiefs within a year. And yes unfortunately the central bank officials have limited independence doing their job in Turkey contrary to the rest of the world. ...


The post Feature Importance in Random Forest first appeared on R-bloggers."
2021,7,1,Spatially weighted averages in R with sf,https://www.r-bloggers.com/2021/07/spatially-weighted-averages-in-r-with-sf/," Spatial joins allow to augment one spatial dataset with information from another spatial dataset by linking overlapping features. In this … Read More →


The post Spatially weighted averages in R with sf first appeared on R-bloggers."
2021,7,1,Think of `&&` as a stricter `&`,https://www.r-bloggers.com/2021/06/think-of-as-a-stricter/,"In programming languages we find logical operators for and
and or. In fact Python uses the actual words and and or
for these operators.
# Python via the reticulate package
x = True
y = False
x and y
#__ False
x or y
#__ True
In Javascript we ...
The post Think of `&&` as a stricter `&` first appeared on R-bloggers."
2021,7,1,How to find z score in R-Easy Calculation-Quick Guide,https://www.r-bloggers.com/2021/06/how-to-find-z-score-in-r-easy-calculation-quick-guide/," z score how to find? z-score provides how many standard deviations away a value is from the mean. We can use the following formula...
The post How to find z score in R-Easy Calculation-Quick Guide appeared first on finnstats.


The post How to find z score in R-Easy Calculation-Quick Guide first appeared on R-bloggers."
2021,7,1,Depth Quantile Functions,https://www.r-bloggers.com/2021/06/depth-quantile-functions/,"
Figure 1: Depth quantile functions for the wine data (d=13) class 2 vs class 3. Blue curves correspond to between class comparisons red/pink correspond to within class comparisons.
A common technique in modern statistics is the so-called kernel ...


The post Depth Quantile Functions first appeared on R-bloggers."
2021,7,1,Building a Named Entity Recognition model using a BiLSTM-CRF network,https://blog.dominodatalab.com/named-entity-recognition-ner-challenges-and-model/,"What is the Named Entity Recognition problem and how can a BiLSTM-CRF model be fitted? Learn how by using a freely available annotated corpus and Keras. The model achieves relatively high accuracy and all data and code is freely available in the article.
The post Building a Named Entity Recognition model using a BiLSTM-CRF network appeared first on Data Science Blog by Domino."
2021,7,1,Member Training: A (Gentle) Introduction to k-Nearest Neighbor,https://www.theanalysisfactor.com/k-nearest-neighbor/,"Missing data is a common problem in data analysis. One of the successful approaches is k-Nearest Neighbor (kNN) a simple approach that leverages known information to impute unknown values with a relatively high degree of accuracy. In this gentle introduction to kNN we discuss the key concepts and applications including: Measuring distance Selecting the optimal [&#8230;]
The post Member Training: A (Gentle) Introduction to k-Nearest Neighbor appeared first on The Analysis Factor."
2021,7,1,Neural Models for Tabular Data,http://practicalquant.blogspot.com/2021/07/neural-models-for-tabular-data.html,The Data Exchange Podcast: Sercan Arik on a canonical neural network architecture for tabular data.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Take the 2021 NLP Industry Survey and get a free pass to the 2021 NLP Summit.Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,30,3 Mistakes That Transformed My Machine Learning Career,https://towardsdatascience.com/3-mistakes-that-transformed-my-machine-learning-career-de76679e0084?source=rss----7f60cf5620c9---4,Part of Embracing Mistakes Involves Sharing ThemContinue reading on Towards Data Science »
2021,6,30,How to Analyze Continuous Data from Two Groups,https://towardsdatascience.com/how-to-analyze-continuous-data-from-two-groups-8d101510790f?source=rss----7f60cf5620c9---4,Statistical Hypothesis Testing + Visuals with SciPy and SeabornContinue reading on Towards Data Science »
2021,6,30,Dashboard using Streamlit with data from SQL database,https://towardsdatascience.com/dashboard-using-streamlit-with-data-from-sql-database-f5c1ee36b51?source=rss----7f60cf5620c9---4,An interactive dashboard from customer and sales dataContinue reading on Towards Data Science »
2021,6,30,"1+1=3. Wait, no, 1+1=2. How to have GPT sanity check itself.",https://towardsdatascience.com/1-1-3-wait-no-1-1-2-how-to-have-gpt-sanity-check-itself-136e846987bf?source=rss----7f60cf5620c9---4,A Python walkthrough of using GPT to double-check its answer using GPT-J in ColabLarge language models have a problem where they tend to just make stuff up. This can be because of the training data the prompt or even just ambiguity. This can be mitigated by engineering the prompt to have GPT sanity check its output and it works with both GPT3 and GPT-J (the latter of which you can use for free).What do I mean by a sanity check? Well it turns out by setting up your prompts to double check the output using something like:Initial Answer: 6394 + 250 = 6643Double Checking: 6643 looks wrong. 6394 + 250 = 6644Initial Answer: {a} + {b} =you can improve performance compared to using a traditional prompt that looks more like:6394 + 250 = 6644{a} + {b}=Also for those like me who don’t read good you can skip this post and just run the code using GPT-J yourself using the linked colab.For background there are a few other methods that people use to mitigate this sort of problem. The most popular is just manually curating for answers you like. That’s boring and doesn’t really help if you’re trying to automate processes. There’s another method which involves having a model show its work through intermediary steps which can improve reasoning which is cool. However I figure it’s worth taking a look at self correcting to see how well GPT can look at what it spat out and decide whether it needs improving because it doesn’t seem like people have looked into it much (although there were some cool tweets about anecdotally guess-and-checking random algebraic solutions last year).For a toy example I went with arithmetic. Why arithmetic? Well first of all we can measure if an answer’s correct which is often good to know. Second while there’s an argument that the reason language models don’t do great at arithmetic is due to the way GPT’s tokens are encoded I suspect that there’s a search issue with multi-digit arithmetic where you have to add from right to left to carry digits and when you’re just predicting ahead it’s hard to guess where the digits are gonna get carried. GPT isn’t great at that type of search and if search is contributing to this issue then doing a double check should help it.Anyway in this post I’ll show you how to generate contexts with between 1–30 examples run their completions using a standard few shot and an improved self-checking few shot and then we’ll plot the errors (figure 1) and see that indeed the self-checking prompts have a lower error and a higher exact match than the traditional few shot. Here’s what the graph’ll look like (Note for exact match we’re getting 16% top accuracy whereas GPT3 large with commas you can get over 90% so there can be quite a difference).Figure 1: A self-correcting prompt has lower mean absolute percent error (left) and higher exact match (right) than regular few shots on 4 digit arithmetic using GPT-J.Setting up the query functionThis works on the GPT-J as well the GPT-3 models you’ll just have to set up your query function differently which is where we’ll start. When we query a completion for this sort of task we want to use 0 temperature because we want to get the most likely tokens instead of randomly throwing in other numbers. Here’s what my query function in J looks like (borrowed from the infer function in the GPT-J demo notebook which is pretty cool):def query(context top_p=0 temp=0 gen_len=50):  tokens = tokenizer.encode(context)  provided_ctx = len(tokens)  pad_amount = seq - provided_ctx  padded_tokens = np.pad(tokens ((pad_amount 0))).astype(np.uint32)  batched_tokens = np.array([padded_tokens] * total_batch)  length = np.ones(total_batch dtype=np.uint32) * len(tokens)  output = network.generate(batched_tokens length gen_len {&quot;top_p&quot;: np.ones(total_batch) * top_p &quot;temp&quot;: np.ones(total_batch) * temp})  samples = []  decoded_tokens = output[1][0]  for o in decoded_tokens[: : 0]:    samples.append(tokenizer.decode(o))  return samples[0]Setting up the PromptNow we’ll go ahead and set up our prompt. We’ll create 30 random examples to be our prompt and 100 examples as the test set.import pandas as pd json randomrandom.seed(42)fourDigitDict = []fourDigitTest = []for i in range(30):  a = int(random.random()*10**4)  b = int(random.random()*10**4)  fourDigitDict.append({&#39;first&#39;: a &quot;second&quot;: b &#39;sum&#39;: a+b})fourDigitTrainDF = pd.DataFrame(fourDigitDict)for i in range(100):  a = int(random.random()*10**4)  b = int(random.random()*10**4)  fourDigitTest.append({&#39;first&#39;: a &quot;second&quot;: b &#39;sum&#39;: a+b})fourDigitTestDF = pd.DataFrame(fourDigitTest)For the pure/traditional case we’d just have the examples all lined up as the prompt but instead what we’ll do here for the self-correcting is randomly set up some of these to be wrong. We’ll call the wrong answers (or right ones) originalSum so that we can tell the model to correct it to the right answer later.fourDigitTrainDF[&#39;randomlyWrong&#39;] = fourDigitTrainDF[&#39;sum&#39;].apply(lambda x: random.random() &lt; .5)fourDigitTrainDF[&#39;offset&#39;] = fourDigitTrainDF[&#39;randomlyWrong&#39;].apply(lambda x: .5-random.random() if x==True else 0)fourDigitTrainDF[&#39;offset&#39;] = fourDigitTrainDF[&#39;offset&#39;] * 2000fourDigitTrainDF[&#39;originalSum&#39;] = fourDigitTrainDF[&#39;sum&#39;] + fourDigitTrainDF[&#39;offset&#39;]Next we just create our prompts. We’ll have one set of prompts for the pure few shots and one set of prompts for the corrected ones. And to see how many examples we need we’ll just do a grid search and literally try running from 1–30 examples.correctionPrompts = {}purePrompts = {}for i in range(130):  correctionPrompt = &quot;&quot;  purePrompt = &quot;&quot;  for row in fourDigitTrainDF[:i].iterrows():    correctionPrompt += &#39;Initial Answer: {} + {} = {}&#39;.format(row[1][&#39;first&#39;] row[1][&#39;second&#39;] int(row[1][&#39;originalSum&#39;]))    correctionPrompt += &#39;\n&#39;    interjection = &#39;looks correct.&#39; if not row[1][&#39;randomlyWrong&#39;] else &#39;looks off by a bit.&#39;    correctionPrompt += &#39;Double Checking: {} {} {} + {} = {}&#39;.format(int(row[1][&#39;originalSum&#39;]) interjection row[1][&#39;first&#39;] row[1][&#39;second&#39;] row[1][&#39;sum&#39;])    correctionPrompt += &#39;\n\n&#39;    purePrompt += &#39;{} + {} = {}&#39;.format(row[1][&#39;first&#39;] row[1][&#39;second&#39;] row[1][&#39;sum&#39;]) + &#39;\n&#39;    correctionPrompt += &#39;Initial Answer: &#39;  correctionPrompts[i] = correctionPrompt  purePrompts[i] = purePromptNow we’ve got all of our prompts set up time to try it against the test set!Running the TestTo run the test we’ll begin by going and setting up the actual query (a + b =) and then adding it to the end of each of our prompts (either with self correction or not). Then we’ll just run the whole thing and sit back as it does the 6000 queries. We’ll also dump it to JSON each time we go through the test set in case things break so we can recover.import jsonfourDigitTestDF[&#39;formatted&#39;] = fourDigitTestDF.apply(lambda x: &quot;{} + {} =&quot;.format(x[&#39;first&#39;] x[&#39;second&#39;]) axis=1)correctionResults = {}pureResults = {}#for each size of example length in 1-30 run on the test setfor trainSize in range(130):  if trainSize not in correctionResults:  print(trainSize)  correctionResults[trainSize] = []  pureResults[trainSize] = []  for example in fourDigitTestDF.formatted:    correctionResults[trainSize].append( query(correctionPrompts[trainSize]+example gen_len=50))  pureResults[trainSize].append(query(purePrompts[trainSize]+example gen_len=50))  with open(&#39;correctionResults.json&#39; &#39;w&#39;) as fh:    json.dump(correctionResults fh)  with open(&#39;pureResults.json&#39; &#39;w&#39;) as fh:    json.dump(pureResults fh)EvaluationK so now that we got our 6000 files we’ll evaluate it. I’ll start by renaming the test set to test because… why not. Since I haven’t figured out how to do a stop sequence with GPT-J (My degree’s in journalism math’s hard) we’ll have to parse out the answer we’re trying to get. For the traditional method that’s just the first ‘word’ out the door. For the self-correcting method it’s the last ‘word’ of the first set of outputs.def parsePureInt(x):  base = x.split(&#39;\n&#39;)[0]  try:    return int(base)  except:    return 0def parseCorrectedInt(x):  base = x.split(&#39;\n\n&#39;)[0].split(&#39; &#39;)[-1]  try:    return int(base)  except:    return 0Now we’ll apply this to all of our results and calculate the errors.for key in pureResults.keys():  test[key] = pureResults[key]  test[key] = test[key].apply(lambda x: parsePureInt(x))pureMape = pd.DataFrame()  for col in test.columns[3:]:  pureMape[col] = (abs(test[col] - test[&#39;sum&#39;]))/test[&#39;sum&#39;]pureEM = pd.DataFrame()for col in test.columns[3:]:  pureEM[col] = test[col] == test[&#39;sum&#39;]for key in correctionResults.keys():  test[key] = correctionResults[key]  test[key] = test[key].apply(lambda x: parseCorrectedInt(x))correctedMape = pd.DataFrame()for col in test.columns[3:]:  correctedMape[col] = (abs(test[col] - test[&#39;sum&#39;]))/test[&#39;sum&#39;]correctedeEM = pd.DataFrame()  for col in test.columns[3:]:  correctedeEM[col] = test[col] == test[&#39;sum&#39;]And now we just plot the resultsfig axs = plt.subplots(ncols=2 nrows=1 figsize=(128) facecolor=&#39;white&#39;)correctedMape.mean().plot(label=&#39;self correcting&#39; ax=axs[0])pureMape.mean().plot(ax=axs[0] label=&#39;pure few shot&#39;)axs[0].legend()axs[0].set_xlabel(&#39;# of examples&#39;)axs[0].set_title(&#39;Mean Error&#39;)correctedeEM.sum().plot(label=&#39;self correcting&#39; ax=axs[1])pureEM.sum().plot(ax=axs[1] label=&#39;pure few shot&#39;)axs[1].legend()axs[1].set_xlabel(&#39;# of examples&#39;)axs[1].set_title(&#39;Exact Match&#39;)fig.suptitle(&#39;4 Digit Arithmetic on 100 Tests&#39;)Figure 2. Wait no figure 1 again.In ClosingYou can run the above code with GPT3 as well as J and you’ll get the same sort of improvement. I leave it as an exercise to the reader to do it with commas as well (you can just use f”{x:}” to format things with commas). Pretty cool that it works though.Also if you do it in GPT3 with commas you get something more like this depending on the model size / fine tune. Not sure what happened there at 17 examples though.I think what’s going on here is that since it can’t predict ahead sometimes it starts making up random digits until it gets to the end and then that gives it the context on checking for what the first couple digits should be the next time through. I’m not entirely sure though. These models are weird.1+1=3. Wait no 1+1=2. How to have GPT sanity check itself. was originally published in Towards Data Science on Medium where people are continuing the conversation by highlighting and responding to this story.
2021,6,30,A Beginners Guide to Machine Learning Operations,https://www.analyticsvidhya.com/blog/2021/06/mlops-a-beginners-guide-to-machine-learning-operations/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction I bet all of you might have been witness to ... 
The post A Beginners Guide to Machine Learning Operations appeared first on Analytics Vidhya."
2021,6,30,How to Detect COVID-19 Cough From Mel Spectrogram Using Convolutional Neural Network,https://www.analyticsvidhya.com/blog/2021/06/how-to-detect-covid19-cough-from-mel-spectrogram-using-convolutional-neural-network/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon COVID-19 COVID-19 (coronavirus disease 2019) is a disease that causes respiratory ... 
The post How to Detect COVID-19 Cough From Mel Spectrogram Using Convolutional Neural Network appeared first on Analytics Vidhya."
2021,6,30,Beginner’s Guide To Natural Language Processing Using SpaCy,https://www.analyticsvidhya.com/blog/2021/06/beginners-guide-of-natural-language-processing-using-spacy/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Pre-requisites Basic Knowledge of Natural Language Processing Hands-on practice of Python ... 
The post Beginner&#8217;s Guide To Natural Language Processing Using SpaCy appeared first on Analytics Vidhya."
2021,6,30,Creating Android Machine Learning Apps Using KivyMD,https://www.analyticsvidhya.com/blog/2021/06/creating-android-ml-app-kivymd/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Android Apps have truly captured our lives. Almost every day ... 
The post Creating Android Machine Learning Apps Using KivyMD appeared first on Analytics Vidhya."
2021,6,30,Everything happening in Computer Vision that you should know,https://www.analyticsvidhya.com/blog/2021/06/everything-happening-in-computer-vision-that-you-should-know/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Since the onset of life human vision is essential beautiful ... 
The post Everything happening in Computer Vision that you should know appeared first on Analytics Vidhya."
2021,6,30,Deploying ML Models as API Using FastAPI and Heroku,https://www.analyticsvidhya.com/blog/2021/06/deploying-ml-models-as-api-using-fastapi-and-heroku/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Most of the machine learning projects are stuck in the ... 
The post Deploying ML Models as API Using FastAPI and Heroku appeared first on Analytics Vidhya."
2021,6,30,Download 15 years of Nifty Index Options Data using NSEpy Package,https://www.analyticsvidhya.com/blog/2021/06/download-15-years-of-nifty-index-options-data-using-nsepy-package/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon &#160; In my previous article on fat tails in the NSE ... 
The post Download 15 years of Nifty Index Options Data using NSEpy Package appeared first on Analytics Vidhya."
2021,6,30,Managing Your Reusable Python Code as a Data Scientist,https://www.kdnuggets.com/2021/06/managing-reusable-python-code-data-scientist.html,Here are a few approaches that I have settled on for managing my own reusable Python code as a data scientist presented from most to least general code use and aimed at beginners.
2021,6,30,"Ethics, Fairness, and Bias in AI",https://www.kdnuggets.com/2021/06/ethics-fairness-ai.html,As more AI-enhanced applications seep into our daily lives and expand their reach to larger swaths of populations around the world we must clearly understand the vulnerabilities trained machine leaning models can exhibit based on the data used during development. Such issues can negatively impact select groups of people so addressing the ethical decisions made by AI--possibly unknowingly--is important to the long-term fairness and success of this new technology.
2021,6,30,From Scratch: Permutation Feature Importance for ML Interpretability,https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html,Use permutation feature importance to discover which features in your dataset are useful for prediction — implemented from scratch in Python.
2021,6,30,"KDnuggets™ News 21:n24, Jun 30: What will the demand for Data Scientists be in 10 years?; Add A New Dimension To Your Photos Using Python",https://www.kdnuggets.com/2021/n24.html,What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?; Add A New Dimension To Your Photos Using Python; Data Scientists are from Mars and Software Developers are from Venus; How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3; In-Warehouse Machine Learning and the Modern Data Science Stack
2021,6,30,Code performance in R: Parallelization,https://www.r-bloggers.com/2021/06/code-performance-in-r-parallelization/,"This is the third part of our series about code performance in R. In the first part I introduced methods to measure which part of a given code is slow. The second part lists general techniques to make R code faster. In this part ...
The post Code performance in R: Parallelization first appeared on R-bloggers."
2021,6,30,Speeding Up R Shiny – The Definitive Guide,https://www.r-bloggers.com/2021/06/speeding-up-r-shiny-the-definitive-guide/,"
Better App Performance – It Can Be Done! Prototyping apps in Shiny is fast and easy but once an app grows performance issues may arise. Speeding up Shiny is possible and the methods described below can prevent or resolve these issues. There are a few good practices to have in mind ...


The post Speeding Up R Shiny – The Definitive Guide first appeared on R-bloggers."
2021,6,30,Intraclass Correlation Coefficient in R-Quick Guide,https://www.r-bloggers.com/2021/06/intraclass-correlation-coefficient-in-r-quick-guide/," Intraclass Correlation Coefficient in R ICC is used to determine if subjects can be rated reliably by different raters. In some kind of situation...
The post Intraclass Correlation Coefficient in R-Quick Guide appeared first on finnstats.


The post Intraclass Correlation Coefficient in R-Quick Guide first appeared on R-bloggers."
2021,6,30,Euro 2020: Will Switzerland kick out Spain too?,https://www.r-bloggers.com/2021/06/euro-2020-will-switzerland-kick-out-spain-too/,"One of the big sensations of the UEFA Euro 2020 is that Switzerland kicked out world champion France. We take this as an opportunity to share with you a simple statistical model to predict football (soccer) results with R so read on! Football is a highly stochastic game which is one ...
The post Euro 2020: Will Switzerland kick out Spain too? first appeared on R-bloggers."
2021,6,30,Create a custom metric with tidymodels and NYC Airbnb prices,https://www.r-bloggers.com/2021/06/create-a-custom-metric-with-tidymodels-and-nyc-airbnb-prices/," This is the latest in my series of
screencasts demonstrating how to use the
tidymodels packages from just getting started to tuning more complex models. This week’s episode of
SLICED a competitive data science prediction challenge introduced a ch...


The post Create a custom metric with tidymodels and NYC Airbnb prices first appeared on R-bloggers."
2021,6,30,"Tired: PCA + kmeans, Wired: UMAP + GMM",https://www.r-bloggers.com/2021/06/tired-pca-kmeans-wired-umap-gmm/,"
Introduction
Combining principal component analysis (PCA) and kmeans clustering seems to be a pretty popular 1-2 punch in data science. While there is some debate about whether combining dimensionality reduction and clustering is something we should...


The post Tired: PCA + kmeans Wired: UMAP + GMM first appeared on R-bloggers."
2021,6,30,Draw me a project,https://www.r-bloggers.com/2021/06/draw-me-a-project/,"I’ll be giving a remote keynote talk at the Rencontres R (French R conference) on July the 12th all in French. This blog post is a written version of my presentation but in English. I decided to not talk about package development for once but ...
The post Draw me a project first appeared on R-bloggers."
2021,6,30,Reasons to Use Tidymodels,https://www.r-bloggers.com/2021/06/reasons-to-use-tidymodels/,"
I was listening to episode 135 of ‘Not so standard deviations’ - Moderate confidence
The hosts Hilary and Roger
talked about when to use tidymodels packages and when not.
Here are my 2 cents for when I think it makes sense to use these packages and
...


The post Reasons to Use Tidymodels first appeared on R-bloggers."
2021,6,30,Tally Mobile Apps: Analytics with Anytime Access and Results!,https://www.smarten.com/blog/tally-mobile-apps-analytics-with-anytime-access-and-results/,The Tally ERP Solution is a popular accounting and financial application used by business professionals around the world to complete accounting and finance related tasks. Businesses that use Tally ERP have amassed a large volume of crucial data within the confines of the Tally app [&#8230;]
2021,6,30,"Self-Serve Advanced Analytics Offers a Collaborative, Empowered Environment to SMEs!",https://www.smarten.com/blog/self-serve-advanced-analytics-offers-a-collaborative-empowered-environment-to-smes/,The use of business intelligence and business analytics is growing in every industry business function and in companies of every size. 48% of small and medium sized business CIOs responding to a Gartner survey revealed that business intelligence (BI) data and analytics is one of [&#8230;]
2021,6,30,What to Expect When You Are Expecting the Delta Covid-19 Variant,https://www.r-bloggers.com/2021/06/what-to-expect-when-you-are-expecting-the-delta-covid-19-variant/,"SARS-Cov-2 (COVID-19) has been the defining driver of the market since its emergence. Understanding the progression of the disease through the world has been the secret sauce of alpha generation. Correctly parsing the early infectivity and morality data allowed some to avoid the COVID crash and a solid reading into ...
The post What to Expect When You Are Expecting the Delta Covid-19 Variant first appeared on R-bloggers."
2021,6,30,Concentration Inequalities in Machine Learning,https://www.r-bloggers.com/2021/06/concentration-inequalities-in-machine-learning/,"
The fifth “One World webinar” organized by YoungStatS will take place on September 15th 2021. Selected young European researchers active in the areas of probability and machine learning will present their recent contributions. The webinar is joint...


The post Concentration Inequalities in Machine Learning first appeared on R-bloggers."
2021,6,29,Automated Spam E-mail Detection Model(Using common NLP tasks),https://www.analyticsvidhya.com/blog/2021/06/automated-spam-e-mail-detection-modelusing-common-nlp-tasks/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Hope you all are doing Good !!! Welcome to my blog! ... 
The post Automated Spam E-mail Detection Model(Using common NLP tasks) appeared first on Analytics Vidhya."
2021,6,29,Predictive Modelling – Rain Prediction in Australia With Python,https://www.analyticsvidhya.com/blog/2021/06/predictive-modelling-rain-prediction-in-australia-with-python/,"ArticleVideo Book Introduction: In this article I will be implementing a predictive model on Rain Dataset to predict whether or not it will rain ... 
The post Predictive Modelling &#8211; Rain Prediction in Australia With Python appeared first on Analytics Vidhya."
2021,6,29,Time Series Analysis – A Comprehensive Guide,https://www.analyticsvidhya.com/blog/2021/06/time-series-analysis-a-comprehensive-guide/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Table of Content Let us have a quick overview of this ... 
The post Time Series Analysis &#8211; A Comprehensive Guide appeared first on Analytics Vidhya."
2021,6,29,Deploy Your ML/DL Streamlit Application on Heroku,https://www.analyticsvidhya.com/blog/2021/06/deploy-your-ml-dl-streamlit-application-on-heroku/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Did you developed a Machine Learning or Deep Learning application ... 
The post Deploy Your ML/DL Streamlit Application on Heroku appeared first on Analytics Vidhya."
2021,6,29,Part 3: Topic Modeling and Latent Dirichlet Allocation (LDA) using Gensim and Sklearn,https://www.analyticsvidhya.com/blog/2021/06/part-3-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Overview In the previous two installments we had understood in detail ... 
The post Part 3: Topic Modeling and Latent Dirichlet Allocation (LDA) using Gensim and Sklearn appeared first on Analytics Vidhya."
2021,6,29,Beginner’s Guide to Universal Approximation Theorem,https://www.analyticsvidhya.com/blog/2021/06/beginners-guide-to-universal-approximation-theorem/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction Neural networks are used for so many tasks in the ... 
The post Beginner&#8217;s Guide to Universal Approximation Theorem appeared first on Analytics Vidhya."
2021,6,29,Part 20: Step by Step Guide to Master NLP – Information Retrieval,https://www.analyticsvidhya.com/blog/2021/06/part-20-step-by-step-guide-to-master-nlp-information-retrieval/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction This article is part of an ongoing blog series on ... 
The post Part 20: Step by Step Guide to Master NLP &#8211; Information Retrieval appeared first on Analytics Vidhya."
2021,6,29,Part- 19: Step by Step Guide to Master NLP – Topic Modelling using LDA (Matrix Factorization Approach),https://www.analyticsvidhya.com/blog/2021/06/part-19-step-by-step-guide-to-master-nlp-topic-modelling-using-lda-matrix-factorization-approach/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Introduction This article is part of an ongoing blog series on ... 
The post Part- 19: Step by Step Guide to Master NLP &#8211; Topic Modelling using LDA (Matrix Factorization Approach) appeared first on Analytics Vidhya."
2021,6,29,Real-time data brings real-time business value,https://www.ibm.com/blogs/journey-to-ai/2021/06/real-time-data-brings-real-time-business-value/,"Exploitation of data is critical to business success and quicker data processing improves an organization’s ability to react to business events in real time. As a result organizations are bringing together new types of data from a variety of internal and external sources for real-time data or near-real-time analytics. This can involve building data lakes [&#8230;]
The post Real-time data brings real-time business value appeared first on Journey to AI Blog."
2021,6,29,Unleashing the Power of MLOps and DataOps in Data Science,https://www.kdnuggets.com/2021/06/power-mlops-dataops-data-science.html,Organizations trying to move forward with analytics and data science initiatives -- while floating in an ocean of data -- must enhance their overall approach and culture to embrace a foundation on DataOps and MLOps. Leveraging these operational frameworks are necessary to enable the data to generate real business value.
2021,6,29,10 Mistakes You Should Avoid as a Data Science Beginner,https://www.kdnuggets.com/2021/06/10-mistakes-avoid-data-science-beginner.html,Read this article on how to gain a competitive advantage in the data science job market.
2021,6,29,Transition plot in R-change in time visualization,https://www.r-bloggers.com/2021/06/transition-plot-in-r-change-in-time-visualization/," Transition Plot in R when we have quantitative data for change in time visualization is straight forward but in the case of a categorical...
The post Transition plot in R-change in time visualization appeared first on finnstats.


The post Transition plot in R-change in time visualization first appeared on R-bloggers."
2021,6,29,"Missing Migrants, tracking human deaths along migratory routes",https://www.r-bloggers.com/2021/06/missing-migrants-tracking-human-deaths-along-migratory-routes/," Hi there  	 	 	 	  Several years ago I received the book “Como si nunca hubieran sido” as a Christmas present. In this poem Javier Gallego and Juan Gallegotalk about the real drama suffered by immigrants trying to reach Europe through  the Medi...
The post Missing Migrants tracking human deaths along migratory routes first appeared on R-bloggers."
2021,6,29,Probabilistic ensemble forecasting of Australian COVID-19 cases,https://robjhyndman.com/seminars/covid-forecasting/,Talk for the International Symposium on Forecasting 29 June 2021 and the Australian and New Zealand Statistics Conference 8 July 2021. In March 2020 I joined a team responsible for providing probabilistic forecasts of COVID-19 cases to all Australian state &amp; territory Chief Health Officers. We use case-level data of all Australian positive COVID cases along with nationwide surveys and mobility data from Google Facebook and Apple. Three separate models have been built: (1) a stochastic susceptible-exposed-infectious-recovered (SEEIIR) compartmental model; (2) a stochastic epidemic model; and (3) a global autoregressive model based on public case data from 31 countries.
2021,6,29,StreamSets DataOps Platform – Summer ‘21 Public Beta. Sign up today!,https://www.kdnuggets.com/2021/06/streamsets-dataops-platform-summer-public-beta.html,Introducing StreamSets DataOps Platform - Summer ‘21 Public Beta! Bringing DataOps to the Cloud for Enterprises.
2021,6,29,Computational Complexity of Deep Learning: Solution Approaches,https://www.kdnuggets.com/2021/06/computational-complexity-deep-learning-solution-approaches.html,Why has deep learning been so successful? What is the fundamental reason that deep learning can learn from big data? Why cannot traditional ML learn from the large data sets that are now available for different tasks as efficiently as deep learning can?
2021,6,29,Stephan Kadauke from R/Medicine talks to R Consortium about Racial Bias,https://www.r-bloggers.com/2021/06/stephan-kadauke-from-r-medicine-talks-to-r-consortium-about-racial-bias/," Stephan Kadauke MD PhD is an Assistant Professor of Clinical Pathology and Lab Medicine. He runs the Cell and Gene Therapy Lab at the Children’s Hospital of Philadelphia (CHOP) and...
The post Stephan Kadauke from R/Medicine talks to R Consortium about Racial Bias appeared first on R Consortium.


The post Stephan Kadauke from R/Medicine talks to R Consortium about Racial Bias first appeared on R-bloggers."
2021,6,29,Get the Odds of Euro 2020 Games based on FIFA World Ranking,https://www.r-bloggers.com/2021/06/get-the-odds-of-euro-2020-games-based-on-fifa-world-ranking/," We will provide an example of how you can estimate the outcome of a Euro 2020 Game based on FIFA ... Read moreGet the Odds of Euro 2020 Games based on FIFA World Ranking


The post Get the Odds of Euro 2020 Games based on FIFA World Ranking first appeared on R-bloggers."
2021,6,29,Euro 2020 Predictive Model based on FIFA Ranking System,https://www.r-bloggers.com/2021/06/euro-2020-predictive-model-based-on-fifa-ranking-system/," In a previous post we built a Predictive Model based on FIFA Ranking and making the assumption that the points ... Read moreEuro 2020 Predictive Model based on FIFA Ranking System


The post Euro 2020 Predictive Model based on FIFA Ranking System first appeared on R-bloggers."
2021,6,29,Gentle Introduction to Forecasting with Modeltime [Video Tutorial],https://www.r-bloggers.com/2021/06/gentle-introduction-to-forecasting-with-modeltime-video-tutorial/,"
  A gentle introduction to our forecasting package modeltime. Modeltime extends the Tidymodels ecosystem for time series forecasting. Learn how to forecast with ARIMA Prophet and linear regression time series models in this short video tutorial.
...


The post Gentle Introduction to Forecasting with Modeltime [Video Tutorial] first appeared on R-bloggers."
2021,6,29,Cyclical learning rate with R and Keras,https://www.r-bloggers.com/2021/06/cyclical-learning-rate-with-r-and-keras/," Efficientnet with R and Tf2
In this blog post I will share a way to perform cyclical learning rate with R. I worked on top of some source code I found on a other blog by chance but I adjusted things to make it more similar to the fast.ai ...


The post Cyclical learning rate with R and Keras first appeared on R-bloggers."
2021,6,29,Microsoft announces Arc-enabled Azure SQL general availability,https://www.zdnet.com/article/microsoft-announces-arc-enabled-azure-sql-general-availability/#ftag=RSSbaffb68,The ability to run Azure SQL Managed Instance -- a cloud version of SQL Server -- on non-Azure clouds and even on-premises will be generally available and production-ready on July 30th 2021.
2021,6,29,2021 Data engineering Survey,http://practicalquant.blogspot.com/2021/06/2021-data-engineering-survey.html,"Take the survey and get a copy of the findings—and be entered into a drawing for a free Data Teams book & other prizes. (function(tesn){var oac;t.SMCX=t.SMCX||[]e.getElementById(n)||(o=e.getElementsByTagName(s)a=o[o.length-1]c=e.createElement(s)c.type=""text/javascript""c.async=!0c.id=nc.src=""https://widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgdyp2tu2Z1S4d_2FL4Fl_2F518Cp4RhSqzdS_2F_2FCxPuP8jKmel.js""a.parentNode.insertBefore(ca))})(windowdocument""script""""smcx-sdk""); Create your own user feedback survey "
2021,6,28,Join the DataFrames like SQL tables in Python using Pandas,https://www.analyticsvidhya.com/blog/2021/06/join-the-dataframes-like-sql-tables-in-python-using-pandas/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon In the age of relational databases Joining and Merging tables is ... 
The post Join the DataFrames like SQL tables in Python using Pandas appeared first on Analytics Vidhya."
2021,6,28,Tricks for Data visualization using Plotly Library,https://www.analyticsvidhya.com/blog/2021/06/tricks-for-data-visualization-plotly-library/,"ArticleVideo Book This article was published as a part of the Data Science Blogathon Data is everywhere you just need an eye to select which ... 
The post Tricks for Data visualization using Plotly Library appeared first on Analytics Vidhya."
2021,6,28,Accelerate the modernization of your information architecture with expert tools and advice,https://www.ibm.com/blogs/journey-to-ai/2021/06/accelerate-the-modernization-of-your-information-architecture-with-expert-tools-and-advice/,"IT architectures have witnessed an increasing amount of dispersal and segmentation over the last decade of their life cycle as new data and new technology have made their impact with 62% of enterprises planning to modernize their existing on-premises data platforms[1]. These changes are needed in order to address the lack of cohesion and widespread [&#8230;]
The post Accelerate the modernization of your information architecture with expert tools and advice appeared first on Journey to AI Blog."
2021,6,28,Data utility can be preserved while enhancing data privacy,https://www.ibm.com/blogs/journey-to-ai/2021/06/data-utility-can-be-preserved-while-enhancing-data-privacy/,"Organizations need to strike a perplexing balance when launching strategic AI initiatives: data needs to be accessible without compromising privacy regulation compliance or the speed of business innovation. Customer trust and brand reputation are key competitive advantages so accelerated digital transformation and growth relies on businesses being smart about protecting sensitive customer data while still [&#8230;]
The post Data utility can be preserved while enhancing data privacy appeared first on Journey to AI Blog."
2021,6,28,"Top Stories, Jun 21-27: Data Scientists Will be Extinct in 10 Years; Analytics Engineering Everywhere",https://www.kdnuggets.com/2021/06/top-news-week-0621-0627.html,Also: Pandas vs SQL: When Data Scientists Should Use Each Tool; How to Land a Data Analytics Job in 6 Months; What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?; How to create an interactive 3D chart and share it easily with anyone
2021,6,28,Add A New Dimension To Your Photos Using Python,https://www.kdnuggets.com/2021/06/new-dimension-photos-python.html,Read this to learn how to breathe new life into your photos with a 3D Ken Burns Effect.
2021,6,28,Data Scientists are from Mars and Software Developers are from Venus,https://www.kdnuggets.com/2021/06/data-scientists-mars-software-developers-venus.html,Within the broad universe of IT in the business world the approaches for deploying solutions by traditional software engineers and trendy new data scientists couldn't be more different. However appreciating these differences are incredibly important because great business value can be gained by integrating both worlds of development into driving more efficiency and effectiveness into an organization.
2021,6,28,How to Train a Joint Entities and Relation Extraction Classifier using BERT Transformer with spaCy 3,https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html,A step-by-step guide on how to train a relation extraction classifier using Transformer and spaCy3.
2021,6,28,R user or R Developer? Your opinion matters.,https://www.r-bloggers.com/2021/06/r-user-or-r-developer-your-opinion-matters/," What makes one an R Developer and how does it differ from being an R User?
Running up to the Panel Discussion “R User or R Developer? This is the question”)  at useR!2021 we have realized that there is absolutely no consensus regarding the definiti...


The post R user or R Developer? Your opinion matters. first appeared on R-bloggers."
2021,6,28,Shiny Environmental Video Game Wins Grand Prize at Rstudio Shiny Contest,https://www.r-bloggers.com/2021/06/shiny-environmental-video-game-wins-grand-prize-at-rstudio-shiny-contest/,"
Rstudio Shiny Contest – The Results Are In!  Appsilon engineer Marcin Dubel has been named a Grand Prize Winner in the 3rd Annual Rstudio Shiny Contest for his app – Shark Attack. This marks the second year in a row an Appsilon team member has won a Grand Prize at the Shiny ...


The post Shiny Environmental Video Game Wins Grand Prize at Rstudio Shiny Contest first appeared on R-bloggers."
2021,6,28,RStudio Professional Drivers 1.8.0,https://www.r-bloggers.com/2021/06/rstudio-professional-drivers-1-8-0/,"Announcing full support for the Snowflake driver
In the previous release we included a preview of the Snowflake driver. This new release provides full ODBC dbplyr and Python support for Snowflake as well as corrects some issues that oc...
The post RStudio Professional Drivers 1.8.0 first appeared on R-bloggers."
2021,6,28,Downloading Sentinel-2 archives from Google Cloud with sen2r,https://www.r-bloggers.com/2021/06/downloading-sentinel-2-archives-from-google-cloud-with-sen2r/,"
In short…
Retrieving Sentinel-2 data from official repositories (Copernicus Open Access Hub)
for time series analysis has recently become particularly laborious
since products older than 30 days cannot be directly downloaded
(users have to order...


The post Downloading Sentinel-2 archives from Google Cloud with sen2r first appeared on R-bloggers."
2021,6,28,Hitachi Vantara acquires data governance player Io-Tahoe,https://www.zdnet.com/article/hitachi-vantara-acquires-data-governance-player-io-tahoe/#ftag=RSSbaffb68,Data discovery and governance player Io-Tahoe a division of UK energy provider Centrica is acquired by data and analytics-focused Hitachi Vantara which had previously acquired Pentaho and Waterline Data.
2021,6,28,SPMF 2.48 + The Pattern Mining Wiki,https://data-mining.philippe-fournier-viger.com/spmf-2-48-the-pattern-mining-wiki/,Hi all I have not been very active on the blog during the last month. This is because I had many thinsg going on in my personal and professional life that I will not reveal here. But I will be &#8230; Continue reading &#8594;
2021,6,27,Using NEOS Optimization Solver in R code,https://www.r-bloggers.com/2021/06/using-neos-optimization-solver-in-r-code/,"    This post explains how to use ROI and ROI.plugin.neos packages in R code which provide an interface to NEOS. NEOS (Network-Enabled Optimization System) Server is a free internet-based service for solving numerical optimization problems. For underst...


The post Using NEOS Optimization Solver in R code first appeared on R-bloggers."
2021,6,27,Equality of Variances in R-Homogeneity test-Quick Guide,https://www.r-bloggers.com/2021/06/equality-of-variances-in-r-homogeneity-test-quick-guide/," Equality of Variances in R in this article we are describing variance comparison of 2 or more samples. There are different types of tests...
The post Equality of Variances in R-Homogeneity test-Quick Guide appeared first on finnstats.


The post Equality of Variances in R-Homogeneity test-Quick Guide first appeared on R-bloggers."
2021,6,27,simplevis – simple methods to adjust titles and scales,https://www.r-bloggers.com/2021/06/simplevis-simple-methods-to-adjust-titles-and-scales/,"
library(simplevis)
library(dplyr)
library(palmerpenguins)
Overview
simplevis provides gglot2 (and leaflet) wrapper functions with an objective to help users make beautiful visualisation with less brainpower and typing.
In the first simplevis blog p...


The post simplevis – simple methods to adjust titles and scales first appeared on R-bloggers."
2021,6,26,What Are People Sayin’ About Instagram Lite?,https://www.r-bloggers.com/2021/06/what-are-people-sayin-about-instagram-lite/,"
In the beginning of May I used RSelenium to scrape the Google Play Store reviews for Instagram Lite to demonstrate how the package can be used to automate browser behavior. Its taken longer than I had initially planned to do this follow-up on the analysis of that data. But better ...


The post What Are People Sayin’ About Instagram Lite? first appeared on R-bloggers."
2021,6,26,Quantifying Relative Soccer League Strength,https://www.r-bloggers.com/2021/06/quantifying-relative-soccer-league-strength/,"
Introduction
Arguing about domestic league strength is something that soccer fans seems to never tire of. (“Could Messi do it on a cold rainy night in Stoke?”) Many of these conversations are anecdotal leading to “hot takes” that are unfalsifiabl...


The post Quantifying Relative Soccer League Strength first appeared on R-bloggers."
2021,6,25,"Data privacy isn&#8217;t a compliance checkbox, but a competitive advantage",https://www.ibm.com/blogs/journey-to-ai/2021/06/data-privacy-isnt-a-compliance-checkbox-but-a-competitive-advantage/,"In the post-GDPR era data privacy has taken center stage yet again due to digital transformation across the globe.  Governments everywhere are enforcing more robust data protection guidelines to address new digital interactions between enterprises and consumers as well as to increase accountability from enterprises in the use and protection of data. Accordingly there has [&#8230;]
The post Data privacy isn&#8217;t a compliance checkbox but a competitive advantage appeared first on Journey to AI Blog."
2021,6,25,Applied Language Technology: A No-Nonsense Approach,https://www.kdnuggets.com/2021/06/applied-language-technology.html,Here is a free entry-level applied natural language processing course that can fit into any beginner's roadmap to understanding NLP. Check it out.
2021,6,25,"High-Performance Deep Learning: How to train smaller, faster, and better models – Part 2",https://www.kdnuggets.com/2021/06/high-performance-deep-learning-part2.html,As your organization begins to consider building advanced deep learning models with efficiency in mind to improve the power delivered through your solutions the software and hardware tools required for these implementations are foundational to achieving high-performance.
2021,6,25,How to create an interactive 3D chart and share it easily with anyone,https://www.kdnuggets.com/2021/06/create-interactive-3d-chart-share.html,This is a short tutorial on a great Plotly feature.
2021,6,25,Create and Preview RMarkdown Documents with QBit Workspace,https://www.r-bloggers.com/2021/06/create-and-preview-rmarkdown-documents-with-qbit-workspace/," Create and Preview RMarkdown Documents with QBit Workspace
RMarkdown is an excellent format to create documents which combine code outputs with text—a programming paradigm called Literate Programming first introduced by Donald Knuth. Although RMarkdow...


The post Create and Preview RMarkdown Documents with QBit Workspace first appeared on R-bloggers."
2021,6,25,Techguides Update: Shiny CI/CD in Actions!,https://www.r-bloggers.com/2021/06/techguides-update-shiny-ci-cd-in-actions/,"GitHub Actions has been maturing as a CI / CD tool therefore we updated our techguides to focus more on it.
There are many CI / CD tools available which people most often use together with GitHub. Since GitHub Actions is the first such tool that in...
The post Techguides Update: Shiny CI/CD in Actions! first appeared on R-bloggers."
2021,6,25,rJava with User-defined R Functions in Eclipse,https://www.r-bloggers.com/2021/06/rjava-with-user-defined-r-functions-in-eclipse/,"      This post shows how to call user-defined functions in R script from Eclipse Java with rJava package. This work will improve code readability and minimize the likelihood of errors in such a way that it reduces multiples lines of...


The post rJava with User-defined R Functions in Eclipse first appeared on R-bloggers."
2021,6,25,Smarten Support Portal Updates – June – 2021!,https://www.smarten.com/blog/smarten-support-portal-updates-june-2021/,We invite you to explore our latest knowledgebase articles and to join the Smarten user community on&#160;Smarten Support Portal. If you have not registered yet&#160;Click Here&#160;to obtain your login credentials. Knowledgebase Articles Smarten Application Security: Banner Grabbing vulnerabilities and solutions Smarten Application Security: HTTP Method [&#8230;]
2021,6,25,Gradient Flow Snapshot #60: Self-supervised Learning; SaaS CTO Security Checklist; 2021 NLP Survey,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-60-self.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,24,Reducing neonate mortality rates with AI and Edge computing,https://www.ibm.com/blogs/journey-to-ai/2021/06/reducing-neonate-mortality-rates-with-ai-and-edge-computing/,"She sees the concern in their eyes. She hears &#8220;we&#8217;ll take good care of you&#8211;both.&#8221; Then she awakes to learn her child is in intensive care 3 lb. 7 oz. and eight weeks early. Days into his stay he is jaundiced—a complication of sepsis. Thus begins the adventure into the unknown.  –Reflections from a preemie&#8217;s [&#8230;]
The post Reducing neonate mortality rates with AI and Edge computing appeared first on Journey to AI Blog."
2021,6,24,Season 1 Of Data Science Perspectives Webcast Released,https://www.kdnuggets.com/2021/06/bill-frinks-season-1-data-science-perspectives-webcast.html,Season 1 of Data Science Perspectives is now live and ready for viewing where I interview many of the executives and professionals I’ve met to enable viewers to learn about how their careers unfolded what skills they look for when hiring what trends they think are coming next and more.
2021,6,24,What will the demand for Data Scientists be in 10 years? Will Data Scientists be extinct?,https://www.kdnuggets.com/2021/06/poll-demand-data-scientists-10-years.html,Participate in the latest KDnuggets survey and share your opinion: what does the next decade have in  store for data scientist demand?
2021,6,24,In-Warehouse Machine Learning and the Modern Data Science Stack,https://www.kdnuggets.com/2021/06/in-warehouse-machine-learning-modern-data-science-stack.html,As your organization matures its data science portfolio and capabilities establishing a modern data stack is vital to enabling such growth. Here we overview various in-data warehouse machine learning services and discuss each of their benefits and requirements.
2021,6,24,10 Python Code Snippets We Should All Know,https://www.kdnuggets.com/2021/06/10-python-code-snippets.html,Check out these Python code snippets and start using them to solve everyday problems.
2021,6,24,Updated forecasts for the UEFA Euro 2020 knockout stage,https://www.r-bloggers.com/2021/06/updated-forecasts-for-the-uefa-euro-2020-knockout-stage/,"
    After all group stage matches at the UEFA Euro 2020 we have updated the knockout stage forecasts by re-training our hybrid random forest model on the extended data. This shows that England profits most from the realized tournament draw.
...


The post Updated forecasts for the UEFA Euro 2020 knockout stage first appeared on R-bloggers."
2021,6,24,Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi,https://www.r-bloggers.com/2021/06/working-with-databases-in-r-video-presentation-from-nairobir-and-r-ladies-nairobi/," During this Working with Databases in R online presentation Christopher Maronga shares his years of practical experience in accessing and working with Databases in R. R Consortium assisted by providing...
The post Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi appeared first on R Consortium.


The post Working with Databases in R – Video Presentation from NairobiR and R-Ladies Nairobi first appeared on R-bloggers."
2021,6,24,Sentiment Analysis on Reddit using R,https://www.r-bloggers.com/2021/06/sentiment-analysis-on-reddit-using-r/,"According to Wikipedia Reddit is an American social news aggregation web content rating and discussion website. Registered members submit content to the site such as links text posts images and videos which are then voted up or down by other members. Posts are organized by subject into user-created boards called “...
The post Sentiment Analysis on Reddit using R first appeared on R-bloggers."
2021,6,24,"useR! 2021 Preview: Scaling Shiny, User Tests, and shiny.fluent",https://www.r-bloggers.com/2021/06/user-2021-preview-scaling-shiny-user-tests-and-shiny-fluent/,"
useR! – The Global Virtual R Conference Do you have a dashboard that’s not quite where you want it to be? Or are you curious about how to streamline your beautiful Shiny app projects? If you’re working with Shiny or want to enhance your data storytelling abilities with R ...


The post useR! 2021 Preview: Scaling Shiny User Tests and shiny.fluent first appeared on R-bloggers."
2021,6,24,Why Shiny? Opinions from a Shiny developer,https://www.r-bloggers.com/2021/06/why-shiny-opinions-from-a-shiny-developer/,"EARL 2021 will start with a week of afternoon workshops hosted by our expert Mango Solutions Data Scientists. This morning...
The post Why Shiny? Opinions from a Shiny developer appeared first on Mango Solutions.
The post Why Shiny? Opinions from a Shiny developer first appeared on R-bloggers."
2021,6,24,Who is going to Win the Euro 2020,https://www.r-bloggers.com/2021/06/who-is-going-to-win-the-euro-2020/," We have reached the knock-out phase of Euro 2020 (or 2021) where the final-16 teams and the games can be ... Read moreWho is going to Win the Euro 2020


The post Who is going to Win the Euro 2020 first appeared on R-bloggers."
2021,6,24,ggpairs in R- A Brief Introduction to ggpairs,https://www.r-bloggers.com/2021/06/ggpairs-in-r-a-brief-introduction-to-ggpairs/," In this article we are going to compare pairs and ggpairs functions in R. 1. pairs() in R pairs() function mainly used to plot...
The post ggpairs in R- A Brief Introduction to ggpairs appeared first on finnstats.


The post ggpairs in R- A Brief Introduction to ggpairs first appeared on R-bloggers."
2021,6,24,May 2021: “Top 40” New CRAN Packages,https://www.r-bloggers.com/2021/06/may-2021-top-40-new-cran-packages/,"Two hundred five packages made it to CRAN in May but seven were removed before this post went to print. Here are my “Top40” picks in ten categories: Computational Methods Data Genomics Machine Learning Medicine Science Statistics Time Series Utilities and Visualization.
Computational Methods
madgrad v0.1.0: Implements MADGRAD a Momentumized ...
The post May 2021: “Top 40” New CRAN Packages first appeared on R-bloggers."
2021,6,24,Strategic Analytics at Monash University: How RStudio Accelerated the Transformation,https://www.r-bloggers.com/2021/06/strategic-analytics-at-monash-university-how-rstudio-accelerated-the-transformation/,"
This is a guest post from Dr. Behrooz Hassani-Mahmooei Director of Strategic Intelligence and Insights Unit and his team at Monash University Australia
Photo credit: Monash University
Similar to any other large and complex organisatio...


The post Strategic Analytics at Monash University: How RStudio Accelerated the Transformation first appeared on R-bloggers."
2021,6,24,Winners of the 3rd annual Shiny Contest,https://www.r-bloggers.com/2021/06/winners-of-the-3rd-annual-shiny-contest/,"
Once again the Shiny community has wowed us with their contributions to the 3rd annual Shiny Contest that we announced back in March 2021.
We had 179 submissions from 164 unique Shiny developers to the contest this year over the two-mon...


The post Winners of the 3rd annual Shiny Contest first appeared on R-bloggers."
2021,6,24,How We Curate Our Monthly Newsletter,https://www.r-bloggers.com/2021/06/how-we-curate-our-monthly-newsletter/,"How to keep up with rOpenSci?
We agree that we’re doing so much good work that it’s hard. 😉
More seriously we’ve been curating and sharing a news digest with our community for years because we believe it to be useful.
Over time its ...
The post How We Curate Our Monthly Newsletter first appeared on R-bloggers."
2021,6,24,Cloud data warehouse startup Firebolt closes $127M Series B funding round,https://www.zdnet.com/article/cloud-data-warehouse-startup-firebolt-closes-127m-series-b-funding-round/#ftag=RSSbaffb68,The cloud data warehouse startup which is focused on application-oriented analytics over big data will use the new funds for expansion of its product engineering and go-to-market teams.
2021,6,24,HPE and Nutanix partner to add Database-as-a-Service on GreenLake cloud service,https://www.zdnet.com/article/hpe-and-nutanix-partner-to-add-database-as-a-service-on-greenlake-cloud-service/#ftag=RSSbaffb68,HPE is partnering with Nutanix to expand HPE GreenLake’s DBaaS portfolio. The joint offering will make legacy databases first-class citizens in a hybrid environment that operates and soon will be billed as a cloud service.
2021,6,24,Using Predictive Analytics to Understand Your Business Future!,https://www.smarten.com/blog/using-predictive-analytics-to-understand-your-business-future/,Can Predictive Analytics REALLY Help My Business During These Uncertain Times? How accurate is predictive analytics? Is it worth using for my business? How can forecasting and prediction help me in such an uncertain environment? These are all valid questions and they are they are [&#8230;]
2021,6,24,Training and Sharing Large Language Models,http://practicalquant.blogspot.com/2021/06/training-and-sharing-large-language.html,The Data Exchange Podcast: Connor Leahy on building models and datasets for the natural language research community.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS. Take the 2021 NLP Industry Survey and get a free pass to the 2021 NLP Summit. Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,23,Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0,https://www.ibm.com/blogs/journey-to-ai/2021/06/infuse-intelligent-automation-at-scale-with-ibm-cloud-pak-for-data-4-0/,"When’s the last time you considered if you’re operating in a truly predictive enterprise furthermore if it’s easy for your data consumers models and apps to access the right data? More often than not the answer is a resounding “not very”. Between the proliferation of data types and sources and tightening regulations data is often [&#8230;]
The post Infuse intelligent automation at scale with IBM Cloud Pak for Data 4.0 appeared first on Journey to AI Blog."
2021,6,23,Workflow Orchestration with Prefect and Coiled,https://www.kdnuggets.com/2021/06/coiled-workflow-orchestration-prefect.html,Coiled helps data scientists use Python for ambitious problems scaling to the cloud for computing power ease and speed—all tuned for the needs of teams and enterprises.  In this demo example see how to spin up a Coiled cluster to execute Prefect jobs during runtime.
2021,6,23,Create and Deploy Dashboards using Voila and Saturn Cloud,https://www.kdnuggets.com/2021/06/create-deploy-dashboards-voila-saturn-cloud.html,Working with and training large datasets maintaining them all in one place and deploying them to production is a challenging job. In this article we covered what Saturn Cloud is and how it can speed up your end-to-end pipeline how to create dashboards using Voila and Python and publish them to production in just a few easy steps.
2021,6,23,Data Careers in Demand: Crowd Solutions Architect Explained,https://www.kdnuggets.com/2021/06/data-careers-crowd-solutions-architect.html,How can crowdsourcing support the applications of data teams at an organization? With an ever-increasing demand for more and higher quality data a new role of the Crowd Solutions Architect (CSA) can leverage the potential of the masses to bring an advantage to a business's capability to deliver effective AI-driven solutions.
2021,6,23,Fine-Tuning Transformer Model for Invoice Recognition,https://www.kdnuggets.com/2021/06/fine-tuning-transformer-model-invoice-recognition.html,The author presents a step-by-step guide from annotation to training.
2021,6,23,"KDnuggets™ News 21:n23, Jun 23: Pandas vs SQL: When Data Scientists Should Use Each Tool; How to Land a Data Analytics Job in 6 Months",https://www.kdnuggets.com/2021/n23.html,Pandas vs SQL: When Data Scientists Should Use Each Tool; How to Land a Data Analytics Job in 6 Months; A Graph-based Text Similarity Method with Named Entity Information in NLP; The Best Way to Learn Practical NLP?; An introduction to Explainable AI (XAI) and Explainable Boosting Machines (EBM)
2021,6,23,Infuse automation at scale with IBM Cloud Pak for Data 4.0,https://www.ibm.com/blogs/journey-to-ai/2021/06/infuse-intelligent-automation-at-scale-with-ibm-cloud-pak-for-data-4-0/,"When’s the last time you considered if you’re operating in a truly predictive enterprise furthermore if it’s easy for your data consumers models and apps to access the right data? More often than not the answer is a resounding “not very”. Between the proliferation of data types and sources and tightening regulations data is often [&#8230;]
The post Infuse automation at scale with IBM Cloud Pak for Data 4.0 appeared first on Journey to AI Blog."
2021,6,22,The Word “WORD” Has 13 Meanings,https://www.kdnuggets.com/2021/06/expert-word-has-13-meanings.html,Thoughts around Knowledge Graphs the semantic nature of language and the two main types of word ambiguity.
2021,6,22,Amazing Low-Code Machine Learning Capabilities with New Ludwig Update,https://www.kdnuggets.com/2021/06/ludwig-update-includes-low-code-machine-learning-capabilities.html,Integration with Ray MLflow and TabNet are among the top features of this release.
2021,6,22,Analytics Engineering Everywhere,https://www.kdnuggets.com/2021/06/analytics-engineering-everywhere.html,Many new roles have appeared in the data world ever since the rise of the Data Scientist took the spotlight several years ago. Now there is a new core player ready to take center stage and we may see in five years nearly every organization will have an Analytics Engineering team.
2021,6,22,What is Segmentation?,https://www.kdnuggets.com/2021/06/what-segmentation.html,Segmentation refers to many things and is one of the most frequently used words in marketing This article looks at segmentation from a somewhat different-than-usual perspective.
2021,6,22,More than words: Shedding light on the data terminology mess,https://www.zdnet.com/article/more-than-words-shedding-light-on-the-data-terminology-mess/#ftag=RSSbaffb68,Data management data governance data observability data fabric data mesh DataOps MLOps AIOps. It's a data terminology mess out there. Let's try and untangle it because there's more to words than lingo.
2021,6,22,Databricks cofounder’s next act: Shining a Ray on serverless autoscaling,https://www.zdnet.com/article/databricks-cofounders-next-act-shining-a-ray-on-compute-autoscaling/#ftag=RSSbaffb68,After helping shepherd Spark to surmount the data bottleneck UC Berkeley’s Ion Stoica is helping unleash Ray an emerging open source project to get over the compute bottleneck for scaling machine learning models into production. It will allow any developer to launch their own serverless cluster through a universal API that works anywhere.
2021,6,22,Seasonal functional autoregressive models,https://robjhyndman.com/publications/sfar/,Functional autoregressive models are popular for functional time series analysis but the standard formulation fails to address seasonal behaviour in functional time series data. To overcome this shortcoming we introduce seasonal functional autoregressive time series models. For the model of order one we derive sufficient stationarity conditions and limiting behaviour and provide estimation and prediction methods. Moreover we consider a portmanteau test for testing the adequacy of this model and we derive its asymptotic distribution.
2021,6,22,What you need to know before you get started: A brief tour of Calculus Pre-Requisites,https://machinelearningmastery.com/what-you-need-to-know-before-you-get-started-a-brief-tour-of-calculus-pre-requisites/,"We have previously seen that calculus is one of the core mathematical concepts in machine learning that permits us to [&#8230;]
The post What you need to know before you get started: A brief tour of Calculus Pre-Requisites appeared first on Machine Learning Mastery."
2021,6,22,Why Generalized Linear Models Have No Error Term,https://www.theanalysisfactor.com/generalized-linear-models-no-error-term/,"Even if you’ve never heard the term Generalized Linear Model you may have run one. It’s a term for a family of models that includes logistic and Poisson regression among others. It’s a small leap to generalized linear models if you already understand linear models. Many many concepts are the same in both types of [&#8230;]
The post Why Generalized Linear Models Have No Error Term appeared first on The Analysis Factor."
2021,6,21,"Top Stories, Jun 14-20: Data Scientists Will be Extinct in 10 Years",https://www.kdnuggets.com/2021/06/top-news-week-0614-0620.html,Also: Get Interactive Plots Directly With Pandas; How to Generate Automated PDF Documents with Python; Top 10 Data Science Projects for Beginners; Five types of thinking for a high performing data scientist
2021,6,21,Using External Data to Accelerate Business in a Post-Vaccinated World,https://www.kdnuggets.com/2021/06/roidna-external-data-accelerate-business-webinar.html,Join this webinar Jun 24 2021 to learn how companies are developing insights to better prepare for growth opportunities improve business performance and mitigate risk in a post-pandemic economy.
2021,6,21,Overview of AutoNLP from Hugging Face with Example Project,https://www.kdnuggets.com/2021/06/overview-autonlp-hugging-face-example-project.html,AutoNLP is a beta project from Hugging Face that builds on the company’s work with its Transformer project. With AutoNLP you can get a working model with just a few simple terminal commands.
2021,6,21,Pandas vs SQL: When Data Scientists Should Use Each Tool,https://www.kdnuggets.com/2021/06/pandas-vs-sql.html,Exploring data sets and understanding its structure content and relationships is a routine and core process for any Data Scientist. Multiple tools exist for performing such analysis and we take a deep dive into the benefits and different approaches of two important tools SQL and Pandas.
2021,6,21,How to troubleshoot memory problems in Python,https://www.kdnuggets.com/2021/06/troubleshoot-memory-problems-python.html,Memory problems are hard to diagnose and fix in Python. This post goes through a step-by-step process for how to pinpoint and fix memory leaks using popular open source python packages.
2021,6,21,Data Visualization and Data Analysis in Python — using the OkCupid dataset (Part 2),https://data36.com/data-visualization-analysis-python/,"This article is about dating and data science! Please welcome our guest author Amy Birdee who has done multiple data science hobby projects recently and built a truly...
The post Data Visualization and Data Analysis in Python &#8212; using the OkCupid dataset (Part 2) appeared first on Data36."
2021,6,21,Tips for Using Photos in Data Storytelling,https://www.juiceanalytics.com/writing/tips-for-using-photos-in-data-stories,"Data storytelling needs more than a collection of data visualizations. It is about weaving an engaging message that combines data exploration with narrative descriptions and graphics. Photographs in particular can play a vital role to help your data story grab attention and provide emotional engagement.Photos have always been an important element that we integrate into our data storytelling designs. Here’s why:Photos can underscore a theme or message;Photos can bring an element of humanity and emotion to a data story;Photos provide some visual space between information-dense charts and text;Photos can be used to attract and guide the attention of your audience. People will be instantly drawn to the photos first.Now we’ve made it trivially easy for you to add photos to your data stories in Juicebox.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We’ve integrated into Unsplash which has over 2 million photos and includes a license that allows for free and commercial use of the photos (though you may not sell them). You can also upload your own images when you want to add company logos product images or a picture of a happy customer.How should you think about picking photos to include in your data story? Here are the most important do’s and don’ts:Balance the colors👍You want the colors of your photos to complement the primary color of your data story. If the content is largely white/black/grey a photo with bright colors will bring energy. If you are already using bold colors find a photo that shares the color scheme or is in black &amp; white.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Connecting colors in the photo to other colors in your design
          
        
      
        
      

    
  


  


👎 You don't want the colors in your photo to compete with the colors used in the rest of your data story. A vivid photo will leave your reader wondering where to place their attention.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Purples and greens aren’t good friends. Consult your color wheel.
          
        
      
        
      

    
  


  


Match the theme or mood👍The photo should align with the overall message of your data story. Is your theme dark hopeful energetic serious? The right photo will help you set the tone you are looking for.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A spooky picture to go with the UFO theme.
          
        
      
        
      

    
  


  


👎 You want to avoid dissonance between the story theme and the images. A serious topic is not well suited to a silly image. A data story about people should include photos of people rather than machines or abstract concepts.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Hooray the kangaroos are excited about your key metrics!
          
        
      
        
      

    
  


  


Avoid on-the-nose-ness👍The best photos will alude to your message or theme without being overly literal. To do so the photos might pick out a specific example or on the other hand express broad feelings or concepts that are relevant to your data story.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A discussion about architecting your story
          
        
      
        
      

    
  


  


👎 Avoid using photos that literally present the subject of your data story. The photos are more likely to receive groans from your audience as they realize what you've done. They are the ""Dad Jokes"" of design.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Get it?! Story + Structure.
          
        
      
        
      

    
  


  


Avoid stock photos👍You want to look for photos that bring a sense of authenticity or reality to your data story. Better to find an image of a specific location product customer or activity -- rather than something that feels generic.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Show the top paid athletes
          
        
      
        
      

    
  


  


👎We’re all tired of the unnaturally happy business people sitting at a conference table. Or the close up image of shaking hands. This is one reason we value our integration with Unsplash which provides photos by photographers with a specific eye for authentic details.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A generic “athlete”
          
        
      
        
      

    
  


  


Abstraction vs. Detail👍There is a place for abstract photos (when you are setting a mood or feeling) and detailed photos (when you want to show something important for your readers). Consider what the story needs to enhance your message.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A photo that suggests zooming-in
          
        
      
        
      

    
  


  


👎If you are showing a lot of data a detailed photo may distract the reader from the content you want to be the focus of your message. On the other hand add a detailed photo -- and give it some visual space -- if you want the reader to linger on the image.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            A complex visualization competes for attention with a crowd of people.
          
        
      
        
      

    
  


  


Data Storytelling and Photo Composition


	Juicebox ❤️ Photos"
2021,6,21,Calculus in Machine Learning: Why it Works,https://machinelearningmastery.com/calculus-in-machine-learning-why-it-works/,"Last Updated on June 21 2021 Calculus is one of the core mathematical concepts in machine learning that permits us [&#8230;]
The post Calculus in Machine Learning: Why it Works appeared first on Machine Learning Mastery."
2021,6,19,Brief report about ICIVIS 2021,https://data-mining.philippe-fournier-viger.com/brief-report-about-icivis-2021/,This week-end I have attended the International Conference on Image Vision and Intelligent system from 18 to 20 June 2021 in Changsha city China. It is a medium-sized conference (about 100 participants) but It is well-organized and there was many &#8230; Continue reading &#8594;
2021,6,19,Real-Time Machine Learning: Why It’s Vital and How to Do It,https://www.predictiveanalyticsworld.com/blog/real-time-machine-learning-why-its-vital-and-how-to-do-it/,"By:  Eric Siegel Predictive Analytics World This article is sponsored by IBM. SUMMARY: Organizations often miss the greatest opportunities that machine learning has to offer because tapping them requires real-time predictive scoring. In order to optimize the very largest-scale processes – which is a vital endeavor for your business – predictive scoring must take place [&#8230;]
The post Real-Time Machine Learning: Why It&#8217;s Vital and How to Do It appeared first on Predictive Analytics World."
2021,6,18,"Major changes: Where Analytics, Data Science, Machine Learning were applied in 2020/21",https://www.kdnuggets.com/2021/06/poll-where-analytics-data-science-ml-applied.html,Our latest poll shows major change in where AI Data Science Machine Learning are being applied with decline in interest in traditional fields like CRM/Consumer Analytics and growth in applications to Computer Vision COVID Agriculture and Education.
2021,6,18,"High Performance Deep Learning, Part 1",https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html,Advancing deep learning techniques continue to demonstrate incredible potential to deliver exciting new AI-enhanced software and systems. But training the most powerful models is expensive--financially computationally and environmentally. Increasing the efficiency of such models will have profound impacts in many ways so developing future models with this intension in mind will only help to further expand the reach applicability and value of what deep learning has to offer.
2021,6,18,"Data Science is Not Becoming Extinct in 10 Years, Your Skills Might",https://www.kdnuggets.com/2021/06/data-science-not-becoming-extinct-10-years.html,4 reasons why data science is here to stay and what you need to do to ensure that your skillset stays in demand.
2021,6,18,Useful extensions for online books,https://robjhyndman.com/hyndsight/fpp-extensions/,"I&rsquo;ve had two recent questions from readers of my online textbook (with George Athanasopoulos) which could be solved using Google Chrome extensions.
 Hi I&rsquo;m an MSc student and am shortly starting my project/dissertation on time series data. I&rsquo;ve started reading Version 3 of your book and improving my R skills but am wondering if there&rsquo;s any way I can read V3 that will allow annotation? Thanks
 For personal annotation of websites the Hypothesis extension is very useful."
2021,6,18,Data Storytelling: What's Easy and What's Hard,https://www.juiceanalytics.com/writing/data-storytelling-whats-easy-and-whats-hard,"Putting data on a screen is easy. Making it meaningful is so much harder. Gathering a collection of visualizations and calling it a data story is easy (and inaccurate). Making data-driven narrative that influences people...hard.Here are 25 more lessons we've learned (the hard way) about what's easy and what's hard when it comes to telling stories with data. We also included links to our 🎓 Data Storytelling Lessons where they might help make things a little less hard for you.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Michał Parzuchowski
          
        
      
        
      

    
  


  


Easy — Picking a good visualization to answer a data question …🎓 How to Choose the Right ChartHard — Discovering the core message of your data story that will move your audience to action …🎓 Story ElementsEasy — Knowing who is your target audienceHard — Knowing what motivates your target audience at a personal level by understanding their everyday frustrations and career goalsEasy — Collecting questions your audience wants to answerHard — Delivering answers your audience can act onEasy — Providing flexibility to slice and dice dataHard — Balancing flexibility with prescriptive guidance to help focus on the most important things …🎓 Explore vs. ExplainEasy — Labeling visualizationsHard — Explaining the intent and meaning of visualizations …🎓 Relatable and SpecificEasy — Choosing dimensions to showHard — Choosing the right metrics to show …🎓 Metrics: Your Story CharactersEasy — Getting an export of the data you needHard — Restructuring data for high-performance analytical queriesEasy — Discovering inconsistencies in your dataHard — Fixing those inconsistenciesEasy — Designing a data story with a fixed data setHard — Designing a data story where the data changes …🎓 Explore vs. ExplainEasy — Categorical dimensionsHard — Dates and timesEasy — Showing data values within expected rangesHard — Dealing with null valuesEasy — Determining formats for data fieldsHard — Writing a human-readable definition of data fieldsEasy — Getting people interested in analytics and visualizationHard — Getting people to use data regularly in their job …🎓 Data Personality ProfilesEasy — Picking theme colorsHard — Using colors judiciously and with meaning …🎓 Color and ContrastEasy — Setting the context for your storyHard — Creating intrigue and suspense to move people past the introduction …🎓 Narrating Data StoriesEasy — Showing selections in a visualizationHard — Carrying those selections through the duration of the storyEasy — Creating a long shaggy data storyHard — Creating a concise meaningful data story …🎓 Story StructureEasy — Adding more dataHard — Cutting out unnecessary dataEasy — Serving one audienceHard — Serving multiple audiences to enable new kinds of discussions …🎓 Data Personality ProfilesEasy — Helping people find insightsHard — Explaining what to do about those insights …How to Ensure Your Actionable Insights Lead to ActionEasy — Explaining data to expertsHard — Explaining data to novices …🎓 Relatable and SpecificEasy — Building a predictive modelHard — Convincing people they should trust your predictive modelEasy — Visual mock-ups with stubbed-in dataHard — Visual mock-ups that support real-world dataEasy — Building a visualization toolHard — Building a data storytelling tool


	Try Juicebox Free"
2021,6,18,Key Concepts in Calculus: Rate of Change,https://machinelearningmastery.com/key-concepts-in-calculus-rate-of-change/,"Last Updated on June 19 2021 The measurement of the rate of change is an integral concept in differential calculus [&#8230;]
The post Key Concepts in Calculus: Rate of Change appeared first on Machine Learning Mastery."
2021,6,18,Gradient Flow Snapshot #59: From Cloud → Sky computing; Automation in DataOps; Top Technology Trends,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-59-from-cloud.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,17,Trustworthy AI helps Regions Bank better serve customers,https://www.ibm.com/blogs/journey-to-ai/2021/06/trustworthy-ai-helps-regions-bank-better-serve-customers/,"Financial institutions worldwide are feeling the scrutiny from both customers and regulators alike. Perceptions of an institution’s governance practices including its commitment to ethics fairness explainability and transparency of decisions are critical to its standing. No wonder those poised to gain a competitive advantage today want to ensure their AI is fair trustworthy and explainable. A member of the S&#38;P 500 Index Regions Financial Corporation is one of the United States’ largest full-service [&#8230;]
The post Trustworthy AI helps Regions Bank better serve customers appeared first on Journey to AI Blog."
2021,6,17,"Submit Your Algorithm for a Chance to Win Prizes Totaling $700,000+",https://www.kdnuggets.com/2021/06/nij-recidivism-forecasting-challenge.html,Can your algorithm make fair and accurate #recidivism forecasts? Take part in US National Institute of Justice “Recidivism Forecasting Challenge” with prize money totaling over $700K.
2021,6,17,How to Land a Data Analytics Job in 6 Months,https://www.kdnuggets.com/2021/06/land-data-analytics-job-6-months.html,Go from zero to hero in under six months.
2021,6,17,"Data storytelling: brains are built for visuals, but hearts turn on stories",https://www.kdnuggets.com/2021/06/data-storytelling.html,Today we need much more than just numbers about our organization to understand gain insights and take relevant actions. While visualizations of the data are important making an emotional connection with the stories behind the data is key. If you want to sell a story send a missile to the heart.
2021,6,17,Dashboards for Interpreting & Comparing Machine Learning Models,https://www.kdnuggets.com/2021/06/dashboards-interpreting-comparing-machine-learning-models.html,This article discusses using Interpret to create dashboards for machine learning models.
2021,6,17,"The biggest investment in database history, the biggest social network ever, and other graph stories from Neo4j",https://www.zdnet.com/article/the-biggest-investment-in-database-history-the-biggest-social-network-ever-and-other-graph-stories-from-neo4j/#ftag=RSSbaffb68,A $325 million Series F funding round bringing Neo4j's valuation to over $2 billion. A social network of 3 billion people distributed across 1000 servers. The latter is a demo; the former is not. But both are real signs that the graph market and Neo4j are getting huge.
2021,6,17,Timescale scales out and sets its sights on analytics,https://www.zdnet.com/article/timescale-scales-out-and-sets-its-sights-on-analytics/#ftag=RSSbaffb68,Yes there’s another time series database on the radar screen and unlike most of them it runs on PostgreSQL. In addition the latest release adds support for multi-node distributed deployment.
2021,6,17,What is Calculus?,https://machinelearningmastery.com/what-is-calculus/,"Last Updated on June 19 2021 Calculus is the mathematical study of change.&#160; The effectiveness of calculus to solve a [&#8230;]
The post What is Calculus? appeared first on Machine Learning Mastery."
2021,6,17,Why Do I Need to Analyze eCommerce Results?,https://www.smarten.com/blog/why-do-i-need-to-analyze-ecommerce-results/,Do You Want Your eCommerce Growth to Explode? Why does an eCommerce or online shopping business need analytics? After all eCommerce is exploding. There is no limit to what a business can do! Some of what you just read is true. Some of it isn’t! [&#8230;]
2021,6,17,Questioning the Efficacy of Neural Recommendation Systems,http://practicalquant.blogspot.com/2021/06/questioning-efficacy-of-neural.html,The Data Exchange Podcast: Paolo Cremonesi and Maurizio Ferrari Dacrema on the reproducibility complexity and inefficiency of neural methods for recommenders.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.    Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.
2021,6,16,"Operationalize AI: You built an AI model, now what?",https://www.ibm.com/blogs/journey-to-ai/2021/06/operationalize-ai-you-built-an-ai-model-now-what/,"Global AI Adoption Index 2021 reports the top drivers of AI adoption in organizations are: 1. Advances in AI that make it more accessible (46%); 2. Business needs (46%); and 3. Changing business needs due to COVID-19 (44%). To bring AI models into production businesses are also mitigating the following AI modeling and management issues: [&#8230;]
The post Operationalize AI: You built an AI model now what? appeared first on Journey to AI Blog."
2021,6,16,How a Polytechnic Helps You Make the Tech-Business Connection,https://www.kdnuggets.com/2021/06/wpi-polytechnic-make-tech-business-connection.html,WPI welcomes professionals of all levels to its 100% online MS in Business Analytics — no GRE or GMAT required. Get started here.
2021,6,16,The Best Way to Learn Practical NLP?,https://www.kdnuggets.com/2021/06/best-way-learn-practical-nlp.html,Hugging Face has just released a course on using its libraries and ecosystem for practical NLP and it appears to be very comprehensive. Have a look for yourself.
2021,6,16,An introduction to Explainable AI (XAI) and Explainable Boosting Machines (EBM),https://www.kdnuggets.com/2021/06/explainable-ai-xai-explainable-boosting-machines-ebm.html,Understanding why your AI-based models make the decisions they do is crucial for deploying practical solutions in the real-world. Here we review some techniques in the field of Explainable AI (XAI) why explainability is important example models of explainable AI using LIME and SHAP and demonstrate how Explainable Boosting Machines (EBMs) can make explainability even easier.
2021,6,16,A Graph-based Text Similarity Method with Named Entity Information in NLP,https://www.kdnuggets.com/2021/06/graph-based-text-similarity-method-named-entity-information-nlp.html,"In this article the author summarizes the 2017 paper ""A Graph-based Text Similarity Measure That Employs Named Entity Information"" as per their understanding. Better understand the concepts by reading along."
2021,6,16,"KDnuggets™ News 21:n22, Jun 16: Data Scientists Extinct in 10 Years? Generate Automated PDF Documents with Python",https://www.kdnuggets.com/2021/n22.html,Data Scientists be extinct in 10 years? How to generate PDF Documents with Python; Top 10 Data Science Projects for Beginners; Five types of thinking for a high performing data scientist; and how to get interactive plots directly with Pandas.
2021,6,16,How Can I Make Search and Analytics Easier for My Staff?,https://www.smarten.com/blog/how-can-i-make-search-and-analytics-easier-for-my-staff/,Wouldn’t it be great if you could give your business users an easy way to surf all that data you have within your walls? What if they could just ask a question? Just ask a simple question and get an answer that would help them [&#8230;]
2021,6,15,KDnuggets Top Blogs Rewards for May 2021,https://www.kdnuggets.com/2021/06/top-blogs-rewards-may.html,We announce the winners of the first KDnuggets Top Blog Rewards Program.
2021,6,15,The Data Matters: Choosing the right data to analyze can make or break your analysis,https://www.kdnuggets.com/2021/06/nomad-data-matters.html,We started Nomad Data to help data scientists and business analysts quickly find the right commercial datasets to match their specific use case. We catalog use cases of data and use machine learning and AI to match analysis goals with datasets.
2021,6,15,7 Data Security Best Practices for 2021,https://www.kdnuggets.com/2021/06/7-data-security-best-practices-2021.html,Here are seven data security best practices to adopt this year.
2021,6,15,Beginners Guide to Debugging TensorFlow Models,https://www.kdnuggets.com/2021/06/beginners-guide-debugging-tensorflow-models.html,If you are new to working with a deep learning framework such as TensorFlow there are a variety of typical errors beginners face when building and training models. Here we explore and solve some of the most common errors to help you develop a better intuition for debugging in TensorFlow.
2021,6,15,Facebook Launches One of the Toughest Reinforcement Learning Challenges in History,https://www.kdnuggets.com/2021/06/facebook-launches-toughest-reinforcement-learning-challenges.html,The FAIR team just launched the NetHack Challenge as part of the upcoming NeurIPS 2021 competition. The objective is to test new RL ideas using a one of the toughest game environments in the world.
2021,6,15,Communal Computing,https://www.oreilly.com/radar/communal-computing/,Home assistants and smart displays are being sold in record numbers but they are built wrong. They are designed with one person in mind: the owner. These technologies need to fit into the communal spaces where they are placed like homes and offices. If they don’t fit they will be unplugged and put away due [&#8230;]
2021,6,15,DataStax launches beta of Astra Streaming service,https://www.zdnet.com/article/datastax-launches-beta-of-astra-streaming-service/#ftag=RSSbaffb68,DataStax is placing its streaming bet on Apache Pulsar an emerging rival to Kafka unveiling a new cloud-managed service in its Astra portfolio.
2021,6,15,Accelerating model velocity through Snowflake Java UDF integration,https://blog.dominodatalab.com/accelerating-model-velocity-through-snowflake-java-udf-integration/,"Integrating Domino and Snowflake and using in-database machine learning / data processing techniques via user defined functions (UDF).
The post Accelerating model velocity through Snowflake Java UDF integration appeared first on Data Science Blog by Domino."
2021,6,15,Differential Evolution from Scratch in Python,https://machinelearningmastery.com/differential-evolution-from-scratch-in-python/,"Last Updated on June 19 2021 Differential evolution is a heuristic approach for the global optimisation of nonlinear and non- [&#8230;]
The post Differential Evolution from Scratch in Python appeared first on Machine Learning Mastery."
2021,6,15,What is a Composable Enterprise and Why You Need It?,https://www.mdmgeek.com/2021/06/15/what-is-a-composable-enterprise-and-why-you-need-it/,"Before we explore Composable Enterprise consider this scenario. Have you ever come across a situation where you realize your colleague is working on the same thing that you are? Perhaps it is a quarterly revenue report if you are looking at the business&#8217;s health. And my friends in technology you may be writing the exact [&#8230;]
The post What is a Composable Enterprise and Why You Need It? appeared first on MDMgeek."
2021,6,14,How bakery company Vaasan used AI to upgrade their planning,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-bakery-company-vaasan-used-ai-to-upgrade-their-planning/,"The Finnish baker Vaasan knows a thing or two about fast delivery. After all the company’s roots date back to year 1849 which makes Vaasan one of the oldest nationwide bakeries in Finland. Vaasan is best known as the producer of Finland’s most popular bread Vaasan Ruispalat. The company has to be fast because the [&#8230;]
The post How bakery company Vaasan used AI to upgrade their planning appeared first on Journey to AI Blog."
2021,6,14,"Top Stories, Jun 7-13: 5 Tasks To Automate With Python; Five types of thinking for a high performing data scientist",https://www.kdnuggets.com/2021/06/top-news-week-0607-0613.html,Also: How to Generate Automated PDF Documents with Python; Five types of thinking for a high performing data scientist; How I Doubled My Income with Data Science and Machine Learning; Top 10 Data Science Projects for Beginners
2021,6,14,Data Scientists Will be Extinct in 10 Years,https://www.kdnuggets.com/2021/06/data-scientists-extinct-10-years.html,And why it’s not a bad thing.
2021,6,14,Get Interactive Plots Directly With Pandas,https://www.kdnuggets.com/2021/06/interactive-plots-directly-pandas.html,Telling a story with data is a core function for any Data Scientist and creating data visualizations that are simultaneously illuminating and appealing can be challenging. This tutorial reviews how to create Plotly and Bokeh plots directly through Pandas plotting syntax which will help you convert static visualizations into interactive counterparts -- and take your analysis to the next level.
2021,6,14,Building a Knowledge Graph for Job Search Using BERT,https://www.kdnuggets.com/2021/06/knowledge-graph-job-search-bert.html,A guide on how to create knowledge graphs using NER and Relation Extraction.
2021,6,14,Where is IBM’s hybrid cloud launchpad?,https://www.zdnet.com/article/where-is-ibms-hybrid-cloud-launchpad/#ftag=RSSbaffb68,IBM is not the only major technology provider that has embraced hybrid and multicloud. But with vertical industry clouds IBM could have a unique strength.
2021,6,14,How to Ensure Your Actionable Insights Lead to Action,https://www.juiceanalytics.com/writing/how-to-ensure-your-actionable-insights-lead-to-action,"“Actionable insights” is the Holy Grail of analytics. It is the point at which data achieves value when smarter decisions are made and when the hard work of the analytics team pays off. Actionable insights can also be elusive — a perfectly brilliant insight gets ignored or a comprehensive report gathers dusts on a shelf.The myth persists: If you build it they will come. The purity of your insight will compel your audience to action.Of course that’s not how it works. There is rarely an analytical finding that is by itself so obvious clever and compelling that other people in your organization are forced to act. The reality is that an analytic insight needs to be nurtured and sold. Like a new product people need to understand it see how it fits into their life compare it to the status quo and see the case for making change.Your actionable insight needs to connect to the things that are important to your audience. The following diagram shows the eight things that you’ll want to consider to evaluate whether your insight is likely to deliver the action you are hoping for. Consider it a checklist starting from the top (“Attention”) and proceeding clockwise.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


These are the stumbling blocks that we’ve seen as perfectly intelligence and researched insights go to a pre-mature grave. A good insight is a terrible thing to waste.


	Try Juicebox Free"
2021,6,13,Modeling Pipeline Optimization With scikit-learn,https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/,"Last Updated on June 19 2021 This tutorial presents two essential concepts in data science and automated learning. One is [&#8230;]
The post Modeling Pipeline Optimization With scikit-learn appeared first on Machine Learning Mastery."
2021,6,11,Top 10 Data Science Projects for Beginners,https://www.kdnuggets.com/2021/06/top-10-data-science-projects-beginners.html,Check out these projects for ideas to strengthen your skills and build a portfolio that stands out.
2021,6,11,Five types of thinking for a high performing data scientist,https://www.kdnuggets.com/2021/06/five-types-thinking-data-scientist.html,The way you think about a problem and the conceptual process you go through to find a solution may be guided by your personal skills or the type of problem at hand. Many mental models exist representing a variety of thinking patterns -- and as a Data Scientist appreciating different approaches can help you more effectively model data in the business world and communicate your results to the decision-makers.
2021,6,11,9 Deadly Sins of Machine Learning Dataset Selection,https://www.kdnuggets.com/2021/06/9-deadly-sins-ml-dataset-selection.html,Avoid endless pain in model debugging by focusing on datasets upfront.
2021,6,11,Gradient Flow Snapshot #58: Delta Live Tables; Knowledge Graphs; Data Portability,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-58-delta-live.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,10,"Top May Stories: A Guide On How To Become A Data Scientist; Data Scientist, Data Engineer & Other Data Careers, Explained",https://www.kdnuggets.com/2021/06/top-stories-2021-may.html,A Guide On How To Become A Data Scientist; Data Scientist Data Engineer &#038; Other Data Careers Explained; Vaex: Pandas but 1000x faster; Data Preparation in SQL with Cheat Sheet
2021,6,10,Numerics V: Integrality – When Being Close Enough is not Always Good Enough,https://www.kdnuggets.com/2021/06/fico-numerics-vs-integrality-close-enough.html,Wow already the fifth blog in this series…What is left to tell about numerics? There is another place where a MIP solver can sneak in minor violations that we have not yet discussed: The integrality conditions.
2021,6,10,"The Essential Guide to Transformers, the Key to Modern SOTA AI",https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html,You likely know Transformers from their recent spate of success stories in natural language processing computer vision and other areas of artificial intelligence but are familiar with all of the X-formers? More importantly do you know the differences and why you might use one over another?
2021,6,10,Feature Selection – All You Ever Wanted To Know,https://www.kdnuggets.com/2021/06/feature-selection-overview.html,"Although your data set may contain a lot of information about many different features selecting only the ""best"" of these to be considered by a machine learning model can mean the difference between a model that performs well--with better performance higher accuracy and more computational efficiency--and one that falls flat. The process of feature selection guides you toward working with only the data that may be the most meaningful and to accomplish this a variety of feature selection types methodologies and techniques exist for you to explore."
2021,6,10,How to Generate Automated PDF Documents with Python,https://www.kdnuggets.com/2021/06/generate-automated-pdf-documents-python.html,Discover how to leverage automation to create dazzling PDF documents effortlessly.
2021,6,10,Quest enhances its Erwin data modeling and data intelligence platforms,https://www.zdnet.com/article/quest-enhances-its-erwin-data-modeling-and-data-intelligence-platforms/#ftag=RSSbaffb68,After acquiring Erwin Inc. this past January Quest rolls out new versions of its data modeling data catalog and data stewardship components.
2021,6,10,Negotiating the data science job salary. Should you do that? How do you do that?,https://data36.com/negotiating-the-data-science-job-salary-how-to/,"Should you negotiate your salary before you get into a junior data scientist role? To start: yes! You should always negotiate. Tech companies in particular EXPECT you to...
The post Negotiating the data science job salary. Should you do that? How do you do that? appeared first on Data36."
2021,6,10,Gradient Descent With AdaGrad From Scratch,https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent With AdaGrad From Scratch appeared first on Machine Learning Mastery."
2021,6,10,Interpreting the Shapes of Hazard Functions in Survival Analysis,https://www.theanalysisfactor.com/survival-analysis-interpreting-shapes-of-hazard-functions/,"by Steve Simon PhD Hazard functions are a key tool in survival analysis. But they&#8217;re not always easy to interpret. In this article we&#8217;re going to explore the definition purpose and meaning of hazard functions. Then we&#8217;ll explore a few different shapes to see what they tell us about the data. Motivating example This is [&#8230;]
The post Interpreting the Shapes of Hazard Functions in Survival Analysis appeared first on The Analysis Factor."
2021,6,10,Automation in Data Management and Data Labeling,http://practicalquant.blogspot.com/2021/06/automation-in-data-management-and-data.html,The Data Exchange Podcast: Hyun Kim on building automation and DataOps tools to help companies unlock computer vision data.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.    Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,9,How to speed up a Deep Learning Language model by almost 50X at half the cost,https://www.kdnuggets.com/2021/06/determined-ai-speed-up-deep-learning-language-model.html,In this blog post we show how to accelerate fine-tuning the ALBERT language model while also reducing costs by using Determined’s built-in support for distributed training with AWS spot instances.
2021,6,9,"Data Scientists, You Need to Know How to Code",https://www.kdnuggets.com/2021/06/data-scientists-need-know-code.html,You need to know how to code &#8212; and not just code but write good code.
2021,6,9,The 7 Best Open Source AI Libraries You May Not Have Heard Of,https://www.kdnuggets.com/2021/06/7-open-source-ai-libraries.html,AI researchers today have many exciting options for working with specialized tools. Although starting original projects from scratch is often not necessary knowing which existing library to leverage remains a challenge. This list of generally unknown yet awesome open-source libraries offers an interesting collection to consider for state-of-the-art research that spans from automatic machine learning to differentiable quantum circuits.
2021,6,9,How a Single Mistake Wasted 3 Years of My Data Science Journey,https://www.kdnuggets.com/2021/06/single-mistake-wasted-3-years-data-science.html,Self-paced courses are just sleeping pills; Industry experts are the right choice.
2021,6,9,AzureR update: new in May/June,https://blog.revolutionanalytics.com/2021/06/mayjune-azurer-update.html,by Hong Ooi This is a summary of the updates to AzureR family of packages in May and June 2021. AzureAuth Change the default caching behaviour to disable the cache if running inside Shiny. Update Shiny vignette to clean up redirect page after authenticating (thanks to Tyler Littlefield). Add a create_AzureR_dir function to create the caching directory manually. This can be useful not just for non-interactive sessions but also Jupyter and R notebooks which are not technically interactive in the sense that they cannot read user input from a console prompt. AzureGraph Add enhanced support for the paging API. Many...
2021,6,9,Tally ERP Analytics: Providing a Comprehensive Picture of Business!,https://www.smarten.com/blog/tally-erp-analytics-providing-a-comprehensive-picture-of-business/,The Tally ERP Solution is a popular accounting and financial application that is used by many business professionals. It acts as a repository for crucial business information and allows for data entry and reporting. But today every team member is expected to contribute to business [&#8230;]
2021,6,8,Grafana 8.0 integrates with Prometheus alerting,https://www.zdnet.com/article/grafana-8-0-integrates-with-prometheus-alerting/#ftag=RSSbaffb68,Alerting is finally unified in the latest update of the Grafana open source stack.
2021,6,8,Fitting Support Vector Machines via Quadratic Programming,https://blog.dominodatalab.com/fitting-support-vector-machines-quadratic-programming/,"A deep dive inside Support Vector Machines by deriving a Linear SVM classifier explain its advantages and show the fitting process.
The post Fitting Support Vector Machines via Quadratic Programming appeared first on Data Science Blog by Domino."
2021,6,8,7 Must-Have Data Visualization Skills for Data Analysts,https://www.juiceanalytics.com/writing/7-must-have-data-visualization-skills-for-data-analysts,"When I speak to people new to the analytics field they often wonder what skills will make them successful in their career. For all the data science skills tools and boot camps that are available I still find that the missing link for many data analysts is the ability to communicate and convince after they’ve analyzed data. All your hard work is wasted if it doesn’t spur someone to change. The smart folks at MIT agree:“The skill of data storytelling is removing the noise and focusing people's attention on the key insights.”To be a good data storyteller and communicator of data you need a collection of “soft skills” that are overlooked when people think of data analysis. Here are the 7 skills I think are most important:1. Ask the tough questionsThe common trap for data analysts is to become an order taker. Your boss asks a series of off-the-cuff questions and you run off to answer them — only to find that the answers don’t get to the crux of the issue. The cycle continues.A skilled analyst asks tough questions before they start getting into the data. “What actions could we take if you knew that answer?” Nancy Duarte a leader in business storytelling emphasizes the phrase “therefore we need to…” appended to the results of an analysis. Know the purpose of your work and how it will effect change.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


2. Develop audience understandingAs a data analyst you need to get into the shoes of your audience. What are their priorities? What actions can they take in their role? The more you know about the people who will consume your insights the more you’ll be able to shape them in a way that they are useful. Our Data Personality Profile is one way to build this type of understanding. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


3. Basic visual design skillsWhen you are sharing your analysis you will impress a lot of people if you apply some basic design principles.Learn how to properly use color and contrast in your charts apply our Simple Font Framework to make your text look great and remove distracting visual elements (e.g. chartjunk). You’ll be considered the data artist 👩🏾‍🎨🎨in many organizations. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


4. Edit and simplify to preserve attentionIn your role as a data analyst you will be challenged to get the attention of your manager or peers when you share data. Therefore it is incumbent on you to bring focus and clarity to your message. You’ll want to remove data that is merely interesting and guide attention to the data that is most actionable.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


5. Practice rapid prototypingWhen you first create a report dashboard or data presentation you will inevitably leave some important questions unanswered. Your audience won’t fully understand your message or how to read you data visualizations. That’s ok. Putting data solutions in front of customer and then learning from their reactions is part of the process. Make sure to listen carefully and move quickly to respond.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


6. Gather feedbackDelivering a presentation analysis or report can feel like the finish line. Often it is not. As noted above you may need more cycles of refinement to ensure your hard work is having the impact it deserves.Seek out feedback: Did they understand your metric definitions or the charts you used? Did they interpret the data in the same way as you? The more you ask the more you will learn and grow.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


7. Learn storytelling with dataStorytelling with data is a powerful skill for analysts to connect with their audiences. Our collection of lessons on data storytelling includes guidance around structuring your data stories choosing the right metrics and writing a guided narrative.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;


	Get Your Free Workspace"
2021,6,8,Gradient Descent Optimization With AMSGrad From Scratch,https://machinelearningmastery.com/gradient-descent-optimization-with-amsgrad-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent Optimization With AMSGrad From Scratch appeared first on Machine Learning Mastery."
2021,6,8,Clickless Analytics is the Future of Business User Analytics!,https://www.smarten.com/blog/clickless-analytics-is-the-future-of-business-user-analytics/,What Does a Business Need to Do To Ensure That Users Adopt Analytics? If your business is trying to incorporate data analytics into the fabric of day-to-day work you will need to get your users to adopt analytical tools. The way forward is not all [&#8230;]
2021,6,7,Machine learning at the edge: TinyML is getting big,https://www.zdnet.com/article/machine-learning-at-the-edge-tinyml-is-getting-big/#ftag=RSSbaffb68,Being able to deploy machine learning applications at the edge is the key to unlocking a multi-billion dollar market. TinyML is the art and science of producing machine learning models frugal enough to work at the edge and it's seeing rapid growth.
2021,6,6,Gradient Descent Optimization With AdaMax From Scratch,https://machinelearningmastery.com/gradient-descent-optimization-with-adamax-from-scratch/,"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]
The post Gradient Descent Optimization With AdaMax From Scratch appeared first on Machine Learning Mastery."
2021,6,4,How do you drive exponential growth in the healthcare industry?,https://www.ibm.com/blogs/journey-to-ai/2021/06/how-do-you-drive-exponential-growth-in-the-healthcare-industry/,"The healthcare industry is adapting to changes resulting from the coronavirus pandemic but many complex challenges prevail. How do we anticipate and prevent hospitalization of high-risk patients? How can we reduce the length of stay without compromising quality of care? How do we improve patient experience? How do we obtain the insights needed to drive [&#8230;]
The post How do you drive exponential growth in the healthcare industry? appeared first on Journey to AI Blog."
2021,6,4,Understanding links between water-quality variables and nitrate concentration in freshwater streams using high-frequency sensor data,https://robjhyndman.com/publications/water-quality-gam/,Real time monitoring using in situ sensors is becoming a common approach for measuring water quality within watersheds. High frequency measurements produce big data sets that present opportunities to conduct new analyses for improved understanding of water quality dynamics and more effective management of rivers and streams. Of primary importance is enhancing knowledge of the relationships between nitrate one of the most reactive forms of inorganic nitrogen in the aquatic environment and other water quality variables.
2021,6,4,Gradient Flow Snapshot #57: Monitoring Machine Learning Models; Greykite for Time-series Forecasting,http://practicalquant.blogspot.com/2021/06/gradient-flow-snapshot-57-monitoring.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,6,3,Dremio's Dart initiative further consolidates lake and warehouse paradigms,https://www.zdnet.com/article/dremios-dart-initiative-further-consolidates-lake-and-warehouse-paradigms/#ftag=RSSbaffb68,A variety of improvements and optimizations give Dremio's platform warehouse-league capabilities over file-based data lakes.
2021,6,3,Movie Clip- NSA,https://decisionstats.com/2021/06/03/movie-clip-nsa/,
2021,6,3,STR: Seasonal-Trend decomposition using Regression,https://robjhyndman.com/publications/str/,We propose a new method for decomposing seasonal data: STR (a Seasonal-Trend decomposition using Regression). Unlike other decomposition methods STR allows for multiple seasonal and cyclic components covariates seasonal patterns that may have non-integer periods and seasonality with complex topology. It can be used for time series with any regular time index including hourly daily weekly monthly or quarterly data. It is competitive with existing methods when they exist but tackles many more decomposition problem than other methods allow.
2021,6,3,What is forecasting?,https://robjhyndman.com/hyndsight/assa-video/,
2021,6,3,A Gentle Introduction to Premature Convergence,https://machinelearningmastery.com/premature-convergence/,"Convergence refers to the limit of a process and can be a useful analytical tool when evaluating the expected performance [&#8230;]
The post A Gentle Introduction to Premature Convergence appeared first on Machine Learning Mastery."
2021,6,3,Give Citizen Data Scientists the Tools They Need!,https://www.smarten.com/blog/give-citizen-data-scientists-the-tools-they-need/,Don’t Be Intimidated! Citizen Data Scientists Don’t Need Rocket Science! When you take on the mantle of Citizen Data Scientist there is a lot to process! Much of the reason business users feel overwhelmed at the idea of the Citizen Data Scientist role is the [&#8230;]
2021,6,3,Reinforcement Learning For the Win,http://practicalquant.blogspot.com/2021/06/reinforcement-learning-for-win.html,The Data Exchange Podcast: Nicolas Hohn on challenges and best practices for using RL and machine learning in the enterprise.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS. Nic Hohn is part of an outstanding speaker lineup at the 2021 Ray Summit a FREE virtual conference that brings together developers machine learning practitioners data scientists DevOps and cloud-native architects interested in building scalable data &amp; AI applications.Full show notes can be found&nbsp;on the Data Exchange web site.A video version of this conversation is available&nbsp;on YouTube.
2021,6,2,Amazon DocumentDB adds Global Clusters,https://www.zdnet.com/article/amazon-documentdb-adds-global-clusters/#ftag=RSSbaffb68,AWS’s MongoDB-compatible database service fills a gap with a new capability for cross-region replication for disaster recovery.
2021,6,2,What’s next for Cloudera?,https://www.zdnet.com/article/whats-next-for-cloudera/#ftag=RSSbaffb68,While the exit to private equity was the headline the sleeper could be the pair of technology acquisitions that accompanied the announcement. They provide missing pieces that could transform Cloudera’s data platform into a true self-service SaaS offering.
2021,6,2,Digital Transformation (Dx) Requires Augmented Analytics Technology to Support Business Users!,https://www.smarten.com/blog/digital-transformation-dx-requires-augmented-analytics-technology-to-support-business-users/,If your business has embraced the concept of Digital Transformation (Dx) and Data Literacy it must plan for these initiatives in order to ensure that they are successful. The best way to achieve Dx and data literacy is to establish a technology environment that will [&#8230;]
2021,6,1,Fast forecast reconciliation using linear models,https://robjhyndman.com/publications/lhf/,Forecasting hierarchical or grouped time series usually involves two steps: computing base forecasts and reconciling the forecasts. Base forecasts can be computed by popular time series forecasting methods such as Exponential Smoothing (ETS) and Autoregressive Integrated Moving Average (ARIMA) models. The reconciliation step is a linear process that adjusts the base forecasts to ensure they are coherent. However using ETS or ARIMA for base forecasts can be computationally challenging when there are a large number of series to forecast as each model must be numerically optimized for each series.
2021,6,1,Visualizing probability distributions across bivariate cyclic temporal granularities,https://robjhyndman.com/publications/gravitas/,Deconstructing a time index into time granularities can assist in exploration and automated analysis of large temporal data sets. This paper describes classes of time deconstructions using linear and cyclic time granularities. Linear granularities respect the linear progression of time such as hours days weeks and months. Cyclic granularities can be circular such as hour-of-the-day quasi-circular such as day-of-the-month and aperiodic such as public holidays. The hierarchical structure of granularities creates a nested ordering: hour-of-the-day and second-of-the-minute are single-order-up.
2021,6,1,Member Training: An Introduction into the Grammar of Graphics,https://www.theanalysisfactor.com/grammar-of-graphics-introduction/,"As it has been said a picture is worth a thousand words and so it is with graphics too. A well constructed graph can summarize information collected from tens to hundreds or even thousands of data points. But not every graph has the same power to convey complex information clearly. Underlying every statistical graph is [&#8230;]
The post Member Training: An Introduction into the Grammar of Graphics appeared first on The Analysis Factor."
2021,5,31,Data Cleaning and Exploratory Data Analysis Using the OkCupid Dataset (Part 1),https://data36.com/data-cleaning-and-exploratory-data-analysis-project/,"This article is about dating and data science! Please welcome our guest author Amy Birdee who has done multiple data science hobby projects recently and built a truly...
The post Data Cleaning and Exploratory Data Analysis Using the OkCupid Dataset (Part 1) appeared first on Data36."
2021,5,31,"Google Data Studio Dashboard, Remixed",https://www.juiceanalytics.com/writing/remaking-a-google-data-studio-dashboard,"When Najmah Salam at Notion demonstrated how to build a Google Data Studio dashboard to show email campaign data I had to create the Juicebox version. This dashboard is a common use case for many marketers who want to see how their email efforts are performing and identify what is working.I appreciate that Najmah used a tool they were comfortable with. Here’s how their dashboard came out:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://datastudio.google.com/u/0/reporting/c03801dd-e707-4ca6-a1fe-11daad8d390c/page/FthGC
          
        
      
        
      

    
  


  


Data Studio is eager to fit all the content on one page in the traditional “collage of data visualizations” style.Juicebox offers a different approach more reminiscent of scrollable websites and data journalism. Here’s what I put together:This exercise helps highlight a few things our design team cares about. We want to make presenting data…Easier than a HelloFresh meal kit. We are dead-set on making it dead-simple whether you are calculating measures adding visualizations or laying out your app.More collaborative than Among Us. Google Data Studio shows the data in a largely static view. We want to enable discussions through interaction and exploration. Good data visualization will raise more questions than it answers. Interactivity lets you answer the next level of questions…like Who’s the imposter?!?Better looking than a Telsa Cybertruck. No offense Cybertruck. We want to make it easy to deliver an attractive balanced modern web app not something from a ‘90s Paul Verhoeven film.As portable as an iPhone. What if I could fit 1000 data points in my pocket…or a million. A modern visualization tool should be responsive for viewing on mobile devices. Data Studio is still working on that.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Email Report Dashboard
          
        
      
        
      

    
  


  


To sum up: Juicebox is like HelloFresh prepared in a Ford F150 Lightning while playing Among Us on your phone.


	Get Your Free Workspace

"
2021,5,31,Machine Maintenance Using Smarten Assisted Predictive Modelling!,https://www.smarten.com/blog/machine-maintenance-using-smarten-assisted-predictive-modelling/,1. &#160;Machine Maintenance is always cheaper then downtime! Sooner or later all machines run to fail and monitoring the condition of the machine is crucial for any enterprise as any unplanned downtime can have greater economic impact resulting in reduced productivity and ultimately losing the [&#8230;]
2021,5,28,Oracle Function Returns Two Values,https://www.deep-data-mining.com/2021/05/oracle-function-returns-two-values.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} There is a table in a schema that contains three columns p low and hi. In the table p is the primary key. I want to develop a function to return low and hi based on an input variable p. First I create a type.create or replace type t_low_hi as object ( low number hi number);   Then I create a function that finds low and hi based on p constructs a type object and returns it.   create or replace function f_prob (p_p number)  return t_low_hi is  p_Low number;  p_Hi number;  Str_sql varchar2(2000);  begin  Str_sql := 'Select low hi from t_lookup where p=:1';  Execute immediate str_sql into p_low p_hi using p_p;  return t_low_hi(p_low p_hi);  end;  /  I call the function and retrieve low and hi for p with a value of 0.99.  select x.v.low  x.v.hi from (select f_prob(0.99) v from dual) x;    
2021,5,28,Situational assessment of COVID-19 in Australia,https://robjhyndman.com/publications/covid19b/,
2021,5,28,Liftoff: The Basics of Predictive Model Deployment,https://www.predictiveanalyticsworld.com/blog/liftoff-the-basics-of-predictive-model-deployment/,"By: Eric Siegel Predictive Analytics World This article is based on the transcript of one of 142 videos in Eric Siegel’s online course Machine Learning Leadership and Practice – End-to-End Mastery. Developing a good predictive model with machine learning isn&#8217;t the end of the story &#8212; you also need to use it. Predictions don’t help [&#8230;]
The post Liftoff: The Basics of Predictive Model Deployment appeared first on Predictive Analytics World."
2021,5,28,Gradient Flow Snapshot #56: Airflow + Ray; Data Warehouse → Lakehouse; CSV file → Knowledge Graph,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-56-airflow-ray.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going&nbsp;HERE.
2021,5,27,Databricks ups AI ante with new AutoML engine and feature store,https://www.zdnet.com/article/databricks-ups-ai-ante-with-new-automl-engine-and-feature-store/#ftag=RSSbaffb68,Building on yesterday's announcements around data sharing pipelines and catalog Databricks today shifts focus to AI providing capabilities for data science specialists and non-specialists alike.
2021,5,27,eCommerce Analytics is Mandatory to Business Success!,https://www.smarten.com/blog/ecommerce-analytics-is-mandatory-to-business-success/,Are You Succeeding at eCommerce? If You Don’t Know You NEED Integrated Analytics! When eCommerce and online shopping businesses reflect on their results it can sometimes be difficult to decide on a strategy going forward. You may be able to produce reports and have a [&#8230;]
2021,5,27,How Companies Are Investing in AI Risk and Liability Minimization,http://practicalquant.blogspot.com/2021/05/how-companies-are-investing-in-ai-risk.html,The Data Exchange Podcast: Andrew Burt on the state of AI risk mitigation and responsible AI.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.  Andrew Burt spoke at our very popular virtual event: Responsible AI in Practice. Click HERE to watch it on-demand.  Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,26,"Databricks rolls out data sharing, automated pipelines, data catalog",https://www.zdnet.com/article/databricks-rolls-out-data-sharing-automated-pipelines-data-catalog/#ftag=RSSbaffb68,At its Data + AI Summit Databricks rolls out its new Delta Sharing Delta Live Tables and Unity Catalog initiatives. For now only Delta Sharing is open source as the company looks to fill out its platform with all the bells and whistles.
2021,5,26,A closer look at Google’s spring data and analytics rollouts,https://www.zdnet.com/article/a-closer-look-at-googles-spring-data-and-analytics-rollouts/#ftag=RSSbaffb68,Google is announcing previews of new data analytics services that are focusing on connectivity security and governance. It’s part of an emerging trend among cloud data platform providers for adding connective tissue to connect and extend the reach of their portfolios.
2021,5,26,How to Succeed With AI: Do It Backwards,http://feedproxy.google.com/~r/jtonedm/~3/-Elew1DNyck/,Copyright © 2021 https://jtonedm.com MarketingThe single most critical and most neglected aspect of artificial intelligence (AI) projects is problem definition. All too often teams start with data determine what kind of machine learning (ML)/AI insights they can generate and then go off to find someone in the business who can benefit from it. The result? [&#8230;]
2021,5,26,Data Storytelling Is a Sandwich,https://www.juiceanalytics.com/writing/data-storytelling-is-a-sandwich,"What is made out of layers and becomes more delicious when you put them together?Why…a data story of course! And a sandwich.(This is what happens when you write a blog post after learning your favorite sandwich shop is closing down…we’ll miss you Clawson’s 😫)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Sara Cervera via Unsplash
          
        
      
        
      

    
  


  


What are those layers?An effective data story should have four distinct parts not unlike the layers of a sandwich:The filling: Whether deli meat or peanut butter &amp; jelly the filling is the heart of a sandwich. For data stories that filling is the data visualizations.The foundation bread. This is the bedrock on which the story is built. Big picture numbers (a.k.a. Big-Ass Numbers) that set the stage of the story.The finishing bread. This is the final layer that tells us it is time for action.The condiments and toppings. These are the flavor boosters that connect the filling to the bread. In data stories these flavor boosters are often specific examples call-outs or insights that capture the attention of the audience.As a data storytelling artist your job is to get quality ingredients and assemble them in a logical order. Let’s take a bigger bite of this metaphor…The fillingSometimes people make the mistake of thinking that a lonely data visualization is itself a data story. That’s like saying a slice of ham is a ham sandwich. Sure the data visualization is the star of the show. But it needs to be wrapped in a few more layers to make it complete.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Karo Kujanpaa via Unsplash
          
        
      
        
      

    
  


  


The foundation breadThe foundation of a data story are the key metrics. They are often shown as ‘Big Ass Numbers’ — a summarized simplified powerful statement about what matters (“metrics are the characters of your story”). These BANs have the potential to deliver an impact if you can communicate them in the right way.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Hans Vivek via Unsplash
          
        
      
        
      

    
  


  


The finishing breadWhen you put that slice of bread on the top you know it is time for action. Eating action! Same with a data story. You want to end the story with an action your audience can do something about. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Killer Heat Interactive Tool
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Jordan Nix via Unsplash
          
        
      
        
      

    
  


  


The condiments and toppingsThe bread and fillings fulfill the basic requirements of a sandwich; taking it extraordinary requires condiments and toppings. This is where the sandwich and data storytelling artists distinguish themselves.Specific examples that provide human-sized connections;Narrative text that ties the story together;Design touches with color emojis fonts that make the story irresistible.(By the way here’s a favorite April Fools joke my team played on me a mayo-lover)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox
          
        
      
        
      

    
  


  











  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Eaters Collective via Unsplash
          
        
      
        
      

    
  


  


Of course your data story/sandwich is only as good as the ingredients you use.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            amirali mirhashemian via Unsplash
          
        
      
        
      

    
  


  





	Try Juicebox Free"
2021,5,25,AI Powered Misinformation and Manipulation at Scale #GPT-3,https://www.oreilly.com/radar/ai-powered-misinformation-and-manipulation-at-scale-gpt-3/,OpenAI’s text generating system GPT-3 has captured mainstream attention. GPT-3 is essentially an auto-complete bot whose underlying Machine Learning (ML) model has been trained on vast quantities of text available on the Internet. The output produced from this autocomplete bot can be used to manipulate people on social media and spew political propaganda argue about [&#8230;]
2021,5,25,"Build 2021: Microsoft reveals enhancements to Power BI, Cosmos DB",https://www.zdnet.com/article/microsoft-announces-enhancements-to-power-bi-cosmos-db-at-build/#ftag=RSSbaffb68,Microsoft's annual developer event Build introduces no revolutionary changes on the data and analytics front but there's a long manifest of new evolutionary features and service tiers.
2021,5,21,The Akronomicon: an Extreme-Scale Leaderboard,https://nuit-blanche.blogspot.com/2021/05/the-akronomicon-extreme-scale.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**As larger models seem to be providing more context and more ability for zero-shot learning&nbsp;Julien&nbsp;just created&nbsp;the Akronomicon: an Extreme-Scale Leaderboard featuring the world's largest Machine Learning Models. And yes LightOn is on that board for the moment!&nbsp;Want to contribute? https://github.com/lightonai/akronomicon&nbsp;&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,5,21,"Gradient Flow Snapshot #55: Reinforcement Learning in the Enterprise, Knowledge Graphs in Finance",http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-55-reinforcement.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,20,Superconductive scores $21M Series A funding to sustain growth of its Great Expectations,https://www.zdnet.com/article/superconductive-scores-21m-series-a-funding-to-sustain-growth-of-its-great-expectations-open-source-framework-for-data-quality/#ftag=RSSbaffb68,Ensuring data quality is essential for analytics data science and machine learning. Superconductive's Great Expectations open source framework wants to do for data quality what test-driven development did for software quality
2021,5,20,ML internals: Synthetic Minority Oversampling (SMOTE) Technique,https://blog.dominodatalab.com/smote-oversampling-technique/,"In this article we discuss why fitting models on imbalanced datasets is problematic and how class imbalance is typically addressed. We present the inner workings of the SMOTE algorithm and show a simple &#8220;from scratch&#8221; implementation of SMOTE. We use an artificially constructed imbalance dataset (based on Iris) to generate synthetic observations via our SMOTE [&#8230;]
The post ML internals: Synthetic Minority Oversampling (SMOTE) Technique appeared first on Data Science Blog by Domino."
2021,5,20,The Future of Machine Learning Lies in Better Abstractions,http://practicalquant.blogspot.com/2021/05/the-future-of-machine-learning-lies-in.html,The Data Exchange Podcast: Travis Addair on how higher levels of abstractions enable non-experts to build efficient machine learning models.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.  Travis Addair is part of an outstanding speaker lineup at the 2021 Ray Summit a FREE virtual conference that brings together developers machine learning practitioners data scientists DevOps and cloud-native architects interested in building scalable data &amp; AI applications.Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,19,Keys to Data Fluency: Shared Understanding,https://www.juiceanalytics.com/writing/keys-to-data-fluency-shared-understanding,"Building a Data Fluent organization requires getting everyone on the same page. This common understanding spans everything from cultural expectations to accessing and using data. Below I’ve outlined six areas where creating alignment will have long-term benefit…and where you can go to get started today.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Photo by Nicole Baster via Unsplash
          
        
      
        
      

    
  


  


&nbsp;Shared ExpectationsLeaders need to define and communicate how they expect data to be used in the organization. As I’ve written time and again it incumbent on executives to set the standard for data culture.Where to get started: The Data Lodge provides guidance and training to help data leaders build their organizational culture and capabilities.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared SkillsYour team needs the know-how to understand analyze and communicate data. These skills are not always a prevalent as we’d like.Where to get started: DataLiteracy.com provides a growing collection of courses for data skills.Where else to get started: Quanthub offers an adaptive educational platform focused on building data skills in your organization.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Definitions and TerminologyYou want everyone in your organization to know what is meant by the data being shared. Who qualifies as a lead? What is an active customer? How is revenue calculated? Without arriving at shared definitions and terminology your data discussion will get stuck in fruitless debates.Where to get started: There are many high-tech Master Data Management solutions…not the place to start. Create a shared document where you define: 1) how your key data/metrics are calculated; 2) where this data comes from; 3) how this data might be improved. Link to it when you present data in a dashboard report or data story.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Data AccessIncreasingly we’ve run into IT teams who have found a way out of the endless back-and-forth data requirements cycle. Instead they generate frequently-updated and thoroughly-tested tables for analysis. Now business users can have the flexibility to create with less risk of misinterpreting data.Where to get started: One of the best tools we’ve found for this is Keboola a flexible platform for connecting to data ETL and providing a data catalog.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Shared Metrics and GoalsData fluent organizations have a shared set of key metrics and can explain how these metrics link to organizational goals.Where to get started: We  appreciate the simplicity and clarity of Matt Lerner’s Metrics that Matter. His framework provides a roadmap for defining your most important metrics.Where else to get started: TeamOnUp provides guidance and software for aligning around shared goals and defining clear responsibility.    Matt Lerner | Metrics that Matter | BoS USA Online 2020  from Business of Software Conference Shared Data ProductsA data fluent organization leans on a curated set of data products — dashboards reports presentations — for focus and insight.Where to get started: Juicebox is a lightweight and versatile solution for business users to create reports presentations dashboards and data stories. It is designed to make creating and sharing easier than anything else on the planet.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


"
2021,5,19,What is a Chi-Square Test?,https://www.theanalysisfactor.com/what-is-a-chi-square-test/,"Just about everyone who does any data analysis has used a chi-square test. Probably because there are quite a few of them and they’re all useful. But it gets confusing because very often you’ll just hear them called “Chi-Square test” without their full formal name. And without that context it’s hard to tell exactly what [&#8230;]
The post What is a Chi-Square Test? appeared first on The Analysis Factor."
2021,5,17,Monash Time Series Forecasting Archive,https://robjhyndman.com/publications/monash-forecasting-data/,Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets.
2021,5,17,Data Storytelling and Photo Composition,https://www.juiceanalytics.com/writing/data-storytelling-and-photo-composition,"Photographers are intentional and often instinctual about how they arrange elements to capture and hold the attention of their viewers. Composition is the term for the art of composing an image through framing. While there are “rules of thumb” for how to compose photographs (or any visual expression) visual artists would just as quickly say that great composition is beyond rules subjective and a natural ability. This recognition in real life of a rhythm of surfaces lines and values is for me the essence of photography; composition should be a constant of preoccupation being a simultaneous coalition—an organic coordination of visual elements. —Henri Cartier-BressonData Storytelling is of course a visual medium. And as a new medium we are well served to build on the shoulders of giants. What are a few lessons we can draw from photography to compose our data stories? Here are a few:Leading LinesIn photographs: “The eye of the viewer will make its way through the frame of the photograph. The path is not always predicable but how you arrange objects in the photograph or how you frame the scene can serve as a guide for the eye’s (hopefully) pleasing journey through your image—a journey that allows the viewer to understand the meaning of your photograph.” Todd Vorenkamp








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Image by&nbsp;Pierre Metivier.
          
        
      
        
      

    
  


  


In data stories: As an author your role is to guide the reader through the information. You can help people navigate through pointers leading text and numbering steps. Give them the signposts to know where to start and where to go next.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Foreground the SubjectIn photographs: Photos are inherently two-dimensional. By composing a distinct foreground and background the artist is able to create depth and interest.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: It is critical to set the context of your story up-front (foregrounding) then draw your reader into the depths of the story gradually. In this example a data story about Cicadas grabs attention with a title image before leading into the data exploration.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Balance Symmetry and PatternsIn photographs: People are naturally drawn to patterns symmetry and balance. It provides a sense of harmony that allows the viewer to linger on an image.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: The analytical tool on the right provides a sense of balance by centering the text and providing the same number of choices for the two chart axes. For analytical interfaces providing a balanced UI can alleviate some of the user’s inherent fear of complexity. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Lessons on Data Storytelling
          
        
      
        
      

    
  


  


SimplicityIn photographs: “Cut out all unnecessary details to keep keep the viewer's attention focused on the subject.” Photographymad








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Image by Hien Nguyen.
          
        
      
        
      

    
  


  


In data stories: Focus on the core message or key question you want your audience to understand. Remove data and content that will distract from this message.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Top 20 Best Data Storytelling Examples
          
        
      
        
      

    
  


  


Negative SpaceIn photographs: By giving visual “breathing room” around the main subject you make it easier for the viewer to engage with the subject.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            https://petapixel.com/2016/09/14/20-composition-techniques-will-improve-photos/
          
        
      
        
      

    
  


  


In data stories: Traditional dashboards and report attempt to fill-up the entire screen to show as much data as possible at once. Providing negative space will give your readers the opportunity to see what is most important and focus their attention. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Lessons on Data Storytelling
          
        
      
        
      

    
  


  


&nbsp;


	Try Juicebox Free"
2021,5,16,Time series cross-validation using fable,https://robjhyndman.com/hyndsight/tscv-fable/,"Time series cross-validation is handled in the fable package using the stretch_tsibble() function to generate the data folds. In this post I will give two examples of how to use it one without covariates and one with covariates.
Quarterly Australian beer production Here is a simple example using quarterly Australian beer production from 1956 Q1 to 2010 Q2. First we create a data object containing many training sets starting with 3 years (12 observations) and adding one quarter at a time until all data are included."
2021,5,14,Interview with SourceForge: Bringing Data-driven Decisions to a Broad Audience,https://www.juiceanalytics.com/writing/interview-with-sourceforge-bringing-data-driven-decisions-and-communications-to-a-broad-audience,"I recently spoke with the team at SourceForge a leading platform for the distribution and discovery of software solutions. The interview ended up summarizing our journey as a company to transform how people communicate with data. Here’s the transcript:SourceForge: You have said that the challenges faced by the analytics industry are more social than technical. What did you mean by that?We’ve been in the analytics space for nearly two decades. The technology has advanced particularly in advanced analytics but the same problems persist. There is still a lack of engagement with data by many people in organizations. People have discomfort with using data to drive everyday decisions. That stuff doesn’t get solved through more features. And for many leaders there is a feeling of frustration for all the money they have spent on data projects. Where is the payback? How long are they going to have to wait? We can’t climb our way out of these problems by always betting on the machines to do more.For all the advances in artificial intelligence and machine learning it seems to me that the people-side of analytics continues to be neglected. When I say people-side I mean: what skills do everyday information workers need to be successful? How does the culture of an organization need to change to embrace using data? How do we meet people where they are to help them become more data fluent?SourceForge: Given that why have you focused your company on building yet another tool?








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Fair point. It is because creating change in the workplace is often the intersection of new behavior and new easier ways to enable those behaviors. Take Slack and how they transformed workplace communication. Our email inboxes were exploding and adding more features to email clients wasn’t solving the problem. Slack came along and re-thought how to make it easier for people to collaborate in teams.That’s how we think about Juicebox. There needs to be a fresh approach to how people take spreadsheets of data and turn it into something useful. The new approach needs to put people first not by making it more complex or feature bloated.SourceForge: But there are a lot of tools for visualizing data. Why did you feel like the world needed another one?The world certainly doesn’t need another dashboard-creating tool that’s for sure. Nor do we need something to try to replace the visual analytics behemoths Tableau and PowerBI. Those tools are essentially Excel on steroids. More capable. More visual. And more complicated.What these tools don’t focus on is how do we make sure the data gets communicated effectively. That’s the missing link. We sometimes call it the last mile of analytics. What has been missing is a solution that provides an easy accessible bridge between people who work with data and the minds of the decision-makers who should understand that data.With Juicebox we created a solution that is lightweight and accessible to everyone. It is easy to learn easy to get started. The everyday information worker doesn’t want to have to get an advanced certification to be able to visualize present and share data in their organization. They need something radically simpler. But also something radically more powerful than the Excel and PowerPoint that they are currently using to present data.That’s where Juicebox fits in. We experienced first-hand the frustration people feel. We set out to deliver a better mousetrap for communicating data.SourceForge: Let’s talk about those people. What have they struggled with and how does Juicebox help them?ZG: I believe there is a silent majority of people in the workplace who want to do more with data but don’t yet have the skills or tools.Think of it like all the want-to-be cooks who admire recipes online but find it too much effort to gather all the ingredients and learn to make the meal. For these people meal prep solutions came along like Blue Apron and HelloFresh. Suddenly anyone could whip up a darn good at-home meal. What did it take? It took some guidance some simplifying of the recipes and more convenience.It’s exactly the same for data in the workplace. Sure people have Excel and maybe access to a powerful analytics BI platform…but that doesn’t make it easy. With Juicebox we want to make it easy for these people to whip up something delicious with their data.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;SourceForge: You are also the author of a book called ‘Data Fluency’ in which you present a path toward more effective use of data in organizations. How does your product fit into this framework?We wrote that book because it was clear that many organizations were struggling to really unlock the power of their data. I’m not talking about hiring more data scientists or applying machine learning models. They just want to know what is most important define key metrics see trends and find insights they could act on. It is the world of small data that still has so much untapped potential. We saw that the issues were about mindset and skillset not technology.In our book we propose four pillars that an organization needs to build to become data fluent. The pillars are: data consumers that are data literate; data authors that know how to communicate effectively; an organizational data-driven culture; and an ecosystem for designing and sharing what we call data products.Juicebox is a key that can help unlock some of these challenges. It gives data authors the most user-friendly solution for communicating and it serves as an integral part of a data product ecosystem. More than ever we believe that we need to make data a medium for communication. Juicebox is one piece of the puzzle.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;&nbsp;


	Try Juicebox Free"
2021,5,14,Gradient Flow Snapshot #54: NLP Index and Getting Read for New AI Regulations,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-54-nlp-index-and.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,13,Why You Should Optimize Your Deep Learning Inference Platform,http://practicalquant.blogspot.com/2021/05/why-you-should-optimize-your-deep.html,The Data Exchange Podcast: Yonatan Geifman and Ran El-Yaniv on the benefits that accrue from using an inference acceleration platform.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,5,11,Quantile forecasting with ensembles and combinations,https://robjhyndman.com/publications/quantile-ensembles/,
2021,5,11,Missing Data Mechanisms: A Primer,https://www.theanalysisfactor.com/causes-of-missing-data/,"Missing data are a widespread problem as most researchers can attest. Whether data are from surveys experiments or secondary sources missing data abounds. But what&#8217;s the impact on the results of statistical analysis? That depends on two things: the mechanism that led the data to be missing and the way in which the data analyst [&#8230;]
The post Missing Data Mechanisms: A Primer appeared first on The Analysis Factor."
2021,5,10,W. Edwards Deming’s Contributions to the Practice of Analytics,https://analyticstrategy.com/demings-contributions-to-the-practice-of-analytics/?utm_source=rss&utm_medium=rss&utm_campaign=demings-contributions-to-the-practice-of-analytics,"How Deming integrated deep technical knowledge about statistical quality control good practical knowledge of how to implement statistical quality control and a great ability to teach.
The post W. Edwards Deming&#8217;s Contributions to the Practice of Analytics appeared first on Analytic Strategy Partners."
2021,5,10,Junior Data Scientist Job Interview Questions (and How to Answer Them),https://data36.com/junior-data-scientist-job-interview-questions-answers/,"In this article I&#8217;ll show you a few junior data scientist job interview questions… And also how to answer them. Before we get started&#8230; This article is part...
The post Junior Data Scientist Job Interview Questions (and How to Answer Them) appeared first on Data36."
2021,5,10,Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality,https://robjhyndman.com/publications/global-forecasting/,Forecasting of groups of time series (e.g. demand for multiple products offered by a retailer server loads within a data center or the number of completed ride shares in zones within a city) can be approached locally by considering each time series as a separate regression task and fitting a function to each or globally by fitting a single function to all time series in the set. While global methods can outperform local for groups composed of similar time series recent empirical evidence shows surprisingly good performance on heterogeneous groups.
2021,5,7,14 Best Data Storytelling Tools 2021,https://www.juiceanalytics.com/writing/best-data-storytelling-solutions,"Data storytelling is quickly becoming a popular mode for presenting data. It combines text and graphics with data visualizations to guide an audience. Traditionally people have used tools like PowerPoint and Excel as well as traditional dashboard and business intelligence platforms to communicate in this way. But these solutions are limited in their ability to balance the explanatory and exploratory elements of an effective data story. We are seeing a new category of tool emerge: the data storytelling platform. It emphasizes features such as human-friendly visualizations integration of text and visuals narrative flow connected stories easy-to-learn authoring and effortless sharing.The demand for better data storytelling is being met by a growing collection of data storytelling tools. We evaluated tools that resembled the description above leaving out more technical tools visualization libraries and old-school dashboard/report tools. In the end we identified four unique categories:Guided AnalysisStand-alone VisualizationsData Storytelling as a FeatureDesign over DataStories with WordsGuided AnalysisThese solutions combine exploratory data visualization with explanatory text and graphical elements. The interactive data storytelling applications created by these platforms are intended as an alternative to traditional dashboards and reports.JuiceboxJuicebox combines modern data journalism style with exploratory visualizations that are automatically connected to enable analysis. A focus on easy authoring makes Juicebox the only tool in this category that is accessible to non-technical or non-analyst users. In their words: Create beautiful data visualizations that make you look like a proStrengths: Lightweight easy editing professional web design automatically connected visualizations.Cost: Free plan (up to 3 users). Team plan is $49/month for 5 editors 15 viewers.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Toucan TocoToucan Toco is one of the earliest solutions for data storytelling. This platform targets enterprise buyers and has a unique approach to presenting data stories. Sharing annotation and drill-in story views give you a chance to communicate a comprehensive overview of a topic.In their words: Communicate actionable insights at scale using Toucan’s built-in no-code framework for storytelling.Strengths: Dashboard-style layout; user management features; sharing via presentation-mode for sharing.Cost: Annual subscription. Reach out for a quote.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


NugitNugit has flown under-the-radar for some customers but represents one of the most complete data storytelling solutions on the market. Attractive design combined with powerful text features make this a solution worth watching.In their words: A better way to share data with colleagues and customers. Automated tools for creating data stories on web and email.Strengths: Live API integrations report/email automation automated natural language generation infographic-style graphics.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Stand-alone VisualizationThese visualization solutions offer flexibility and beautiful design to build individual visualizations. The end-product is generally intended to be embedded in a webpage often as part of an online article.FlourishFlourish has built a loyal customer base by delivering creative and beautifully-designed visualizations. They are well-known for their racing bar-chart but have many other visual options.In their words: Easily turn your data into stunning charts maps and interactive stories.Strengths: Animated visualization easy embedding fine-grain configuration of visualiations. Cost: Free tier. Paid plans start at $69/mo.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


RAWGraphsRAWGraphs is one of the quickest easiest ways to create advanced visualizations. An open source project with a long history this tool provides a simple step-by-step process to create downloadable images for embedding in webpages.In their words: The missing link between spreadsheets and data visualization.Strengths: Open source lightweight editing advanced visualizations data doesn’t leave your browser.Cost: Free.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DatawrapperDatawrapper is a popular tool for data journalist around the world. With a collection of attractive visualizations and advanced maps Datawrapper gives you the configuration flexibility to craft the precise visual you need.In their words: Serving charts and maps for millions of readers every day. Datawrapper helps some of the world’s best teams to tell their stories with data.Strengths: Maps chart configuration options labeling features scaling for millions of views.Cost: Free plan. Pro plan $599/month.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Data Storytelling as a FeatureThis set of solutions are comprehensive business intelligence and visual analytics platforms. Data storytelling is presented as a feature or technique that can be accomplished within the larger platform.Tableau Story PointsTableau a leader in visual analytics saw the potential for data storytelling early on. They released a feature called ‘Story Points’ in 2014. The feature has not achieved wide-adoption among their customer base and Tableau appears to be focusing on PowerPoint export options instead.In their words: Story Points is a way to build a narrative from data. People tend to understand and remember concepts through stories. Story Points gives anyone the tools to create a narrative with data.Strengths: Wide-adoption of Tableau; powerful data manipulation and visualization tools.Cost: $70/editor/month








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Qlik Sense Stories ﻿Qlik Sense is a well-established analytics platform with strong visualization capabilities. While it gets less press than its competitors Tableau and PowerBI Qlik understands the need to reach broader audiences in the enterprise through data storytelling.In their words: The purpose of data storytelling is to turn data discoveries into a story. Emphasizing important elements helps create convincing stories and supports stakeholders in decision-making.Strengths: Powerful querying technology enables rapid analysis.Cost: $30/user/month








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PowerBIPowerBI is Microsoft’s answer to the success of visual analytics powerhouse Tableau. Like the other solutions in this category PowerBI provides guidance features and instruction around data storytelling without providing a focused solution for users.In their words: The job of a data analyst is not just technical. It entails more than just transforming data into information. It is also about clearly communicating the key messages derived from this data.Strengths: Comprehensive BI platform; integrations with deeply-adopted technologies.Cost: Pro starts at $10/user/month.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Design over DataThese solutions for designers are focused on creating infographics and presentations that may include charts and graphs as part of the document. The data is one of many media elements that tell the story.InfogramInfogram is a flexible design platform that includes capabilities for adding lightweight charts. It offers an array of formats for presenting information including everything from dashboards and reports to social media posts and posters.In their words: Create engaging infographics and reports in minutesStrengths: Consistent branding pre-defined templates animations output formatsCost: Free plan. Pro starts at $25/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


VismeWhether you want to create infographics posters social media graphics or even videos Visme is a designer’s toolbox. Like the other design-first tools charts are intended to show a few data points rather than to enable analysis.In their words: Create visual brand experiences for your business whether you are a seasoned designer or a total novice.Strengths: A vast collection of icons and widgets; 1000s of templates.Cost: Free plan. Pro starts at $25/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PiktochartPiktochart is a design tool for building infographics posters flyers social media graphics and presentations. Data seems to be mostly an afterthought for a solution that focuses on brand styling and templates.In their words: Improve your internal and external communication with Piktochart. Quickly turn any text- or data-heavy content into a visual story that your audience will love.Strengths: Colors and branding video stories.Cost: Free plan. Pro starts at $29/month/user








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Stories with WordsThese solutions focus on using words as the primary way to convey the story in the data. Their algorithms identify insights in the data and present those insights in sentences and bullet points.SiSense NarrativesSiSense is a traditional business intelligence and dashboard solution that has added narrative capabilities.In their words: With Sisense Narratives we use natural language generation (NLG) to automatically present you with calculations and insights in plain easy to understand language based on what the engine recognizes as interesting.Strengths: Integrated as part of a complete BI solution.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Lexio by Narrative ScienceNarrative Science has been a leader for years in extracting analytical insights and presenting results as text. They believe that pre-defined dashboards should be replaced by automatically-generated text stories.In their words: No more dashboards. Data should be easy to use and understand. Start data storytelling with Lexio.Strengths: Machine learning to analyze your data without the need for analysts.Cost: Not available.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


"
2021,5,7,Gradient Flow Snapshot #53: Data Validation for Machine Learning; Modernizing Data Governance,http://practicalquant.blogspot.com/2021/05/gradient-flow-snapshot-53-data.html,Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,5,6,Forecasting elements that stand the test of time,https://robjhyndman.com/seminars/lokadtv/,"Interview for Lokad TV Lokad is a supply chain software company based in Paris. They have a TV channel on which they discuss supply chain issues.
Recently I was interviewed on Lokad TV discussing my R packages for forecasting."
2021,5,6,AI Beyond Automation,http://practicalquant.blogspot.com/2021/05/ai-beyond-automation.html,The Data Exchange Podcast: Jerry Overton on building an Artificial Intelligence Center of Excellence to incubate serious AI talent.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS  Download the 2021 Business At The Speed Of AI Report and learn how leading companies are using and implementing data and machine learning technologies. Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.  
2021,5,5,Approximate Algorithms for High Utility Itemset Mining,https://data-mining.philippe-fournier-viger.com/approximate-algorithms-for-high-utility-itemset-mining/,On this blog I have previously given an introduction to a popular data mining task called high utility itemset mining. Put simply this task aims at finding all the sets of values (items) that have a high importance in a &#8230; Continue reading &#8594;
2021,5,3,11 Best Data Storytelling Courses 2021,https://www.juiceanalytics.com/writing/best-data-storytelling-courses-2021,"Are you looking to upgrade your Data Storytelling skills? There are many options for learning including this list of best data storytelling workshops grabbing our free data storytelling lessons and absorbing the lessons of masters from 20 amazing data storytelling examples.If you are looking for a packaged course that will teach you about data visualization narrative and engaging your audience we’ve tracked down some of the best options. In our search we wanted to find solutions that were accessible to everyone delivered by an experienced instructor and did not focus on a particular piece of software.Story IQCourse: Data Storytelling for Business provides learners with a solid grounding in fundamental data storytelling learning concepts. By the end of the course learners will have the skills needed to produce impactful data visualizations layered with compelling narratives.Access: Self-paced onlineInstructor: StoryIQ is a small group of training professionals focused on hands-on practical teaching for a business audience.Cost: Starts at $99








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Bill ShandlerCourse Data Storytelling &amp; Visualization Workshop teaches the art and science the practical hands-on tactics of creating compelling communication experiences.Access: By RequestInstructor: Bill Shandler has been “working with clients on information design and data storytelling/visualization projects for over 25 years. This includes governments NGOs and commercial enterprises across just about every industry.”Cost: N/A








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Juice AnalyticsCourse: Data Storytelling Lessons. With more than 20 short lessons this course provides a complete overview of the skills tips and tricks required to become a data storyteller. The hands-on interactive lessons are self-paced and take 5-10 minutes to complete.Access: On-demand self-pacedInstructor: Zach Gemignani has spent 15 years helping organizations design and develop interactive analytical applications presentations and data stories. He is author of the book Data Fluency Empowering Your Organization with Effective Data Communication and has guided the development of a leading data storytelling platform Juicebox.Cost: Free








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


MIT Executive EducationCourse: Persuading with Data — highly practical and collaborative this course combines visualization and strategic communication best practices to help you communicate data more effectively and influence others to take action based on data through data storytelling.Access: Live Online on specific dates.Instructor: Miro Kazakoff is a Senior Lecturer in Managerial Communication at the MIT Sloan School of Management where he focuses on how individuals use data to persuade others.Cost: $4300








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Data Story AcademyCourse: Data Story Academy is a three-part framework built for business professionals providing the tools they need to grow their career and access to frameworks that virtually guarantee success in doing more with data.Access: On-demandInstructor: Zack Mazzoncini has helped hundreds of organizations and individuals develop data-driven cultures centered around data storytelling. Zack's personal mission statement is: ""Change people's lives for the better by being the best version of myself"".Cost: $697








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Plural SightCourse: Data Storytelling: Moving Beyond Static Data Visualizations. Learn how to package a data story for different mediums and audiences and how to craft a data story by defining your audience and end goals. Explore how to create animations and motion graphics to present an impactful moment.Access: On-demandInstructor: Troy Kranendonk is a Curriculum Manager for Data Access and Analytics as well as an author with Pluralsight. He considers himself to be a Pixel Ninja.Cost: $199-299 per year (Plural Sight subscription)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Knight CenterCourse: Data Visualization for Storytelling and Discovery. The four-week course which was powered by Google took place from June 11 to July 8 2018. We are now making the content free and available to students who took the course and anyone else who is interested in learning how to create data visualizations to improve their reporting and storytelling.Access: On-demandInstructor: Alberto Cairo is an information designer and professor. Cairo is the Knight Chair in Visual Journalism at the School of Communication of the University of Miami.Cost: Free








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


LinkedIn LearningCourse: Telling Stories with Data. The same techniques that are used to tell stories with words—structure conflict resolution emotion and surprise—can be used with data. You can craft compelling narratives that help audiences visualize information without complex charts or graphs.Access: On-demandInstructor: Paul A. Smith is author of the best-selling book Sell with a Story: How to Capture Attention Build Trust and Close the Sale.Cost: $30/month (for LinkedIn Learning)








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


PurdueCourse: Data Storytelling 101 offers an introduction to the concept of Data Storytelling why it matters and how it can transform the results of your research into impactful narratives from which your audience learns new things remembers important findings and acts on them.Access: OnlineInstructor: Sorin Adam Matei Data Storytelling Program Director and Associate Dean of Research Purdue University. Cost: Free. Additional paid courses are $350-$1000








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


BI BrainzCourse: Master BI Data Storytelling. An online course that will teach you how to easily setup build and design your first compelling data story!Access: On-demandInstructor: Mico Yuk Founder of BI Brainz and creator of the Analytics on Fire Podcast.Cost: $497-$697








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;Udemy (multiple courses)Tell a Story with Data. Learn how to pique and keep your audience's attention so they will understand and remember your data presentation. The instructor Mike X Cohen is a neuroscientist and associate professor at the Radboud University in the Netherlands.Data Storytelling and Data Visualization. You'll learn the skills that make up the entire art of speaking the language of data: from communicating with data to creating impactful data visualizations to storytelling with data to driving action with data-driven decisions and finally to creating stunning communications that will leave a lasting impression on an audience and get results. The instructor Joshua Brindley says: “I love telling great stories with data and driving results with visualization that resonate with an audience.”Access: On-demandCost: $14-18








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  





	Try Juicebox Free"
2021,5,3,Member Training: Writing Study Design and Statistical Analysis Plans,https://www.theanalysisfactor.com/study-design-statistical-analysis-plans/,"One component often overlooked in the ‘Define &#38; Design’ phase of a study is writing the analysis plan. The statistical analysis plan integrates a lot of information about the study including the research question study design variables and data used and the type of statistical analysis that will be conducted. In this training we consider [&#8230;]
The post Member Training: Writing Study Design and Statistical Analysis Plans appeared first on The Analysis Factor."
2021,5,1,The Art of Data Storytelling,https://www.juiceanalytics.com/writing/the-art-of-data-storytelling-pixar-style,"Data Storytelling is a powerful way to present data in ways that influence your audience. It is a skill that combines elements of artistic expression and structured methods. In this article we will start by learning from the mindset of a leading storytelling organization Pixar. Then we will discuss how to structure data stories to guide your audience through data.Part 1: Lessons in Data Storytelling from PixarPixar is the gold standard in storytelling. With their 17 feature films they’ve redefined how to create animated worlds and compelling characters.What if you could know the secrets of Pixar’s storytelling success? Now you can. Pixar announced recently that they would team up with Khan Academy to deliver free lessons on how they deliver storytelling magic.We’re long-time fans of Pixar at Juice because their methods don’t just apply to good storytelling but to good data storytelling as well.Pixar’s lessons offer some great principles that can be used to improve the narratives you create with your data. We’ve pulled out some of the best tips that can be applied to data storytelling:1. On Asking the “What If” QuestionAlthough our movies involve hundreds of people and take years to make they all begin with a simple idea about some world and character. What if there’s life out there in the universe? What if a rat wanted to cook haute cuisine? What if our toys that are all around us actually were sentient and can come alive? These what-if questions invite the imagination into a story we want to explore.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


“The best ‘what ifs’ are questions that sort of feel like a key that unlocks the door.”Asking “what if” is a great question to ask yourself when you’re first deciding what direction you want to take your narrative. Not only does asking this question guide how you structure your story but it also allows you to determine what information is most important to your audience and what can be left on the cutting room floor. It can be easy to overwhelm with data; narrowing your focus is the greatest favor you can offer your audience.Here are some “What If” questions you could apply to your data storytelling:What If my sales team knew exactly which prospects needed the most attention today?What If nurses could tell which patients were at risk for sepsis?What If human resources leaders could explore the complexity of their organization in the same way they explore Google maps zooming out to see how all the parts connect and zooming in to see what’s going on on the ground?What If teachers could visually see how each of their students was doing on their learning journey and quickly identify the knowledge gaps and resources to fill those gaps?2. On WorldA ‘what if’ statement is ultimately connected to a world and a character… When we say ‘rule’ what we really mean is the environment or set of rules in which our story will take place.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Choosing your world can be most closely associated with the effort to set the context in our data stories. It’s important to ground the audience in the “world” before you start introducing the “characters” - your data. In our data apps we are careful to set the stage for the audience by explaining the purpose and context before thrusting users into a series of charts.3. On Flawed CharactersEntertaining characters are often deeply flawed...these flaws can also be the key to why audiences care about them.This lesson reminds us of “flawed” data points. Often the outliers and the unexpected data points are the most interesting. Sure they don’t tell the whole story but they definitely give more insight into what’s actually happening with your data and can provide some colorful detail in your data story.4. On Fully-Developed CharactersWe call these characters fully developed. This means we’ve gotten to know them so well that we can imagine them in almost any situation.Providing full context around the characters in your data allows you to be able to look at your data points from multiple perspectives and draw out three-dimensional insights an important step in data storytelling.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


5. On Behavioral CharacteristicsWe can talk about characters in two ways. They have external features which are their design their clothes what they look like. Then much more interesting is the internal features. Are they insecure are they brave are they jealous?With this lesson the distinction between descriptive and behavioral data comes to mind. For example we can look at descriptive data about customers but really the behavioral data is much more interesting. How do customers react to stimuli? That’s where the real story is.6. On Authentic ExperiencesCharacters have to come from authentic human emotions and experiences.When working constantly with numbers it’s easy to sometimes forget that behind each data point is a living breathing person. How do you connect the data to the actual real-life actions that are taking place to create that data? One of the best ways to do this is by bringing specific examples into your data story. As John Hodgman likes to say: “Specificity is the soul of narrative.”We created a complete lesson in our Data Storytelling curriculum about making data relatable and specific to help connect to your audience.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


7. On Story FlowWhat happens when I tell the story to another person is that these other things show up without me asking for them even while I’m telling them. The story starts to come alive. The characters start to come alive. And then also the person you told the story to will tell you what they thought of it notes they’re free. They actually are helping you make your story and characters better.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


This is very true of data storytelling. The more you run through the story flow with users the more insight you receive into how they think about the data they are seeing and what they need to know. Based on this feedback you can adapt and change the way you present your data to make a better overall experience for your users.These lessons from Pixar give us some of the mindset it takes to tell a data story. Now let’s look at the structure that provides a strong architecture for your data story.Part 2: Data Story StructureWhile traditional storytelling and data storytelling are not identical mediums there is quite a bit of overlap between the two and many of the best practices for one can be applied to the other. Take for example the idea of structure when it comes to storytelling. Structure or in simpler terms “what do you want the audience to know and when?” is hugely important when it comes to the practice of data storytelling.It may seem counterintuitive to consider modeling your data presentations after traditional storytelling structure. After all storytelling is an inherently subjective act. The storyteller is crafting something that helps the audience learn about a theme that the storyteller finds important and consequently a moral that should be learned. Applying this to data can seem like enemy territory for analysts who feel that their job in presenting data is to “let the data tell the story.” It’s important to note however that the data doesn’t have an opinion on what is important. For example I was speaking to an HR Analytics team recently and it was clear to me that they wanted to use data to share important lessons with the business. It was less clear that they felt empowered to do so because they felt the data should speak for itself. Data often needs a voice to give it meaning.When creating the structure of your data stories keep in mind that it often takes a while to get to the structure that works best for what you are trying to accomplish. That is why it is important to create something ‒ even in a rough form ‒ and get it in front of people who will give you feedback. Does it resonate and connect with the audience ‒ or is it more like the unpopular original structure of Finding Nemo? Without this knowledge you’re more lost than Dory and Marlin ever were.&nbsp;Dive deeper into a lesson on story structure from our collection of Data Storytelling Lessons.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Story Beats &amp; Story SpineAn effective way of organizing story structure is by utilizing story beats the most important moments in your story and story spine a pattern into which most stories can fit. While your data story most likely won’t open with “once upon a time…” and end with “and ever since then…” the lesson can still be applied. Using a structure that is broadly familiar to audiences and hitting familiar story beats will help ensure that a data story leverages the hooks that storytelling already has in people.Your audience is looking for certain things in a data story just like they would in a Pixar film. Who or what are the key players? What’s the conflict? How can it be resolved? Utilizing these when appropriate will make your data stories much more effective. The traditional 3-part play provides a template for designing data stories.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Act 1The first act of a film serves to introduce the audience to a protagonist establish the setting provide information into how the characters’ world works and introduce an obstacle that sets the rest of the story in motion.In traditional dashboards and reports this information is often missing and leads to users not knowing where to start. If your audience is going to go on a data adventure with you they should start off by caring about the situation that exists. Data stories should start with a high-level summary that then lets users progressively and logically drill into more complex details and context.Act 2Pixar states that the second act of a story as “a series of progressive complications.” My favorite way of describing act two is “the part of the story in which you throw rocks at your characters.” Either way what happens in the next part of your data story is clear: addressing conflict.When it comes to data stories act two is the back-and-forth exploration of the problem. In the traditional story spine they refer to it as “because of that…”; for analytics we call it “slicing-and-dicing.” Throughout act two of your data story you are showing your audience the drivers of problems and identify any outliers.Act 3In traditional storytelling the third act is the part of the story where the main character learns what she truly needs — as opposed to what she thought she wanted. The character has gone on a transformation along the course of the story and that is evidenced in the final act.This is much harder to pull off in data storytelling. In data storytelling I believe the protagonist is the audience. Much like the main character the audience needs to be transformed and understand something new and important. A satisfying story is when a problem is fixed and the world is set right in some way. Great data stories deliver that change -- but to do so they need to do more than change the audience’s perspective. They need to make the audience act on not just discuss this transformation.Work BackwardThe best bit of advice from the Pixar storytellers is simple: work backward. This is how we do it at Juice: we consider what is the endpoint the change or impact that we want to make on the audience and then craft the story that can help get us there.


	Try Juicebox Free"
2021,4,30,Gradient Flow Snapshot #52: Data Integration; Language Benchmarks; Online Resource Allocation with Ray,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-52-data.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,4,29,The Future of Data Science – Mining GTC 2021 for Trends,https://blog.dominodatalab.com/the-future-of-data-science-mining-gtc-2021-for-trends/,"Deep learning enthusiasts are increasingly putting NVIDIA’s GTC at the top of their gotta-be-there conference list. I enjoyed mining this year’s talks for trends that foreshadow where our industry is headed. Three of them were particularly compelling and inspired a new point of view on transfer learning that I feel is important for analytical practitioners [&#8230;]
The post The Future of Data Science &#8211; Mining GTC 2021 for Trends appeared first on Data Science Blog by Domino."
2021,4,29,Injecting Software Engineering Practices and Rigor into Data Governance,http://practicalquant.blogspot.com/2021/04/injecting-software-engineering.html,The Data Exchange Podcast: Steve Touw on why data governance needs to go from the boardroom into code.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.   Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube.
2021,4,28,A Review of Data Storytelling Solutions,https://www.juiceanalytics.com/writing/a-review-of-data-storytelling-solutions,"If you are in the data or analytics industry it is worth getting to know Ted Cuzzillo. He’s been following our industry since 2007 was covering Tableau as an industry analyst before anyone else and wrote for esteemed publications TDWI and Information Management.He knows the analytics landscape. And he’s always looking for what’s next.His new venture DataDoodle is explicit in where he has set his sights: he is exploring nascent trends and vendors in the area of smarter cities and data narrative.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


I had the opportunity to share what we’ve learned about data narrative (i.e. data storytelling) in the form of our own emerging solution Juicebox.As part of the launch of his new DataDoodle site Ted posted an article entitled Two recent “storytelling” tools for public audiences.The article begins by highlighting the value of data storytelling — particularly as it relates to municipalities but for all organizations. He profiles both Juicebox and our esprits apparentés Toucan Toco using a delightful cooking analogy. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


A few delicious morsels:Comparing presentation of data to serving a meal:The Juice Analytics’s product Juicebox is more like dinner at home with Blue Apron or other meal kit service.We’ve always taken a people-first approach to technology. It was encouraging to hear Ted see that focus in the product:Juicebox makes data consumption easy while it prods the data-shy into gradual self-confidence…Juicebox’s active approach grows analysts. A certain portion of its users will no doubt sprout legs such as when they see that cooking by number is just a short leap to cooking by touch smell and sizzle. They may have not quite as satisfying a dinner the first few times they try that but in the end they’re better cooks.Finally in comparison to Toucan Toco which has committed to a more centralized controlled approach Ted’s verdict is thatThe enterprise that wants to breed deep enterprise-wide intelligence will prefer Juicebox."
2021,4,28,Virtual Workshop: Conceptual Understanding of Deep Learning (May 17th 9am-4pm PST),https://nuit-blanche.blogspot.com/2021/04/virtual-workshop-conceptual.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**Just got an email from Rina PanigrahyHi IgorI am an algorithms researcher at Google (http://theory.stanford.edu/~rinap) and I am organizing this workshop on ""Conceptual Understanding of Deep Learning"" (details below). It's trying to understand the Brain/Mind as an algorithm from a mathematical/theoretical perspective. I believe that a mathematical/algorithmic approach for understanding the Mind is crucial and very much missing. I'd appreciate any help I can get with advertising this on your blog/mailing-lists/twitter.BestRinaHere is the invite:Please join us for a virtual Google workshop on “Conceptual Understanding of Deep Learning”When: May 17th 9am-4pm PST.Where: Live over YoutubeGoal: How does the Brain/Mind (perhaps even an artificial one) work at an algorithmic level? While deep learning has produced tremendous technological strides in recent decades there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form. The goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning characterizing the class of functions that can be learned coming up with the right learning architecture that may (provably) learn multiple functions concepts and remember them over time as humans do theoretical understanding of language logic RL meta learning and lifelong learning.The speakers and panelists include Turing award winners Geoffrey Hinton Leslie Valiant and Godel Prize winner Christos Papadimitriou (full-details).Panel Discussion: There will also be a panel discussion on the fundamental question of “Is there a mathematical model for the Mind?”. We will explore basic questions such as “Is there a provable algorithm that captures the essential capabilities of the mind?” “How do we remember complex phenomena?” “How is a knowledge graph created automatically?” “How do we learn new concepts function and action hierarchies over time?” and “Why do human decisions seem so interpretable?”Twitter: #ConceptualDLWorkshop.Please help advertise on mailing-lists/blog-posts and Retweet.Hope to see you there!Rina Panigrahy&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2021,4,27,Randomized Algorithms for Scientific Computing (RASC),https://nuit-blanche.blogspot.com/2021/04/randomized-algorithms-for-scientific.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**At LightOn we build photonic hardware that performs random projections and it is nice to find a source of materials on the subject in one document.&nbsp;Here is a report comprehensively presenting how randomized algorithms are key to the future of computing:Randomized Algorithms for Scientific Computing (RASC)&nbsp;by&nbsp;Aydin Buluc Tamara G. Kolda Stefan M. Wild Mihai Anitescu Anthony DeGennaro John Jakeman Chandrika Kamath Ramakrishnan (Ramki)Kannan Miles E. Lopes Per-Gunnar Martinsson Kary Myers Jelani Nelson Juan M. Restrepo C. Seshadhri Draguna Vrabie Brendt Wohlberg Stephen J. Wright Chao Yang Peter ZwartRandomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science astrophysics fusion advanced materials combustion and quantum computing all require randomized algorithms for surmounting challenges of complexity robustness and scalability. This report summarizes the outcomes of that workshop ""Randomized Algorithms for Scientific Computing (RASC)"" held virtually across four days in December 2020 and January 2021.&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2021,4,26,Beautiful Soup Tutorial 2. – How to Scrape Multiple Web Pages,https://data36.com/scrape-multiple-web-pages-beautiful-soup-tutorial/,"Scraping one web page is fun but scraping more web pages is more fun. In this tutorial you’ll learn how to do just that; along the way you’ll...
The post Beautiful Soup Tutorial 2. – How to Scrape Multiple Web Pages appeared first on Data36."
2021,4,25,Keys to Data Fluency: New Decision-Making Behaviors,https://www.juiceanalytics.com/writing/keys-to-data-fluency-data-into-decision-making,"How do you know that your organization is becoming more data fluent? You’ll see new behaviors such as the language people use the things they focus on and the way meetings are run. Data analysis and key performance measures are elevated from after-thought to starting-point. Here are five behaviors that you should start to expect from your team as your data fluency matures:Everyone Knows the Key Metrics — and Debates ThemData fluent organizations have a common understanding of how progress is measured. People at all levels have become familiar with these metrics and think about how their work relates to these numbers.At Juice we have a North Star Metric (the unchanging measure of progress) and three “key drivers” that are the current bottlenecks or leverage-points for improving the North Star Metric. We encourage discussion about how to best measure and what the results imply.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Product Features or Investments Are Weighed Against Key MetricsData fluent organizations evaluate new investments by how they are likely to impact key performance metrics. At Juice our product roadmap is increasingly guided by our quantified understanding of user behaviors. We look for the most frequent blockers to success and consider new features as ways to knock down those walls. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Investment ROI calculator http://labs.juiceanalytics.com/valuation/index.html
          
        
      
        
      

    
  


  


Anecdotes Get TestedWe talk about data storytelling all the time. However individual stories are often just a clue to a pattern — or simply a one-off outlier.Does it happen often? What is the implication? Why did it occur?At Juice we’ve discovered all sorts of user behaviors in Juicebox that we had not anticipated. For example our European users often load CSV (comma delimited files) that are delimited by semi-colons. We discovered this through one user’s story but then validated the frequency through data.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            via Saturday Morning Breakfast Cereal https://www.smbc-comics.com/index.php?db=comics&amp;id=2159
          
        
      
        
      

    
  


  


Important Processes Get Focus Through Data ProductsFor each of the priorities in the organization someone should create a data product that sheds light on the progress toward this goal. The data product may be a quarterly summary of results or a real-time dashboard of operations.Is there an important element of what your organization does that is not transparent and could benefit from an effective data product?








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;One-off Analyses Bloom EverywhereData fluent organizations aren’t satisfied with tracking data. People want to get to The Why. They are hungry for data sources that will help them explain customer behaviors operational issues and marketing performance. They want to put key metric results into context: What is the goal? How does that compare to industry benchmarks?At Juice our focus on key metrics has spawned dozens of analyses to understand what it takes to get a new user to succeed with Juicebox. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Photo by keith davey on Unsplash
          
        
      
        
      

    
  


  





	Try Juicebox Free"
2021,4,23,Gradient Flow Snapshot #51: What is DataOps; 2021 Technology Radar,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-51-what-is.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,4,22,Hot Video: More Accuracy Fallacies – Predicting Criminality and Psychosis,https://www.predictiveanalyticsworld.com/blog/hot-video-more-accuracy-fallacies-predicting-criminality-and-psychosis/,"By:  Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; Can AI &#8220;tell&#8221; if you&#8217;re a criminal? Or whether you&#8217;ll develop psychosis? These are perfect examples of the accuracy fallacy which misleads the public into believing that machine learning can distinguish between positive and negative cases [&#8230;]
The post Hot Video: More Accuracy Fallacies – Predicting Criminality and Psychosis appeared first on Predictive Analytics World."
2021,4,22,Building a data store for unstructured data and deep learning applications,http://practicalquant.blogspot.com/2021/04/building-data-store-for-unstructured.html,The Data Exchange Podcast: Davit Buniatyan on tensorial data stores optimized for deep learning.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS   Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,4,21,How Dataquest Helped an SEO Expert Save Tons of Time,https://www.dataquest.io/blog/data-skills-python-for-seo/,"Antoine Eripret the SEO Lead at Liligo.com decided to learn Python for SEO because he realized he was wasting time.He got interested in search engine optimization as a student and entered the industry full-time after getting his Masters in 2016.As he built experience in the field he says “I realized that I was sometimes doing [&#8230;]
The post How Dataquest Helped an SEO Expert Save Tons of Time appeared first on Dataquest."
2021,4,21,11 Reasons To Learn Bash (A.K.A. Command Line),https://www.dataquest.io/blog/why-learn-the-command-line/,"Learn the command line (also called terminal bash or shell) a skill that is critical for doing data science work and building data pipelines efficiently.
The post 11 Reasons To Learn Bash (A.K.A. Command Line) appeared first on Dataquest."
2021,4,21,"Credit Card Fraud Detection using XGBoost, SMOTE, and threshold moving",https://blog.dominodatalab.com/credit-card-fraud-detection-using-xgboost-smote-and-threshold-moving/,"In this article we&#8217;ll discuss the challenge organizations face around fraud detection how machine learning can be used to identify and spot anomalies that the human eye might not catch. We&#8217;ll use a gradient boosting technique via XGBoost to create a model and I&#8217;ll walk you through steps you can take to avoid overfitting and [&#8230;]
The post Credit Card Fraud Detection using XGBoost SMOTE and threshold moving appeared first on Data Science Blog by Domino."
2021,4,21,Keys to Data Fluency: Matching Tools to User Needs,https://www.juiceanalytics.com/writing/keys-to-data-fluency-right-tools-for-the-jobs,"A data fluent organization should have a massive appetite for data. As you build your data fluency in front-line decision-makers and create a vibrant ecosystem the demand for data products will grow. And if there is one truism in analytics it is:Good analytics generates better questions.In what form do you answer the growing array of questions and needs? What data solutions or products do your data consumers needs? There are many choices:DashboardsReportsSelf-service BI toolsPredictive modelsOne-off analyses using slidesSpreadsheet modelsIt is a confusing array of ways to deliver data to these data consumers.What’s the right tool for the job?Of course there isn’t a single answer; it depends on the specific needs. Start by considering these two dimensions:How much flexibility and control does the data consumer need? Do they need to be able to dig deeply into the data or can results be shared with a static presentation of insights?How much will the raw data be enhanced with analysis modeling and pre-digested insights? For some audiences simply knowing the trend of key metrics is sufficient.Across these two axes it becomes clear there are a wide variety of different forms of data products.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Take the marketing function as an example:Analytics Tools (upper left): A Marketing Analyst wants to explore the performance of different advertising campaigns to understand what creatives are working best.Data Storytelling (upper right): A Director of Marketing needs to present a data-driven plan for spending to convinced the executive team to allocate budget.Data Access (lower left): A Data Analyst needs to extract data from a 3rd-party platform to explore the behaviors of new users.Performance Reporting (lower right): The CEO wants an overview of marketing performance to share with the sales product development and the Board.With so many different needs and use cases it seems evident that there isn’t just one tool that can fill all these situations. My friend at GoodData Roman Stanek has been talking about ‘Data as a Service’ the transformation from traditional tightly-coupled data platforms to a new model:The data industry now has a unique opportunity. Cloud-based data infrastructure can allow every decision to be data-driven. And as both people and machines make decisions today this new infrastructure needs to support automated decision-making as well. We need to break down the monolithic nature of existing BI tools and we need to deliver Data as a Service to every device and person so that access to data becomes truly pervasive.He recognizes the diverse needs of data consumers that we see as organizations become more mature in their data fluency. A restrictive dashboard tool isn’t the right answer for telling a data story nor does it serve the data scientist who wants to spend less time extracting data and more time exploring the data.If you’ve made the commitment to becoming a data fluency organization you’re already thinking about how to better serve all the people who might be working with that data. Mapping the right tool to each specific job-to-be-done is an essential step.


	Try Juicebox to tell your data story"
2021,4,20,Data Storytelling Lessons: Be Relatable and Specific,https://www.juiceanalytics.com/writing/lessons-in-data-storytelling-relatable-and-specific,"Data &amp; RealityData is an abstract representation of reality. We take real things processes and actions and turn them into numbers.This is useful for analysis. However it creates a conceptual disconnect from the reality you are interested in explaining. Take these numbers for example:56000&nbsp;people.840000&nbsp;square miles.Those are the population and size of Greenland. Alone those numbers are a little hard to conceptualize. Are they big or small? How do they relate?The graphic below (by&nbsp;UCI) delivers far greater impact:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


By making the data relatable (as above) or re-connecting it to reality with examples you have the opportunity to help your audience and deliver a more compelling data story.In the following interactive Juicebox learning app we share examples of making data&nbsp;relatable&nbsp;and&nbsp;specific. This is the special sauce that separates data storytelling from traditional dashboards and reports. Looking for more lessons on data storytelling? We’ve got you covered with our collection of data storytelling lessons.&nbsp;








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


&nbsp;&nbsp;


	Try Juicebox Free"
2021,4,20,FREE Report: 2021 Business at the Speed of AI Report,http://practicalquant.blogspot.com/2021/04/free-report-2021-business-at-speed-of.html,DOWNLOAD
2021,4,19,AI Adoption in the Enterprise 2021,https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/,During the first weeks of February we asked recipients of our Data and AI Newsletters to participate in a survey on AI adoption in the enterprise. We were interested in answering two questions. First we wanted to understand how the use of AI grew in the past year. We were also interested in the practice [&#8230;]
2021,4,19,18 Things You’ll Learn about Data Science Only on the Job,https://data36.com/data-science-on-the-job/,"Data Science on the job is different from the things you see about it in tutorials. I wish someone would&#8217;ve told me about these when I was an...
The post 18 Things You&#8217;ll Learn about Data Science Only on the Job appeared first on Data36."
2021,4,19,Keys to Data Fluency: Believe in Your Front-Line Decision-Makers,https://www.juiceanalytics.com/writing/keys-to-data-fluency-believe-in-your-front-line-decision-makers,"In 2007 Professor Thomas Davenport wrote an influential book called Competing on Analytics: The New Science of Winning. At the time he stoked a smoldering ember into a flame by examining the power of analytics to improve organizations. The book was a catalyst for a generation of business leaders looking to find value in their data.For all its influence we had a quibble with Davenport promotion of a centralized model for analytics where the data is managed at an enterprise-level by a cadre of data scientists building complex models to drive decisions throughout the organization. He believed that the best organizational structure is:central analytics and data science organization based in a Strategy function with analysts assigned to and rotated among business units and functions: This is I think the optimal structure and home for analytics and data science. The central function allows for a critical mass of quants and for central coordination of their skill and career development. It should be a shared service where anyone with the money and the high-priority requirements can get the help they need.To this day the question of where analytics should happen is still unclear for many organizations. Research by Deloitte shows that many organizations are confused or conflicted:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


We are advocates for bringing analytics to the front-line decision-makers of your organization. The marketers operators managers salespeople and customer service teams all need to understand data to be better at their jobs. For us management guru Peter Drucker sums it up best:Most discussions of decision making assume that only senior executives make decisions or that only senior executives' decisions matter. This is a dangerous mistake. Decisions are made at every level of the organization beginning with individual professional contributors and frontline supervisors. These apparently low-level decisions are extremely important in a knowledge-based organization.Senior leaders in your organization may make the so-called “big strategic” decisions in effect choosing the path to travel down. But the speed with which you travel toward your goal and stay on course when distractions arise—these decisions are controlled by your front-line staff. This belief that data can inform better decisions throughout an organization is part of our motivation for Juicebox.Data needs to be formed into targeted purposeful solutions to be of use to most people. The common practice of delivering a general-purpose analytical tool to end-users and expecting something useful to happen with it typically results in little added value. People are busy with their jobs. The last thing most information workers have time for is to learn how to use a new analysis tool figure out what data might be relevant to them and dive deep into a data analysis exercise. It is the difference between throwing someone an anchor and throwing them a lifeline.We have also made clear our belief in people over technology. There are many suitable technologies for capturing managing manipulating and presenting data. Better technology or tools is seldom the problem. Actually many of the data challenges that required large information technology investments a decade ago can be done quickly and economically today. 








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


The challenges are in the skills and collaboration of the people that use those technologies. Poor communication misalignment of values limited data communication skills unfocused messages…these are the challenges that most organizations we work with face today. The good news is that these are all solvable by focusing on the skills of your people.


	Try Juicebox Free"
2021,4,19,The Precondition for Machine Learning Success: Bridge the Quant/Business Culture Gap,https://www.predictiveanalyticsworld.com/blog/the-precondition-for-machine-learning-success-bridge-the-quant-business-culture-gap/,"By: Eric Siegel Predictive Analytics World Over the last few years I poured thousands of working hours and 25 years of consulting and teaching experience into making the online course Machine Learning Leadership and Practice – End-to-End Mastery. Why? I developed this training program because teaching is in my blood and I was dying to [&#8230;]
The post The Precondition for Machine Learning Success: Bridge the Quant/Business Culture Gap appeared first on Predictive Analytics World."
2021,4,18,Data Analyst Skills – 8 Skills You Need to Get a Job,https://www.dataquest.io/blog/data-analyst-skills/,"What are 5 real-world tasks that cover most of the skills someone needs to be hired as a data analyst?
The post Data Analyst Skills – 8 Skills You Need to Get a Job appeared first on Dataquest."
2021,4,16,Gradient Flow Snapshot #50: Data Engineering jobs in the U.S; Algorithms That Make Instacart Roll,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-50-data.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,4,15,UDML 2021 @ ICDM 2021,https://data-mining.philippe-fournier-viger.com/udml-2021-icdm-2021/,Hi all This is to let you know that the UDML workshop on utility driven mining and learning is back again this year at ICDM for the fourth edition. The topic of this workshop is the concept of utility in &#8230; Continue reading &#8594;
2021,4,15,Microsoft365R 2.1.0 with Outlook support now on CRAN,https://blog.revolutionanalytics.com/2021/04/microsoft365r-210-with-outlook-support-now-on-cran.html,by Hong Ooi I’m happy to announce that Microsoft365R 2.1.0 is now on CRAN with Outlook email support! Here’s a quick summary of the new features: Send reply to and forward emails optionally composed with blastula or emayili Copy and move emails between folders Create delete copy and move folders Add remove and download attachments Here’s a sample of how to write an email using blastula: library(Microsoft365R) # 1st one is for your personal Microsoft account # 2nd is for your work & school account outl <- get_personal_outlook() outlb <- get_business_outlook() # compose an email with blastula library(blastula) bl_body <-...
2021,4,15,How Technology Companies Are Using Ray,http://practicalquant.blogspot.com/2021/04/how-technology-companies-are-using-ray.html,The Data Exchange Podcast: Zhe Zhang describes how companies are using Ray in large-scale production applications.Subscribe: Apple • Android • Spotify • Stitcher • Google • RSS.  Attend the 2021 Ray Summit a FREE virtual conference that brings together developers machine learning practitioners data scientists DevOps and cloud-native architects interested in building scalable data &amp; AI applications. Full show notes can be found on the Data Exchange web site. A video version of this conversation is available on YouTube. 
2021,4,14,MLiSE 2021 @ PKDD 2021 – a new workshop!,https://data-mining.philippe-fournier-viger.com/mlise-2021-pkdd-2021-a-new-workshop/,I am glad to announce that I am co-organizing a new workshop called MLiSE 2021 (1st international workshop on Machine Learning in Software Engineering) held in conjunction with the ECML PKDD 2021 conference. Briefly the aim of this workshop is &#8230; Continue reading &#8594;
2021,4,14,Learn R the Right Way in 5 Steps,https://www.dataquest.io/blog/learn-r-for-data-science/,"R is in an increasingly popular language for data analysis and data science. Here's how you can learn R and be sure it sticks so you can get the career you want.
The post Learn R the Right Way in 5 Steps appeared first on Dataquest."
2021,4,14,Forecasting podcasts,https://robjhyndman.com/hyndsight/podcasts/,"I&rsquo;ve been interviewed for several podcasts over the last year or so. It&rsquo;s always fun to talk about my work and I hope there is enough differences between them to make it interesting for listeners. Here is a full list of them.
(Updated: 30 May 2021)
   Date Podcast Episode     24 May 2021 Data Skeptic Forecasting principles and practice   12 April 2021 Seriously Social Forecasting the future: the science of prediction   6 February 2021 Forecasting Impact Rob Hyndman   19 July 2020 The Curious Quant Forecasting COVID time series and why causality doesnt matter as much as you think‪   27 May 2020 The Random Sample Forecasting the future &amp; the future of forecasting   9 October 2019 Thought Capital Forecasts are always wrong (but we need them anyway)"
2021,4,13,FREE Report: 2020 NLP Industry Survey Report,http://practicalquant.blogspot.com/2021/04/free-report-2020-nlp-industry-survey.html,DOWNLOAD
2021,4,12,How to Apply for a Data Science Job and How to Prepare for Interviews,https://data36.com/apply-prepare-data-science-job-interviews/,"You have an awesome resume a jaw-dropping portfolio and a Pulitzer Prize winning cover letter. What’s next? Now we’re getting to the good stuff: applying for a data...
The post How to Apply for a Data Science Job and How to Prepare for Interviews appeared first on Data36."
2021,4,12,11 Real World Applications for Python Skills,https://www.dataquest.io/blog/real-world-python-use-cases/,"Python is one of the most frequently-recommended programming languages. You’ve probably heard people say that’s because it’s relatively easy to learn — and that’s true! But is Python actually useful? What are some of the real-world applications for Python skills once you’ve got them?In this post we’ll look at some of the most common use-cases [&#8230;]
The post 11 Real World Applications for Python Skills appeared first on Dataquest."
2021,4,12,"Data Engineer, Data Analyst, Data Scientist — What’s the Difference?",https://www.dataquest.io/blog/data-analyst-data-scientist-data-engineer/,"In the fast-growing field of data the ""big three"" job roles are data engineer data analyst and data scientist. Figure out which is the best fit for you.
The post Data Engineer Data Analyst Data Scientist — What’s the Difference? appeared first on Dataquest."
2021,4,12,Seriously social podcast,https://robjhyndman.com/seminars/assa-podcast/,Podcast interview for Seriously Social Recently I was interviewed for the podcast &ldquo;Seriously Social&rdquo;. You can listen to the episode here.
2021,4,12,Keys to Data Fluency: Creating the Data Product Ecosystem,https://www.juiceanalytics.com/writing/keys-to-data-fluency-creating-the-data-product-ecosystem,"For data-driven thinking to flourish in your organization you need to give people easy access to ‘data products’ that will answer their pressing questions.Easier said than done.In fact for most organizations the collection of dashboards reports and analysis tools feels like a chaotic mess. When we worked for a global manufacturer a survey of information workers revealed that the top problem was an inability to find data products that served their needs. Also a big concern: the quality and usefulness of those data products.This is the challenge of creating a data product ecosystem. Creating a vibrant ecosystem for data products requires processes and tools. Processes set standards and ensure that the right priorities and qualities are built into every data product. Tools gather data visualize the results and distribute data products to users. Here are the six conditions (“the Six Ds” shown below) that are essential to building this ecosystem:








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DemandWhat are the most important areas that would benefit from the insights and guidance of better data?There is nothing so useless as doing efficiently that which should not be done at all.—Peter DruckerWe begin with the end in mind. The consumers of data have needs. A healthy ecosystem will support those needs through the right data products. Discovering the information that will best serve the organization is the first step.Understanding data consumer demand is not a one-time endeavor. It requires a process of continually mapping the important decisions made by the organization and evaluating whether and how data can improve those decisions.One framework to use: Map the expressed needs of your data product consumers into a matrix to evaluate a) whether the data product will bring real value to the organization; b) whether a solution can truly drive better decision with data. This model will reveal data product concepts with the potential to deliver the greatest impact.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


DesignWhat processes and tools can help ensure the effective design of data products?Less than 30 percent of the potential users of organizations’ standard business intelligence (BI) tools use the technology today. This low take-up is due to the fact that longstanding tools and approaches to BI are often too difficult to use slow to respond or deliver content of limited relevance.—GartnerThe three reasons cited by Gartner for this problem are:1. Ease of use (“is hard to work with”)2. Performance (“users are frustrated by delays”)3. Relevance (“does not express content in line with their frame of reference”)The first and last reasons link directly to issues of poor data product design.In our role as dashboard and analytical application designers this is an area that is close to home. We see it all the time: reports and dashboards that lack focus and a message that targets their audience which is often undefined. We see poor choices in data visualization that distract from the important elements in the data and put the burden of deciphering meaning on the readers. We see data products that lack an obvious starting point and logical flow to conclusions.Poor design is wasteful. It results in solutions that users don’t want to use as noted by Gartner. It wastes the audience’s valuable time as it struggles to comprehend the data. And it wastes the development and distribution efforts necessary to deliver the data product.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Juicebox delivers beautiful data presentations with good design decisions built-in.
          
        
      
        
      

    
  


  


DevelopWhat processes and tools support the efficient production of data products including gathering multiple data sources presenting this data providing user customization and delivering the information to data consumers?Ideally you want to have a small set of data tools that support the variety of types of data products your organization needs. A single solution is unlikely to offer the breadth of capabilities necessary. In our experience four to five tools for data presentation are usually sufficient for most organizations.There are many forms your data products may take. And for every form there are many technology options. However here are some common features that are worth evaluating in almost every case:End-user customization—Some presentations may target a single audience. This is the exception to the rule. Most often a data product goes out into the world alone and is used by many people each of whom comes from a unique perspective. Whether it is their department region or products all audience members will want to see data that is customized and scoped to their situation. Many interactive applications can support this ability to filter the relevant data.Sharing support—Data should spur conversation. However some solutions for data products create an isolating environment. The data product should make it easy to share discuss and capture insights— whether the discussion happens online offline on a desktop or on mobile devices.Quality visualization—It matters how data is visualized. Clean clear charts can make it easy for readers to quickly understand the data. The default settings for data visualizations should adhere to the fundamentals shared by well-known data visualization authors like Stephen Few and Edward Tufte.Fit workflows—Finally it is important that data products integrate into how people do their jobs. If the consumer of data is constantly on the run bombarded by information of all types an effective data product will deliver simple narrow content to this person. If the consumer wants to drill deeply into the data to understand underlying assumptions the functionality should exist to allow for this need.DiscoverHow can you help people discover the many data products in your organization and find the right information for their individual needs?Data product discovery should mirror the capabilities of online content subscription services. Podcasts blogs or Twitter all have established features for ensuring an audience can find and access the latest content. These include:1. Searching of metadata about the content including title author and description2. Browsing of content sorted into categories and ranked by popularity or ratings3. Surfacing of related content based on the consumer’s expressed areas of interest4. Subscribing to allow consumers to sign up to receive updates to content5. Automated pushing that allows consumers to receive updated content automatically rather than having to remember to return to the source6. User permissions to control who has access to applications and content








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Browsing Spotify
          
        
      
        
      

    
  


  


DiscussWhat capabilities encourage data consumers to take the insights they find in the data and share these insights with others?The best data ecosystems don’t simply assume discussions will occur. They encourage discussions through mechanisms for sharing capturing and saving information and insights. The discoveries found in the data are treated as precious assets—after all they are the purpose of all the effort put into creating data products. Finally the ecosystem should encourage people to take action when the discussion is complete.Some organizations consider data products a one-way information broadcast. They implicitly assume that a dashboard is intended to deliver an information result not drive action.Discussions on data—like most of data fluency—are more a social and human problem than a technology problem. The technology approaches may be simple. For each data product create a document or folder for capturing insights. The document may simply be screenshots of the relevant part of the content along with an annotation explaining why it is interesting. As a historical artifact this document will reveal patterns of common issues and best practice approaches for responding to those issues. More important than a complex technology solution is an organizational culture that encourages dialogue and action after the insights are first found.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
          
          
            Source: https://www.ontotext.com/knowledgehub/fundamentals/dikw-pyramid/
          
        
      
        
      

    
  


  


DistillHow do you filter out the irrelevant content and provide feedback to enhance those data products that remain?The scourge of data in most organizations is the ever-growing collection of reports that get generated month after month. New reports are created but seldom killed. The mass of data products quickly becomes difficult to navigate and the right information is hard to find. Even for the data that has found a suitable audience there is seldom a feedback loop. The direction and needs of the organization may change yet the content does not change to fit evolving demands.Data products should be living documents. They should improve over time or be removed if they are no longer relevant. It is a matter of survival of the fittest.Data fluent organizations recognize that too much content—particularly data content—will clog up the channels of communication. The data products must be distilled to the essential information. You want to clean out distractions and emphasize the most useful remaining parts.To filter and clean your data product ecosystem you need processes in place to gather feedback from your users. The feedback needs to impact how data products are designed and produced. There are at least three ways to continuously distill the best data products:Create a lightweight feedback mechanism like a simple “star” system.Track usage both on the volume of usage and the levels of engagement.Conduct content reviews by gathering the audience for a product and having a focus group-style discussion.Because knowledges are so specialized we need also a methodology a discipline a process to turn this potential into performance. Otherwise most of the available knowledge will not become productive; it will remain mere information. To make knowledge productive we will have to learn to connect.—Peter Drucker Pro-Capitalist SocietyIt is hard work to create an environment that enables the creation of high-quality data products ensures those products get into the right hands and has mechanisms for self-improvement. But without doing this work all the data investments your organization makes will struggle to reach the important decision-makers and consumers of that data.


	Try Juicebox"
2021,4,11,Mining Episode Rules (video),https://data-mining.philippe-fournier-viger.com/mining-episode-rules-video/,In this blog post I will share the&#160;video&#160;of our most recent paper presented last week at ACIIDS 2021. It is about a new algorithm named POERM for about&#160;analyzing sequences of events or symbols. The algorithm will find rules called &#8220;episode &#8230; Continue reading &#8594;
2021,4,10,Hot Video: The Accuracy Fallacy – Bogus Machine Learning Results,https://www.predictiveanalyticsworld.com/blog/hot-video-the-accuracy-fallacy-bogus-machine-learning-results/,"By:  Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; Can AI &#8220;tell&#8221; if you&#8217;re gay? When machine learning practitioners claim their model achieves &#8220;high accuracy&#8221; it&#8217;s often bogus. This video reveals the undeniable yet common &#8220;accuracy fallacy&#8221;.
The post Hot Video: The Accuracy Fallacy – Bogus Machine Learning Results appeared first on Predictive Analytics World."
2021,4,9,Lessons from the Ever Given and Archegos: Four Ways Predictive Models Fail,https://analyticstrategy.com/lessons-from-the-ever-given-and-archegos-four-ways-predictive-models-fail/?utm_source=rss&utm_medium=rss&utm_campaign=lessons-from-the-ever-given-and-archegos-four-ways-predictive-models-fail,"Understanding the hierarchy of uncertainty and four ways predictive models fail - from false positives to black swans and deep uncertainty.
The post Lessons from the Ever Given and Archegos: Four Ways Predictive Models Fail appeared first on Analytic Strategy Partners."
2021,4,9,Gradient Flow Snapshot #49: Data Cascades; Exploiting ML Models; Prisma Migrate,http://practicalquant.blogspot.com/2021/04/gradient-flow-snapshot-49-data-cascades.html, Subscribe to our Newsletter YouTube channel and to the Data Exchange podcast by going HERE.
2021,4,8,"Explainable Machine Learning, Model Transparency, and the Right to Explanation",https://www.predictiveanalyticsworld.com/blog/explainable-machine-learning-model-transparency-and-the-right-to-explanation/,"By: Eric Siegel Predictive Analytics World Check out this topical video from Predictive Analytics World founder Eric Siegel: &#160; A computer can keep you in jail or deny you a job a loan insurance coverage or housing – and yet you cannot face your accuser. The predictive models generated by machine learning to drive these [&#8230;]
The post Explainable Machine Learning Model Transparency and the Right to Explanation appeared first on Predictive Analytics World."
2021,4,7,Data Discussion Lessons from Brad Pitt,https://www.juiceanalytics.com/writing/data-discussion-etiquette-from-brad-pitt,"Before Matt Damon impersonates an investigator in Ocean’s Eleven Brad Pitt’s character delivers a little pep talk.&nbsp;Watch this 40 second clip:Rusty Ryan (Brad Pitt) explains the rules of undercover conversation to Linus (Matt Damon). From: Ocean's Eleven (2001)Now imagine yourself giving a pep talk to the next email PowerPoint slide or dashboard that you are about to send out.&nbsp;Presumably your data is not meant to distort yet we can gather from this short scene some practical communication tips to improve data-informed discussions.Let’s break down the key moments.Be natural.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



[Damon takes an unnatural stiff stance] “No good. Don’t touch your tie. Look at me.”&nbsp;&nbsp;His first posture is fidgety and self-conscious with an overly professional stance.&nbsp;First impressions endure when it comes to perceived levels of interest and credibility. Most of us have an uncanny ability to sniff out a fake and how data enters the discussion is no exception. We’re not computers&nbsp;so we don’t enjoy an overwhelming data dump of facts findings and insights. Two paragraphs and 15 slides in everyone wonders “Where is this going? What’s the point?” Messages must be clear and focused and eliminate the unnatural mechanical chart headings and the unnecessarily complex statistical jargon.&nbsp;Be honest.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“I ask you a question. You have to think of the answer. Where do you look? No good. You look down; they know you’re lying. And up; they know you don’t know the truth.”&nbsp;Be honest with what you do and do not know and what data you do and do not have. Your audience expects to have certain questions answered in order to take your information seriously. Your audience wants to both hear and understand answers to questions like these:How do I know I can trust this data? How was it collected and who was involved?How exactly is this metric calculated?I see the number is X but how do I know whether that is good or bad?What’s the history of this number and the frequency of its collection?How quickly does this number usually change?Why is this useful for me to know? How will it change what I care about?These questions aren’t novel. They follow the 5W’s basics. Yet they are often either left out or overcomplicated in most data discussions. The goal here is to acknowledge these needs in the simplest most useful way.Start with a (very) short story.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Don’t use 7 words when 4 will do.”&nbsp;&nbsp;&nbsp;With data as with words precision is as much an art as a science. Still helpful tools exist. Ann Gibson wrote a relevant post and I highly recommend reading the article for all the details but here’s the magical excerpt:


  
    &#147;Once upon a time there was a [main character] living in [this situation] who [had this problem]. [Some person] knows of this need and sends the [main character] out to [complete these steps]. They [do things] but it’s really hard because [insert challenges]. They overcome [list of challenges] and everyone lives happily ever after.&#148;
  
  

The beauty of this frame narrative is that it provides a structure for those who are too long-winded to focus on the essence of their own message and it helps others whose ideas tend to dart all over the place to preserve a sequential flow.Each of these [placeholders] are candidates for data context that help satisfy the previous ""Be Honest"" section. I mocked up a quick scenario that demonstrates a short story with useful data context:







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



Set your mark.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Don’t shift your weight. Look always at your mark but don’t stare.”&nbsp;&nbsp;You’ve likely heard of S.M.A.R.T. goals before but are your charts smart? Something as simple as a target value by a specific date on a chart can work wonders at moving towards something tangible. People crave purpose&nbsp;so set and communicate your goals. But don’t be that presenter who stares incessantly at your metrics and goals.&nbsp;Be enjoyably useful.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“Be specific but not memorable. Be funny but don’t make him laugh. He’s got to like you; then forget you the moment he’s left your sight.”&nbsp;&nbsp;“Jazz it up”&nbsp;“Make it shine”&nbsp;and “Make it pretty” are all phrases you’ve either heard or used yourself. Few situations are more disappointing than when a company tries to overcompensate with their insufficient irrelevant data by lathering on the “wow factor.” Don’t succumb to making your data memorable for the wrong reasons. For businesses the goal isn’t memorable chart-junk but that does not mean your data should be lifeless and shallow.Don’t leave people hanging.







 

  
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  



“And for God’s sake whatever you do don’t under any circumstances…”&nbsp;&nbsp;The worst move you can make is to omit the call to action. End with clear next steps key questions posed or an action button that allows your audience to engage with immediacy while your solid ideas are fresh and ripe for action.


	Try Juicebox Free"
2021,4,7,9 Habits of Data Fluent Organizations — and How to Learn Them,https://www.juiceanalytics.com/writing/10-habits-of-data-fluent-organizations,"With our book resources and workshops we’ve shared guidance about what it takes to become a data fluent organization. Most of all it starts with cultural habits that get people focused on using data in their decision-making. At Juice we are working everyday to create these habits and we wanted to share how we are building a data-first mindset and where we look for inspiration.Habit 1: Define shared metricsData fluency requires getting everyone on the same page as to what matters most. Matt Lerner in conjunction with Business of Software delivers online workshops that help you determine your “North Star Metric” and the set of key drivers that are bottlenecks to achieving that overall success. He also emphasizes how the key drivers will change over time as you improve.Habit 2: Create a shared vocabulary for your dataWhat is an “active user”? How do we track “first success” for a user? These are terms that need to be carefully defined and documented so we can move on to how we are going to improve them.Val Logan of The Data Lodge is one of the premier thinkers on how organizations can build shared skills in using data. Check out her podcast: Speaking Data: Information as a Second LanguageThe Data LodgeHabit 3: Evaluate the strengths and weaknesses of data sourcesWhen we measure user activities for Juicebox we have multiple 3rd-party tools and internal data sources to choose from. We needed to determine what sources are more reliably accurate and what is the trade-off for convenience. If you are going to lean on data you want to understand its quality. Here’s an overview article from Neil Patel about assessing data quality.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 4: Ensure transparency into how data is manipulated modeled and presentedYou need to build alignment to avoid constantly reverting to discussions about the quality or meaning of your data. The discussion needs to move on to: What are we going to do to improve?Alberto Cairo is a preeminent advocate for truth in presentation of data his book How Charts Lie is a must-read on this topic.Another thought leader in this area is Alan Schwarz a journalist who has consistently used data to uncover hard truths. 
Habit 5: Use metrics as the starting point for everyday discussionsGetting focused on a few key metrics has started to transform how we work. We start meetings by reviewing how we are performing then focus on what activities are going to move those metrics. For difficult choices we have shared baseline: How will it impact our North Star Metric?Fortunately we have a tool in Juicebox that fluidly integrates data visualization with the ability to explain context priorities and next steps. It acts like a dashboard combined with a project management tool.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 6: Established guidelines to create purposeful data productsA data-eager organization will spawn plenty of reports dashboards and data presentation in an effort to communicate and explore. A data fluent organization will be purposeful about why each of those data products exists. You want to start with a clear understanding of the problem you want to solve and who is going to use the information.We created a short Data Product Checklist to help evaluate if your solution is ready to share.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 7: Develop a feedback mechanism for data products to evolve and improveYour report or dashboard is a product. Like any other product it needs to show value — even if you are only asking for your users’ attention. To fully discover that value you are going to need to talk to users improve and iterate on the data and how you show it.There are many ways to think about product development. We’ve found the product-led growth framework a useful set of guidelines.
Habit 8: Celebrate examples of quality data productsWhen you create data products that have made a difference make a big deal out of it. These winning examples will give other groups in your organization a benchmark to pursue.Here’s our 20 best data storytelling examples that show how to change minds with data.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  


Habit 9: Leadership promotes a data-driven cultureData fluency needs to emanate from the top because people emulate the behaviors of their leaders.Few people have been studying and advocating for data-driven cultures longer than Tom Davenport the Surgeon General of Analytics in the Enterprise.








  

    
  
    

      

      
        
          
        
        

        
          
            
          
            
          
        
          
        

        
      
        
      

    
  


  





	Learn more about Juicebox

"
2021,4,6,A Brief Report about ACIIDS 2021 (13th Asian Conference on Intelligent Information and Database Systems),https://data-mining.philippe-fournier-viger.com/a-brief-report-about-aciids-2021-13th-asian-conference-on-intelligent-information-and-database-systems/,In this blog post I will give a brief report about the ACIIDS 2021 conference that I am attending from April 7–10 2021. What is ACIIDS? ACIIDS is an international conference focusing on intelligent information and database systems. The conference &#8230; Continue reading &#8594;
2021,4,6,Python Practice: Free Ways To Improve Your Python Skills,https://www.dataquest.io/blog/python-practice/,"Getting good Python practice can help solidify your coding skills. Here are some of the best resources for practicing Python:
The post Python Practice: Free Ways To Improve Your Python Skills appeared first on Dataquest."
2021,4,6,Ray for Data Science: Distributed Python tasks at scale,https://blog.dominodatalab.com/ray-for-data-science-distributed-python-tasks-at-scale/,"In this article Dr Dean Wampler provides an overview of Ray including raising the question of why we need it.  The article covers practical techniques and some walk through code to help users get started.
The post Ray for Data Science: Distributed Python tasks at scale appeared first on Data Science Blog by Domino."
2021,4,6,"The $1,000 GPT-3",https://nuit-blanche.blogspot.com/2021/04/the-1000gpt-3.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**Progress usually comes from a steady technology bootstrap…until it doesn’t.Take for instance the race for the $1000 genome that started in the early 2000s. Initially sequencing the human genome meant a race between the well-funded public and private sectors but more importantly the resources for the first breakthrough ended up costing upwards of $450M. Yet despite all the economic promise of genome sequencing had Moore’s law been applied sequencing one full genome would still cost $100000 today. However once the goal became clearer to everyone a diversity of technologies and challengers emerged. This intense competition eventually yielded a growth faster than Moore’s Law. The main takeaway is that one cannot rely on the steady progress of one specific technology alone to commoditize tools.Figure from NIH “Facts sheets about genomics: The cost of Sequencing a Human Genome” Dec 7th&nbsp;2020.What does this have to do with the current state of silicon computing and the new demand for Large Language Models (LLMs)? Everything if you ask us and here is how.Less than a year into existence Large Language Models like GPT-3 have already spawned a new generation of startups built on the ability of the model to respond to requests for which it was not trained. More importantly for us hardware manufacturers are positing that one or several customers will be willing to put a billion dollars on the table to train an even larger model in the coming years.Interestingly much like the mass industrialization in the 1930s the good folks at OpenAI are sketching new scaling laws for the industrialization of these larger models.The sad truth is that extrapolating their findings to the training of a 10 Trillion parameters model involves a supercomputer running continuously for two decades. The minimum capital expenditure of this adventure is estimated in the realm of several hundreds of million dollars.Much like what happened in sequencing while silicon improvement and architecture may achieve speedups in the following years it is fair to say that even with Moore’s law no foreseeable technology can reasonably train a fully scaled-up GPT-4 and grab the economic value associated with it.Rebooting silicon with a different physics light and NvNsFor a real breakthrough to occur much like what happened in the sequencing story different technologies need to be jointly optimized. In our case this means performing co-design with new hardware and physics but also going rogue on full programmability.LightOn’s photonic hardware can produce massively parallel matrix-vector multiplications with an equivalent of 2 trillion parameters “for free”: this is about one-fifth of the number of parameters needed for GPT-4. Next comes revisiting the programmability. Current LightOn’s technology keeps these weights fixed by design. Co-design means finding the algorithms for which CPUs and GPUs can perform some of the most intelligent computations and how LightOn’s massive Non-von Neumann (NvN) hardware can do the heavy lifting. We already published how we are replacing backpropagation the workhorse of Deep Learning with an algorithm that unleashes the full potential of our hardware in distributed training. We are also working similarly on an inference step that will take full advantage of the massive number of parameters at our disposal. This involved effort relies in a heavy part thanks to our access to ½ million GPU hours on some of France’s and Europe’s largest supercomputers.And this is just the beginning. There is a vast untapped potential for repurposing large swaths of optical technologies directed primarily for entertainment and telecommunication into computing.The road towards a $1000 GPT-3Based on the GPT-3 training cost estimates achieving a $1000 GPT-3 requires four orders of magnitude improvements. Much like what occurred in 2007 with the genome sequencing revolution Moore’s law may take care of the first two orders of magnitude in the coming decade but the next two rely on an outburst of new efficient technologies — hardware and algorithms. It just so happens that GPT-3 has close to 100 layers so achieving two orders of magnitude savings may arise faster than you can imagine. Stay tuned!Igor Carron is the CEO and co-founder at LightOn&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,4,5,You Need Data Skills to Future-Proof Your Career,https://www.dataquest.io/blog/data-skills-to-future-proof-your-career/,"No matter what industry you're in you need data skills to future-proof your career.&#160;&#160;You might be thinking: Vik is the CEO of a company that teaches data science - of course he'd say that! But stick with me for a few more paragraphs I'll walk you through how data was key to all of the [&#8230;]
The post You Need Data Skills to Future-Proof Your Career appeared first on Dataquest."
2021,3,31,Streamline - tidy data as a service,https://simplystatistics.org/2021/03/31/streamline-data-science/,"Tldr: We started a company called Streamline Data Science https://streamlinedatascience.io/ that offers tidy data as a service. We are looking for customers partnerships and employees as we scale up after closing our funding round!

Most of my career I have worked in the muck of data cleaning. In the world of genomics a lot of my research has focused on batch effects synthesizing big genomic data into usable formats and generally making data easier to use. A couple of years ago we also started a company called Problem Forward Data Science. Problem Forward offered fractional data science services to a variety of businesses around the country from the very big corporations to startups just getting started. We were asked to do a lot of different types of data work everything from turning spreadsheets into dashboards to building complicated forecasting models. But no matter the project whether in government academia or industry we always ended up with the same problem.


We needed to clean the data before we could do the data science.


This will be no surprise to anyone who has worked in data science or analytics but the data almost always led to setbacks and frustration when we were working with our clients. Customers wanted complex AI insightful dashboards or easy reports but the data just weren’t ready for that yet. And we wasted a huge amount of time cleaning the data over and over again.

We realized that the most common challenge companies have is that their data processing and management pipelines aren’t ready for analytics. Or as Google so eloquently puts it:


“Everyone wants to do the model work not the data work”


We realized that this was a service that many businesses needed. They needed someone who could come in and set up a data processing pipeline for them manage it and make sure the data were up to date. Some people call this Extract Load Transform (ELT) but we found it goes a bit beyond that. It is figuring out what format is most useful for the people who rely on data and working backward to create a customized and unique data pipeline that gets the data ready to use.

The ELT pipeline we set up is designed to consistently output “tidy data” that makes it easy for our customers to use BI tools like Tableau or Looker and to ingest their data without having to do all the ugly data work that is painful and time-consuming.

We piloted this for one of our startup customers - we built their data pipeline and provided ongoing management maintenance and upkeep. When they hired their first data scientist they were able to quickly create dashboards for their whole business because they already had easy-to-use tidy data.

We got so excited about this data dry cleaning idea that we started a new company called Streamline Data Science that solely focuses on tidy data as a service. We just closed our seed round and are now working with our first set of customers to set up their data pipelines. The cool thing is we found that our most excited customers were the ones that already had a data scientist on the team. This seems a little counter-intuitive until you realize that we handle the painful/boring bits of data management so they can focus on the fun part.

The interesting thing about Streamline is that it isn’t a product. There are a ton of complicated tools out there that you can use to set up your own data pipeline. Streamline is a service that handles all your data issues for you so the data “just works”. It can often be a lot cheaper than building out a full stack data engineering team in house.

If you are a company that is worried about the state of your data - they are difficult to share to manage and to quality control - we’d love to hear from you! We would also love to hear from you if you are a data scientist or analyst at a company that is frustrated about how much time you are spending on data management and cleaning.

I’ll write more in the future about how we figured out setting up a data pipeline efficiently and the problems Streamline solves. We will also be releasing our first public data Streamlines that you can play with. In the meantime I wanted to share how excited I am to finally be working on solving the first mile of data science and building a company that can help Baltimore grow its data science community."
2021,3,30,Tutorial: Web Scraping with Python Using Beautiful Soup,https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/,"Learn how to scrape the web with Python! The internet is an absolutely massive source of data — data that we can access using web scraping and Python!&#160;In fact web scraping is often the only way we can access data. There is a lot of information out there that isn't available in convenient CSV exports [&#8230;]
The post Tutorial: Web Scraping with Python Using Beautiful Soup appeared first on Dataquest."
2021,3,30,Outlook client support in Microsoft365R available for beta test,https://blog.revolutionanalytics.com/2021/03/outlook-client-in-microsoft365r-beta.html,"by Hong Ooi This is an announcement that a beta Outlook email client is now part of the Microsoft365R package. You can install it from the GitHub repository with: devtools::install_github(""Azure/Microsoft365R"") The client provides the following features: Send reply to and forward emails optionally composed with blastula or emayili Copy and move emails between folders Create delete copy and move folders Add remove and download attachments The plan is to submit this to CRAN sometime next month after a period of public testing. Please give it a try and give me your feedback: either via email or by opening an issue..."
2021,3,29,Beautiful Soup Tutorial 1. – An Introduction to Web Scraping with Python,https://data36.com/beautiful-soup-tutorial-web-scraping/,"As a data scientist or data analyst sooner or later you’ll come to a point where you have to collect large amounts of data. Be it a hobby...
The post Beautiful Soup Tutorial 1. – An Introduction to Web Scraping with Python appeared first on Data36."
2021,3,29,On-Demand Spark clusters with GPU acceleration,https://blog.dominodatalab.com/on-demand-spark-clusters-with-gpu-acceleration/,"Apache Spark has become the de facto standard for processing large amounts of stationary and streaming data in a distributed fashion. The addition of the MLlib library consisting of common learning algorithms and utilities opened up Spark for a wide range of machine learning tasks and paved the way for running complex machine learning workflows [&#8230;]
The post On-Demand Spark clusters with GPU acceleration appeared first on Data Science Blog by Domino."
2021,3,28,Random Forest Tutorial: Predicting Goals in Football,https://algobeans.com/2021/03/29/random-forest-tutorial-predicting-goals-in-football/,Learn how a random forest model can help us to predict the probability of a goal with applications ranging from performance appraisal to match-fixing detection.
2021,3,26,Dimension reduction for outlier detection using DOBIN,https://robjhyndman.com/publications/dobin/,This paper introduces DOBIN a new approach to select a set of basis vectors tailored for outlier detection. DOBIN has a solid mathematical foundation and can be used as a dimension reduction tool for outlier detection tasks. We demonstrate the effectiveness of DOBIN on an extensive data repository by comparing the performance of outlier detection methods using DOBIN and other bases. We further illustrate the utility of DOBIN as an outlier visualization tool.
2021,3,26,Forecasting the old-age dependency ratio to determine a sustainable pension age,https://robjhyndman.com/publications/pensionage/,We forecast the old-age dependency ratio for Australia under various pension age proposals and estimate a pension age scheme that will provide a stable old-age dependency ratio at a specified level. Our approach involves a stochastic population forecasting method based on coherent functional data models for mortality fertility and net migration which we use to simulate the future age-structure of the population. Our results suggest that the Australian pension age should be increased to 68 by 2030 69 by 2036 and 70 by 2050 in order to maintain the old-age dependency ratio at 23% just above the 2018 level.
2021,3,24,Computing with Light: How LightOn intends to unlock Transformative AI,https://nuit-blanche.blogspot.com/2021/03/computing-with-light-how-lighton.html,I gave a talk at #mathia2021 conference on March 9th 2021 where I drew a parallel between the scaling laws that enabled industrialization in the 1920's and the new scaling laws in AI of the 2020's. AI is at its infancy and it needs to have guiding principles (as embedded in these empirical laws) and it also needs to develop new hardware. I showed how in this context LightOn can help unlock Transformative AI. Enjoy!All these other presentations by Yann LeCun Kathryn Hess Michael Jordan Emmanuel Candès and others can be found in this collection of videos on Vimeo. Let me note that Michael made a similar argument as mine where we think of current stage of AI at its infancy in terms of industrialization.&nbsp;&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,23,"InfoTribes, Reality Brokers",https://www.oreilly.com/radar/infotribes-reality-brokers/,It seems harder than ever to agree with others on basic facts let alone to develop shared values and goals: we even claim to live in a post-truth era1. With anti-vaxxers QAnon Bernie Bros flat earthers the intellectual dark web and disagreement worldwide as to the seriousness of COVID-19 and the effectiveness of masks have [&#8230;]
2021,3,23,Developing good research habits,https://robjhyndman.com/seminars/research_habits/,Presentation for the 2021 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2021,3,21,"Phrasebank, an interesting tool to improve your academic writing.",https://data-mining.philippe-fournier-viger.com/phrasebank-an-interesting-tool-to-improve-your-academic-writing/,Writing a research paper is not easy! It is a skill that takes time to master. Luckily most research papers follow more or less the same structure. In previous blog posts I have given an overview of how to write &#8230; Continue reading &#8594;
2021,3,20,Machine Learning’s Missing Link: Business Leadership,https://www.predictiveanalyticsworld.com/blog/machine-learnings-missing-link-business-leadership/,"Machine learning. Your team needs it your boss demands it and your career loves it. After all LinkedIn places it as one of the top few “Skills Companies Need Most” and as the very top emerging job in the U.S. But this number-crunching craze tends to tragically overlook one key point: Of all the ingredients [&#8230;]
The post Machine Learning&#8217;s Missing Link: Business Leadership appeared first on Predictive Analytics World."
2021,3,18,Data Analytics Certification: Do You Need a Certificate to Get a Job as a Data Analyst?,https://www.dataquest.io/blog/data-analytics-certification/,"If you’re interested in becoming a data analyst or even just interested in adding some data skills to your resume you’ve probably wondered: do I need some kind of data analytics certification?Finding the real answer to this question is tricky. There are a million data analytics certificate programs out there and they all have a [&#8230;]
The post Data Analytics Certification: Do You Need a Certificate to Get a Job as a Data Analyst? appeared first on Dataquest."
2021,3,16,SQL Operators: 6 Different Types (w/ Examples),https://www.dataquest.io/blog/sql-operators/,"We have previously covered why you need to learn SQL to get a data job in 2021 as well as publishing a full list of SQL commands to help you get started. Next we’re going to be looking at SQL operators.We’re going to cover what exactly SQL operators are before providing a comprehensive list of [&#8230;]
The post SQL Operators: 6 Different Types (w/ Examples) appeared first on Dataquest."
2021,3,16,Trending Toward Concept Building – A Review of Model Interpretability for Deep Neural Networks,https://blog.dominodatalab.com/trending-toward-concept-building-a-review-of-model-interpretability-for-deep-neural-networks/,"We are at an interesting time in our industry when it comes to validating models &#8211; a crossroads of sorts when you think about it. There is an opportunity for practitioners and leaders to make a real difference by championing proper model validation. That work has to include interpretability and explainability techniques. Explaining how deep [&#8230;]
The post Trending Toward Concept Building &#8211; A Review of Model Interpretability for Deep Neural Networks appeared first on Data Science Blog by Domino."
2021,3,15,Profiles in Analytics: Frank Knight,https://analyticstrategy.com/profiles-in-analytics-frank-knight/?utm_source=rss&utm_medium=rss&utm_campaign=profiles-in-analytics-frank-knight,"Frank Knight's important distinction between risk and uncertainty and its relevance to analytics
today.
The post Profiles in Analytics: Frank Knight appeared first on Analytic Strategy Partners."
2021,3,15,"Before you apply for a data science position… (resume, cover letter, website, GitHub help)",https://data36.com/data-science-cv-resume-cover-letter-github/,"This article is all about how to make your application stand out for data science and analytics positions. I&#8217;ll show you what will catch recruiters’ eyes when sifting...
The post Before you apply for a data science position&#8230; (resume cover letter website GitHub help) appeared first on Data36."
2021,3,15,How to Get a Job in Data Science and Analytics (episode #1),https://data36.com/get-job-data-science-analytics/,"Hello fellow job seekers! My name is Peter. About a year ago I was in your shoes. I wanted one of those high-paying jobs in data science and...
The post How to Get a Job in Data Science and Analytics (episode #1) appeared first on Data36."
2021,3,15,Why Jorge Prefers Dataquest Over DataCamp for Learning Data Analysis,https://www.dataquest.io/blog/dataquest-datacamp-learn-data-analysis/,"When Jorge Varade decided he wanted to learn data analysis he tried both DataCamp and Dataquest and found he strongly preferred the latter. Here's why.
The post Why Jorge Prefers Dataquest Over DataCamp for Learning Data Analysis appeared first on Dataquest."
2021,3,11,Choosing the right Machine Learning Framework,https://blog.dominodatalab.com/choosing-the-right-machine-learning-framework/,"Discover key considerations for selecting the right machine learning framework for your project and learn about 4 popular ML frameworks.
The post Choosing the right Machine Learning Framework appeared first on Data Science Blog by Domino."
2021,3,10,Papers without code (and the problem of non-reproducible research),https://data-mining.philippe-fournier-viger.com/paper-without-code-non-reproducible-research/,Recently there has been some debate on the Machine Learning sub-Reddit about the reproducibility or I should say the lack of reproducibility of numerous machine learning papers. Several Reddit users complained that they spent much time (sometimes weeks) to try &#8230; Continue reading &#8594;
2021,3,10,How Long Does It Take to Learn SQL?,https://www.dataquest.io/blog/how-long-learn-sql/,"How long does it take to learn SQL? It depends on your goals and your background so we've broken down a variety of scenarios for you.
The post How Long Does It Take to Learn SQL? appeared first on Dataquest."
2021,3,10,Teams support in Microsoft365R,https://blog.revolutionanalytics.com/2021/03/teams-support-in-microsoft365r.html,"by Hong Ooi I’m happy to announce that version 2.0 of Microsoft365R the R interface to Microsoft 365 is now on CRAN! This version adds support for Microsoft Teams a much-requested feature. To access a team in Microsoft Teams use the get_team() function and provide the team name or ID. You can also list the teams you’re in with list_teams(). These return objects of R6 class ms_team which has methods for working with channels and drives. list_teams() team <- get_team(""My team"") # list the channels in a team (including your private channels) team$list_channels() # get the primary channel for a..."
2021,3,9,The Next Generation of AI,https://www.oreilly.com/radar/the-next-generation-of-ai/,Programs like AlphaZero and GPT-3 are massive accomplishments: they represent years of sustained work solving a difficult problem. But these problems are squarely within the domain of traditional AI. Playing Chess and Go or building ever-better language models have been AI projects for decades. The following projects have a different flavor: In February PLOS Genetics [&#8230;]
2021,3,9,An Overview of Pattern Mining Techniques,https://data-mining.philippe-fournier-viger.com/an-overview-of-pattern-mining-techniques-by-data-types/,In this blog post I will give an overview of some of the main pattern mining tasks to explain what kind of patterns can be found in different types of symbolic data. I will describe some main types of data &#8230; Continue reading &#8594;
2021,3,8,Unveiling LightOn Appliance,https://nuit-blanche.blogspot.com/2021/03/unveiling-lighton-appliance.html,Today is a big day at LightOn as we unveil a hardware product the Appliance the world's first commercially available photonic co-processor for AI and HPCIf interested pre-ordering information is here:&nbsp;http://lighton.ai/lighton-appliance&nbsp;We have had a few of these optical processing units in our own LightOn Cloud for the past two years and just retired one after more than 800 days working full time.&nbsp;&nbsp;Here is the press release:&nbsp;The future is now!&nbsp;Leasing starts at 1900€/month or about US$2250/month&nbsp;&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,5,MLFlow on Azure Databricks,https://decisionstats.com/2021/03/05/mlflow-on-azure-databricks/,On Azure Databricks you can create experiments using MLFlow https://mlflow.org/ notebook_path = &#8216;/Users/Ajay/Folder&#8217; notebook_path = notebook_pathmlflow.set_experiment(notebook_path + &#8216;_experiments&#8217;) with mlflow.start_run(run_name=&#8221;ExperimentRun&#8221;+curr_ts):mlflow.log_params({&#8216;RSME&#8217;: RSME&#8216;AUC&#8217;: AUC })mlflow.end_run() https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/ https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/quick-start-python
2021,3,5,Fireside Chat: Stig Pedersen from Topdanmark,https://blog.dominodatalab.com/fireside-chat-stig-pedersen-from-topdanmark/,"Stig Pedersen joins David Bloch to discuss scaling data science inside the modern enterprise based on his experience leading the ML function at Topdanmark
The post Fireside Chat: Stig Pedersen from Topdanmark appeared first on Data Science Blog by Domino."
2021,3,4,SQL vs T-SQL: Understanding the Differences,https://www.dataquest.io/blog/sql-vs-t-sql/,"SQL or T-SQL — which one do you need to learn? SQL and T-SQL both see heavy use in the database and data science industries. But what exactly are they? These two query languages are very similar both in name and in what they can do so the distinction between them can be difficult to [&#8230;]
The post SQL vs T-SQL: Understanding the Differences appeared first on Dataquest."
2021,3,4,Keep Your Business Logic Out of (Code) Jail,http://feedproxy.google.com/~r/jtonedm/~3/souQ77yEqyg/,Copyright © 2021 https://jtonedm.com James Taylor Our friends at Data Decisioning forwarded an article from The Register recently &#8211; Inflexible prison software says inmates due for release should be kept locked up behind bars. The basic building blocks of this story is that there is a module calculating release dates for prisoners that was clearly [&#8230;]
2021,3,4,Video: LightOn unlocks Transformative AI,https://nuit-blanche.blogspot.com/2021/03/video-lighton-unlocks-transformative-ai.html,In the coming days we'll be making another announcement but I wanted to first share a video we did recently. At LightOn we don't build photonic computing hardware because it's fancy or cool (even though it is cool) but because computing hardware is hitting the limits. I know what some say about Moore's law not being dead but the recent focus on Transformers and their attendant scaling laws makes it obvious that in order for more people to have access to these models we need a new computing paradigm. Indeed not everyone can afford to spend a billion dollars in training these models. As Azeem was recently pointing out in one of his newsletters this is how bad things will become:The amazing thing is that we can start to compare the cost of training single AI models with the cost of building the physical fabs that make chips. TSMC’s state-of-the-art 3nm&nbsp;fab&nbsp;will run to around $20bn&nbsp;when it is completed in two years. A&nbsp;fab&nbsp;like this may be competitive for 5-7 years which means it’ll need to churn out $7-8m worth of chips every day before it pays back.And so at LightOn we think that a combination of algorithms and (cool) hardware as the only pathway forward for computing large-scale AI. The video is right here enjoy!&nbsp; Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2021,3,2,Forecasting for Social Good,https://robjhyndman.com/publications/fsg/,Forecasting plays a critical role in the development of organisational business strategies. Despite a considerable body of research in the area of forecasting the focus has largely been on the financial and economic outcomes of the forecasting process as opposed to societal benefits. Our motivation in this study is to promote the latter with a view to using the forecasting process to advance social and environmental objectives such as equality social justice and sustainability.
2021,2,26,Product Management for AI,https://www.oreilly.com/radar/product-management-for-ai/,A couple of years ago Pete Skomoroch Roger Magoulas and I talked about the problems of being a product manager for an AI product. We decided that would be a good topic for an article and possibly more. After Pete and I wrote the first article for O’Reilly Radar it was clear that there was [&#8230;]
2021,2,24,How to Learn Python (Step-by-Step) in 2021,https://www.dataquest.io/blog/learn-python-the-right-way/,"Learn Python the right way avoid the ""cliff of boring"" and give yourself the best chance to actually learn to code by following these steps.
The post How to Learn Python (Step-by-Step) in 2021 appeared first on Dataquest."
2021,2,19,5 things on our data and AI radar for 2021,https://www.oreilly.com/radar/5-things-on-our-data-and-ai-radar-for-2021/,Here are some of the most significant themes we see as we look toward 2021. Some of these are emerging topics and others are developments on existing concepts but all of them will inform our thinking in the coming year. MLOps FTW MLOps attempts to bridge the gap between Machine Learning (ML) applications and the [&#8230;]
2021,2,18,The Best Way to Learn SQL (According to Seasoned Devs),https://www.dataquest.io/blog/best-way-to-learn-sql/,"What's the best way to learn SQL? With all of the resources available learning SQL the “right way” can be difficult. Finding the best way to learn SQL is tricky because everyone learns things differently. But after training tens of thousands of students — seeing what works and what doesn’t — we’ve come up with [&#8230;]
The post The Best Way to Learn SQL (According to Seasoned Devs) appeared first on Dataquest."
2021,2,17,SQL Commands: The Complete List (w/ Examples),https://www.dataquest.io/blog/sql-commands/,"What can you do with SQL? Here's a reference guide to the most commonly-used SQL commands with code examples.
The post SQL Commands: The Complete List (w/ Examples) appeared first on Dataquest."
2021,2,15,How to Navigate the Challenging Journey from an AI Algorithm to an AI Product,https://analyticstrategy.com/the-journey-from-an-ai-algorithm-to-an-ai-product/?utm_source=rss&utm_medium=rss&utm_campaign=the-journey-from-an-ai-algorithm-to-an-ai-product,"It's a long challenging journey from an AI algorithm to an AI product. 
The post How to Navigate the Challenging Journey from an AI Algorithm to an AI Product appeared first on Analytic Strategy Partners."
2021,2,15,Manifold learning with approximate nearest neighbours,https://robjhyndman.com/publications/mlann/,Manifold learning algorithms are valuable tools for the analysis of high-dimensional data many of which include a step where nearest neighbors of all observations are found. This can present a computational bottleneck when the number of observations is large or when the observations lie in more general metric spaces such as statistical manifolds which require all pairwise distances between observations to be computed. We resolve this problem by using a broad range of approximate nearest neighbor algorithms within manifold learning algorithms and evaluating their impact on embedding accuracy.
2021,2,13,Assessing mortality inequality in the US: What can be said about the future?,https://robjhyndman.com/publications/us-longevity/,This paper investigates mortality inequality across U.S. states by modelling and forecasting mortality rates via a forecast reconciliation approach. Understanding the heterogeneity in state-level mortality experience is of fundamental importance as it can assist decision-making for policy makers health authorities as well as local communities who are seeking to reduce inequalities and disparities in life expectancy. A key challenge of multi-population mortality modeling is high dimensionality and the resulting complex dependence structures across sub-populations.
2021,2,11,SQL vs MySQL: A Simple Guide to the Differences,https://www.dataquest.io/blog/sql-vs-mysql/,"SQL and MySQL are important tools for working with data. But what are they exactly and how are they different? Let's clear that up. 
The post SQL vs MySQL: A Simple Guide to the Differences appeared first on Dataquest."
2021,2,9,Microsoft365R: an R interface to the Microsoft 365 suite,https://blog.revolutionanalytics.com/2021/02/microsoft365r.html,"I’m very happy to announce Microsoft365R a package for working with the Microsoft 365 (formerly known as Office 365) suite of cloud services. Microsoft365R extends the interface to the Microsoft Graph API provided by the AzureGraph package to provide a lightweight yet powerful interface to SharePoint and OneDrive with support for Teams and Outlook soon to come. Microsoft365R is now available on CRAN or you can install the development version from GitHub with devtools::install_github(""Azure/Microsoft365R""). Authentication The first time you call one of the Microsoft365R functions (see below) it will use your Internet browser to authenticate with Azure Active Directory (AAD)..."
2021,2,8,Data Science on Azure,https://ryanswanstrom.com/2021/02/08/data-science-azure/,"Learn about Building Data Science Solutions on Azure Join us for a discussion on data science on Azure. I was lucky enough to get Priyanshi Singh and Julian Soh to join me for a conversation about their newest book and their data science careers. This should be a fun conversation and highly informative. Please feel [...]
The post Data Science on Azure appeared first on Ryan Swanstrom."
2021,2,6,Forecasting impact podcast,https://robjhyndman.com/seminars/iif-podcast/,Podcast interview for Forecasting Impact Recently I was interviewed for the IIF podcast &ldquo;Forecasting Impact&rdquo;. You can listen to the episode here.
2021,2,6,Leave-one-out kernel density estimates for outlier detection,https://robjhyndman.com/publications/lookout/,This paper introduces lookout a new approach to detect outliers using leave-one-out kernel density estimates and extreme value theory. Outlier detection methods that use kernel density estimates generally employ a user defined parameter to determine the bandwidth. Lookout uses persistent homology to construct a bandwidth suitable for outlier detection without any user input. We demonstrate the effectiveness of lookout on an extensive data repository by comparing its performance with other outlier detection methods based on extreme value theory.
2021,2,1,SQL Interview Questions — Real Questions to Prep for Your Job Interview,https://www.dataquest.io/blog/sql-interview-questions/,"A lot of the SQL interview questions you'll find on the web are generic: ""What is SQL?"" You'll never be asked that. We've got real questions to help you prep.
The post SQL Interview Questions — Real Questions to Prep for Your Job Interview appeared first on Dataquest."
2021,2,1,SQL Basics — Hands-On Beginner SQL Tutorial Analyzing Bike-Sharing,https://www.dataquest.io/blog/sql-basics/,"Learn the SQL basics and go hands-on querying databases as you analyze bike rental data in this free beginner SQL tutorial.
The post SQL Basics — Hands-On Beginner SQL Tutorial Analyzing Bike-Sharing appeared first on Dataquest."
2021,1,29,Want a Job in Data? Learn SQL.,https://www.dataquest.io/blog/why-sql-is-the-most-important-language-to-learn/,"Learning SQL might not be as ""sexy"" as learning Python or R but it's a fundamental skill for almost every data scientist and data analyst job. Here's why.
The post Want a Job in Data? Learn SQL. appeared first on Dataquest."
2021,1,25,"Where Programming, Ops, AI, and the Cloud are Headed in 2021",https://www.oreilly.com/radar/where-programming-ops-ai-and-the-cloud-are-headed-in-2021/,In this report we look at the data generated by the O’Reilly online learning platform to discern trends in the technology industry—trends technology leaders need to follow. But what are “trends”? All too often trends degenerate into horse races over languages and platforms. Look at all the angst heating up social media when TIOBE or [&#8230;]
2021,1,25,Data Management & Data Stories with Scott Taylor,https://ryanswanstrom.com/2021/01/25/data-management-data-stories-with-scott-taylor/,"Data Management with Scott Taylor Scott better known as The Data Whisperer has spent over two decades solving DATA problems. Scott is an expert in Data Management and storytelling. He recently wrote a book Telling Your Data Story to help companies do just that. He is also a live streamer video creator podcaster and entertaining [...]
The post Data Management &#038; Data Stories with Scott Taylor appeared first on Ryan Swanstrom."
2021,1,21,AzureCosmosR: interface to Azure Cosmos DB,https://blog.revolutionanalytics.com/2021/01/azurecosmosr-interface-to-azure-cosmos-db.html,by Hong Ooi Last week I announced AzureCosmosR an R interface to Azure Cosmos DB a fully-managed NoSQL database service in Azure. This post gives a short rundown on the main features of AzureCosmosR. Explaining what Azure Cosmos DB is can be tricky so here’s an excerpt from the official description: Azure Cosmos DB is a fully managed NoSQL database for modern app development. Single-digit millisecond response times and automatic and instant scalability guarantee speed at any scale. Business continuity is assured with SLA-backed availability and enterprise-grade security. App development is faster and more productive thanks to turnkey multi region...
2021,1,20,Starting a Data Career,https://ryanswanstrom.com/2021/01/20/starting-a-data-career/,"Starting a Data Career If you are just starting a career in data you will not want to miss this live interview. Join me as I speak with Henrique Senra from Simbiose Ventures to discuss the book Getting Started with Data. The book was written out of a desire to provide a better future for [...]
The post Starting a Data Career appeared first on Ryan Swanstrom."
2021,1,20,SQL Cheat Sheet — SQL Reference Guide for Data Analysis,https://www.dataquest.io/blog/sql-cheat-sheet/,"Whether you’re learning SQL through one of our interactive SQL courses or by some other means it can be really helpful to have a SQL cheat sheet.Bookmark this article or download and print the PDF and keep it handy for quick reference the next time you’re writing an SQL query!Our SQL cheat sheet goes a [&#8230;]
The post SQL Cheat Sheet — SQL Reference Guide for Data Analysis appeared first on Dataquest."
2021,1,20,Forecast reconciliation: A geometric view with new insights on bias correction,https://robjhyndman.com/publications/hierarchical-geometry/,A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular a general framework is established that nests many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric.
2021,1,19,Stress Free Goal Setting with Deb Eckerling,https://ryanswanstrom.com/2021/01/19/stress-free-goal-setting-deb-eckerling/,"Stress-free Goal Setting with Deb Eckerling Debra Eckerling is a goal-setting expert. She offers coaching workshops and online support for people looking to achieve goals in any aspect of life. She has organized conferences served as an editor for numerous publications and authored 3 books. Deb joined to talk about goal setting and her latest [...]
The post Stress Free Goal Setting with Deb Eckerling appeared first on Ryan Swanstrom."
2021,1,19,Do You Need a SQL Certification to Get a Data Job in 2021?,https://www.dataquest.io/blog/sql-certification/,"If you want to work in data do you need a SQL certification? That’s a question that can be difficult to answer especially with different organizations pushing to get you to spend money on their certificate programs. Table Of Contents (click to expand) 1Do you need to learn SQL? Yes.2Do you need a SQL certificate? [&#8230;]
The post Do You Need a SQL Certification to Get a Data Job in 2021? appeared first on Dataquest."
2021,1,18,SQL Joins Tutorial: Working with Databases,https://www.dataquest.io/blog/sql-joins-tutorial/,"Learn how to master joins in the SQL joins tutorial. Learn to use inner left right and outer joins while analyzing CIA factbook data.
The post SQL Joins Tutorial: Working with Databases appeared first on Dataquest."
2021,1,17,Python script for requesting user input repeatedly,http://themainstreamseer.blogspot.com/2021/01/python-script-for-requesting-user-input.html,"&nbsp;Someone recently asked how to write a Python script that took user input into account and took some action based on the validity of the user input.&nbsp; Here are some code snippets that I developed for a few different scenarios:1) A game is being played and it has just ended.&nbsp; How would you ask the user if they wanted to play again?game_running = True #assuming the game is running play_again = input(""Play again? "")&nbsp; #get user inputwhile True: #restart game if answer is ""y"" or ""Y""&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if play_again == 'y' or play_again == ""Y"":&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; game_running = True&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else: #print thank you message and exit the program&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Thank you for playing!"")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; game_running = False&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; breakHere if the user answers either ""y"" or ""Y"" they get to play the game again.&nbsp; If they type anything else they receive a thank you message and the game ends.2) Create a while loop which validates that the user input is a float before it takes an action on it.#create a while loop to seek user input for fuel to burn in the next second until valid input is provided#cast valid user input as a float#use try and except statements to account for invalid user input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (some condition):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = (input(""Enter a number: ""))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; try:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = float(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; except ValueError as e:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(str(x) + "" is not a valid number.&nbsp; Please try again."")3) Define a function that asks a user for input does something if the answer is ""yes"" does something else if the answer is ""no"" and prompts the user again if the answer is neither ""yes"" or ""no"".def user_input():&nbsp;&nbsp;&nbsp; while True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = input(""Enter yes or no: "")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (x == ""yes"" or x == ""Yes""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (x == ""no"" or x == ""No""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Invalid input.&nbsp; Please try again."")&nbsp;user_input()4) Similar to #3 except that the function has a parameter.def user_input(x):&nbsp;&nbsp;&nbsp; while True:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = input(""Enter y or n: "")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (x == ""y"" or x == ""Y""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; continue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (x == ""n"" or x == ""N""):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(""Invalid input.&nbsp; Please try again."")user_input(""Enter y or n"")"
2021,1,15,Ryan Swanstrom a Guest on #PirateBroadcast,https://ryanswanstrom.com/2021/01/15/ryan-swanstrom-a-guest-on-piratebroadcast/,"I am super excited to have been invited as a guest on another live show. The #PirateBroadcast with Russ Johns. It will be happening January 15 2021 at 8:00am Central Time. I would love to have you stop by if you are available. I will be talking more about my latest project The Example Show [...]
The post Ryan Swanstrom a Guest on #PirateBroadcast appeared first on Ryan Swanstrom."
2021,1,15,The road to recovery from COVID-19 for Australian tourism,https://robjhyndman.com/publications/covidtourism/,COVID-19 has had a devastating effect on many industries around the world including tourism and policy makers are interested in mapping out what the recovery path will look like. In this paper we focus on Australian tourism analysing international arrivals and domestic flows. Both sectors have been severely affected by travel restrictions in the form of international and interstate border closures and regional lockdowns. We use statistical models of historical data to generate COVID-free counterfactual forecasts pretending that the pandemic never occurred.
2021,1,14,Three Reasons All Corporate Boards Need Someone Who Understands Both Analytic Innovation and Analytic Strategy,https://analyticstrategy.com/three-reasons-corporate-boards-need-expertise-in-ai/?utm_source=rss&utm_medium=rss&utm_campaign=three-reasons-corporate-boards-need-expertise-in-ai,"According to a 2019 report from CB Insights [1] between 2010 and 2019 there were 635 AI acquisitions. The acquisitions break into three groups as can be seen in the visualization below (Figure 1) from CB Insights. Facebook Apple Google Microsoft Amazon (FAGMA) and Intel accounted for 67 acquisitions each making 7 or more acquisitions [&#8230;]
The post Three Reasons All Corporate Boards Need Someone Who Understands Both Analytic Innovation and Analytic Strategy appeared first on Analytic Strategy Partners."
2021,1,13,45 Fun (and Unique) Python Project Ideas for Easy Learning,https://www.dataquest.io/blog/python-projects-for-beginners/,"Building projects is an extremely succesful way to learn but building Python projects for beginners can be difficult. Learn how to build with success!
The post 45 Fun (and Unique) Python Project Ideas for Easy Learning appeared first on Dataquest."
2021,1,13,Updated Online Chinese Document Analytics Tool,https://www.deep-data-mining.com/2021/01/updated-online-chinese-document.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} We have updated our free online tool for analyzing Chinese documents: https://aistrike.us/text-analysis.html A user fills in a textbox with the content and click Submit button. The tool identifies words displays a word cloud picture and calculates a sentiment index for each sentence. Enjoy!  Identifying words in a sentence is necessary.  Chinese words in a sentence are next to each other without spaces separating them e.g.  Chinesewordsinasentencearenexttoeachotherwithoutspacesseparatingthem. And yes the division of words could be ambiguous. For example ""结婚和尚未结婚的"" could mean ""married and unmarried"" (""结婚 | 和 | 尚未结婚的"") or ""married monk unmarried"" (""结婚 | 和尚 | 未结婚的"") .    "
2021,1,13,What's new with AzureR,https://blog.revolutionanalytics.com/2021/01/whats-new-with-azurer.html,by Hong Ooi This is an update on what’s been happening with the AzureR suite of packages. First you may have noticed that just before the holiday season the packages were updated on CRAN to change their maintainer email to a non-Microsoft address. This is because I’ve left Microsoft for a role at Westpac bank here in Australia; while I’m sad to be leaving I do intend to continue maintaining and updating the packages. To that end here are the changes that have recently been submitted to CRAN or will be shortly: AzureAuth now allows obtaining tokens for the “organizations”...
2021,1,12,SQL Tutorial: Selecting Ungrouped Columns Without Aggregate Functions,https://www.dataquest.io/blog/sql-tutorial-selecting-ungrouped-columns-without-aggregate-functions/,"When is a SQL query that returns the correct answer actually wrong? In this tutorial we're going to take a close look at a very common mistake. It's one that will actually return the right answer but it's still a mistake that's important to avoid.That probably sounds rather mysterious so let's dive right in. We'll [&#8230;]
The post SQL Tutorial: Selecting Ungrouped Columns Without Aggregate Functions appeared first on Dataquest."
2021,1,8,Data Science 101 Blog gets a New Home,https://ryanswanstrom.com/2021/01/08/data-science-101-blog-gets-a-new-home/,"The Data Science 101 blog was started almost 9 years ago and the goal was to collect useful information for people looking to break into data science. It did that well but now there are many other resources for learning data science. Thus I have changed the URL and will be altering the focus a [...]
The post Data Science 101 Blog gets a New Home appeared first on Ryan Swanstrom."
2021,1,5,Extract date from datetime in Pandas column,https://decisionstats.com/2021/01/05/extract-date-from-datetime-in-pandas-column/,use .dt.date df[&#8216;column&#8217;] = pd.to_datetime(df[&#8216;column&#8217;] format=&#8217;%Y-%m-%d&#8217;).dt.date Source- https://stackoverflow.com/questions/16176996/keep-only-date-part-when-using-pandas-to-datetime
2021,1,4,Decision-Driven Data Analytics,http://feedproxy.google.com/~r/jtonedm/~3/XzX9UH1-EU8/,Copyright © 2021 https://jtonedm.com James Taylor Bart de Langhe and Stefano Puntoni recently published a great article in the MIT Sloan Management Review called &#8220;Leading With Decision-Driven Data Analytics.&#8221; In contrast to so much of the literature that focuses first on data they focus on decision-making. In fact they go so far as to say [&#8230;]
2021,1,1,Anomaly detection in high-dimensional data,https://robjhyndman.com/publications/stray/,The HDoutliers algorithm is a powerful unsupervised algorithm for detecting anomalies in high-dimensional data with a strong theoretical foundation. However it suffers from some limitations that significantly hinder its performance level under certain circumstances. In this article we propose an algorithm that addresses these limitations. We define an anomaly as an observation that deviates markedly from the majority with a large distance gap. An approach based on extreme value theory is used for the anomalous threshold calculation.
2021,1,1,Forecasting Swiss Exports using Bayesian Forecast Reconciliation,https://robjhyndman.com/publications/swiss-exports/,This paper conducts an extensive forecasting study on 13118 time series measuring Swiss goods exports grouped hierarchically by export destination and product category. We apply existing state of the art methods in forecast reconciliation and introduce a novel Bayesian reconciliation framework. This approach allows for explicit estimation of reconciliation biases leading to several innovations: Prior judgment can be used to assign weights to specific forecasts and the occurrence of negative reconciled forecasts can be ruled out.
2021,1,1,Hierarchical Probabilistic Forecasting of Electricity Demand with Smart Meter Data,https://robjhyndman.com/publications/hpf-electricity/,Decisions regarding the supply of electricity across a power grid must take into consideration the inherent uncertainty in demand. Optimal decision-making requires probabilistic forecasts for demand in a hierarchy with various levels of aggregation such as substations cities and regions. The forecasts should be coherent in the sense that the forecast of the aggregated series should equal the sum of the forecasts of the corresponding disaggregated series. Coherency is essential since the allocation of electricity at one level of the hierarchy relies on the appropriate amount being provided from the previous level.
2021,1,1,Nonlinear mixed effects models for time series forecasting of smart meter demand,https://robjhyndman.com/publications/nlme-smart-meters/,Buildings are typically equipped with smart meters to measure electricity demand at regular intervals. Smart meter data for a single building have many uses such as forecasting and assessing overall building performance. However when data are available from multiple buildings there are additional applications that are rarely explored. For instance we can explore how different building characteristics influence energy demand. If each building is treated as a random effect and building characteristics are handled as fixed effects a mixed effects model can be used to estimate how characteristics affect energy usage.
2020,12,29,Inventory Optimization - Calculate Safety Stock,https://www.deep-data-mining.com/2020/12/inventory-optimization-calculate-safety.html,"   h1h2h3h4 {   text-align: center; } pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue}  Safety stock provides a ""cushion"" in inventory to address the uncertainty in a customer's demand. It is important to maintain the ""right"" amount of safety stock. If it is too low we may not be able to fulfill a customers' orders in a timely fashion. On the other hand safety stock that is too high incurs significant financial and/or logistics burden to the business.   The calculation of safety stock is based on a number of factors include historical customer demands product lead time and fill rate (a.k.a. demand satisfaction). The team at safetystockcalc.com builds a website and describe a popular approach to calculate safety stock using an example.  On the website you may also find two free tools that are useful an online calculator and a downloadable spreadsheet with all the formulas. Enjoy! Online Safety Stock Calculator Screenshot     Spreadsheet Safety Stock Calculator Screenshot     "
2020,12,29,The Awesome Implicit Neural Representations Highly Technical Reference Page,https://nuit-blanche.blogspot.com/2020/12/the-awesome-implicit-neural.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Here is a new curated page on the topic of Implicit Neural Representations aptly called Awesome Implicit Neural Representations. It is curated by Vincent Sitzmann&nbsp;(@vincesitzmann) and has been added to the Highly Technical Reference Page:From the page:A curated list of resources on implicit neural representations inspired by&nbsp;awesome-computer-vision. Work-in-progress.This list does not aim to be exhaustive as implicit neural representations are a rapidly evolving &amp; growing research field with hundreds of papers to date.Instead this list aims to list papers introducing key concepts &amp; foundations of implicit neural representations across applications. It's a great reading list if you want to get started in this area!For most papers there is a short summary of the most important contributions.Disclosure: I am an author on the following papers:Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene RepresentationsMetaSDF: MetaSDF: Meta-Learning Signed Distance FunctionsImplicit Neural Representations with Periodic Activation FunctionsInferring Semantic Information with 3D Neural Scene RepresentationsWhat are implicit neural representations?Implicit Neural Representations (sometimes also referred to coordinate-based representations) are a novel way to parameterize signals of all kinds. Conventional signal representations are usually discrete - for instance images are discrete grids of pixels audio signals are discrete samples of amplitudes and 3D shapes are usually parameterized as grids of voxels point clouds or meshes. In contrast Implicit Neural Representations parameterize a signal as a&nbsp;continuous function&nbsp;that maps the domain of the signal (i.e. a coordinate such as a pixel coordinate for an image) to whatever is at that coordinate (for an image an RGB color). Of course these functions are usually not analytically tractable - it is impossible to ""write down"" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.Why are they interesting?Implicit Neural Representations have several benefits: First they are not coupled to spatial resolution anymore the way for instance an image is coupled to the number of pixels. This is because they are continuous functions! Thus the memory required to parameterize the signal is&nbsp;independent&nbsp;of spatial resolution and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have ""infinite resolution"" - they can be sampled at arbitrary spatial resolutions.This is immediately useful for a number of applications such as super-resolution or in parameterizing signals in 3D and higher dimensions where memory requirements grow intractably fast with spatial resolution.However in the future the key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the ""convolutional neural network"" equivalent of a neural network operating on images represented by implicit representations? Questions like these offer a path towards a class of algorithms that are independent of spatial resolution!..........h/t Shubhendu Trivedi (@_onionesque)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,12,21,Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment,https://nuit-blanche.blogspot.com/2020/12/hardware-beyond-backpropagation.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;We presented this work at the Beyond Backpropagation workshop at NeurIPS. A great conjunction between computational hardware and algorithm!&nbsp;Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment&nbsp;by&nbsp;Julien Launay Iacopo Poli Kilian Müller Gustave Pariente Igor Carron Laurent Daudet Florent Krzakala Sylvain GiganThe scaling hypothesis motivates the expansion of models past trillions of parameters as a path towards better performance. Recent significant developments such as GPT-3 have been driven by this conjecture. However as models scale-up training them efficiently with backpropagation becomes difficult. Because model pipeline and data parallelism distribute parameters and gradients over compute nodes communication is challenging to orchestrate: this is a bottleneck to further scaling. In this work we argue that alternative training methods can mitigate these issues and can inform the design of extreme-scale training hardware. Indeed using a synaptically asymmetric method with a parallelizable backward pass such as Direct Feedback Alignement communication needs are drastically reduced. We present a photonic accelerator for Direct Feedback Alignment able to compute random projections with trillions of parameters. We demonstrate our system on benchmark tasks using both fully-connected and graph convolutional networks. Our hardware is the first architecture-agnostic photonic co-processor for training neural networks. This is a significant step towards building scalable hardware able to go beyond backpropagation and opening new avenues for deep learning.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,19,Diffraction-unlimited imaging based on conventional optical devices,https://nuit-blanche.blogspot.com/2020/12/diffraction-unlimited-imaging-based-on.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Aurélien&nbsp;sent me an email back in October and we are now in December! Time flies.Dear IgorI hope things are well.I have been following your NuitBlanche blog for quite a few years. It would thus be great for us if you consider a recent paper of ours to appear in your blog entitled “Diffraction-unlimited imaging based on conventional optical devices”. This paper has been published in Optics Express this year and its link is: https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-8-11243This manuscript proposes a new imaging paradigm for objects that are too far away to be illuminated or accessed which allows them to be resolved beyond the limit of diffraction---which is thus distinct from the microscopy setting. Our concept involves an easy-to-implement acquisition procedure where a spatial light modulator (SLM) is placed some distance from a conventional optical device. After acquisition of a sequence of images for different SLM patterns the object is reconstructed numerically. The key novelty of our acquisition approach is to ensure that the SLM modulates light before information is lost due to diffraction.Feel free to let us know what you think and happy to provide more information/pictures if needed. Thanks a lot for your time and consideration!Best regardsAurélien BourquardThank you&nbsp;Aurélien!&nbsp;&nbsp;Here is the paper's abstract:Diffraction-unlimited imaging based on conventional optical devices by&nbsp;Nicolas Ducros and Aurélien BourquardWe propose a computational paradigm where off-the-shelf optical devices can be used to image objects in a scene well beyond their native optical resolution. By design our approach is generic does not require active illumination and is applicable to several types of optical devices. It only requires the placement of a spatial light modulator some distance from the optical system. In this paper we first introduce the acquisition strategy together with the reconstruction framework. We then conduct practical experiments with a webcam that confirm that this approach can image objects with substantially enhanced spatial resolution compared to the performance of the native optical device. We finally discuss potential applications current limitations and future research directions.I note that Aurélien has also published some exciting research on Differential Imaging Forensics. His co-author Nicolas has also some interesting work on&nbsp;Single Pixel cameras.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,17,Azure Functions with R and plumber,https://blog.revolutionanalytics.com/2020/12/azure-functions-with-r.html,Azure Functions is a cloud service that allows you to deploy “serverless” microservices that are triggered by events (timers HTTP POST events etc) and automatically scale to serve demand while minimizing latency. The service natively supports functions written in C# Java JavaScript PowerShell Python and TypeScript and now supports other languages as well thanks to the launch last week of custom handlers for Azure Functions. A new tutorial walks you through the process of creating a custom handler for a “hello world” R function. The process is fairly straightforward: use a couple of Azure CLI commands to set up a...
2020,12,14,Five Steps to Improve the Analytic Maturity of Your Company – 2021 Edition,https://analyticstrategy.com/five-steps-to-improve-analytic-maturity-of-your-company/?utm_source=rss&utm_medium=rss&utm_campaign=five-steps-to-improve-analytic-maturity-of-your-company,"Half of AI Projects Fail A good rule of thumb is that about half of all AI and analytic projects fail to bring business value. Here are some recent articles that remind us of this: Gil Press writing in Forbes [Press2019] summarized some of the statistics around the failure rate of AI projects. One of [&#8230;]
The post Five Steps to Improve the Analytic Maturity of Your Company &#8211; 2021 Edition appeared first on Analytic Strategy Partners."
2020,12,11,R at Microsoft,https://blog.revolutionanalytics.com/2020/12/r-at-microsoft.html,"I was my great pleasure yesterday to be a presenter in the ""Why R Webinar"" series on the topic R at Microsoft. In the talk (which you can watch below) I recounted the history of Microsoft's acquisition of Revolution Analytics and the various way the Microsoft supports R: its membership of the R Consortium integration with many products (including demos of Azure ML Service with GitHub Actions and Azure Functions) and how Microsoft has driven adoption of R in large organizations by making R ""IT approved"". Many thanks to the Why R Foundation for hosting the talk and to everyone..."
2020,12,9,LightOn at #NeurIPS2020,https://nuit-blanche.blogspot.com/2020/12/lighton-at-neurips2020.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;I posted the following on LightOn's Blog.We live in interesting times!A combination of post-Moore’s law era and the advent of very large ML models require all of us to think up new approaches to computing hardware and AI algorithms at the same time. LightOn is one of the few (20) companies in the world publishing in both AI and hardware venues to engage both communities into thinking how theories and workflows may eventually be transformed by the photonic technology we develop.This year thanks to the awesome Machine Learning team at LightOn we have two accepted papers at NeurIPS the AI flagship conference and have five papers in its“Beyond Backpropagation” satellite workshop that will take place on Saturday. This is significant on many levels not the least being that these papers have been nurtured and spearheaded by two Ph.D. students (Ruben Ohana and Julien Launay) who are doing their thesis as LightOn engineers.Here is the list of the different papers accepted at NeurIPS this year that involved LightOn members:Reservoir Computing meets Recurrent Kernels and Structured Transforms Jonathan Dong Ruben Ohana Mushegh Rafayelyan Florent Krzakala. Links: Oral poster paper (presenter: Ruben Ohana). Poster Session 4 on Wed Dec 9th 2020 @ 18:00–20:00 CET. GatherTown: Deep learning ( Town E1 — Spot C0 ) Join GatherTown. Only if and only if poster is crowded join ZoomDirect Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures Jonathan Dong Ruben Ohana Mushegh Rafayelyan Florent Krzakala. Links: Poster paper (Presenter: Julien Launay). Poster Session 6 on Thu Dec 10th 2020 @ 18:00–20:00 CET. GatherTown: Neuroscience and Cognitive Science ( Town A3 — Spot B0 )And at the NeurIPS Beyond Backpropagation workshop taking place on Saturday December 12:Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment Julien Launay Iacopo Poli Kilian Muller Igor Carron Laurent Daudet Florent Krzakala Sylvain GiganDirect Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures Julien Launay François Boniface Iacopo Poli Florent Krzakala (Presenter: Julien Launay).Ignorance is Bliss: Adversarial Robustness by Design through Analog Computing and Synaptic Asymmetry Alessandro Cappelli Ruben Ohana Julien Launay Iacopo Poli Florent Krzakala (Presenter: Alessandro Cappelli). We had a blog post on this recently.Align then Select: Analysing the Learning Dynamics of Feedback Alignment Maria Refinetti Stéphane d’Ascoli Ruben Ohana Sebastian Goldt paper (Presenter: Ruben Ohana).How and When does Feedback Alignment Work Stéphane d’Ascoli Maria Refinetti Ruben Ohana Sebastian Goldt. paper (Presenter: Ruben Ohana)Some of these presentations are given in French at the “Déjeuners virtuels de NeurIPS” Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,12,7,Apply to Dataquest and AI Inclusive’s Under-Represented Genders 2021 Scholarship!,https://www.dataquest.io/blog/dataquest-scholarship-women-underrepresented-genders/,"Dataquest is launching another data science scholarship for women and anyone who identifies as an underrepresented gender in data science!
The post Apply to Dataquest and AI Inclusive&#8217;s Under-Represented Genders 2021 Scholarship! appeared first on Dataquest."
2020,12,5,Distributionally Robust Contextual Bandit Learning,http://www.machinedlearnings.com/2020/12/distributionally-robust-contextual.html,This blog post is about improved off-policy contextual bandit learning via distributional robustness.  I'll provide some theoretical background and also outline the implementation in vowpal wabbit. Some of this material is in a NeurIPS expo talk video and additional material is in the accepted paper.  Motivation In off-policy learning in contextual bandits our goal is to produce the best policy possible from historical data and we have no control over the historical logging policy which generated the data.  (Note production systems that run in closed-loop configurations nonetheless are in practice doing off-policy learning because of delays between inference training and model update.)  Off-policy learning reduces to optimization of a policy value estimator analogously to supervised learning; however the accuracy of policy value estimation depends upon the mismatch between the policy being evaluated and the policy that generated the data and therefore can be quite different for different policies (unlike supervised learning where the differences in estimator resolution across the hypothesis class are less pronounced in practice).  To appreciate this effect consider the IPS policy value estimator $$ \hat{V}(\pi; \mu) = \frac{1}{N} \sum_{n \in N} \frac{\pi(a_n|x_n)}{\mu(a_n|x_n)} r_n $$ where $\mu$ is the historical policy $\pi$ is the policy being estimated and our historical data consists of tuples $\{ (x_n a_n r_n) \}$.  The importance weight $\frac{\pi(a_n|x_n)}{\mu(a_n|x_n)}$ can be quite large if $\pi$ frequently takes an action that $\mu$ rarely takes causing the estimator to be highly sensitive to a few examples with large importance weights.  Even if we initialize learning with $\pi = \mu$ as optimization progresses $\pi$ will induce increasingly different distributions over actions than $\mu$ as the learning algorithm encounters rare events with high reward.  To combat this overfitting technique we will introduce regularization.  Distributionally Robust Optimization Distributionally robust optimization is a generic method for regularizing machine learning objectives.  The basic idea is to consider the observed data as one possible distribution of data (the “empirical distribution”) and then to optimize a worst-case outcome over all distributions that are “sufficiently close” to the empirical distribution.  In the case of IPS we can find the smallest policy value estimate over a set of distributions that are close in KL divergence to the empirical distribution $$ \begin{alignat}{2} &amp;\!\min_{P \in \Delta} &amp; \qquad &amp; \mathbb{E}_P\left[w r\right] \\ &amp;\text{subject to} &amp; &amp; \mathbb{E}_P\left[w\right] = 1 \\ &amp; &amp; &amp; \mathbb{E}_N \left[\log\left( P\right) \right] \geq \phi. \end{alignat} $$ where $w \doteq \frac{\pi(a|x)}{\mu(a|x)}$.  It turns out you can do this cheaply (in the dual) and the value of $\phi$ can be computed from a desired asymptotic confidence level.  These results follow from classic work in the field of Empirical Likelihood.  The above problem finds a lower bound; finding an upper bound is analogous resulting in the confidence intervals from the paper:  Empirical Likelihood Confidence Intervals are tighter Gaussian intervals.&nbsp; Not shown: coverage of Empirical Likelihood CIs is better calibrated than Binomial (Clopper-Pearson). When we do distributionally robust optimization we are actually optimizing the lower bound on the policy value.  The green curve in the above plot is a Clopper-Pearson interval which does have guaranteed coverage but is so wide that optimizing the lower bound wouldn't do much until the amount of data is large.  The tighter intervals generated by the blue Empirical Likelihood curve imply that lower bound optimization will induce an interesting policy ordering with only modest data.  In practice even when the empirical mean (empirical IPS value) is fixed the lower bound is:   smaller when the policy generates value via few events with importance weights much larger than 1 and many events with importance weights near zero; and  larger when the policy generates value via many events with importance weights near 1.This is precisely the kind of regularization we desired.  Intuitively any estimator which is sensitive to a few of the observed examples (aka high leverage) will have a larger penalty because it is “cheap” as measured by KL divergence to reduce the probability of those points.  Implementation in Vowpal Wabbit To activate the functionality add the --cb_dro flag to your contextual bandit command line in VW.  Note it only effects training so if you are only predicting you will not see a difference.  Hopefully with the default hyperparameters you will see an improvement in the quality of your learned policy such as in this gist.  Internally VW is solving the lower bound optimization problem from above on every example.  There are some modifications: As stated above this would be too expensive computationally but switching from the KL divergence to the Cressie-Read power divergence allows us to derive a closed form solution which is fast to compute.As stated above the lower bound requires remembering all policy decisions over all time.  Instead we accumulate the sufficient statistics for the Cressie-Read power divergence in $O(1)$ time and space.To track nonstationarity we use exponentially weighted moving averages of the sufficient statistics.  The hyperparameter --cb_dro_tau specifies the decay time constant.As always YMMV.
2020,12,3,Machine learning algorithms surprises at deployment? (article on Medium),http://www.bzst.com/2020/12/machine-learning-algorithms-surprises.html,"Machine
 learning (ML) algorithms are being used to generate predictions in 
every corner of our decision-making life. Methods range from “simple” 
algorithms such as trees forests naive Bayes..."
2020,12,3,Maximizing the ROI of your business rules investment,http://feedproxy.google.com/~r/jtonedm/~3/hApk14JWzCE/,Copyright © 2021 https://jtonedm.com James Taylor We help a lot of clients select install and adopt a Business Rules Management Systems (BRMS). These clients are looking to get automate decision-making with transparency deliver business control of their critical decision-making logic and establish an ability to drive continuous improvement through simulation and impact analysis. Adopted correctly [&#8230;]
2020,12,1,An Online Tool for Analyzing Chinese Text,https://www.deep-data-mining.com/2020/11/an-online-tool-for-analyzing-chinese.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} We have developed a free online tool for analyzing Chinese document. The URL of the tool is located at  here.  A user fills in the textbox with the content and click Submit button. The tool identifies words calculates frequencies for those words and display a word cloud picture.   . Enjoy!       
2020,11,30,Kernel Density Plots,https://algobeans.com/2020/11/30/kernel-density-plots/,Visualizing soccer data we identify hot spots where most shots occur shot preferences across players and comparisons between different teams like Liverpool and Manchester United.
2020,11,25,ASSA New Fellows Presentations,https://robjhyndman.com/seminars/fassa/,Presentation given to the Academy of the Social Sciences in Australia My talk starts at 42:41
2020,11,25,"Attend the Create:Data free online event, December 7",https://blog.revolutionanalytics.com/2020/11/createdata-free-online-event-december-7.html,The Microsoft Create: series is back again now with Create: Data! Join us for a half-day of conversations at Microsoft Create: Data and connect with the experts and community to learn and discuss everything data - from the upcoming trends to best practices and data for good. Join our virtual event Create: Data Date: 7 December 2020 Time: 8:00AM – 11:10AM PDT / 4:00PM – 7:10PM GMT Join us! In this event you will hear from our keynote speakers Arun Ulagaratchagan and Heather Newman share how to drive a data culture in a world of remote everything. Learn about Apache...
2020,11,24,The Four Jobs of the Data Scientist,https://simplystatistics.org/2020/11/24/the-four-jobs-of-the-data-scientist/,"In 2019 I wrote a post about The Tentpoles of Data Science that tried to distill the key skills of the data scientist. In the post I wrote:


When I ask myself the question “What is data science?” I tend to think of the following five components. Data science is (1) the application of design thinking to data problems; (2) the creation and management of workflows for transforming and processing data; (3) the negotiation of human relationships to identify context allocate resources and characterize audiences for data analysis products; (4) the application of statistical methods to quantify evidence; and (5) the transformation of data analytic information into coherent narratives and stories.

My contention is that if you are a good data scientist then you are good at all five of the tentpoles of data science. Conversely if you are good at all five tentpoles then you’ll likely be a good data scientist.


I still feel the same way about these skills but my feeling now is that actually that post made the job of the data scientist seem easier than it is. This is because it wrapped all of these skills into a single job when in reality data science requires being good at four jobs. In order to explain what I mean by this we have to step back and ask a much more fundamental question.

What is the Core of Data Science?

This is a question that everyone is asking and I think struggling to answer. With any field there’s always a distinction between the questions of


What is the core of the field?
What do people in that field do on a regular basis?


In case it’s not clear these are not the same question. For example in Statistics based on the curriculum from most PhD program the core of the field involves statistical methods statistical theory probability and maybe some computing. Data analysis is generally not formally taught (i.e. in the classroom) but rather picked up as part of a thesis or research project. Many classes labeled “Data Science” or “Data Analysis” simply teach more methods like machine learning clustering or dimension reduction. Formal software engineering techniques are also not generally taught but in practice are often used.

One could argue that data analysis and software engineering is something that statisticians do but it’s not the core of the field. Whether that is correct or incorrect is not my point. I’m only saying that a distinction has to be made somewhere. Statisticians will always do more than what would be considered the core of the field.

With data science I think we are collectively taking inventory of what data scientists tend to do. The problem is that at the moment it seems to be all over the map. Traditional statistics does tend to be central to the activity but so does computer science software engineering cognitive science ethics communication etc. This is hardly a definition of the core of a field but rather an enumeration of activities.

The question then is can we define something that all data scientists do? If we had to teach something to all data science students without knowing where they might end up afterwards what would it be? My opinion is that at some point all data scientists have to engage in the basic data analytic iteration.

Data Analytic Iteration

The basic data analytic iteration comes in four parts. Once a question has been established and a plan for obtaining or collecting data is available we can do the following:


Construct a set of expected outcomes
Design/Build/Apply a data analytic system to the data
Diagnose any anomalies in the analytic system output
Make a decision about what to do next


While this iteration might be familiar or obvious to many its familiarity masks the complexity involved. In particular each step of the iteration requires that the data scientist play a different role involving very different skills. It’s like a one-person play where the data scientist has to change costumes when going from one step to the next. This is what I refer to as the the four jobs of the data scientist.

The Four Jobs

Each of the steps in the basic data analytic iteration requires the data scientist to be four different people: (1) Scientist; (2) Statistician; (3) Systems Engineer; and (4) Politician. Let’s take a look at each job in greater detail.

Scientist

The scientist develops and asks the question and is responsible for knowing the state of the science and what the key gaps are. The scientist also designs any experiments for collecting new data and executes the data collection. The scientist must work with the statistician to design a system for analyzing the data and ultimately construct a set of expected outcomes from any analysis of the data being collected.

The scientist plays a key role in developing the system that results in our set of expected outcomes. Components of this system might include a literature review meta-analysis preliminary data or anecdotal data from colleagues. I use the term “Scientist” broadly here. In other settings this could be a policy-maker or product manager.

Statistician

The statistician in concert with the scientist designs the analytic system that will analyze the data generated by any data collection efforts. They specify how the system will operate what outputs it will generate and obtain any resources needed to implement the system. The statistician draws on statistical theory and personal experience to choose the different components of the analytic system that will be applied.

The statistician’s role here is to apply the data analytic system to the data and to produce the data analytic output. This output could be a regression coefficient a mean a plot or a prediction. But there must be something produced that we can compare to our set of expected outcomes. If the output deviates from our set of expected outcomes then the next task is to identify the reasons for that deviation.

Systems Engineer

Once the analytic system is applied to the data there are only two possible outcomes:


The outputs meet our expectations or
The output does not meet our expectations (an anomaly).


In the case of an anomaly the systems engineer’s responsibility is to diagnose the potential root causes of the anomaly based on knowledge of the data collection process the analytic system and the state of scientific knowledge.

Recently Emma Vitz wrote on Twitter:


How do you teach debugging to people who are more junior? I feel like it’s such an important skill and yet we seem to have no structured framework for teaching it


For software and for data analysis alike the challenge is that bugs or unexpected behavior can originate from anywhere. Any complex system is composed of multiple components some of which may be your responsibility and many of which are someone else’s. But bugs and anomalies do not respect those boundaries! There may be an issue that occurs in one component that only becomes known when you see the data analytic output.

So if you are responsible for diagnosing a problem it is your responsibility to investigate the behavior of each component of the system. If it is something you are not that familiar with then you need to become familiar with it either by learning on your own or (more likely) talking to the person who is in fact responsible.

A common source of unexpected behavior in data analytic output is the data collection process but the statistician who analyzes the data may not be responsible for that aspect of the project. Nevertheless the systems engineer who identifies an anomaly has to go back through and talk to the statistician and the scientist to figure out exactly how each component works.

Ultimately the systems engineer is tasked with taking a broad view of all the activities that affect the output from a data analysis in order to identify any deviations from what we would expect. Once those root causes have been explained we can then move on to decide how we should act on this new information.

Politician

The politician’s job is to make decisions while balancing the needs of the various constituents to achieve a reasonable outcome. Most statisticians and scientist that I know would recoil at the idea of being considered a politician or that politics in any form would play a role in doing any sort of science. However my thinking here is a bit more basic: In any data analysis iteration we are constantly making decisions about what to do keeping in mind a variety of conflicting factors. In order to resolve these conflicts and come to a reasonable agreement one has to engage a key skill which is negotiation.

At various stages of the data analytic iteration the politician must negotiate about (1) the definition of success in the analysis; (2) resources for executing the analysis; and (3) the decision for what to do after we have seen the output from the analytic system and have diagnosed the root causes of any anomalies. Decisions about what to do next fundamentally involve factors outside the data and the science.

Politicians have to identify who the stakeholders of the problem are and what is it that they ultimately want (as opposed to what their position is). For example an investigator might say “We need a p-value less than 0.05”. That’s their position. But what they want is more likely “a publication in a high profile journal”. Another example  might be an investigator who needs to meet a tight publication deadline while another investigator who wants to run a time-consuming (but more robust) analysis. Clearly the positions conflict but arguably both investigators share the same goal which is a rigorous high-impact publication.

Identifying positions versus underlying needs is a key task in negotiating a reasonable outcome for everyone involved. Rarely in my experience does this process have to do with the data (although data may be used to make certain arguments). The dominating elements of this process tend to be the nature of relationships between each constituent and the constraints on resources (such as time).

Applying the Iteration

If you’re reading this and find yourself saying “I’m not an X” where X is either scientist statistician systems engineer or politician then chances are that is where you are weak at data science. I think a good data scientist has to have some skill in each of these domains in order to be able to complete the basic data analytic iteration.

In any given analysis the iteration may be applied anywhere from once to dozens if not hundreds of times. If you’ve ever made two plots of the same dataset you’ve likely done two iterations. While the exact details and frequency of the iterations may vary widely across applications the core nature and the skills involved do not change much."
2020,11,19,A Typology of Data Relationships,https://statswithcats.net/2020/11/19/a-typology-of-data-relationships/,Nine patterns of three types of relationships that aren’t spurious. When analysts see a large correlation coefficient they begin speculating about possible reasons. They’ll naturally gravitate toward their initial hypothesis (or&#160;preconceived notion) which set them to investigate the data relationship &#8230; Continue reading &#8594;
2020,11,12,Machine Learning vs AI Business Models – What’s New with the Economics of AI?,https://analyticstrategy.com/machine-learning-vs-ai-business-models/?utm_source=rss&utm_medium=rss&utm_campaign=machine-learning-vs-ai-business-models,"some of the barriers in building a successful AI company
The post Machine Learning vs AI Business Models &#8211; What&#8217;s New with the Economics of AI? appeared first on Analytic Strategy Partners."
2020,11,8,Being Creative with Video on LinkedIn,https://ryanswanstrom.com/2020/11/07/being-creative-with-video-on-linkedin/,"https://youtu.be/TSP5BJs8LsQ
The post Being Creative with Video on LinkedIn appeared first on Ryan Swanstrom."
2020,11,5,Beginner Python Tutorial: Analyze Your Personal Netflix Data,https://www.dataquest.io/blog/python-tutorial-analyze-personal-netflix-data/,"How much time have you spent watching The Office on Netflix? Find out with this entry-level tutorial on analyzing your own Netflix usage data!
The post Beginner Python Tutorial: Analyze Your Personal Netflix Data appeared first on Dataquest."
2020,11,3,Self-promotion for researchers,https://robjhyndman.com/seminars/self-promotion/,Talk given at the 2020 ACEMS retreat I discuss maintaining a public profile and using social media to your advantage. I cover personal websites online repositories such as arXiv Google Scholar profiles ORCID profiles and using twitter and other social media platforms.
2020,11,2,Modern strategies for time series regression,https://robjhyndman.com/publications/moderntsreg/,This paper discusses several modern approaches to regression analysis involving time series data where some of the predictor variables are also indexed by time. We discuss classical statistical approaches as well as methods that have been proposed recently in the machine learning literature. The approaches are compared and contrasted and it will be seen that there are advantages and disadvantages to most currently available approaches. There is ample room for methodological developments in this area.
2020,10,28,COVID-19 impacts on our energy system,https://robjhyndman.com/seminars/covid19-energy/,Presentation given for the Energy Research Institutes Council of Australia My talk starts at 19:00.
2020,10,27,Ten years of forecast reconciliation,https://robjhyndman.com/seminars/reconciliation_review_talk/,"Keynote talk given at International Symposium on Forecasting 2020 It is common to forecast at different levels of aggregation. For example a retail company will want national forecasts state forecasts and store-level forecasts. And they will want them for all products for groups of products and for individual products.
Ten years ago anyone doing such forecasts needed to select between bottom-up top-down or middle-out methods. Then optimal forecast reconciliation was introduced and a new and better approach was available."
2020,10,26,Call for papers: Innovations in hierarchical forecasting,https://robjhyndman.com/hyndsight/ijf-hierarchical/,"There is a new call for papers for a special issue of the International Journal of Forecasting on &ldquo;Innovations in hierarchical forecasting&rdquo;.
Guest editors: George Athanasopoulos Rob J Hyndman Anastasios Panagiotelis and Nikolaos Kourentzes.
Submission deadline: 31 August 2021."
2020,10,26,The Importance of Data Sharing in Organizations,https://www.mdmgeek.com/2020/10/26/the-importance-of-data-sharing-in-organizations/,"According to Gartner by 2023 organizations that promote data sharing will outperform their peers on most business value metrics. Gartner says data and analytics leaders who share data externally generate three times more measurable economic benefits than those who do not. The impact on business performance due to data sharing is real and organizations seem [&#8230;]
The post The Importance of Data Sharing in Organizations appeared first on MDMgeek."
2020,10,24,Nhung Ho – Data Science in a Cloud World,https://ryanswanstrom.com/2020/10/24/nhung-ho-data-science-in-a-cloud-world/,"This is a great talk for data scientists and managers of technology teams. If you do data science in 2020 or beyond there is a good chance the cloud will be involved. Topics covered: Lessons learned when migrating data science (or technology in general) to the cloudAI services available via different cloud providersWorkflows in the [...]
The post Nhung Ho &#8211; Data Science in a Cloud World appeared first on Ryan Swanstrom."
2020,10,24,Six Ways Machine Learning Threatens Social Justice,https://www.predictiveanalyticsworld.com/blog/six-ways-machine-learning-threatens-social-justice/,"By: Eric Siegel Predictive Analytics World Originally published in Big Think When you harness the power and potential of machine learning there are also some drastic downsides that you&#8217;ve got to manage. Deploying machine learning you face the risk that it be discriminatory biased inequitable exploitative or opaque. In this article I cover six ways [&#8230;]
The post Six Ways Machine Learning Threatens Social Justice appeared first on Predictive Analytics World."
2020,10,22,Our Favorite Questions,https://www.oreilly.com/radar/our-favorite-questions/,&#8220;On peut interroger n&#8217;importe qui dans n&#8217;importe quel état; ce sont rarement les réponses qui apportent la vérité mais l&#8217;enchaînement des questions.&#8220; &#8220;You can interrogate anyone no matter what their state of being.&#160; It&#8217;s rarely their answers that unveil the truth but the sequence of questions that you have to ask.&#8220;–&#160; Inspector Pastor in La [&#8230;]
2020,10,22,The Evolution of Data Science … As I Remember It,https://statswithcats.net/2020/10/22/the-evolution-of-data-science-as-i-remember-it/,Those who cannot remember the past are condemned to repeat it. George Santayana History isn’t always clear-cut. It’s written by anyone with the will to write it down and the forum to distribute it. It’s valuable to understand different perspectives &#8230; Continue reading &#8594;
2020,10,21,R vs Python for Data Analysis — An Objective Comparison,https://www.dataquest.io/blog/python-vs-r/,"Python vs. R — which is better for data science? We compare the two languages side by side and see how Python and R perform on the same analysis steps.
The post R vs Python for Data Analysis — An Objective Comparison appeared first on Dataquest."
2020,10,21,Model selection in reconciling hierarchical time series,https://robjhyndman.com/publications/chfr/,Model selection has been proven an effective strategy for improving accuracy in time series forecasting applications. However when dealing with hierarchical time series apart from selecting the most appropriate forecasting model forecasters have also to select a suitable method for reconciling the base forecasts produced for each series to make sure they are coherent. Although some hierarchical forecasting methods like minimum trace are strongly supported both theoretically and empirically for reconciling the base forecasts there are still circumstances under which they might not produce the most accurate results being outperformed by other methods.
2020,10,20,Co-authorships for sale,https://robjhyndman.com/hyndsight/coauthorships-for-sale/,"This is an interesting development! How many papers are published by bogus authors and what is the going price for a coauthorship? Needless to say this is appalling and contrary to every academic integrity policy I&rsquo;ve seen. See the Monash authorship policy for example.
 Dear Hyndman Rob J.
Hope you are doing well.
I write this letter on behalf of authors seeking to co-publish. We have seen your previous works (https://www."
2020,10,19,Job Opening at Decision Management Solutions – Delivery Management,http://feedproxy.google.com/~r/jtonedm/~3/1cps1d_dKz0/,Copyright © 2021 https://jtonedm.com James Taylor POSITION FILLED Decision Management Solutions is growing and looking for a Delivery Manager for its projects. The Delivery Manager will be an experienced hybrid agile project manager and will be responsible for managing several concurrent discipline based high visibility projects using agile and fixed milestone methods in a fast-paced [&#8230;]
2020,10,16,PII. The Great Taboo of Data Analysis,https://statswithcats.net/2020/10/16/pii-the-great-taboo-of-data-analysis/,Many of us have had our personal information stolen from the Internet some more than once. Even governments can’t prevent the thefts. Professionals who work with data come under a lot of scrutiny because of PII. Continue reading &#8594;
2020,10,14,"Weight Agnostic Neural Networks, a virtual presentation by Adam Gaier, Thursday October 15th, LightOn AI meetup #7",https://nuit-blanche.blogspot.com/2020/10/weight-agnostic-neural-networks-virtual.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;Ever since we started LightOn we have been putting some emphasis on having great minds think how new algorithms are possible and how they can be enabled with our photonic chips.&nbsp; We also have a regular meetup where we see how other great minds are devising new algorithms.&nbsp;Tomorrow Thursday (October 15th) we are continuing this&nbsp;journey by having Adam Gaier&nbsp;who will talk to us about Weight Agnostic Neural Networks. The virtual meetup will start at:16:00 (UTC+2) Paris time but also&nbsp;7AM PST&nbsp;10AM CST&nbsp;11PM JST.&nbsp;To have more information about connecting to the meetup please register here: https://meetup.com/LightOn-meetup/events/273660363/Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,10,13,AI Product Management After Deployment,https://www.oreilly.com/radar/ai-product-management-after-deployment/,The field of AI product management continues to gain momentum. As the AI product management role advances in maturity more and more information and advice has become available. Our previous articles in this series introduce our own take on AI product management discuss the skills that AI product managers need and detail how to bring [&#8230;]
2020,10,12,Why Great Machine Learning Models are Never Enough: Three Lessons About Data Science from Dr. Foege’s Letter,https://analyticstrategy.com/data-science-lessons-from-the-foege-letter/?utm_source=rss&utm_medium=rss&utm_campaign=data-science-lessons-from-the-foege-letter,"Foege&#8217;s Letter In September William H. Foege MD MPH sent a private letter to Robert Redfield the Director of the CDC reminding him that the &#8220;best decisions come from the best science&#8221; and the &#8220;best results come from the best management.&#8221; The letter became public on October 6 2020 in a USA today article written [&#8230;]
The post Why Great Machine Learning Models are Never Enough: Three Lessons About Data Science from Dr. Foege&#8217;s Letter appeared first on Analytic Strategy Partners."
2020,10,10,As The World Turns: Implementations now on ArXiv thanks to Paper with Code,https://nuit-blanche.blogspot.com/2020/10/as-world-turns-implementations-now-on.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;**&nbsp;It's the little things.&nbsp;In the 2000s after featuring good work on Nuit Blanche I was usually following through by asking authors where their codes were. This is how the implementation tag was born. Some of the answers were along the lines of: ""I didn't make it available because I thought it was not worthy"". But what I usually responded was that in effect releasing one's code had a compounding effect on the community:&nbsp;""You may not think it's worthy of release but somehow someone somewhere needs your code for reasons you cannot fathom""&nbsp;As a result I made a conscious choice of featuring those papers that were actively featuring their implementations. The earliest post with featured implementations was February 28th 2007 with a blog post featuring three different implementations of reconstruction solver for compressed sensing. Yes implementations were already available before that but within the compressive sensing community it was a point in time with a collective realization that releasing one's code would bring others to reuse one's work and advance the field as a result. At some point I started making a long list of implementation available but got swamped after a while because it became most of the time the default behavior (a good thing).Five years ago Samim Winiger started GitXiv&nbsp;around Machine Learning papers. I was ecstatic but the site eventually stopped working. Two years ago the&nbsp;Paper with code site started around the same issue and flourished. Congratulations to Robert Ross Marcin Viktor and Ludovic&nbsp;on starting a vibrant community around this need for listing papers with their attendant code. Two days ago the next logical step occurred with the featuring of codes within ArXiv a fantastic advance for Science. Woohoo!Congratulations to&nbsp;Robert&nbsp;Ross&nbsp;Marcin&nbsp;Viktor and&nbsp;Ludovic&nbsp;on making this happen!&nbsp;My next question is:&nbsp;When are they going to get a prize for this? Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,10,8,How to Learn Fast: 7 Science-Backed Study Tips for Learning New Skills,https://www.dataquest.io/blog/how-to-learn-fast-science-based-study-tips/,"Want to learn a new skill as fast as possible? These science-based tips will show you how you can learn more efficiently.
The post How to Learn Fast: 7 Science-Backed Study Tips for Learning New Skills appeared first on Dataquest."
2020,10,6,“Not Enough Memory” — How Data Skills Ended an Excel Nightmare,https://www.dataquest.io/blog/dataquest-changed-my-life/,"How learning data science and Python with Dataquest helped Curtly Critchlow solve a massive Excel problem at his job.
The post “Not Enough Memory” — How Data Skills Ended an Excel Nightmare appeared first on Dataquest."
2020,10,6,Machine Learning Week 2021 Call for Speakers,http://feedproxy.google.com/~r/jtonedm/~3/9I7_tQdYQYk/,Copyright © 2021 https://jtonedm.com James Taylor The team at Machine Learning Week/Predictive Analytics World has announced the schedule for 2021 (virtual conference May 24-28 2021) and issued their call for speakers. This is a great conference and will be a great opportunity to present. As always those with case studies and real experience will be [&#8230;]
2020,10,4,How to Write a Great Data Science Resume,https://www.dataquest.io/blog/how-data-science-resume-cv/,"How can you get a data science job? It all starts with a great resume: one that frames your data analysis and data science projects in the right way.
The post How to Write a Great Data Science Resume appeared first on Dataquest."
2020,10,2,Getting Executive Support for Machine Learning – Backwards,http://feedproxy.google.com/~r/jtonedm/~3/1G5SM28_RE8/,Copyright © 2021 https://jtonedm.com James Taylor Eric Siegel and I had a great discussion about doing Machine Learning BACKWARDS recently &#8211; you can watch the recording below or on our YouTube Channel. Eric if you don&#8217;t know is the&#160;founder of Predictive Analytics World a leading consultant and author of &#8220;Predictive Analytics&#8220;. You can also check [&#8230;]
2020,10,1,How Long Does it Take to Learn Python?,https://www.dataquest.io/blog/how-long-does-it-take-to-learn-python/,"Are you ready to learn Python for Data Science? With the right program habits and structure you can master it more quickly than you might think.
The post How Long Does it Take to Learn Python? appeared first on Dataquest."
2020,9,27,Anecdotes and Big Data,https://statswithcats.net/2020/09/27/anecdotes-and-big-data/,"An anecdote is a kitten gently licking your face with a warm wet 
raspy tongue. Big data is a three-inch high-pressure firehose held an arm’s length away. They have to be treated quite differently.
 Continue reading &#8594;"
2020,9,22,Practical Data Ethics — How You Can Make Your Data Work More Ethical,https://www.dataquest.io/blog/practical-data-ethics-make-your-data-work-more-ethical/,"How can you as a junior data professional help ensure your company's data work is more ethical? Here are practical strategies and tactics.
The post Practical Data Ethics — How You Can Make Your Data Work More Ethical appeared first on Dataquest."
2020,9,16,Making Learning to Code Friendlier with Art — An Interview with Dr. Allison Horst,https://www.dataquest.io/blog/making-learning-to-code-friendlier-with-art-allison-horst-interview/,"Learning to code for the first time can be intimidating. One way to make it more approachable? Fun pictures of fuzzy monsters.
The post Making Learning to Code Friendlier with Art — An Interview with Dr. Allison Horst appeared first on Dataquest."
2020,9,16,21 Places to Find Free Datasets for Data Science Projects,https://www.dataquest.io/blog/free-datasets-for-projects/,"A collection of the best places to find free data sets for data visualization data cleaning machine learning and data processing projects.
The post 21 Places to Find Free Datasets for Data Science Projects appeared first on Dataquest."
2020,9,15,How to Set AI Goals,https://www.oreilly.com/radar/how-to-set-ai-goals/,AI Benefits and Stakeholders AI is a field where value in the form of outcomes and their resulting benefits is created by machines exhibiting the ability to learn and “understand” and to use the knowledge learned to carry out tasks or achieve goals. AI-generated benefits can be realized by defining and achieving appropriate goals. These [&#8230;]
2020,9,15,Coursera’s “Machine Learning for Everyone” Fulfills Unmet Training Requirements,https://www.predictiveanalyticsworld.com/blog/courseras-machine-learning-for-everyone-fulfills-unmet-training-requirements/,"By: Eric Siegel Predictive Analytics World My new course series on Coursera Machine Learning for Everyone (free access) fulfills two different kinds of unmet learner needs. It’s a conceptually-complete end-to-end course series – its three courses amount to the equivalent of a college or graduate-level course – that covers both the technology side and the [&#8230;]
The post Coursera&#8217;s &#8220;Machine Learning for Everyone&#8221; Fulfills Unmet Training Requirements appeared first on Predictive Analytics World."
2020,9,10,Do You Need a Grand Strategy in Analytics?,https://analyticstrategy.com/do-you-need-a-grand-strategy-in-analytics/?utm_source=rss&utm_medium=rss&utm_campaign=do-you-need-a-grand-strategy-in-analytics,"In foreign affairs and national defense especially among academics it has becoming more common to talk about grand strategies. There is a very popular course at Yale University by John Lewis Gaddis called On Grand Strategy and in 2018 he published a book worth reading with the same name. An emerging definition for grand strategy [&#8230;]
The post Do You Need a Grand Strategy in Analytics? appeared first on Analytic Strategy Partners."
2020,9,10,11 High-Paying Data Analytics Jobs in 2020,https://www.dataquest.io/blog/10-data-analytics-jobs/,"Thinking about kickstarting a career in data analytics? These 10 high-paying jobs may just be the motivation you need to learn more about the data science industry and gain the specific skills you need to succeed. 
The post 11 High-Paying Data Analytics Jobs in 2020 appeared first on Dataquest."
2020,9,8,Pair Programming with AI,https://www.oreilly.com/radar/pair-programming-with-ai/,In a conversation with Kevlin Henney we started talking about the kinds of user interfaces that might work for AI-assisted programming. This is a significant problem: neither of us were aware of any significant work on user interfaces that support collaboration. However as software developers many of us have been practicing effective collaboration for years. [&#8230;]
2020,9,4,Going Live to Grow Your Business – An Interview with Lindy Chapman,https://ryanswanstrom.com/2020/09/04/going-live-to-grow-your-business-an-interview-with-lindy-chapman/,"https://youtu.be/PQ54qHYxAXU
The post Going Live to Grow Your Business &#8211; An Interview with Lindy Chapman appeared first on Ryan Swanstrom."
2020,9,3,How Machine Learning Works – in 20 Seconds,https://www.predictiveanalyticsworld.com/blog/how-machine-learning-works-in-20-seconds/,"By:  Eric Siegel Predictive Analytics World This transcript comes from Coursera’s online course series Machine Learning for Everyone with Eric Siegel. In 57 words here&#8217;s why machine learning’s important: Business needs prediction. Prediction requires machine learning. And machine learning depends on data. Putting that in reverse we have data we give it to machine learning [&#8230;]
The post How Machine Learning Works – in 20 Seconds appeared first on Predictive Analytics World."
2020,9,2,Data Visualization in R with ggplot2: A Beginner Tutorial,https://www.dataquest.io/blog/data-visualization-in-r-with-ggplot2-a-beginner-tutorial/,"A famous general is thought to have said “A good sketch is better than a long speech.” That advice may have come from the battlefield but it's applicable in lots of other areas — including data science. ""Sketching"" out our data by visualizing it using ggplot2 in R is more impactful than simply describing the [&#8230;]
The post Data Visualization in R with ggplot2: A Beginner Tutorial appeared first on Dataquest."
2020,9,2,Tutorial: Web Scraping with Python Using Beautiful Soup,https://www.dataquest.io/blog/web-scraping-tutorial-python/,"Web scraping allows us to extract information from web pages. In this tutorial you'll learn how to perform web scraping with Python and BeautifulSoup.
The post Tutorial: Web Scraping with Python Using Beautiful Soup appeared first on Dataquest."
2020,9,1,How to Use If-Else Statements and Loops in R,https://www.dataquest.io/blog/control-structures-in-r-using-loops-and-if-else-statements/,"Learn to use if-else statements for loops and while loops to build complex conditional programs in R a valuable skill for aspiring data scientists and R programmers alike.
The post How to Use If-Else Statements and Loops in R appeared first on Dataquest."
2020,8,28,Do You Post Too Much? Analyze Your Personal Facebook Data with Python,https://www.dataquest.io/blog/analyze-facebook-data-python/,"As of Q2 2020 Facebook claims more than 2.7 billion active users. That means that if you're reading this article chances are you're a Facebook user. But just how much of a Facebook user are you? How much do you really post? We can find out using Python!&#160;Specifically we're going to use Python to create [&#8230;]
The post Do You Post Too Much? Analyze Your Personal Facebook Data with Python appeared first on Dataquest."
2020,8,28,The geometry of forecast reconciliation,https://robjhyndman.com/seminars/geometry-reconciliation/,"Talk given at Macquarie University Sydney. It is common to forecast at different levels of aggregation. For example a retail company will want national forecasts state forecasts and store-level forecasts. And they will want them for all products for groups of products and for individual products. Forecast reconciliation methods allow for the forecasts at all levels of aggregation to be adjusted so they are consistent with each other.
I will describe a geometric interpretation for reconciliation methods used to forecast time series that adhere to known linear constraints."
2020,8,26,Best Data Science Books in 2020 (Vetted by Experts),https://www.dataquest.io/blog/data-science-books/,"Learn Python R machine learning social media scraping and much more from these free data science books you can download today.
The post Best Data Science Books in 2020 (Vetted by Experts) appeared first on Dataquest."
2020,8,26,Palantir Shows Its Cards,https://simplystatistics.org/2020/08/26/palantir-shows-its-cards/,"File this under long-term followup but just about four years ago I wrote about Palantir the previously secretive but now soon to be public data science company and how its valuation was a commentary on the value of data science more generally. Well just recently Palantir filed to go public and therefore submitted a registration statement (S-1) describing its business. It&rsquo;s a fascinating read if you&rsquo;re into that kind of stuff.

But the important thing is that Palantir itself summarized the question I asked more than 4 years ago. In their enumeration of risk factors one risk factor they highlight is


If our customers are not able or willing to accept our product-based business model instead of a labor-based business model our business and results of operations could be negatively impacted. [emphasis added]


In my original post I wrote about the &ldquo;Data Science Spectrum&rdquo; which has consulting on one end and software on the other.



The point of the diagram was that businesses on the right hand side have huge valuations while businesses on the left side have merely large valuations. The people running Palantir clearly understand this and are trying to push the company in a software-based productized direction.

Here&rsquo;s the rest of their summary of this risk factor:


Our platforms are generally offered on a productized basis to minimize our customers’ overall cost of acquisition maintenance and deployment time of our platforms. Many of our customers and potential customers are instead generally familiar with the practice of purchasing or licensing software through labor contracts where custom software is written for specific applications the intellectual property in such software is often owned by the customer and the software typically requires additional labor contracts for modifications updates and services during the life of that specific software. Customers may be unable or unwilling to accept our model of commercial software procurement. Should our customers be unable or unwilling to accept this model of commercial software procurement our growth could be materially diminished which could adversely impact our business financial condition results of operations and growth prospects.


Those of us who do data analysis for a living already know this to be true. Custom consulting is not scalable and therefore not as valuable as a piece of boxed up software which is infinitely scalable.

Show Me The Numbers

So how is Palantir doing?

At first glance it seems their doing pretty well. Their gross profit (Revenue - Cost of Revenue) suggests about a 72% gross margin percentage for 2018 and 67% for 2019 which both seem high. These gross margin percentages are in software company territory. (For comparison Facebook&rsquo;s percentage runs around 80%.) This suggests that each dollar of Palantir&rsquo;s revenue does not have a lot of direct costs associated with it.

But ulimately Palantir has posted net losses every year indicating there are significant indirect costs to generating that revenue. In particular their Sales and marketing costs almost equal their entire gross profit. Reading through the S-1 this ultimately is not surprising. Palantir itself admits that


Our sales efforts involve considerable time and expense and our sales cycle is often long and unpredictable.


Alas there is some consulting to do after all. My guess is that much of the up front &ldquo;sales&rdquo; work comes down to Palantir having to


Figure out a customer&rsquo;s problem and what question they&rsquo;re asking;
Figure out how a customer&rsquo;s data are organized;
Figure out how to their existing software products to the customer&rsquo;s specific situation.


This should sound familiar to seasoned data scientists. Indeed this is almost all the work of the data scientist. This is expensive because it requires humans to do it and there&rsquo;s typically not much to generalize from customer to customer. Implementing the software and deploying it is work too but is often more straightforward and their are often existing solutions that can be employed.

The Road to Profits

So here&rsquo;s the problem: Palantir&rsquo;s route to profitability involves making these costs go down&hellip;a lot. Maybe not to zero but substantially because each new customer&mdash;with their different problems and different data&mdash;costs a lot to acquire. If they can do this they&rsquo;ve cracked the nut of data science scalability.

Another big expense is Research and Development which the company describes as developing new methods for data analysis (machine learning tools etc.). While it&rsquo;s nice to have room to do open-ended research on new data science tools my guess is that this line item goes down a lot in the near future as it often does at companies that start off with large R&amp;D budgets. Ultimately it would save Palantir ~$300 million a year.

See you in another four years?"
2020,8,25,WHAT ARE THE ODDS?,https://statswithcats.net/2020/08/25/what-are-the-odds/,Probability is the core of statistics. You hear the phrase “what’s the probability of …” all the time in statistics. You also hear that phrase in everyday life too. What’s the probability of rain tomorrow? What’s the probability of winning &#8230; Continue reading &#8594;
2020,8,24,How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial,https://www.dataquest.io/blog/jupyter-notebook-tutorial/,"Use this tutorial to learn how to create your first Jupyter Notebook important terminology and how easily notebooks can be shared and published online.
The post How to Use Jupyter Notebook in 2020: A Beginner’s Tutorial appeared first on Dataquest."
2020,8,20,Top Tips for Learning R from Africa R’s Shelmith Kariuki,https://www.dataquest.io/blog/top-tips-for-learning-r-from-africa-rs-shelmith-kariuki/,"If you’re just at the beginning of your journey learning R programming and you're looking for tips there’s a lot you can learn from Shelmith Kariuki.Shel has years of professional experience working in data science and years of experience teaching statistics and data skills to others. She’s a data analyst an RStudio certified Tidyverse instructor [&#8230;]
The post Top Tips for Learning R from Africa R’s Shelmith Kariuki appeared first on Dataquest."
2020,8,19,How to Become a Data Scientist (Step-By-Step) in 2020,https://www.dataquest.io/blog/how-to-become-a-data-scientist/,"Data science is one of the most buzzed about fields right now and&#160;data scientists are in extreme demand. And with good reason — data scientists are doing everything from&#160;creating self-driving cars&#160;to&#160;automatically captioning images. Given all the interesting applications it makes sense that data science is a very sought-after career.&#160;&#160;Data science is applied in many field [&#8230;]
The post How to Become a Data Scientist (Step-By-Step) in 2020 appeared first on Dataquest."
2020,8,18,Why Best-of-Breed is a Better Choice than All-in-One Platforms for Data Science,https://www.oreilly.com/radar/why-best-of-breed-is-a-better-choice-than-all-in-one-platforms-for-data-science/,So you need to redesign your company’s data infrastructure. Do you buy a solution from a big integration company like IBM Cloudera or Amazon?&#160; Do you engage many small startups each focused on one part of the problem?&#160; A little of both?&#160; We see trends shifting towards focused best-of-breed platforms. That is products that are [&#8230;]
2020,8,15,Python API Tutorial: Getting Started with APIs,https://www.dataquest.io/blog/python-api-tutorial/,"In this data science tutorial learn about APIs by analyzing data from the international space station in this step-by-step Python API tutorial.
The post Python API Tutorial: Getting Started with APIs appeared first on Dataquest."
2020,8,14,Can Anyone Learn to Code? Yes! (It’s Science),https://www.dataquest.io/blog/can-anyone-learn-to-code-yes-its-science/,"Anyone can learn to code — and that's not just a nice-sounding platitude there's real science backing it up. Not a math person? That could actually help...
The post Can Anyone Learn to Code? Yes! (It’s Science) appeared first on Dataquest."
2020,8,14,Ensemble forecasts using fable,https://robjhyndman.com/seminars/nyrc2020/,Talk given at the 2020 R conference New York. For over 50 years we have known that ensemble forecasts perform better than individual methods yet they are not as widely used as they should be. Perhaps this is because users think it is more work or that it is hard to get prediction intervals or that it is difficult to determine the relative weights of the component methods. The fable package solves these problems and makes it easy to produce probabilistic forecasts using ensembles across many time series.
2020,8,14,Calculating variance in the log domain,https://justindomke.wordpress.com/2020/08/14/calculating-variance-in-the-log-domain/,Say you&#8217;ve got a positive dataset and you want to calculate the variance. However the numbers in your dataset are huge so huge you need to represent them in the log-domain. How do you compute the log-variance without things blowing up? I ran into the problem today. To my surprise I couldn&#8217;t find a standard &#8230; Continue reading Calculating variance in the log&#160;domain &#8594;
2020,8,11,How to Learn Python for Data Science In 5 Steps,https://www.dataquest.io/blog/how-to-learn-python-for-data-science-in-5-steps/,"Interested in how to learn Python for data science? It's not as hard as you think! Here's a clear roadmap for learning Python programming and other data science skills.
The post How to Learn Python for Data Science In 5 Steps appeared first on Dataquest."
2020,8,11,WHY YOU NEED TO TAKE STATS 101,https://statswithcats.net/2020/08/11/why-you-need-to-take-stats-101/,Whether you’re in high school college or a continuing education program you may have the opportunity or be required to take an introductory course in statistics call it Stats 101. It’s certainly a requirement for degrees in the sciences and &#8230; Continue reading &#8594;
2020,8,10,When You Need to Deploy Predictive Models Safely,https://analyticstrategy.com/deploying-analytic-models-safely/?utm_source=rss&utm_medium=rss&utm_campaign=deploying-analytic-models-safely,"Little languages. A key insight in the development of Unix was that there was an important role for what became known as little languages which are simple specialized languages for executing important types of tasks. The insight was that it is much easier to design a little language that can be implemented efficiently for a [&#8230;]
The post When You Need to Deploy Predictive Models Safely appeared first on Analytic Strategy Partners."
2020,8,10,Go to war with the data you have,http://feedproxy.google.com/~r/jtonedm/~3/mLRFHCv7QkY/,Copyright © 2021 https://jtonedm.com James Taylor A few months back Scott Adams posted a great Dilbert that I have been meaning to write about for a while (click on the image to see the original). In the strip Dilbert says &#8220;You don&#8217;t go to war with the data you need. You go to war with [&#8230;]
2020,8,5,Tutorial: Getting Started with R and RStudio,https://www.dataquest.io/blog/tutorial-getting-started-with-r-and-rstudio/,"Get your R programming journey off on the right foot with this RStudio tutorial that walks through everything from installation to best practices.
The post Tutorial: Getting Started with R and RStudio appeared first on Dataquest."
2020,8,4,When Learning is Hard: 3 Ways to Make it Easier (Guest Post),https://www.dataquest.io/blog/3-ways-to-make-learning-easier/,"The following is a guest post by Darya Jandossova Troncoso and does not necessarily represent the views or opinions of Dataquest.Learning is a lifelong process. It starts when we're babies and follows us into old age. Education is essential to our development and to how we see the world. The desire for knowledge starts at [&#8230;]
The post When Learning is Hard: 3 Ways to Make it Easier (Guest Post) appeared first on Dataquest."
2020,8,4,"Probabilistic forecast reconciliation: properties, evaluation and score optimisation",https://robjhyndman.com/publications/coherentprob/,We develop a framework for prediction of multivariate data that follow some known linear constraints such as the example where some variables are aggregates of others. This is particularly common when forecasting time series (predicting the future) but also arises in other types of prediction. For point prediction an increasingly popular technique is reconciliation whereby predictions are made for all series (so-called &ldquo;base&rdquo; predictions) and subsequently adjusted to ensure coherence with the constraints.
2020,8,1,PROBABILITY IS SIMPLE … KINDA,https://statswithcats.net/2020/08/01/probability-is-simple-kinda/,Language isn’t very precise in dealing with uncertainty. Probably is more certain than possibly but who knows where dollars-to-doughnuts falls on the spectrum. Statistics needs to deal with uncertainty more quantitatively. That’s where probability comes in. For the most part &#8230; Continue reading &#8594;
2020,7,28,Bringing an AI Product to Market,https://www.oreilly.com/radar/bringing-an-ai-product-to-market/,The Core Responsibilities of the AI Product Manager Product Managers are responsible for the successful development testing release and adoption of a product and for leading the team that implements those milestones. Product managers for AI must satisfy these same responsibilities tuned for the AI lifecycle. In the first two articles in this series we [&#8230;]
2020,7,28,"Power, Harms, and Data",https://www.oreilly.com/radar/power-harms-and-data/,A recent article in The Verge discussed PULSE an algorithm for “upsampling” digital images. PULSE when applied to a low-resolution image of Barack Obama recreated a White man&#8217;s face; applied to Alexandria Ocasio-Cortez it built a White woman&#8217;s face.  It had similar problems with other images of Black and Hispanic people frequently giving them White [&#8230;]
2020,7,24,Contraceptive forecasting competition,https://robjhyndman.com/hyndsight/contraceptive-forecasting/,"Here&rsquo;s an interesting new forecasting competition that came via my inbox this week.
 Contraceptive access is vital to safe motherhood healthy families and prosperous communities. Greater access to contraceptives enables couples and individuals to determine whether when and how often to have children. In low- and middle-income countries (LMIC) around the world health systems are often unable to accurately predict the quantity of contraceptives necessary for each health service delivery site in part due to insufficient data limited staff capacity and inadequate systems."
2020,7,21,"AI, Protests, and Justice",https://www.oreilly.com/radar/ai-protests-and-justice/,Largely on the impetus of the Black Lives Matter movement the public&#8217;s response to the murder of George Floyd and the subsequent demonstrations we&#8217;ve seen increased concern about the use of facial identification in policing. First in a highly publicized wave of announcements IBM Microsoft and Amazon have announced that they will not sell face [&#8230;]
2020,7,21,Distributed ARIMA Models for Ultra-long Time Series,https://robjhyndman.com/publications/darima/,Providing forecasts for ultra-long time series plays a vital role in various activities such as investment decisions industrial production arrangements and farm management. This paper develops a novel distributed forecasting framework to tackle challenges associated with forecasting ultra-long time series by utilizing the industry-standard MapReduce framework. The proposed model combination approach facilitates distributed time series forecasting by combining the local estimators of ARIMA (AutoRegressive Integrated Moving Average) models delivered from worker nodes and minimizing a global loss function.
2020,7,19,Podcast episode: the curious quant,https://robjhyndman.com/seminars/curious-quant-podcast/,Podcast episode for The Curious Quant Last week I had a chat with Michael Kollo for the Curious Quant podcast.
2020,7,18,Convex-concave games off the shelf,http://www.machinedlearnings.com/2020/07/convex-concave-games-off-shelf.html,If you need to solve a convex optimization problem nowadays you are in great shape.  Any problem of the form $$\begin{alignat}{2}&\!\inf_z & \qquad & f(z) \\& \text{subject to} & & h(z) = 0 \\& & & g(z) \preceq 0\end{alignat}$$ where $f$ and $g$ are convex and $h$ is affine can be attacked by several excellent freely available software packages: my current favorite is cvxpy which is a joy to use.  If you have a lot of variables and not a lot of constraints you can instead solve a dual problem.  It ends up looking like $$\begin{alignat}{2}&\!\sup_{x}  & \qquad & L(x) \\& \text{subject to} & & \tilde{h}_x(x) = 0 \\& & & \tilde{g}_x(x) \preceq 0\end{alignat}$$ where $L$ is concave assuming that you get lucky and can analytically eliminate all the primal variables $z$ such that only the dual variables $x$ remain.  But what if you can't eliminate all the primal variables but only most of them?  You might end up with a problem like $$\begin{alignat}{2}&\!\sup_{x} \inf_y & \qquad & L(x y) \\& \text{subject to} & & \tilde{h}_x(x) = 0 \\& & & \tilde{g}_x(x) \preceq 0 \\& & & \tilde{h}_y(y) = 0 \\& & & \tilde{g}_y(y) \preceq 0\end{alignat}$$ where $\tilde{g}_x$ and $\tilde{g}_y$ are convex and $\tilde{h}_x$ and $\tilde{h}_y$ are affine $L(x \cdot)$ is convex in $y$ given fixed $x$ and $L(\cdot y)$ is concave in $x$ given fixed $y$.  It feels like this problem should be easier to solve than the original problem if many primal variables have been analytically eliminated.  Unfortunately none of my favorite convex optimization toolkits will accept a problem of this form.  This is despite the viability of interior-point methods for such problems.  Bummer.One thing I tried was to solve the inner infimum using a standard toolkit compute the gradient of the solution wrt the outer parameters via the sensitivity map and then use a first-order method for the outer supremum.  This did not work for me: it works for toy problems but on real problems the outer supremum has very slow convergence suggesting ill-conditioning.  What I need is the power of interior-point methods to handle ill-conditioning via second-order information.  I'm able to achieve this via sequential quadratic minimax programming: first locally approximate the objective $L(\lambda \mu y)$ with a quadratic around the current point and linearize the constraints. $$\begin{alignat}{2}&\!\sup_{\delta x} \inf_{\delta y} & \qquad & \frac{1}{2} \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right)^\top \left(\begin{array}{cc} P_{xx} & P_{yx}^\top \\ P_{yx} & P_{yy} \end{array}\right) \left(\begin{array}{c} \delta x \\ \delta y \end{array} \right) + \left(\begin{array}{c} q_x \\ q_y \end{array} \right)^\top \left(\begin{array}{c} \delta x \\ \delta y \end{array} \right) \\& \text{subject to} & & \left(\begin{array}{cc} A_x & 0 \\ 0 & A_y \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right) = \left(\begin{array}{c} b_x \\ b_y \end{array}\right) \\& & & \left(\begin{array}{cc} G_x & 0 \\ 0 & G_y \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \end{array}\right) \preceq \left(\begin{array}{c} h_x \\ h_y \end{array}\right) \\\end{alignat}$$ The Wolfe dual converts this problem into a standard QP: $$\begin{alignat}{2}&\!\sup_{\delta x \delta y \lambda \mu} & \qquad &  \frac{1}{2} \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right)^\top \left(\begin{array}{cccc} P_{xx} & 0 & 0 & 0 \\ 0 & -P_{yy} & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) + \left(\begin{array}{c} q_x \\ 0 \\ -b_y \\ -h_y \end{array} \right)^\top \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array} \right) \\& \text{subject to} & & \left(\begin{array}{cc} A_x & 0 & 0 & 0 \\ P_{yx} & P_{yy} & A_y^\top & G_y^\top \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) = \left(\begin{array}{c} b_x \\ -q_y \end{array}\right) \\& & & \left(\begin{array}{cc} G_x & 0 & 0 & 0  \\ 0 & 0 & 0 & -I \end{array} \right) \left(\begin{array}{c} \delta x \\ \delta y \\ \lambda \\ \mu \end{array}\right) \preceq \left(\begin{array}{c} h_x \\ 0 \end{array}\right) \\\end{alignat}$$ If you solve this for $(\delta x \delta y)$ you get a Newton step for your original problem.  The step acceptance criterion here is tricky: if the iterate is feasible you want to leverage the saddle point condition (see equation (11) of Essid et. al.).  If the iterate is infeasible more sophistication is required but fortunately my constraints were actually linear so doing an initial exact inner solve allowed me to iterate while staying feasible.  (Note: if you solve a more general convex problem on each step you don't need to linearize the $x$ constraints.)YMMV!
2020,7,17,Estimating temporal variation in transmission of SARS-CoV-2 and physical distancing behaviour in Australia,https://robjhyndman.com/publications/covid19/,
2020,7,17,What To Look For In Data,https://statswithcats.net/2020/07/17/what-to-look-for-in-data/,Sometimes you have to do things when you have no idea where to start. It can be a stressful experience. If you&#8217;ve ever had to analyze a data set you know the anxiety. Deciding how and where to start exploring &#8230; Continue reading &#8594;
2020,7,16,Analytic Enterprises: Three Critical Success Factors,http://feedproxy.google.com/~r/jtonedm/~3/-eKXobyExb0/,Copyright © 2021 https://jtonedm.com James Taylor Working with companies that are investing in becoming analytic enterprises we have determined that there are three critical success factors. Whether you are focused on business analytics data mining predictive analytics machine learning artificial intelligence or all of the above these factors will be critical. Check out these videos [&#8230;]
2020,7,14,Spatial modelling of the two-party preferred vote in Australian federal elections: 2001-2016,https://robjhyndman.com/publications/elections/,We examine the relationships between electoral socio-demographic characteristics and two-party preference in the six Australian federal elections held between 2001 to 2016. Socio-demographic information is derived from the Australian Census which occurs every five years. Since a Census is not directly available for each election spatio-temporal imputation is employed to estimate Census data for the electorates at the time of each election. This accounts for both spatial and temporal changes in electoral characteristics between Censuses.
2020,7,11,Early classification of spatio-temporal events using partial information,https://robjhyndman.com/publications/eventstream/,This paper investigates early event classification in spatio-temporal data streams where events need to be classified using partial information i.e. while the event is still ongoing. The framework incorporates two early event classification algorithms with different strengths as well as an event extraction algorithm. We apply this framework to synthetic and real world problems and demonstrate its reliability and broad applicability. The algorithms and data are available in the R package eventstream and other code in the supplementary material.
2020,7,11,35 Ways Data Go Bad,https://statswithcats.net/2020/07/11/35-ways-data-go-bad/,When you take your first statistics class your professor will be a kind person who cares about your mental well-being. OK maybe not but what the professor won’t do is give you real-world data sets. The data may represent things &#8230; Continue reading &#8594;
2020,7,6,The Most Important Statistical Assumptions,https://statswithcats.net/2020/07/05/the-most-important-statistical-assumptions/,
2020,6,26,Terminology matters,https://robjhyndman.com/hyndsight/terminology-matters/,"I was reminded again this week that getting the right terminology is important. Some of my colleagues who work in machine learning wrote a paper entitled “Time series regression” which began with “This paper introduces Time Series Regression (TSR): a little-studied task …”. Statisticians and econometricians have done time series regression for many decades so this beginning led to the paper being lampooned on Twitter.
The problem arose due to clashes in terminology being used in different fields."
2020,6,24,COVID-19 and Complex Systems,https://www.oreilly.com/radar/covid-19-and-complex-systems/,In various mailing lists about the COVID-19 pandemic I&#8217;ve seen several discussions of &#8220;complex systems theory&#8221; as possibly a way to understand how the pandemic is playing out in different locations. Specifically: why have Japan and Hong Kong not experienced an explosion in cases even though their governments responded poorly to the crisis? The argument [&#8230;]
2020,6,18,Gartner Top 10 Trends include Decision Intelligence,http://feedproxy.google.com/~r/jtonedm/~3/bqSocPTL7pY/,Copyright © 2021 https://jtonedm.com James Taylor Gartner recently published a piece &#8220;Top 10 Trends in Data and Analytics 2020&#8221; that you can currently get from our friends at ThoughtSpot (registration required). It&#8217;s an interesting report you should definitely check out. My favorite section was the one on Decision Intelligence within which they include the kind [&#8230;]
2020,6,16,Decision-Making in a Time of Crisis,https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/,In the 1996 cult classic film Swingers two friends Trent and Mike (played by Vince Vaughan and Jon Favreau respectively) make an impromptu trip to Las Vegas. At the blackjack table Mike gets dealt an 11 and Trent tells him to double down. Mike responds “What?!” and Trent replies “Double down baby. You gotta double [&#8230;]
2020,6,9,Machine Learning and the Production Gap,https://www.oreilly.com/radar/machine-learning-and-the-production-gap/,The biggest problem facing machine learning today isn&#8217;t the need for better algorithms; it isn&#8217;t the need for more computing power to train models; it isn&#8217;t even the need for more skilled practitioners. It&#8217;s getting machine learning from the researcher&#8217;s laptop to production. That&#8217;s the real gap. It&#8217;s one thing to build a model; it&#8217;s [&#8230;]
2020,6,3,Hierarchical forecast reconciliation with machine learning,https://robjhyndman.com/publications/hfrml/,Hierarchical forecasting methods have been widely used to support aligned decision-making by providing coherent forecasts at different aggregation levels. Traditional hierarchical forecasting approaches such as the bottom-up and top-down methods focus on a particular aggregation level to anchor the forecasts. During the past decades these have been replaced by a variety of linear combination approaches that exploit information from the complete hierarchy to produce more accurate forecasts. However the performance of these combination methods depends on the particularities of the examined series and their relationships.
2020,5,29,Photonic Computing for Massively Parallel AI is out and it is spectacular!,https://nuit-blanche.blogspot.com/2020/05/photonic-computing-for-massively.html,It’s been a long time brewing but we just released our first white paper on Photonic Computing for Massively Parallel AI. The document features the technology we develop at LightOn some of its use some testimonials and how we see the future of computing. It is downloadable here or from our website: LightOn.aiEnjoy! Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,5,27,Reclaiming the stories that algorithms tell,https://www.oreilly.com/radar/reclaiming-the-stories-that-algorithms-tell/,Algorithms tell stories about who people are. The first story an algorithm told about me was that my life was in danger. It was 7:53 pm on a clear Monday evening in September of 1981 at the Columbia Hospital for Women in Washington DC. I was exactly one minute old. The medical team scored me—as [&#8230;]
2020,5,27,Forecasting the Future & the Future of Forecasting,https://robjhyndman.com/seminars/acems-podcast/,Podcast interview for The Random Sample Recently I was interviewed for the ACEMS podcast &ldquo;The Random Sample&rdquo; on the topic of forecasting. You can listen to the episode here.
2020,5,24,Seasonal mortality rates,https://robjhyndman.com/hyndsight/seasonal-mortality-rates/,"


The weekly mortality data recently published by the Human Mortality Database can be used to explore seasonality in mortality rates. Mortality rates are known to be seasonal due to temperatures and other weather-related effects (Healy 2003)."
2020,5,21,Excess deaths for 2020,https://robjhyndman.com/hyndsight/excess-deaths/,"The reported COVID19 deaths in each country are often undercounts due to different reporting practices or people dying of COVID19 related causes without ever being tested. One way to explore the true mortality effect of the pandemic is to look at “excess deaths” — the difference between death rates this year and the same time in previous years.
The Financial Times (and other media outlets) have been collecting data from many countries to try to measure this effect."
2020,5,18,What to Do When AI Fails,https://www.oreilly.com/radar/what-to-do-when-ai-fails/,These are unprecedented times at least by information age standards. Much of the U.S. economy has ground to a halt and social norms about our data and our privacy have been thrown out the window throughout much of the world. Moreover things seem likely to keep changing until a vaccine or effective treatment for COVID-19 [&#8230;]
2020,5,18,Back to Basics: Approximate Bayesian Computation,https://justindomke.wordpress.com/2020/05/18/back-to-basics-approximate-bayesian-computation/,(All based on these excellent slides from Umberto Picchini) &#8220;Approximate Bayesian Computation&#8221; sounds like a broad class of methods that would potentially include things like message passing variational methods MCMC etc. However for historical reasons the term is used for a very specialized class of methods. The core idea is as follows: Sample from the &#8230; Continue reading Back to Basics: Approximate Bayesian&#160;Computation &#8594;
2020,5,17,You are what you vote: the social and demographic factors that influence your vote,https://robjhyndman.com/publications/voting/,"Australia has changed in many ways over the past two decades. Rising house prices country-wide improvements in education an ageing population and a decline in religious affiliation are just some of the ways it has changed. At the same time political power has moved back and forth between the two major parties. How much can we attribute changes in political power to changes in who we are?
Quite a lot as it turns out."
2020,5,15,Tackling Reinforcement Learning with the Aurora OPU,https://nuit-blanche.blogspot.com/2020/05/tackling-reinforcement-learning-with.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Martin Graive did an internship at LightOn and decided to investigate how to use Random Projections in the context of Reinforcement Learning. He just wrote a blog post on the matter entitled ""Tackling Reinforcement Learning with the Aurora OPU"". The attendant GitHub repo is located here. Enjoy!Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2020,5,14,Practical Skills for The AI Product Manager,https://www.oreilly.com/radar/practical-skills-for-the-ai-product-manager/,In our previous article What You Need to Know About Product Management for AI we discussed the need for an AI Product Manager.&#160; This role includes everything a traditional PM does but also requires an operational understanding of machine learning software development along with a realistic view of its capabilities and limitations. In this article [&#8230;]
2020,5,11,When models are everywhere,https://www.oreilly.com/radar/when-models-are-everywhere/,You probably interact with fifty to a hundred machine learning products every day from your social media feeds and YouTube recommendations to your email spam filter and the updates that the New York Times CNN or Fox News decide to push not to mention the hidden models that place ads on the websites you visit [&#8230;]
2020,5,6,4 Ways to Keep Businesses Running During a Crisis,https://www.mdmgeek.com/2020/05/06/4-ways-to-keep-businesses-running-during-a-crisis/,"As business leaders we need to adjust to the circumstances of the rapidly changing world. COVID19 is impacting us in ways we never thought was possible. But there are things we can do to sustain the business and survive the impact of this pandemic. 
The post 4 Ways to Keep Businesses Running During a Crisis appeared first on MDMgeek."
2020,4,30,GRATIS: GeneRAting TIme Series with diverse and controllable characteristics,https://robjhyndman.com/publications/gratis/,The explosion of time series data in recent years has brought a flourish of new time series analysis methods for forecasting clustering classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics named GRATIS with the use of mixture autoregressive (MAR) models.
2020,4,30,Asymptotics of Reproducibility,https://simplystatistics.org/2020/04/30/asymptotics-of-reproducibility/,"Every once in a while I see a tweet or post that asks whether one should use tool X or software Y in order to “make their data analysis reproducible”. I think this is a reasonable question because in part there are so many good tools out there! This is undeniably a good thing and quite a contrast to just 10 years ago when there were comparatively few choices.

The question of toolset though is not a question worth focusing on too much because it’s the wrong question to ask. Of course you should choose a tool/software package that is reasonably usable by a large percentage of your audience. But the toolset you use will not determine whether your analysis is reproducible in the long-run.

I think of the choice of toolset as kind of like asking “Should I use wood or concrete to build my house?” Regardless of what you choose once the house is built it will degrade over time without any deliberate maintenance. Just ask any homeowner! Sure some materials will degrade slower than others but the slope is definitely down.

Discussions about tooling around reproducibility often sound a lot like “What material should I use to build my house so that it never degrades*?” Such materials do not exist and similarly toolsets do not exist to make your analysis permanently reproducible.

I’ve been reading some of the old web sites from Jon Claerbout’s group at Stanford (thanks to the Internet Archive) the home of some of the original writings about reproducible research. At the time (early 90s) the work was distributed on CD-ROMs which totally makes sense given that CDs could store lots of data were relatively compact and durable and could be mailed or given to other people without much concern about compatibility. The internet was not quite a thing yet but it was clearly on the horizon.

But ask yourself this: If you held one of those CD-ROMs in your hand right now would you consider that work reproducible? Technically yes but I don’t even have a CD-ROM reader in my house so I couldn’t actually read the data. And a larger problem is that a CD from the 90s probably degraded to the point where it is likely unreadable anyway.

Claerbout’s group obviously knew about the web and were transitioning in that direction but such a transition costs money. As does keeping a keen eye on emerging trends and technology usage.

Hilary Parker and I recently discussed the how the economics of academic research are not well-suited to support the reproducibility of scientific results. The traditional model is that a research grant pays for the conduct of research over a 3-5 year period after which the grant is finished and there is no more funding. During (or after) that time scientific results are published. While the funding can be used to prepare materials (data software and code) to make the published findings reproducible at the instant of publication there is no funding afterwards for dealing with two key tasks:


Ensuring that the work continues to be reproducible given changes to the software and computing environment (maintenance)
Fielding questions or inquiries from others interested in reproducing the results or in building upon the published work (support)


These two activities (maintenance and support) can continue to be necessary in perpetuity for every study that an investigator publishes. The mismatch between how the grant funding system works and the requirements of reproducible research is depicted in the diagram below.



When I say “value” in the drawing above what I really mean is the “reproducibility value”. In the old model of publishing science there was no reproducibility value because the work was generally not reproducible in the sense that data and code were available. Hence this whole discussion would be moot.

Traditional paper publications held their value because the text on the page did not generally degrade much over time and copies could easily be made. Scientists did have to field the occasional question about the results but it was not the same as maintaining access to software and datasets and answering technical questions therein. As a result the traditional economic model for funding academic research really did match the manner in which research was conducted and then published. Once the results were published the maintenance and support costs were nominal and did not really need to be paid for explicitly.

Fast forward to today and the economic model has not changed but the “business” of academic research has. Now every publication has data and code/software attached to it which come with maintenance and support costs that can extend for a substantial period into the future. While any given publication may not require significant maintenance and support the costs for an investigator’s publications in aggregate can add up very quickly. Even a single paper that turns out to be popular can take up a lot of time and energy.

If you play this movie to the end it becomes soberingly clear that reproducible research from an economic stand point is not really sustainable. To see this it might help to use an analogy from the business world. Most businesses have capital costs where they buy large expensive things &ndash; machinery buildings etc. These things have a long life but are thought to degrade over time (accountants call it depreciation). As a result most businesses have “maintenance capital expenditure” costs that they report to show how much money they are investing every quarter to keep their equipment/buildings/etc. up to shape. In this context the capital expenditure is worth it because every new building or machine that is purchased is designed to ultimately produce more revenue. As long as the revenue generated exceeds the cost of maintenance the capital costs are worth it (not to oversimplify or anything!).

In academia each new publications incurs some maintenance and support costs to ensure reproducibility (the “capital expenditure” here) but it’s unclear how much each new publication brings in more “revenue” to offset those costs. Sure more publications allow one to expand the lab or get more grant funding or hire more students/postdocs but I wouldn’t say that’s universally true. Some fields are just constrained by how much total funding there is and so the available funding cannot really be increased by “reaching more customers”. Given that the budgets for funding agencies (at least in the U.S.) have barely kept up with inflation and the number of publications increases every year it seems the goal of making all research reproducible is simply not economically supportable.

I think we have to concede that at any given moment in time there will always be some fraction of published research for which there is no maintenance or support for reproducibility. Note that this doesn’t mean that people don’t publish their data and code (they should still do that!) it just means they don’t support or maintain it. The only question is which fraction should *no*t be supported or maintained? Most likely it will be older results where the investigators simply cannot keep up with maintenance and support. However it might be worth coming up with a more systematic approach to determining which publications need to maintain their reproducibility and which don’t.

For example it might be more important to maintain the reproducibility of results from huge studies that cannot be easily replicated independently. However for a small study conducted a decade ago that has subsequently been replicated many times we can probably let that one go. But this isn’t the only approach. We might want to preserve the reproducibility of studies that collect unique datasets that are difficult to re-collect. Or we might want to consider term-limits on reproducibility so an investigator commits to maintaining and supporting the reproducibility of a finding for say 5 years after which either the maintenance and support is dropped or longer-term funding is obtained. This doesn’t necessarily mean that the data and code suddenly disappear from the world; it just means the investigator is no longer committed to supporting the effort.

Reproducibility of scientific research is of critical importance perhaps now more than ever. However we need to think harder about how we can support it in both the short- and long-term. Just assuming that the maintenance and support costs of reproducibility for every study are merely nominal is not realistic and simply leads to investigators not supporting reproducibility as a default."
2020,4,29,"3-year PhD studentship in Inverse Problems and Optical Computing, LightOn, Paris, France",https://nuit-blanche.blogspot.com/2020/04/3-year-phd-studentship-in-inverse.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Come and join us at LightOn we have a 3-year PhD fellowship available for someone who can help us build our future photonic cores. Here is&nbsp;As part of the newly EU-funded ITN project “Post-Digital” LightOn has an opening for a fully-funded 3 year Ph.D. studentship to join its R&amp;D team at the crossroads between Computer Science and Physics.&nbsp;The goal of this 3 year Ph.D. position is to theoretically numerically and experimentally investigate how optimization techniques can be used in the design of hybrid computing pipelines including a number of photonic building blocks (“photonic cores”). In particular the optimized networks will be used to solve large-scale physics-based inverse problems in science and engineering - for instance in medical imaging (e.g. ultrasound) or simulation problems. The candidate will first investigate how LigthOn’s current range of photonics co-processors can be integrated within task-specific networks. The candidate will then develop a computational framework for the optimization of electro-optical systems. Finally optimized systems will be built and evaluated on experimental data. This project will be part of LightOn’s internal THEIA project aiming at automating the design of hybrid computing architectures including combinations of LightOn’s photonic cores and traditional silicon chips.In the framework of the EU funded ITN Post-Digital network this project involves collaborations and 3-month secondments with two research groups led by:Daniel Brunner (Université Bourgogne Franche-Comté / FEMTO-ST Besançon) who will be the academic supervisor - The candidate will be registered as a Ph.D. student at UBFC.Pieter Bienstman (IMEC Leuven Belgium).The supervisor at LightOn will be Laurent Daudet CTO - currently on leave from his position of professor of physics at Université de Paris.Due to the EU funding source please make sure you comply with the mobility and eligibility rule before applying. Application: Position to be filled no later than Sept 1st 2020.Send your application with a CV to jobs@lighton.io with [Post-Digital PhD] in the subject line. Shortlisted applicants will be asked to provide references. This project has received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No 860830.For more information: https://lighton.ai/careers/Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,4,29,Amplifying people I trust on COVID-19,https://simplystatistics.org/2020/04/29/amplifying-people-i-trust-on-covid-19/,"Like a lot of people I&rsquo;ve been glued to various media channels trying to learn about the latest with what is going on with COVID-19. I have also been frustrated - like a lot of people - with misinformation and the deluge of preprints and peer reviewed material. Some of this information is critically important and some is hard to trust.

As a biostatistician at a very visible school of public health I have also had a number of media outreaches but I&rsquo;ve been hesitant to do any interviews or talk about COVID-19. The reason is that even thought I have a PhD in Biostatistics and I work in a School of Public Health I actually know very little about infectious disease modeling and response. I think if you aren&rsquo;t really deep in the field its difficult to know the difference between someone like me and someone with real expertise.

While I&rsquo;m not an expert in the area I know many of the real experts professionally or by reputation. So I thought I&rsquo;d make a brief list of people and organizations I find credible and have been following for good information in case it is helpful to others. Many of these folks have already been found by audiences much bigger than ours but I just thought it would be useful to amplify further their work.

Paper review


JHU Novel Coronavirus Research Compendium - Hopkins experts rapidly reviewing preprints and peer reviewed literature to find the gems.


Infectious disease modeling


Trevor Bedford - Fred Hutchinson Cancer Research center expert in phylogenetic modeling of infectious disease his viz work and sober analysis is one of my go-tos.
Justin Lessler - infectious disease professor and epidemiologist at Hopkins who did some of the earliest studies of contact tracing in China.
Kate Grabowski - infectious disease professor and epidemiologist at Hopkins
Nicholas Reich - UMass expert in infectious disease modeling doing a great job of aggregating and evaluating disease models.
Natalie Dean - University of Florida expert statistician in vaccine clinical trials - also one of my favorite pragmatic reviewers of big papers.


Vaccine development


Derek Lowe - drug discovery chemist and blogger who is one of the best out there at distilling progress on vaccines.


Scicom and public outreach


Ellie Murray - Boston University expert epidemiologist professor and communicator providing clear understandable breakdowns of the best practices.
Lucy D&rsquo;Agostino McGowan - Vanderbilt statistics professor and communicator who does an amazing job of breaking down difficult stats and causal inference issues.
Carl Bergstrom - UW Biology Professor and infectious disease expert providing sober reviews and interactions around many of the papers coming out.


Policy


Tom Ingelsby - Professor and director of Johns Hopkins Center for Health Security has been producing solid analysis and policy recommendations on when to re-open.
Caitlin Rivers - Professor at the Hopkins Center for Health Security outbreak specialist also producing solid analysis and policy recommendations.
Andy Slavitt - Ex-Obama health care head and providing solid policy reviews and ideas.
Josh Sharfstein - Professor of the Practice at Johns Hopkins Bloomberg School of Public Health has a great public health podcast with lots of experts on it.
Keshia Pollack-Porter - Professor of Health Policy and Management at Johns Hopkins Bloomberg School of Public Health who has a great take on mobility issues associated with Covid-19.
Lisa Cooper - Bloomberg Professor at the Johns Hopkins Bloomberg School of Public Health who has great content on inequality of impact.


I&rsquo;m sure I&rsquo;ve missed great people to mention as I&rsquo;ve dashed this off pretty quickly so apologies if I missed you!"
2020,4,22,How data privacy leader Apple found itself in a data ethics catastrophe,https://www.oreilly.com/radar/how-data-privacy-leader-apple-found-itself-in-a-data-ethics-catastrophe/,Three months ago Apple released a new credit card in partnership with Goldman Sachs that aimed to disrupt the highly regulated world of consumer finance. However a well-known software developer tweeted that he was given 20x the credit line offered to his wife despite the fact that they have been filing joint tax returns and [&#8230;]
2020,4,21,Real COVID-19 Death Rate,https://www.deep-data-mining.com/2020/04/real-covid-19-death-rate.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} In my last post we did research on COVID-19 death rate based on the ratio between the number of deaths and the number of confirmed cases. However this method is inherently flawed. Some infected people did not show up at a hospital or a testing station to get tested. As a result the death rate is exaggerated.  Blood antibody tests on randomly sampled residents in Santa Clara California in early April  shows that the number of people infected is 55 to 85 times more than confirmed cases (https://www.cnn.com/2020/04/17/health/santa-clara-coronavirus-infections-study/index.html). Thus the real death rate for people who infected with coronavirus is between 0.1% and 0.17% which are similar to that of flu.   We use the following charts to illustrate two ways of calculating death rates.    COVID-19 Death Rate (Flawed) = Number of Deaths/ Number of Confirmed Cases  COVID-19 Death Rate (Real) = Number of Deaths/Number of Infected  As we can see the chance of COVID-19 bullet hitting the bullseye i.e. causing death is much slimmer that appears based on confirmed cases alone. 
2020,4,19,Anomaly detection in streaming nonstationary temporal data,https://robjhyndman.com/publications/oddstream/,This article proposes a framework that provides early detection of anomalous series within a large collection of non-stationary streaming time series data. We define an anomaly as an observation that is very unlikely given the recent distribution of a given system. The proposed framework first forecasts a boundary for the system&rsquo;s typical behavior using extreme value theory. Then a sliding window is used to test for anomalous series within a newly arrived collection of series.
2020,4,8,Optimal non-negative forecast reconciliation,https://robjhyndman.com/publications/nnmint/,The sum of forecasts of disaggregated time series are often required to equal the forecast of the aggregate giving a set of coherent forecasts. The least squares solution for finding coherent forecasts uses a reconciliation approach known as MinT proposed by Wickramasuriya et al (2019). The MinT approach and its variants do not guarantee that the coherent forecasts are non-negative even when all of the original forecasts are non-negative in nature.
2020,4,7,LightOn Cloud 2.0 featuring LightOn Aurora OPUs,https://nuit-blanche.blogspot.com/2020/04/lighton-cloud-20-featuring-lighton.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** At LightOn we just launched LightOn Cloud 2.0 that feature several Aurora Optical Processing Unit for use by the Machine Learning Community. the blog post about this can be found here. You can request access to the Cloud at&nbsp;https://cloud.lighton.ai/We are also having a LightOn Cloud for Research program:&nbsp;https://cloud.lighton.ai/lighton-research/[En] Press Release: LightOn launches LightOn Cloud 2.0 featuring Aurora OPUs April 7th 2020&nbsp;[Fr] Communiqué de presse: LightOn lance le LightOn Cloud 2.0 avec des OPUs Aurora 7 Avril 2020&nbsp;Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,4,5,Why log ratios are useful for tracking COVID-19,https://robjhyndman.com/hyndsight/logratios-covid19/,"There have been some great data visualizations produced of COVID-19 case and deaths data the best known of which is the graph from John Burn-Murdoch in the Financial Times. To my knowledge it was first used by Matt Cowgill from the Grattan Institute and has been widely copied. This is a great visualization and has helped introduce log-scale graphics to a wide audience.
Reproducing the Financial Times cumulative confirmed cases graph To produce something like it we can use the tidycovid19 package from Joachim Gassen:"
2020,4,3,Study on COVID-19 Annualized Death Rate,https://www.deep-data-mining.com/2020/04/study-on-covid-19-annualized-death-rate.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Probably this is the first time you see a chart like this.  When people hear COVID-19 death rate for older people is high they panic. We did research and published a paper on COVID-19 death rate(   Study on COVID-19 Annualized Death Rate). Please notice the death rates for COVID-19 are ""annualized"".  But we have to look at things in context. When COVID-19 death rate is annualized it can be compared with other statistical data that are on annual basis. This is the key contribution of our research. Please click the chart for sharper view. Only when COVID-19 death rate is annualized it can be compared with other statistical data that are on annual basis. This is the key contribution of our research. The following are the conclusions.   Conclusions We propose a method to calculate the annualized death rate (ADR) related to COVID-19. Based on ADR related to COVID-19 and the 2018 death rate for the population of the United States we gain the following insights:Incremental annual death rates related to COVID-19 for age groups 45-54 55-64 65-74 75-84 and 85+ are 0.4% 1.2% 2.2% 3.1% and 6.0%， respectively.Percentages of incremental annual death rates related to COVID-19 for age groups 45-54 55-64 65-74 75-84 and 85+ are 100.9% 131.8% 124.4% 69.8% and 44.5% respectively. If the herd immunity strategy is used in the United States the incremental annual number of deaths related to COVID-19 for people equal to or older than 45 will be between 1.1 and 1.47 million. The PDF file for the paper can be downloaded Study on COVID-19 Annualized Death Rate "
2020,3,31,What you need to know about product management for AI,https://www.oreilly.com/radar/what-you-need-to-know-about-product-management-for-ai/,If you’re already a software product manager (PM) you have a head start on becoming a PM for artificial intelligence (AI) or machine learning (ML). You already know the game and how it is played: you’re the coordinator who ties everything together from the developers and designers to the executives. You’re responsible for the design [&#8230;]
2020,3,31,Developing good research habits,https://robjhyndman.com/seminars/research_habits2020/,Presentation for the 2020 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2020,3,26,Accelerating SARS-COv2 Molecular Dynamics Studies with Optical Random Features,https://nuit-blanche.blogspot.com/2020/03/accelerating-sars-cov2-molecular.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We just published a new blog post at LightOn. This time we used LightOn's Optical Processing Unit to show how our hardware can help in speeding up global sampling studies that are using Molecular Dynamics simulations such as in the case of metadynamics. Our engineer Amélie Chatelain&nbsp;wrote a blog post about it and it is here:&nbsp;Accelerating SARS-COv2 Molecular Dynamics Studies with Optical Random&nbsp;FeaturesWe showed that LightOn's OPU in tandem with the NEWMA algorithm becomes very interesting (compared to CPU implementations of Random Fourier Features and FastFood) for simulations featuring more than 4 000 atoms.&nbsp;&nbsp;Because building computational hardware makes no sense if we don't have a community that lifts us the code used to generate the plots in that blog post is publicly available at the following link:&nbsp;https://github.com/lightonai/newma-md.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,3,24,The unreasonable importance of data preparation,https://www.oreilly.com/radar/the-unreasonable-importance-of-data-preparation/,In a world focused on buzzword-driven models and algorithms you’d be forgiven for forgetting about the unreasonable importance of data preparation and quality: your models are only as good as the data you feed them. This is the garbage in garbage out principle: flawed data going in leads to flawed results algorithms and business decisions. [&#8230;]
2020,3,22,Forecasting COVID-19,https://robjhyndman.com/hyndsight/forecasting-covid19/,"What makes forecasting hard? Forecasting pandemics is harder than many people think. In my book with George Athanasopoulos we discuss the contributing factors that make forecasts relatively accurate. We identify three major factors:
how well we understand the factors that contribute to it; how much data is available; whether the forecasts can affect the thing we are trying to forecast.  For example tomorrow’s weather can be forecast relatively accurately using modern tools because we have good models of the physical atmosphere there is tons of data and our weather forecasts cannot possibly affect what actually happens."
2020,3,19,6 trends framing the state of AI and ML,https://www.oreilly.com/radar/6-trends-framing-the-state-of-ai-and-ml/,O’Reilly online learning is a trove of information about the trends topics and issues tech leaders need to know about to do their jobs. We use it as a data source for our annual platform analysis and we’re using it as the basis for this report where we take a close look at the most-used [&#8230;]
2020,3,18,AI adoption in the enterprise 2020,https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2020/,Last year when we felt interest in artificial intelligence (AI) was approaching a fever pitch we created a survey to ask about AI adoption. When we analyzed the results we determined the AI space was in a state of rapid change so we eagerly commissioned a follow-up survey to help find out where AI stands [&#8230;]
2020,3,14,Au Revoir Backprop ! Bonjour Optical Transfer Learning !,https://nuit-blanche.blogspot.com/2020/03/au-revoir-backprop-bonjour-optical.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We recently used LightOn's Optical Processing Unit to show how our hardware fared in the context of Transfer learning. Our engineer Luca Tommasone wrote a blog post about it and it is here:&nbsp;Au Revoir Backprop! Bonjour Optical Transfer Learning!Because building computational hardware makes no sense if we don't have a community that lifts us the code used to generate the plots in that blog post is publicly available at the following link:&nbsp;https://github.com/lightonai/transfer-learning-opu.Enjoy and most importantly stay safe !Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,3,3,Querying Database Using Command Line Client and Powershell,https://www.deep-data-mining.com/2020/03/querying-database-using-command-line.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I have found Powershell is a powerful tool. When it combines with a command line tool such as mysql we can perform sophisticated tasks easily. The output of Powershell is an object instead of a text string. We can perform SQL-like operations such as selecting columns and filtering rows using where clause. Since I found out the power of Powershell last year I have been using Powershell as my major tool to query databases including SQL Server MySQl and Postgre. In this post I demonstrate how to query MySQL using mysql client and Powershell. First we run the following command to set up our host user name password and port.  mysql_config_editor set --login-path=local --host=localhost                          --user=root --port=3306 --passwordWhen we run the above command at a shell it will prompt us to input password. It will create a file .mylogin.cnf that is not humanly readable under home directory (I am using Mac. For Windows the file is under %APPDATA%\MySQL). The reason of doing this is to avoid an annoying security warning message if we supply the password in mysql command. If you don't mind the warning message you can skip the step.  Then I write a Powershell function to query MySQL database. I save the scripts as mysql_f.ps1 on my Mac. function runsql {param($s)mysql --login-path=local -D dataexp -B -e ""$s"" | convertfrom-csv -delimiter `t}  I issue the following command from the shell prompt>. ""./mysql_f.ps1"" Now the function is defined. I test the function. prompt>runsql ""select 1 as val""val---1It worked. The following query displays the table in the current database. prompt>runsql ""select TABLE_SCHEMA table_name create_time        from information_schema.tables where table_schema=database()""TABLE_SCHEMA TABLE_NAME             CREATE_TIME------------ ----------             -----------dataexp      flyway_schema_history  2019-11-21 11:56:31dataexp      mdl_sampled            2020-01-14 10:40:06dataexp      mdl_set                2020-01-14 09:46:59dataexp      sample_user_id         2020-01-14 09:58:34dataexp      t_all_pk1              2019-12-05 11:53:43dataexp      t_all_pk1_int          2019-12-05 22:14:54dataexp      t_all_tab_cols         2019-11-21 15:39:00 From the Powershell output we select only table_name an create_time by piping the output to ""select"" command of Powershell. prompt>runsql ""select TABLE_SCHEMA table_name create_time        from information_schema.tables   where table_schema=database()"" | select table_name create_timeTABLE_NAME             CREATE_TIME----------             -----------flyway_schema_history  2019-11-21 11:56:31mdl_sampled            2020-01-14 10:40:06mdl_set                2020-01-14 09:46:59sample_user_id         2020-01-14 09:58:34t_all_pk1              2019-12-05 11:53:43t_all_pk1_int          2019-12-05 22:14:54t_all_tab_cols         2019-11-21 15:39:00 The above operation is possible because in runsql function we use convertfrom-csv to convert text output of mysql to Powershell object. Convertfrom-csv is really a magic function! Once we have an Powershell object we can do all sorts of things.   "
2020,2,27,On normalization and algorithm selection for unsupervised outlier detection,https://robjhyndman.com/publications/normalization-outliers/,This paper demonstrates that the performance of various outlier detection methods depends sensitively on both the data normalization schemes employed as well as characteristics of the datasets. Recasting the challenge of understanding these dependencies as an algorithm selection problem we perform the first instance space analysis of outlier detection methods. Such analysis enables the strengths and weaknesses of unsupervised outlier detection methods to be visualized and insights gained into which method and normalization scheme should be selected to obtain the most likely best performance for a given dataset.
2020,2,18,5 key areas for tech leaders to watch in 2020,https://www.oreilly.com/radar/oreilly-2020-platform-analysis/,O’Reilly online learning contains information about the trends topics and issues tech leaders need to watch and explore. It’s also the data source for our annual usage study which examines the most-used topics and the top search terms.[1] This combination of usage and search affords a contextual view that encompasses not only the tools techniques [&#8230;]
2020,2,12,The state of data quality in 2020,https://www.oreilly.com/radar/the-state-of-data-quality-in-2020/,We suspected that data quality was a topic brimming with interest. Those suspicions were confirmed when we quickly received more than 1900 responses to our mid-November survey request. The responses show a surfeit of concerns around data quality and some uncertainty about how best to address those concerns. Key survey results: The C-suite is engaged [&#8230;]
2020,2,7,Electricity demand data in tsibble format,https://robjhyndman.com/hyndsight/electrictsibbles/,"The tsibbledata packages contains the vic_elec data set containing half-hourly electricity demand for the state of Victoria along with corresponding temperatures from the capital city Melbourne. These data cover the period 2012-2014.
Other similar data sets are also available and these may be of interest to researchers in the area.
For people new to tsibbles please read my introductory post.
 Australian state-level demand The rawdata for other states are also stored in the tsibbledata github repository (under the data-raw folder) but these are not included in the package to satisfy CRAN space constraints."
2020,2,5,Dis-aggregated hardware,http://themainstreamseer.blogspot.com/2020/02/dis-aggregated-hardware.html,This post focuses on dis-aggregated hardware follows my earlier post Open Source Networking: a hierarchical approach.&nbsp;Hardware dis-aggregation is an important aspect of open networking.&nbsp; Essentially dis-aggregated hardware involves software being separated from the underlying hardware.&nbsp; Similar to a PC or a server where you can boot with a Linux installer CD to Linux you are able to boot dis-aggregated hardware with an Operating System (OS) installer and install an operating system on it.&nbsp; Most commercial vendors bundle their hardware with software which means you cannot modify the software or change the OS that it comes loaded with.&nbsp; Open dis-aggregated hardware however does not have this restriction.Take for example Ethernet switches which have two main components:A packet switching/processing ASIC chipset controllerA CPU which hosts the software / firmware and the packet switching ASIC.Most commercial vendors bundle the software with their hardware whereas Ethernet switches by vendors of dis-aggregated hardware contain the same hardware components but allow you to install your choice of network OS (for example Open Network Linux or ONL) on it.&nbsp; ONL and other network OS provide hardware compatibility guides which clearly specify what hardware they support.&nbsp; Source: Reza Toghraee and The Linux FoundationThe most simple form of a disaggregated networking hardware is a  standard bare metal x86 machine with multiple network adapter cards. You can  install a Linux OS on that PC activate routing and  transform it into a firewall or a router. &nbsp;There are two main open source projects that are driving developments in the dis-aggregated hardware space:Open Compute Project; andTelecom Infra Project.Open Compute ProjectIn 2009 as Facebook was growing exponentially the company realized that it had to rethink its infrastructure  to accommodate the huge influx of new people and data and also control  costs and energy consumption.&nbsp; Facebook then started a project to design the world’s most energy  efficient data center one that could handle unprecedented scale at the  lowest possible cost.&nbsp; Two years and a small team of engineers later they built a data center from the ground up at Prineville Oregon including software servers  racks power supplies and cooling.&nbsp; It was 38% more energy efficient to build and 24%  less expensive to run than the company’s previous facilities—and has  led to even greater innovation.&nbsp;In 2011 Facebook shared its designs with the  public and—along with Intel and Rackspace Goldman Sachs and Andy  Bechtolsheim—launched the Open Compute Project (OCP) and incorporated the Open  Compute Project Foundation.&nbsp; There are multiple project workgroups within  OCP each including a project charter and a team working towards  producing and enhancing the open source technologies within that  project.&nbsp; Currently these workgroups are:Data Center Facility: focuses on maximizing mechanical performance and thermal and electrical efficiencyHardware Management: incorporates a set of existing tools and best practices and leverages existing tools for remote machine managementNetworking: creating a set of technologies that are dis-aggregated and fully&nbsp;open allowing for rapid innovation in the network spaceOpen System Firmware: creating and deploying at scale an open source hardware platform  initialization and OS load firmware optimized for web-scale cloud  hardwareRack &amp; Power: focuses on rack standards that are designed for data centers integrating the rack into the data center infrastructureSecurity: creates designs and specifications to enable software security for all  IT gear through collaboration with the wider Open Compute communityServer: provides standardized server system specifications for scale computingStorage: focuses on chassis and sleds components and peripherals networked enabled storage and compatibility solutionsTelco: enlists participants from telecom companies and carriers as well as sub  systems software board and semiconductor suppliers who are seeking to  use data center infrastructure to deliver IT servicesMore details on each of these can be found at the OCP website: https://www.opencompute.org/ Telecom Infra ProjectSimilar to the Open  Compute Project which addresses open source hardware and software used  to build data centers&nbsp;and enterprise IT the Telecom Infra Project (TIP) is a  workgroup meant to create open source standards for telecommunication  and service provider companies. The Telecom Infra Project includes  multiple project groups:Access projects:&nbsp; working to identify and create innovative infrastructure solutions  technologies and methodologies to make it easier to connect people to  the internet. Access is focused on removing some of the blockers that  can make the connection to the end user difficult.Transport projects: to keep the pace of exponential growth in network traffic better  backhaul is essential. Our transport project groups are addressing the  scalability fast convergence ease of configuration and extensibility  challenges in wireless and wired backhaul.Core &amp; Services: the most significant costs associated with a network are the ongoing  costs of operations and maintenance. The core and services project  groups are simplifying the core network architecture and improving  efficiency and flexibility while reducing ongoing costs associated with  keeping a network up and running.&nbsp;More details on each of these can be found at the TIP website: https://telecominfraproject.com/&nbsp;My next blog will focus on IO Abstraction and Datapath.&nbsp; 
2020,2,1,"Hospital characteristics, rather than surgical volume, predict length of stay following colorectal cancer surgery",https://robjhyndman.com/publications/hospital-los/,"Objective Length of hospital stay (LOS) is considered a vital component for successful colorectal surgery treatment. Evidence of an association between hospital surgery volume and LOS has been mixed. Data modelling techniques may adversely impact conclusions to the extent of reversing them. This study applied techniques to overcome possible drawbacks.
Method An additive quantile regression model formulated to isolate hospital contextual effects was applied to every colorectal surgery for cancer conducted in Victoria Australia between 2005 and 2015: 28343 admissions in 90 Victorian hospitals."
2020,1,30,How Rmarkdown changed my life,https://robjhyndman.com/seminars/rmarkdown/,Talk given at rstudio::conf San Francisco. Over the last few years Rmarkdown seems to have taken over my life or at least my written communication. These days I use Rmarkdown to maintain my website write my blog write textbooks write academic papers prepare slides for talks keep my CV up-to-date help my students write theses prepare university policy documents write letters prepare exams write reports for clients and more. I haven&rsquo;t quite got to the point of using it for shopping lists but perhaps that&rsquo;s my next Rmarkdown template.
2020,1,27,Tidy time series & forecasting in R,https://robjhyndman.com/seminars/workshop2020/,"knitr::opts_chunk$set(echo = TRUE cache = TRUE) options(digits = 3 width = 75) library(tidyverse) Venue rstudio:conf2020 San Francisco
Course description It is becoming increasingly common for organizations to collect huge amounts of data over time and existing time series analysis tools are not always suitable to handle the scale frequency and structure of the data collected. In this workshop we will look at some new packages and methods that have been developed to handle the analysis of large collections of time series."
2020,1,24,ABS time series as tsibbles,https://robjhyndman.com/hyndsight/abs2tsibble/,"library(tidyverse) library(tsibble) library(readabs) Australian data analysts will know how frustrating it is to work with time series data from the Australian Bureau of Statistics. They are stored as multiple ugly Excel files (each containing multiple sheets) with inconsistent formatting embedded comments meta data stored along with the actual data dates stored in a painful Excel format and so on.
Fortunately there is an R package available to make this a little easier."
2020,1,21,Blazing Trails in Teaching Talend: Meet Rick Sherman,https://athena-solutions.com/blazing-trails-in-teaching-talend-meet-rick-sherman/,From Talend:&#160;Rick&#160;Sherman is an author educator and a managing partner of Athena IT Solutions.&#160; His book&#160;Business Intelligence Guidebook: From Data Integration to Analytics&#160;was published by Morgan […]
2020,1,16,Open Source Networking: a hierarchical approach,http://themainstreamseer.blogspot.com/2020/01/open-source-networking-hierarchical.html,I have been extremely fortunate to be part of several network cloudification projects over the past 18-24 months.&nbsp; It's been very exciting to see the changes that disaggregation and open source projects have been making to move an industry forward.&nbsp; Here I capture some initial thoughts about open source networking.Note: while the on-the-job learning has been immense I have supplemented this with some terrific courses from The Linux Foundation including LinuxFoundationX:      LFS165x             Introduction to Open Source Networking Technologies by Reza Toghraee.    Networking has come a long way from the widespread use of rigid appliances to&nbsp;perform networking functions such as routing firewalling switching and load balancing.&nbsp; A key principle behind this transformation is disaggregation i.e. the de-coupling of the software performing the networking functions with the hardware it is installed on.&nbsp; The main driver of this transformation is the evolution of Cloud computing.Let's explore the impact of Cloud computing on networking further.&nbsp; The Cloud virtualizes Compute Storage and Network.&nbsp; Compute virtualization came first.&nbsp; While things began slowly once virtualization technology became reliable enterprises began migrating physical servers to virtual servers en masse.&nbsp; Today almost all enterprise applications are served on virtual compute environments.&nbsp; Storage virtualization came next with massive disk arrays giving way to distributed filesystems where JBOD (Just a Bunch of Disks) could be converted into a highly available and robust storage system.&nbsp; Both compute and storage virtualization disaggregated software from the underlying hardware.&nbsp; The same changes have now been occurring with networking over the past several years.&nbsp; The open source revolution has been a driving force behind this change and several open source projects exist for both hardware and software elements of networking.To understand all elements of the open source networking landscape it is useful to take a hierarchical approach starting with the open hardware at the bottom and working our way up to the applications layer at the top.&nbsp; Here is the complete hierarchy:Disaggregated HardwareIO Abstraction and DatapathNetwork Operating SystemsNetwork ControlNetwork VirtualizationCloud and Virtual ManagementOrchestration Management PolicyNetwork Data AnalyticsApplication Layer.A diagram with this hierarchy and the various open source projects associated with it is provided below (source: Reza Toghraee and The Linux Foundation):In following blogs I intend to cover the different elements of the hierarchy in detail along with the various open source projects that are driving developments in this space. 
2020,1,15,Reinforcement learning for the real world,https://www.oreilly.com/radar/reinforcement-learning-for-the-real-world/,Roger Magoulas recently sat down with Edward Jezierski reinforcement learning AI principal program manager at Microsoft to talk about reinforcement learning (RL). They discuss why RL&#8217;s role in AI is so important challenges of applying RL in a business environment and how to approach ethical and responsible use questions. Here are some highlights from their [&#8230;]
2020,1,15,Beyond Overfitting and Beyond Silicon: The double descent curve,https://nuit-blanche.blogspot.com/2020/01/beyond-overfitting-and-beyond-silicon.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** We recently tried a small experiment with LightOn's Optical Processing Unit on the issue of generalization. Our engineer&nbsp;Alessandro Cappelli did the experiment and wrote a blog post on it and it is here:&nbsp;Beyond Overfitting and Beyond Silicon: The double descent curve&nbsp;Two days ago&nbsp;Becca Willett&nbsp;was talking on the same subject at the Turing Institute in London.A function space view of overparameterized neural networks Rebecca Willett.Attendant preprint is here:A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case&nbsp;by&nbsp;Greg Ongie Rebecca Willett Daniel Soudry Nathan SrebroA key element of understanding the efficacy of overparameterized neural networks is characterizing how they represent functions as the number of weights in the network approaches infinity. In this paper we characterize the norm required to realize a function&nbsp;f:Rd→R&nbsp;as a single hidden-layer ReLU network with an unbounded number of units (infinite width) but where the Euclidean norm of the weights is bounded including precisely characterizing which functions can be realized with finite norm. This was settled for univariate univariate functions in Savarese et al. (2019) where it was shown that the required norm is determined by the L1-norm of the second derivative of the function. We extend the characterization to multivariate functions (i.e. networks with d input units) relating the required norm to the L1-norm of the Radon transform of a (d+1)/2-power Laplacian of the function. This characterization allows us to show that all functions in Sobolev spaces&nbsp;Ws1(R)&nbsp;s≥d+1 can be represented with bounded norm to calculate the required norm for several specific functions and to obtain a depth separation result. These results have important implications for understanding generalization performance and the distinction between neural networks and more traditional kernel learning.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2020,1,13,Curious about 5G?,http://themainstreamseer.blogspot.com/2020/01/curious-about-5g.html,Are you curious about 5G but not sure what it's all about?&nbsp; Here's a quick summary to bring you up to speed.A quick refresher about 1G through 4G before we get started:1G refers to the first generation of wireless cellular technologies that brought us our very first cell phones.2G was a significant improvement from 1G in the way the radio frequency spectrum was used enabling many more users per frequency band.&nbsp; Importantly for consumers 2G enabled digitally encrypted conversations and SMS text messages!3G was a giant leap forward from 2G.&nbsp; 3G enabled consumers to get online using their cell phones.4G pushed the limits further with improved speeds and increased applications for consumers including IP telephony high def mobile TV and video conferencing.&nbsp;So what does 5G promise?&nbsp; 5G promises to dramatically improve speed latency and scale. It is expected to be 100X faster than 4G (you could download an HD movie in 1 second!). Its data volume capacity is expected to be 1000X that of 4G when fully deployed.&nbsp; As a result 5G is expected to be the foundation for turbocharging applications like Virtual Reality Autonomous Driving and the Internet of Things.So how does 5G do this? &nbsp; Here are 5 key technologies enabling 5G:Millimeter waves: mobile phones and other electronic devices use frequencies under 6GHz on the radio frequency spectrum.&nbsp; As the number of devices online increase these frequencies are getting crowded leading to issues like dropped calls.&nbsp; To solve this researchers are experimenting with using extremely high frequency or millimeter waves (30 to 300 GHz).Small cells: Small cells are low-powered mini base stations that  have a range of 10 meters to a few kilometers. Fewer new macrocell  sites are being built with larger numbers of small cells recognized as  an important method of increasing cellular network capacity quality and  resilience.Massive MIMO: MIMO stands for multiple-input and multiple-output.&nbsp; It is a method for multiplying the capacity of a radio link using  multiple transmission and receiving antennas.&nbsp; 4G base stations have about 12 ports for all traffic.&nbsp; 5G / Massive MIMO base stations are expected to have 100 ports which could increase capacity by 22X or more.Beamforming: Beamforming or spatial filtering is a  signal processing technique used in sensor arrays for directional signal  transmission or reception.&nbsp; Instead of broadcasting signals in every direction beamforming would allow a base station to send a specific signal to a user preventing interference.Full Duplex: Today's base station antennas can only do one job at a time - either transmit or receive - a principle known as reciprocity.&nbsp; Researchers have use silicon transistors to create high speed switches that enable networks to overcome reciprocity thereby increasing speeds of data communication.&nbsp;Here's a great video by IEEE Spectrum on this topic: Are there any other technologies not covered in this post?&nbsp; If so please let me know in the comments column.
2020,1,8,heatmaply 1.0.0 – beautiful interactive cluster heatmaps in R,https://www.r-statistics.com/2020/01/heatmaply-1-0-0-beautiful-interactive-cluster-heatmaps-in-r/,"I&#8217;m excited to announce that heatmaply version 1.0.0 has been published to CRAN! (getting started vignette is available here) What is heatmaply? heatmaply is an R package for easily creating interactive cluster heatmaps that can be shared online as a stand-alone HTML file. Interactivity includes a tooltip display of values when hovering over cells as &#8230; Continue reading ""heatmaply 1.0.0 &#8211; beautiful interactive cluster heatmaps in R""
The post heatmaply 1.0.0 – beautiful interactive cluster heatmaps in R first appeared on R-statistics blog."
2020,1,7,8 AI trends we’re watching in 2020,https://www.oreilly.com/radar/8-ai-trends-were-watching-in-2020/,We see the AI space poised for an acceleration in adoption driven by more sophisticated AI models being put in production specialized hardware that increases AI’s capacity to provide quicker results based on larger datasets simplified tools that democratize access to the entire AI stack small tools that enables AI on nearly any device and [&#8230;]
2020,1,7,Calendar-based graphics for visualizing people's daily schedules,https://robjhyndman.com/publications/calendar-vis/,Calendars are broadly used in society to display temporal information and events. This paper describes a new calendar display for plotting data that includes a layout algorithm with many options and faceting functionality. The functions use modular arithmetic on the date variable to restructure the data into a calendar format. The user can apply the grammar of graphics to create plots inside each calendar cell and thus the displays synchronize neatly with ggplot2 graphics.
2020,1,6,Hierarchical forecasting,https://robjhyndman.com/publications/hierarchical-forecasting/,Accurate forecasts of macroeconomic variables are crucial inputs into the decisions of economic agents and policy makers. Exploiting inherent aggregation structures of such variables we apply forecast reconciliation methods to generate forecasts that are coherent with the aggregation constraints. We generate both point and probabilistic forecasts for the first time in the macroeconomic setting. Using Australian GDP we show that forecast reconciliation not only returns coherent forecasts but also improves the overall forecast accuracy in both point and probabilistic frameworks.
2020,1,4,A new tidy data structure to support exploration and modeling of temporal data,https://robjhyndman.com/publications/tsibble/,Mining temporal data for information is often inhibited by a multitude of formats: irregular or multiple time intervals point events that need aggregating multiple observational units or repeated measurements on multiple individuals and heterogeneous data types. On the other hand the software supporting time series modeling and forecasting makes strict assumptions on the data to be provided typically requiring a matrix of numeric data with implicit time indexes. Going from raw data to model-ready data is painful.
2020,1,3,FFORMA: Feature-based Forecast Model Averaging,https://robjhyndman.com/publications/fforma/,We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model.
2020,1,2,Forecasting in social settings: the state of the art,https://robjhyndman.com/publications/forecasting-sota/,This paper provides a non-systematic review of the progress of forecasting in social settings. It is aimed at someone outside the field of forecasting wanting to appreciate the results of the M4 Competition by reading a survey paper to get informed about the state of the art of this discipline. It discusses the recorded improvements over time in forecast accuracy the need to capture forecast uncertainty and what can go wrong with predictions.
2020,1,1,A brief history of forecasting competitions,https://robjhyndman.com/publications/forecasting-competitions/,Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held and how influential they have been over the years. I briefly review the history of forecasting competitions and discuss what we have learned about their design and implementation and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions and for research about forecasting based on competitions.
2019,12,18,AI is computer science disguised as hard work,https://www.oreilly.com/radar/ai-is-computer-science-disguised-as-hard-work/,Roger Magoulas recently sat down with Rob Thomas and Tim O’Reilly to discuss Thomas’s AI framework called the AI Ladder which according to his recent paper is a framework that describes “the increasing levels of analytic sophistication that lead to and buttress a thriving AI environment.” Thomas notes both in his paper and in a [&#8230;]
2019,12,18,"LightOn’s AI Research Workshop — FoRM #4: The Future of Random Matrices. Thursday, December 19th",https://nuit-blanche.blogspot.com/2019/12/lightons-ai-research-workshop-form-4.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** Tomorrow we will feature LightOn’s 4th AI Research workshop on the Future of Random Matrices (FoRM). It starts at 2pm on Thursday December 19th&nbsp;(That’s 2pm CET/Paris 1pm GMT/UTC/London 8am EST/NY-Montreal 5am PST/California 9pm UTC+8/ Shenzhen). We have an exciting and diverse line-up with talks on compressive learning binarized neural networks particle physics and matrix factorization.Feel free to join us or to catch the event livestream — link to be available on this page on the day of the event.Without further ado here is the program:Program1:45pm — Welcome coffee and opening. A short introduction about LightOn Igor Carron2:00pm — Compressive Learning with Random Projections Ata Kaban2:45pm — Medical Applications of Low Precision Neuromorphic Systems Bodgan Penkovsky3:30pm — Comparing Low Complexity Linear Transforms Gavin Gray4:00pm — Coffee break and discussions4:20pm —LightOn’s OPU+Particle Physics David Rousseau Aishik Ghosh Laurent Basara Biswajit Biswas5:00pm — Accelerated Weighted (Nonnegative) Matrix Factorization with Random Projections Matthieu Puigt5:45pm — Wrapping-up and beers on our rooftopTalks and abstractsAta Kaban University of Birmingham.Compressive Learning with Random ProjectionsBy direct analogy to compressive sensing compressive learning has been originally coined to mean learning efficiently from random projections of high dimensional massive data sets that have a sparse representation. In this talk we discuss compressive learning without the sparse representation requirement where instead we exploit thenatural structure of learning problems.Bodgan Penkovsky Paris-Sud University.Medical Applications of Low Precision Neuromorphic SystemsThe advent of deep learning has considerably accelerated machine learning development but its development at the edge is limited by its high energy cost and memory requirement. With new memory technology available emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation enabling machine learning on the edge devices and avoiding data transfer over the network. In this talk we will discuss strategies to apply BNNs to biomedical signals such as electrocardiography and electroencephalography without sacrificing accuracy and improving energy use. The ultimate goal of this research is to enable smart autonomous healthcare devices.Gavin Gray Edinburgh University.Comparing Low Complexity Linear TransformsIn response to the development of recent efficient dense layers this talk discusses replacing linear components in pointwise convolutions with structured linear decompositions for substantial gains in the efficiency/accuracy tradeoff. Pointwise convolutions are fully connected layers and are thus prepared for replacement by structured transforms. Networks using such layers are able to learn the same tasks as those using standard convolutions and provide Pareto-optimal benefits in efficiency/accuracy both in terms of computation (mult-adds) and parameter count (and hence memory).David Rousseau&nbsp;Aishik Ghosh&nbsp;Laurent Basara Biswajit Biswas. LAL Orsay LRI Orsay BITS University.OPU+Particle PhysicsLightOn’s OPU is opening a new machine learning paradigm. Two use cases have been selected to investigate the potentiality of OPU for particle physics:End-to-End learning: high energy proton collision at the Large Hadron Collider have been simulated each collision being recorded as an image representing the energy flux in the detector. Two classes of events have been simulated: signal are created by a hypothetical supersymmetric particle and background by known processes. The task is to train a classifier to separate the signal from the background. Several techniques using the OPU will be presented compared with more classical particle physics approaches.Tracking: high energy proton collisions at the LHC yield billions of records with typically 100000 3D points corresponding to the trajectory of 10000 particles. Various investigations of the potential of the OPU to digest this high dimensional data will be reported.Matthieu Puigt Université du Littoral Côte d’Opale.Accelerated Weighted (Nonnegative) Matrix Factorization with Random ProjectionsRandom projections belong to the major techniques used to process big data. They have been successfully applied to e.g. (Nonnegative) Matrix Factorization ((N)MF). However missing entries in the matrix to factorize (or more generally weights which model the confidence in the entries of the data matrix) prevent their use. In this talk I will present the framework that we recently proposed to solve this issue i.e. to apply random projections to weighted (N)MF. We experimentally show the proposed framework to significantly speed-up state-of-the-art weighted NMF methods under some mild conditions.The workshop will take place at IPGG 6 Rue Jean Calvin 75005 Paris. The location is close to both the Place Monge and the Censier-Daubenton subway stations on line7. it is also close to the Luxembourg station on the RER B line. The location is close to bus stops on the 21 24 27 47 and 89 routes. Note that strikes are still ongoing and some of these options may not be available.We will be in the main amphitheater downstairs on your right when you enter the building. Please register in advance on our meetup group so as to help us in the organization of the workshop.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&nbsp;About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,12,12,Why you should care about debugging machine learning models,https://www.oreilly.com/radar/why-you-should-care-about-debugging-machine-learning-models/,For all the excitement about machine learning (ML) there are serious impediments to its widespread adoption. Not least is the broadening realization that ML models can fail. And that’s why model debugging the art and science of understanding and fixing problems in ML models is so critical to the future of ML. Without being able [&#8230;]
2019,12,11,"Ce Soir: Paris Machine Learning Meetup #2 Season 7: Symbolic maths, Data Generation thru GAN, ""Prevision Retards"" @SNCF, Retail and AI, Rapids.ai Leveraging GPUs",https://nuit-blanche.blogspot.com/2019/12/ce-soir-paris-machine-learning-meetup-2.html,"**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** A big thank you to Scaleway for hosting us in their inspiring office and sponsoring the networking event afterwards.So this is quite exciting. Our meetup group has 7 999 members and we are going to organize a meetup in a town that is paralyzed by strikes. During the course of existence of this meetup we have seen worse.&nbsp;&nbsp;For those of you who will not be able to make it all information slides and link to streaming are below:https://youtu.be/bvKzKfj-8uE0. Presentation Scaleway Mélodie Morice1. Aurélia Negre Michael Sok Quantmetry ""Data generation through GANs""Tabular data are the most common within companies. Generating synthetic data that respects the statistical properties of the original data can have several applications: a machine learning that respects data privacy improving the robustness of a model in relation to data drift etc. Since 2018 there has been an increasing number of academic publications presenting the use of GANs on this type of data particularly on patient medical data. We have performed a proof of concept on real data and present the results of several models from the research namely the Wasserstein GAN the Wasserstein GAN with Gradient Penalty and the Cramér-GAN with the objective of ""model compatibility"" i.e. the possibility of using synthetic data to replace real data to train a classifier.2. Eloïse Nonne Soumaya Ihihi ""Prévisions Retards""&nbsp;a Machine Learning project led by e.SNCF's Data IoT team.Its goal is to integrate predictions of train delays into the SNCF mobile application. Every day our model predicts delays for the next 7 days at each stop for every train in Paris area network. The challenge of this project is to improve the reliability of passenger information and to provide more relevant routes for the application users. We will present the project from the definition of needs and exploratory data analysis to its industrialization in the cloud and the reliability of its predictions.3. Léa Dalle Lucche  Elina Ashkinazi-Ildis Kasra Mansouri Retail and AI Carrefour Data Lab ArtefactThis talk is focussed on AI and ML applications in retail. Discover how Carrefour is transforming through the introduction of the Google - Carrefour Lab by Elina Ashkinazi-Ildis Director of the Lab. Then go further with the ""shelf out detection"" usecase presented by Kasra Mansouri Data Scientist within Artefact.4.&nbsp;Arnaud Wald&nbsp;Scaleway ""RAPIDS.AI Leveraging GPUs for accelerated data science and data analytics""RAPIDS makes it possible to have end-to-end data science pipelines run entirely on GPU architecture. It capitalizes on the parallelization capabilities of GPUs to accelerate data preprocessing pipelines with a pandas-like dataframe syntax. GPU-optimized versions of scikit-learn algorithms are available and RAPIDS also integrates with major deep learning frameworks.This talk will present RAPIDS and its capabilities and how to integrate it in your pipelines.5.&nbsp;François Charton ""Deep Learning for Symbolic Mathematics""Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper we show that they can be **surprisingly good** at more elaborated tasks in mathematics such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.https://arxiv.org/abs/1912.01412Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroupAbout&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv"
2019,12,10,The road to Software 2.0,https://www.oreilly.com/radar/the-road-to-software-2-0/,Roughly a year ago we wrote “What machine learning means for software development.” In that article we talked about Andrej Karpathy’s concept of Software 2.0. Karpathy argues that we’re at the beginning of a profound change in the way software is developed. Up until now we’ve built systems by carefully and painstakingly telling systems exactly [&#8230;]
2019,12,8,Machine learning applications in time series hierarchical forecasting,https://robjhyndman.com/publications/mlhts/,Hierarchical forecasting (HF) is needed in many situations in the supply chain (SC) because managers often need different levels of forecasts at different levels of SC to make a decision. Top-Down (TD) Bottom-Up (BU) and Optimal Combination (COM) are common HF models. These approaches are static and often ignore the dynamics of the series while disaggregating them. Consequently they may fail to perform well if the investigated group of time series are subject to large changes such as during the periods of promotional sales.
2019,12,8,Predicting the whole distribution with methods for depth data analysis demonstrated on a colorectal cancer treatment study,https://robjhyndman.com/publications/colorectal/,We demonstrate the utility of predicting the whole distribution of an outcome rather than a marginal change. We overcome inconsistent data modelling techniques in a real world problem. A model based on additive quantile regression and boosting was used to predict the whole distribution of length of hospital stay (LOS) following colorectal cancer surgery. The model also assessed the association of hospital and patient characteristics over the whole distribution of LOS.
2019,12,4,Is Artificial Intelligence Revolutionizing Environmental Health?,https://simplystatistics.org/2019/12/04/is-artificial-intelligence-revolutionizing-environmental-health/,"NOTE: This post was written by Kevin Elliott Michigan State University; Nicole Kleinstreuer National Institutes of Health; Patrick McMullen ScitoVation; Gary Miller Columbia University; Bhramar Mukherjee University of Michigan; Roger D. Peng Johns Hopkins University; Melissa Perry The George Washington University; Reza Rasoulpour Corteva Agriscience and Elizabeth Boyle National Academies of Sciences Engineering and Medicine. The full summary for the workshop on which this post is based can be obtained here.

On June 6 and 7 2019 the National Academy of Sciences Engineering and Medicine (NASEM) hosted a workshop on the use of artificial intelligence (AI) in the field of Environmental Health. Rapid advances in machine learning are demonstrating the ability of machines to carry out repetitive “smart” tasks requiring discreet judgments. Machine learning algorithms are now being used to analyze large volumes of complex data to find patterns and make predictions often exceeding the accuracy and efficiency of people attempting the same task. Driven by tremendous growth in data availability as well as computing power and accessibility artificial intelligence and machine learning applications are rapidly growing in various sectors of society including retail such as predicting consumer purchases; the automotive industry as demonstrated by self-driving cars and in health care with advances in automated medical diagnoses.

Building upon the major themes of the NASEM workshop in this blog post we address the following questions:


How might AI advance environmental health?

Does AI change the standards used for conducting environmental health research?

Does the use of AI allow us to change our established research principles?

How does AI impact our training programs for the next generation of environmental health scientists?

Are there barriers within the current academic incentive structures that are hindering the full potential of AI and how might those barriers be overcome?


How might AI advance environmental health?

Environmental health is the study of how the environment affects human health. Due to the complexity of both human biology and the multiplicity of environmental factors that we encounter daily studying environmental impacts on human health presents many data challenges. Due to the data boom we have seen in recent years we now have a multitude of individualized data including genetic sequencing and wearable health and activity monitors.  We have also seen exponential growth in the availability of data on individual environmental exposures.  Wearable sensors and personal chemical samplers are allowing for more detailed exposure models whereas advancements in exposure biomonitoring in a variety of matrices including blood and urine is giving more granular detail about actual chemical body burdens. We have also seen an increase in available population level data on dietary factors the social and built environment climate and many other variables affected by environmental and genetic factors. Concurrently while population data are booming toxicology is creating a variety of experimental models to advance our understanding of how chemicals and environmental exposures may pose risks to human health. Large-scale high-throughput chemical safety screening efforts can now generate data on tens of thousands of chemicals in thousands of biological targets. Integrating these diverse data streams represents a new level of complexity.

AI and machine learning provide many opportunities to make this complexity more manageable such as highly accurate prediction methods to better assess exposures and flexible approaches to allow incorporation of exposure to complex mixtures in population health analyses. Incorporating artificial intelligence and machine learning methods in environmental health research offers the potential to transform how we analyze environmental exposures and our understanding of how these myriad factors influence our health and contribute to disease.

Does AI change the standards used for conducting environmental health research?

While we think the use of AI and machine learning techniques clearly hold great promise for the advancement of environmental health research we also believe such techniques introduce new challenges and magnify existing ones.  While the major standards by which we conduct scientific research do not change our ability to adhere to them will require some adaptation. Transparency and repeatability are key.  We must ensure that the computational reproducibility and replicability of our scientific findings do not suffer at the hands of complex algorithms and poorly assembled data pipelines. Complex data analyses that incorporate more diverse data types from varied sources stretch our ability to track curate and validate these data without robust data curation tools.  Although some data curation tools that establish standard approaches for creating managing and maintaining data are available they are usually field-specific and currently there are no incentives or strict requirements to ensure that investigators use them.

Machine learning and artificial intelligence algorithms have demonstrated themselves to be very powerful. At the same time we also recognize their complexity and general opacity can be cause for concern. While investigators may be willing to overlook the opacity of these algorithms when predictions are highly accurate and precise all is well until it isn’t. When an algorithm does not work as expected it is critical to know why it didn’t work. With transparency and reproducibility of utmost importance machine learning algorithms must ensure that investigators and data analysts have accountability in their analyses and that regulators have confidence in applying AI generated results to inform public health decisions.

Does the use of AI allow us to change our established research principles?

AI does not change established research principles such as sound study designs and understanding threats of bias. However there is a need to create updated guidelines and implement best practices for choosing cleaning structuring and sharing the data used in AI applications. Creating appropriate training datasets engaging in ongoing processes of validation and assessing the domain of applicability for the models that are generated are also important. As in all areas of science it is crucial to clarify whether models solely provide accurate predictions or whether they also provide understanding of relevant mechanisms. The current Open Science movement’s emphasis on transparency is particularly relevant to the use of AI and machine learning. Users of these methods in environmental health should be looking for ways to be open about the model training data to clarify validation methods to create interpretable “models of the models” where possible and to clarify their domains of applicability. Recent innovations like model cards or short documents that go alongside machine learning models to share information that everyone impacted by the model should know is one example of a way model developers can communicate their models’ strengths and weaknesses in a way that is accessible.

How does AI impact our training programs for the next generation of environmental health scientists?

As complex AI methods are increasingly applied to environmental health research it is important to consider effective training of the workforce and its future leaders. Currently training in the application of data science is unstandardized as trainees learn how to apply methods to a specific research application through an apprenticeship type model where a trainee works with a mentor. Classroom training standardizes theory and methods but the mentor teaches the fine details of analyzing data in a specific research area which introduces heterogeneity into the ways in which scientists analyze data. The lack of training standards leads to a worry that analysts may apply cutting-edge computational/algorithmic approaches to data analysis without consideration of fundamental biostatistical and epidemiologic principles such as statistical design sampling and inference.
Fundamental questions taught in biostatistics and epidemiology courses such as &ldquo;Who is in my sample?&rdquo; and &ldquo;What is my target population of inference?&rdquo; are even more relevant in our current era of algorithms and machine learning. Now analysts are agnostically querying databases not designed for population-based research such electronic health records medical claims Twitter Facebook and Google searches for new discoveries in environmental health. It is important to recognize that a lack of proper consideration of issues related to sampling selection bias correlation of multiple exposures exposure and outcome misclassification could lead to erroneous results and false conclusions.  Training programs will need to evolve so that we do not just teach scientists and analysts how to program models and interpret their results but also emphasize how to recognize human biases that can be inadvertently built into the data and model approaches and the continuous need for rigor responsibility and reproducibility.

An increased focus on mathematical theory may also improve training in the application of AI to environmental health. A greater effort in developing standardized theory about how and why a specific research area analyses data in a certain way may help adapt approaches from one research area to another. In addition deeper mathematical exploration of AI methods will help data scientists understand when and why AI methods work well and when they don’t.

Are there barriers within the current academic incentive structures that are hindering the full potential of AI and how might those barriers be overcome?

Rigorous data science requires a team science approach to achieve a variety of functions such as developing algorithms formalizing common data platforms and testing protocols and properly maintaining and curating data sources. Over recent decades we have witnessed how the power of team science has improved the understanding of critical health problems of our time such as in unlocking the human genome and achieving major advancements in cancer treatment.  These advances have demonstrated the payoff of interdisciplinary transdisciplinary and multidisciplinary investigations. Despite these successes there are still barriers to large team science projects because these projects often have goals that do not sit precisely within a single funding agency.  In order for AI to truly advance environmental health federal agencies and institutions that fund environmental health research need to create pathways to support large multi-disciplinary and multi-institutional teams that are conducting this research. An example could be a multi-agency/multi-institute funding consortia. A ten-year investment in a well-coordinated initiative that harnesses AI data opportunities could accelerate new findings in not only the environmental causes of disease but also in informing interventions that can prevent environmentally mediated disease and improve population health.

Final thoughts

We believe machine learning and AI methods have tremendous potential but we also believe they cannot be used in a way that overlooks limitations or relaxes data integrity standards. With these considerations in mind we have tempered enthusiasm for the promises of these approaches. We have to make sure that environmental health scientists stay out in front of these considerations to avoid potential pitfalls such as the allure of hype or chasing after the next new thing because it is novel rather than truly meaningful.  We can do this by fostering ongoing conversations about the challenges and opportunities AI provides for environmental health research. An intentional union of the two cultures of careful (and often overly cautious) stochastic and bold (and often overly optimistic) algorithmic modeling can help to ensuring we are not abandoning principles of proper study design when a new technology comes along but explore how to use the new technology to better understand the myriad ways the environment affects health and disease."
2019,11,26,Moving AI and ML from research into production,https://www.oreilly.com/radar/moving-ai-and-ml-from-the-research-realm-into-production/,In this interview from O&#8217;Reilly Foo Camp 2019 Dean Wampler head of evangelism at Anyscale.io talks about moving AI and machine learning into real-time production environments. Highlights from the interview include: Facilitating the transition from research to production in a robust way introduces a number of complications Wampler says including governance GDPR and traceability rules. [&#8230;]
2019,11,19,There’s a path to an AI ROI,https://www.oreilly.com/radar/theres-a-path-to-an-ai-roi/,In this interview from O&#8217;Reilly Foo Camp 2019 Hands-On Unsupervised Learning Using Python author Ankur Patel discusses the challenges and opportunities in making machine learning and AI accessible and financially viable for enterprise applications. Highlights from the interview include: The biggest hurdle businesses face when implementing machine learning or AI solutions is cleaning and preparing [&#8230;]
2019,11,14,“AI is a lie”,https://www.oreilly.com/radar/ai-is-a-lie/,In this interview from O&#8217;Reilly Foo Camp 2019 Eric Jonas assistant professor at the University of Chicago pierces the hype around artificial intelligence. Highlights from the interview include: Jonas argues that &#8220;AI is a lie&#8221;—meaning that our expectations far outsize the reality of what&#8217;s currently possible. One of the issues arising from that disconnect is [&#8230;]
2019,11,13,"Paris Machine Learning Meetup #1 Season 7: Neuroscience & AI, Time series, Deep Transfert learning in NLP, Media Campaign, Energy Forecasting",https://nuit-blanche.blogspot.com/2019/11/paris-machine-learning-meetup-1-season.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** A big thank-you to Publicis Sapient for welcoming us to their inspiring office. Presentation slides will be available here. The streaming of the event can also be found here: Raphaëlle Abitbol&nbsp;How Publicis Sapient builds Machine Learning products to create value for its clients&nbsp;- Raphaëlle Abitbol Head of Data will introduce the Publicis Sapient Data Science TeamPublicis Sapient is a digital transformation partner helping companies and established organizations get to their future digitally-enabled state both in the way they work and the way they serve their customers. Within Publicis Sapient the Data Science Team builds machine learning products in order to support clients in their transformation.Margot Fournier Publicis Sapient. Classification of first time visitorsA significant share of visitors on a site do not return making it crucial to identify levers that can decrease bouncing rate. For a client in the retail sector we developed several models that are able to predict both the gender and the segment in which unlogged and unknown visitors fit in. This allows to personalize the experience from the first visit and prevent users from bouncing.Maxence Brochard Publicis Sapient. Media campaign optimizationInternet users leave multiple traces of micro-conversions (searches clicks whishlist...) during their visit on an ecommerce site: these micro-conversions can be weak signals of an act of purchase in the near future. To analyze those signals we built a solution to detect visitors that are likely to convert and target them in while optimizing media campaigns budgets.Rémi Louf Hugging Face. Transfert learning in NLPStéphane Sénécal Orange. Neuroscience-Inspired AIThe fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times however communication and collaboration between the two fields has become less commonplace.In&nbsp;https://bit.ly/2WLsMaQ&nbsp;the authors argue that better understanding biological brains could play a vital role in building intelligent machines. They survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. Finally they conclude by highlighting shared themes that may be key for advancing future research in both fields.Guillaume Hochard Quantmetry&nbsp;Statistical and machine learning methods combination for improved energy consumption forecasting performance.The recent M4 forecasting competition (https://www.mcompetitions.unic.ac.cy) has demonstrated that the use of one forecasting method alone is not the most efficient approach in terms of forecasting accuracy. In this talk I will focus on an energy consumption forecasting use case integrating exogenous data such as weather conditions and open data. In particular I will present a forecasting time series challenge and the best practices observed on the best submissions and showcase an interesting approach based on a combination of classical statistical forecasting methods and machine learning algorithms such as gradient boosting for increased performance. Generalizing the use of these methods can be a major help to address the challenge of electricity demand and production adjustment.Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,11,9,"Paris Machine Learning Meetup Hors Série #1: A Talk with François Chollet Hors série with François Chollet, (Creator of the Keras Library)",https://nuit-blanche.blogspot.com/2019/11/paris-machine-learning-meetup-hors.html,**&nbsp;Nuit Blanche is now on Twitter: @NuitBlog&nbsp;** The first Paris Machine Learning Meetup Hors Série #1 of the season is a Talk with François Chollet Hors série with&nbsp;Francois Chollet (Creator of the Keras Library).This event is being recorded.We thank Morning coworking for hosting us and LightOn for their support in organizing this event.&nbsp;Today we welcome Francois Chollet. François is a researcher at Google and creator of the Keras Deep Learning library (https://keras.io). He will talk to us about the new features of the TensorFlow library as well as give us some insight of the latest in Deep Learning Research.Schedule :2pm : Keras &amp; TensorFlow for Deep Learning2.30pm : Q&amp;A2.40pm : Latest research in Deep Learning2.50pm : Q&amp;A3pm : networkingnb :1/ this is **not** a coding session2/ this event does not include a buffet (drink food)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,11,7,A world of deepfakes,https://www.oreilly.com/radar/a-world-of-deepfakes/,Deepfakes have been very much in the news for the past two years. It’s time to think about what deepfakes are and what they mean. Where do they come from? Why now? Is this just a natural evolution in the history of technology? Deepfakes are media that are created by AI. They appear to be [&#8230;]
2019,11,1,"Highlights from TensorFlow World in Santa Clara, California 2019",https://www.oreilly.com/radar/highlights-from-tensorflow-world-2019/,People from across the TensorFlow community came together in Santa Clara California for TensorFlow World. Below you&#8217;ll find links to highlights from the event. Opening keynote Jeff Dean explains why Google open-sourced TensorFlow and discusses its progress. Watch &#8220;Opening keynote&#8220; Accelerating ML at Twitter Theodore Summe offers a glimpse into how Twitter employs machine learning [&#8230;]
2019,11,1,Sticker recommendations and AI-driven innovations on the Hike messaging platform,https://www.oreilly.com/radar/sticker-recommendations-and-ai-driven-innovations-on-the-hike-messaging-platform/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,“Human error”: How can we help people build models that do what they expect,https://www.oreilly.com/radar/human-error-how-can-we-help-people-build-models-that-do-what-they-expect/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,Personalization of Spotify Home and TensorFlow,https://www.oreilly.com/radar/personalization-of-spotify-home-and-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow.js: Bringing machine learning to JavaScript,https://www.oreilly.com/radar/tensorflow-js-bringing-machine-learning-to-javascript/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TFX: An end-to-end ML platform for everyone,https://www.oreilly.com/radar/tfx-an-end-to-end-ml-platform-for-everyone/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,MLIR: Accelerating AI,https://www.oreilly.com/radar/mlir-accelerating-ai/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow Hub: The platform to share and discover pretrained models for TensorFlow,https://www.oreilly.com/radar/tensorflow-hub-the-platform-to-share-and-discover-pretrained-models-for-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,TensorFlow Lite: ML for mobile and IoT devices,https://www.oreilly.com/radar/tensorflow-lite-ml-for-mobile-and-iot-devices/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,11,1,"Videos: IMA Computational Imaging Workshop, October 14 - 18, 2019",https://nuit-blanche.blogspot.com/2019/11/videos-ima-computational-imaging.html,**&nbsp;Nuit Blanche&nbsp;is now on Twitter:&nbsp;@NuitBlog&nbsp;**&nbsp;Stanley Chan&nbsp;Jeff Fessler&nbsp;Justin Haldar&nbsp;Ulugbek Kamilov&nbsp;Saiprasad Ravishankar&nbsp;Rebecca Willett&nbsp;Brendt Wohlberg&nbsp;just organized a workshop at IMA on computational imaging. Short story as this blog just passed the 8 million page views. Understanding of Compressed sensing was in large part at least by looking at the stats:hits on this blog due to an IMA meeting on the subject and the fact that people could watch the videos afterward. Hoping for this workshop to follow the same path. Given the amount of ML in it I wonder if it shouldn't have been called&nbsp;TheGreatConvergence meeting:-)This workshop will serve as a venue for presenting and discussing recent advances and trends in the growing field of computational imaging where computation is a major component of the imaging system. Research on all aspects of the computational imaging pipeline from data acquisition (including non-traditional sensing methods) to system modeling and optimization to image reconstruction processing and analytics will be discussed with talks addressing theory algorithms and mathematical techniques and computational hardware approaches for imaging problems and applications including MRI tomography ultrasound microscopy optics computational photography radar lidar astronomical imaging hybrid imaging modalities and novel and extreme imaging systems. The expanding role of computational imaging in industrial imaging applications will also be explored.Given the rapidly growing interest in data-driven machine learning and large-scale optimization based methods in computational imaging the workshop will partly focus on some of the key recent and new theoretical algorithmic or hardware (for efficient/optimized computation) developments and challenges in these areas. Several talks will focus on analyzing incorporating or learning various models including sparse and low-rank models kernel and nonlinear models plug-and-play models graphical manifold tensor and deep convolutional or filterbank models in computational imaging problems. Research and discussion of methods and theory for new sensing techniques including data-driven sensing task-driven imaging optimization and online/real-time imaging optimization will be encouraged. Discussion sessions during the workshop will explore the theoretical and practical impact of various presented methods and brainstorm the main challenges and open problems.The workshop aims to encourage close interactions between mathematical and applied computational imaging researchers and practitioners and bring together experts in academia and industry working in computational imaging theory and applications with focus on data and system modeling signal processing machine learning inverse problems compressed sensing data acquisition image analysis optimization neuroscience computation-driven hardware design and related areas and facilitate substantive and cross-disciplinary interactions on cutting-edge computational imaging methods and systems.&nbsp;Morning Theme: Optics/PhotographyWelcome to the IMA&nbsp;&nbsp;Saiprasad Ravishankar (Michigan State University) Brendt Wohlberg (Los Alamos National Laboratory)&nbsp;Computational Microscopy&nbsp;Laura Waller (University of California Berkeley)&nbsp;&nbsp;Interpreting Photon and Electron Detections to Form Images&nbsp;Vivek Goyal (Boston University)&nbsp;Q&amp;A / Coffee &nbsp;Tutorial on Julia Programming for Computational Imaging&nbsp;Jeff Fessler (University of Michigan)&nbsp;&nbsp;Surfing the Technology Wave 2019 and Algorithm-Hardware Co-Design - Industry Perspective + Q&amp;A&nbsp;Sergio Goma (QUALCOMM)&nbsp;&nbsp;Afternoon Theme: Optimization for Computational Imaging (CI)Dictionary and model-based methods in quantitative MRI reconstruction&nbsp;Mariya Doneva (Philips Research Laboratory)&nbsp;A simple 2D graphing tool for the convergence of fixed-point iterations and plug-and-play methods&nbsp;Wotao Yin (University of California Los Angeles)&nbsp;&nbsp;Data Compression in Distributed Learning&nbsp;Ming Yan (Michigan State University)&nbsp;Morning Theme: Novel Imaging Domains&nbsp;Rebecca Willett (University of Chicago)&nbsp;Cryo-Electron Microscopy Image Analysis with Multi-Frequency Vector Diffusion Maps&nbsp;Zhizhen (Jane) Zhao (University of Illinois at Urbana-Champaign)&nbsp;Imaging the Unseen: Taking the First Picture of a Black Hole&nbsp;Katie Bouman (California Institute of Technology)&nbsp;&nbsp;Q&amp;A&nbsp;Computational Methods for Large-scale Inverse Problems: Data-driven VS Physics-driven or Combined?&nbsp;Youzuo Lin (Los Alamos National Laboratory)&nbsp;Affiliated Data Science Seminar: Simple Approaches to Complicated Data Analysis&nbsp;Deanna Needell (University of California Los Angeles)&nbsp;&nbsp;Afternoon Theme: Perspectives on Machine Learning and Artificial Intelligence for CI Tutorial on Python Programming for Computational Imaging&nbsp;Frank Ong (Stanford University)&nbsp;&nbsp;Tutorial Part II&nbsp;&nbsp;Computational Imaging with Deep Learning&nbsp;Orazio Gallo (NVIDIA Corporation)&nbsp;Poster Session and Reception&nbsp;Morning Theme: Perspectives on Machine Learning and Artificial Intelligence for CIJeff Fessler (University of Michigan)Coherent Optical Processing with Machine Learning&nbsp;Charles Bouman (Purdue University)&nbsp;Faster Guaranteed GAN-based recovery in Linear Inverse Problems&nbsp;Yoram Bresler (University of Illinois at Urbana-Champaign)&nbsp;&nbsp;Q&amp;A&nbsp;&nbsp;Geometry of Convolutional Neural Networks for Computational Imaging&nbsp;Jong Chul Ye (Korea Advanced Institute of Science and Technology (KAIST)) &nbsp;Afternoon Theme: X-Ray Computed Tomography Imaging&nbsp;Novel CT Data Acquisition and Processing&nbsp;Joseph Stayman (Johns Hopkins University)&nbsp;&nbsp;Machine Learning for Tomographic Imaging&nbsp;Ge Wang (Rensselaer Polytechnic Institute)&nbsp;&nbsp;Q&amp;A/Coffee Break&nbsp;&nbsp;Data and Image Domain Deep Learning for Tomographic Computational Imaging&nbsp;W. Clem Karl (Boston University)&nbsp;&nbsp;Q&amp;A&nbsp;&nbsp;IEEE Computational Imaging Technical Committee Meeting (open to all)&nbsp;Morning Theme: Signal Processing for CI&nbsp;Stanley Chan (Purdue University) Justin Haldar (University of Southern California)&nbsp;Three Short Stories About Image Denoising&nbsp;Mario Figueiredo (Instituto Superior Tecnico)&nbsp;&nbsp;Fourier Multispectral Imaging&nbsp;Keigo Hirakawa (University of Dayton)&nbsp;Q&amp;A&nbsp;Modeling and removal of correlated noise using nonlocal patch-based collaborative filters with applications to direct and inverse imaging&nbsp;Alessandro Foi (Tampere University of Technology)&nbsp;&nbsp;Afternoon Theme: Magnetic Resonance Imaging Computational Imaging: Beyond the limits imposed by lenses&nbsp;Ashok Veeraraghavan (Rice University) &nbsp;Q&amp;A / Coffee&nbsp;Computational Imaging: From structured low-rank methods to model based deep learning.&nbsp;Mathews Jacob (The University of Iowa)&nbsp;Q&amp;A/Coffee Break&nbsp;&nbsp;Rise of the machines (in MR image reconstruction)&nbsp;Florian Knoll (NYU Langone Medical Center)&nbsp;Q&amp;A&nbsp;Theme: Broad CI Modalities&nbsp;Ulugbek Kamilov (Washington University)&nbsp;Computational Radar Imaging&nbsp;Mujdat Cetin (University of Rochester)&nbsp;&nbsp;Q&amp;A&nbsp;Challenges and Opportunities in Magnetic Resonance Fingerprinting&nbsp;Nicole Seiberlich (University of Michigan)&nbsp;&nbsp;Q&amp;A/Closing Remarks&nbsp;Brendt Wohlberg (Los Alamos National Laboratory)Follow @NuitBlog&nbsp;or join the CompressiveSensing Reddit&nbsp;the Facebook page the Compressive Sensing group on&nbsp;LinkedIn&nbsp;&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;LinkedInLiked this entry ? subscribe to Nuit Blanche's feed there's more where that came from.&nbsp;You can also subscribe to Nuit Blanche by Email.Other links:Paris Machine Learning:&nbsp;Meetup.com||@Archives||LinkedIn||Facebook|| @ParisMLGroup&lt; br/&gt;  About&nbsp;LightOn:&nbsp;Newsletter ||@LightOnIO|| on LinkedIn || on CrunchBase || our BlogAbout myself:&nbsp;LightOn || Google Scholar || LinkedIn ||@IgorCarron ||Homepage||ArXiv
2019,10,31,Accelerating ML at Twitter,https://www.oreilly.com/radar/accelerating-ml-at-twitter/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,The latest from TensorFlow,https://www.oreilly.com/radar/the-latest-from-tensorflow/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,TensorFlow World 2019 opening keynote,https://www.oreilly.com/radar/tensorflow-world-2019-opening-keynote/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,31,TensorFlow community announcements,https://www.oreilly.com/radar/tensorflow-community-announcements/,This is a keynote from TensorFlow World in Santa Clara California 2019. See more keynotes from this event.
2019,10,29,The journal game,https://robjhyndman.com/seminars/journalgame/,Presentation for the department retreat  Monash University authorship policy
2019,10,27,Optimizing Inventory Level: Reduce the Inventory Value and Increase Customer Satisfaction Simultaneously,https://www.deep-data-mining.com/2019/10/optimizing-inventory-level-reduce.html," For a large part manufacture company in the transportation industry maintaining the optimal inventory level in their warehouses is crucial to its bottom line. When too many parts are produced and stored it costs the company excessive financial investment and previous warehouse spaces. On the other hand if not enough parts in the warehouses customers will become dissatisfied when orders may not get fulfilled in time. Thus there are two conflicting goals to balance when planning the inventory: reducing inventory value and increasing customer satisfaction. The optimal strategy is to find the sweet spot of inventory level for each individual part that is most economical and maintaining high level of customer satisfaction at the same time.  In a recent project that Dr. Jay Zhou has preformed he is able to reduce the inventory level for his client company by $16 million and still maintain the same level of customer satisfaction. This work is highly received by the client. In this project Dr.Zhou takes advantage of machine learning models and reduces huge number of parts to a much smaller number of homogeneous groups. The ""Demand Satisfaction"" are calculated for these groups.    "
2019,10,24,Make AI “Intelligent” Again,https://www.mdmgeek.com/2019/10/24/make-artificial-intelligence-intelligent-again/,"Today artificial intelligence and machine learning are hot once more. How can we get it right this time? I provide few tips.
The post Make AI “Intelligent” Again appeared first on MDMgeek."
2019,10,17,Highlights from the O’Reilly Artificial Intelligence Conference in London 2019,https://www.oreilly.com/radar/highlights-from-ai-london-2019/,People from across the AI world came together in London for the Artificial Intelligence Conference. Below you&#8217;ll find links to highlights from the event. When flying is cheaper than standing still Raffaello D’Andrea presents his vision of how autonomous indoor drones will drive the next wave of robotics development. Watch &#8220;When flying is cheaper than [&#8230;]
2019,10,17,When to trust AI,https://www.oreilly.com/radar/when-to-trust-ai/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,When flying is cheaper than standing still,https://www.oreilly.com/radar/when-flying-is-cheaper-than-standing-still/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,"Machine learning challenges at LinkedIn: Spark, TensorFlow, and beyond",https://www.oreilly.com/radar/machine-learning-challenges-at-linkedin/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,Accelerate with purpose,https://www.oreilly.com/radar/accelerate-with-purpose-ai-uk-19/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,The quest for high-quality data,https://www.oreilly.com/radar/the-quest-for-high-quality-data-ai-uk-19/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,17,Non-Gaussian forecasting using fable,https://robjhyndman.com/hyndsight/fable2/,"library(tidyverse) library(tsibble) library(lubridate) library(feasts) library(fable) In my previous post about the new fable package we saw how fable can produce forecast distributions not just point forecasts. All my examples used Gaussian (normal) distributions so in this post I want to show how non-Gaussian forecasting can be done.
As an example we will use eating-out expenditure in my home state of Victoria.
vic_cafe &lt;- tsibbledata::aus_retail %&gt;% filter( State == &quot;Victoria&quot; Industry == &quot;Cafes restaurants and catering services&quot; ) %&gt;% select(Month Turnover) vic_cafe %&gt;% autoplot(Turnover) + ggtitle(&quot;Monthly turnover of Victorian cafes&quot;) Forecasting with transformations Clearly the variance is increasing with the level of the series so we will consider modelling a Box-Cox transformation of the data."
2019,10,16,Real-time AI for entity resolution,https://www.oreilly.com/radar/real-time-ai-for-entity-resolution/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,16,The power of knowledge at scale,https://www.oreilly.com/radar/the-power-of-knowledge-at-scale/,This is a keynote highlight from the O&#8217;Reilly Artificial Intelligence Conference in London 2019. Watch the full version of this keynote on the O&#8217;Reilly online learning platform. You can also see other highlights from the event.
2019,10,16,Three opinions on theorems,https://justindomke.wordpress.com/2019/10/16/three-opinions-on-theorems/,1. Think of theorem statements like an API. Some people feel intimidated by the prospect of putting a &#8220;theorem&#8221; into their papers. They feel that their results aren&#8217;t &#8220;deep&#8221; enough to justify this. Instead they give the derivation and result inline as part of the normal text. Sometimes that&#8217;s best. However the purpose of a &#8230; Continue reading Three opinions on&#160;theorems &#8594;
2019,10,9,Forecasts are always wrong (but we need them anyway),https://robjhyndman.com/seminars/thoughtcapital/,Podcast interview for Thought Capital Recently I was interviewed for the Monash Business School podcast “Thought Capital” on the topic of forecasting. You can listen to the episode here (or read the transcript).
2019,10,4,Exponential Families Cheatsheet,https://justindomke.wordpress.com/2019/10/04/exponential-families-cheatsheet/,As part of the graphical models course I taught last spring I developed a &#8220;cheatsheet&#8221; for exponential families. The basic purpose is to explain the standard moment-matching condition of maximum likelihood. The goal of the sheet was to clearly show how this property generalized to maximum likelihood in conditional exponential families with hidden variables or &#8230; Continue reading Exponential Families Cheatsheet &#8594;
2019,9,29,Tidy forecasting in R,https://robjhyndman.com/hyndsight/fable/,"The fable package for doing tidy forecasting in R is now on CRAN. Like tsibble and feasts it is also part of the tidyverts family of packages for analysing modelling and forecasting many related time series (stored as tsibbles).
For a brief introduction to tsibbles see this post from last month.
Here we will forecast Australian tourism data by state/region and purpose. This data is stored in the tourism tsibble where Trips contains domestic visitor nights in thousands."
2019,9,27,Feature-based time series analysis,https://robjhyndman.com/seminars/unsw-fbtsa/,Presentation given at UNSW It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed. I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series.
2019,9,27,Tidy time series analysis in R,https://robjhyndman.com/seminars/tidyverts/,tidyverts R packages
2019,9,26,Tidy time series analysis in R,https://robjhyndman.com/seminars/fable-cardiff/,Presentation at meetup for Cardiff Business School tidyverts R packages
2019,9,25,Registration is open - Deep Learning Autumn School at Bar Ilan University,https://bickson.blogspot.com/2019/09/registration-is-open-deep-learning.html,My friend Ely Porat sent me the following registration notice for the deep learning and AR/VR Autumn courses:https://sites.google.com/datalab.cs.biu.ac.il/biusummerschool/home?authuser=0Registration is pretty low and you can get academic credit from Bar Ilan University.
2019,9,17,A feature-based procedure for detecting technical outliers in water-quality data from in situ sensors,https://robjhyndman.com/publications/oddwater/,Outliers due to technical errors in water‐quality data from in situ sensors can reduce data quality and have a direct impact on inference drawn from subsequent data analysis. However outlier detection through manual monitoring is infeasible given the volume and velocity of data the sensors produce. Here we introduce an automated procedure named oddwater that provides early detection of outliers in water‐quality data from in situ sensors caused by technical issues.
2019,9,16,Feature-based time series analysis,https://robjhyndman.com/hyndsight/fbtsa/,"In my last post I showed how the feasts package can be used to produce various time series graphics.
The feasts package also includes functions for computing FEatures And Statistics from Time Series (hence the name). In this post I will give three examples of how these might be used.
library(tidyverse) library(tsibble) library(feasts) Exploring Australian tourism data I used this example in my talk at useR!2019 in Toulouse and it is also the basis of a vignette in the package and a recent blog post by Mitchell O’Hara-Wild."
2019,8,30,Time series graphics using feasts,https://robjhyndman.com/hyndsight/feasts/,"This is the second post on the new tidyverts packages for tidy time series analysis. The previous post is here.
For users migrating from the forecast package it might be useful to see how to get similar graphics to those they are used to. The forecast package is built for ts objects while the feasts package provides features statistics and graphics for tsibbles. (See my first post for a description of tsibbles."
2019,8,29,Tidy time series data using tsibbles,https://robjhyndman.com/hyndsight/tsibbles/,"There is a new suite of packages for tidy time series analysis that integrates easily into the tidyverse way of working. We call these the tidyverts packages and they are available at tidyverts.org. Much of the work on these packages has been done by Earo Wang and Mitchell O’Hara-Wild.
The first of the packages to make it to CRAN was tsibble providing the data infrastructure for tidy temporal data with wrangling tools."
2019,8,28,You can replicate almost any plot with R,https://simplystatistics.org/2019/08/28/you-can-replicate-almost-any-plot-with-ggplot2/,"Although R is great for quickly turning data into plots it is not widely used for making publication ready figures. But with enough tinkering you can make almost any plot in R. For examples check out the flowingdata blog or the Fundamentals of Data Visualization book.
Here I show five charts from the lay press that I use as examples in my data science courses. In the past I would show the originals but I decided to replicate them in R to make it possible to generate class notes with just R code (there was a lot of googling involved).
Below I show the original figures followed by R code and the version of the plot it produces. I used the ggplot2 package but you can achieve similar results using other packages or even just with R-base. Any recommendations on how to improve the code or links to other good examples are welcomed. Please at to the comments or @ me on twitter: @rafalab.

Example 1
The first example is from this ABC news article. Here is the original:

Here is the R code for my version. Note that I copied the values by hand.
library(tidyverse)
library(ggplot2)
library(ggflags)
library(countrycode)

dat &lt;- tibble(country = toupper(c(&quot;US&quot; &quot;Italy&quot; &quot;Canada&quot; &quot;UK&quot; &quot;Japan&quot; &quot;Germany&quot; &quot;France&quot; &quot;Russia&quot;))
              count = c(3.2 0.71 0.5 0.1 0 0.2 0.1 0)
              label = c(as.character(c(3.2 0.71 0.5 0.1 0 0.2 0.1)) &quot;No Data&quot;)
              code = c(&quot;us&quot; &quot;it&quot; &quot;ca&quot; &quot;gb&quot; &quot;jp&quot; &quot;de&quot; &quot;fr&quot; &quot;ru&quot;))

dat %&gt;% mutate(country = reorder(country -count)) %&gt;%
  ggplot(aes(country count label = label)) +
  geom_bar(stat = &quot;identity&quot; fill = &quot;darkred&quot;) +
  geom_text(nudge_y = 0.2 color = &quot;darkred&quot; size = 5) +
  geom_flag(y = -.5 aes(country = code) size = 12) +
  scale_y_continuous(breaks = c(0 1 2 3 4) limits = c(04)) +   
  geom_text(aes(6.25 3.8 label = &quot;Source UNODC Homicide Statistics&quot;)) + 
  ggtitle(toupper(&quot;Homicide Per 100000 in G-8 Countries&quot;)) + 
  xlab(&quot;&quot;) + 
  ylab(&quot;# of gun-related homicides\nper 100000 people&quot;) +
  ggthemes::theme_economist() +
  theme(axis.text.x = element_text(size = 8 vjust = -16)
        axis.ticks.x = element_blank()
        axis.line.x = element_blank()
        plot.margin = unit(c(1111) &quot;cm&quot;)) 



Example 2
The second example from everytown.org. Here is the original:

Here is the R code for my version. As in the previous example I copied the values by hand.
dat &lt;- tibble(country = toupper(c(&quot;United States&quot; &quot;Canada&quot; &quot;Portugal&quot; &quot;Ireland&quot; &quot;Italy&quot; &quot;Belgium&quot; &quot;Finland&quot; &quot;France&quot; &quot;Netherlands&quot; &quot;Denmark&quot; &quot;Sweden&quot; &quot;Slovakia&quot; &quot;Austria&quot; &quot;New Zealand&quot; &quot;Australia&quot; &quot;Spain&quot; &quot;Czech Republic&quot; &quot;Hungry&quot; &quot;Germany&quot; &quot;United Kingdom&quot; &quot;Norway&quot; &quot;Japan&quot; &quot;Republic of Korea&quot;))
              count = c(3.61 0.5 0.48 0.35 0.35 0.33 0.26 0.20 0.20 0.20 0.19 0.19 0.18 0.16
                        0.16 0.15 0.12 0.10 0.06 0.04 0.04 0.01 0.01))

dat %&gt;% 
  mutate(country = reorder(country count)) %&gt;%
  ggplot(aes(country count label = count)) +   
  geom_bar(stat = &quot;identity&quot; fill = &quot;darkred&quot; width = 0.5) +
  geom_text(nudge_y = 0.2  size = 3) +
  xlab(&quot;&quot;) + ylab(&quot;&quot;) + 
  ggtitle(toupper(&quot;Gun Murders per 100000 residents&quot;)) + 
  theme_minimal() +
  theme(panel.grid.major =element_blank() panel.grid.minor = element_blank() 
        axis.text.x = element_blank()
        axis.ticks.length = unit(-0.4 &quot;cm&quot;)) + 
  coord_flip() 



Example 3
The next example is from the Wall Street Journal. The original is interactive but here is a screenshot:

Here is the R code for my version. Note I matched the colors by hand as the original does not seem to follow a standard palette.
library(dslabs)
data(us_contagious_diseases)
the_disease &lt;- &quot;Measles&quot;
dat &lt;- us_contagious_diseases %&gt;%
  filter(!state%in%c(&quot;Hawaii&quot;&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting) 

jet.colors &lt;- colorRampPalette(c(&quot;#F0FFFF&quot; &quot;cyan&quot; &quot;#007FFF&quot; &quot;yellow&quot; &quot;#FFBF00&quot; &quot;orange&quot; &quot;red&quot; &quot;#7F0000&quot;) bias = 2.25)

dat %&gt;% mutate(state = reorder(state desc(state))) %&gt;%
  ggplot(aes(year state fill = rate)) +
  geom_tile(color = &quot;white&quot; size = 0.35) +
  scale_x_continuous(expand = c(00)) +
  scale_fill_gradientn(colors = jet.colors(16) na.value = &#39;white&#39;) +
  geom_vline(xintercept = 1963 col = &quot;black&quot;) +
  theme_minimal() + 
  theme(panel.grid = element_blank()) +
        coord_cartesian(clip = &#39;off&#39;) +
        ggtitle(the_disease) +
        ylab(&quot;&quot;) +
        xlab(&quot;&quot;) +  
        theme(legend.position = &quot;bottom&quot; text = element_text(size = 8)) + 
        annotate(geom = &quot;text&quot; x = 1963 y = 50.5 label = &quot;Vaccine introduced&quot; size = 3 hjust = 0)



Example 4
The next example is from the New York Times. Here is the original:

Here is the R code for my version:
data(&quot;nyc_regents_scores&quot;)
nyc_regents_scores$total &lt;- rowSums(nyc_regents_scores[-1] na.rm=TRUE)
nyc_regents_scores %&gt;% 
  filter(!is.na(score)) %&gt;%
  ggplot(aes(score total)) + 
  annotate(&quot;rect&quot; xmin = 65 xmax = 99 ymin = 0 ymax = 35000 alpha = .5) +
  geom_bar(stat = &quot;identity&quot; color = &quot;black&quot; fill = &quot;#C4843C&quot;) + 
  annotate(&quot;text&quot; x = 66 y = 28000 label = &quot;MINIMUM\nREGENTS DIPLOMA\nSCORE IS 65&quot; hjust = 0 size = 3) +
  annotate(&quot;text&quot; x = 0 y = 12000 label = &quot;2010 Regents scores on\nthe five most common tests&quot; hjust = 0 size = 3) +
  scale_x_continuous(breaks = seq(5 95 5) limit = c(099)) + 
  scale_y_continuous(position = &quot;right&quot;) +
  ggtitle(&quot;Scraping By&quot;) + 
  xlab(&quot;&quot;) + ylab(&quot;Number of tests&quot;) + 
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank() 
        panel.grid.minor.x = element_blank()
        axis.ticks.length = unit(-0.2 &quot;cm&quot;)
        plot.title = element_text(face = &quot;bold&quot;))



Example 5
This last one is from fivethirtyeight.

Below is the R code for my version. Note that in this example I am essentially just drawing as I don’t estimate the distributions myself. I simply estimated parameters “by eye” and used a bit of trial and error.
my_dgamma &lt;- function(x mean = 1 sd = 1){
  shape = mean^2/sd^2
  scale = sd^2 / mean
  dgamma(x shape = shape scale = scale)
}

my_qgamma &lt;- function(mean = 1 sd = 1){
  shape = mean^2/sd^2
  scale = sd^2 / mean
  qgamma(c(0.10.9) shape = shape scale = scale)
}

tmp &lt;- tibble(candidate = c(&quot;Clinton&quot; &quot;Trump&quot; &quot;Johnson&quot;) 
              avg = c(48.5 44.9 5.0) 
              avg_txt = c(&quot;48.5%&quot; &quot;44.9%&quot; &quot;5.0%&quot;) 
              sd = rep(2 3) 
              m = my_dgamma(avg avg sd)) %&gt;%
  mutate(candidate = reorder(candidate -avg))

xx &lt;- seq(0 75 len = 300)

tmp_2 &lt;- map_df(1:3 function(i){
  tibble(candidate = tmp$candidate[i]
         avg = tmp$avg[i]
         sd = tmp$sd[i]
         x = xx
         y = my_dgamma(xx tmp$avg[i] tmp$sd[i]))
})

tmp_3 &lt;- map_df(1:3 function(i){
  qq &lt;- my_qgamma(tmp$avg[i] tmp$sd[i])
  xx &lt;- seq(qq[1] qq[2] len = 200)
  tibble(candidate = tmp$candidate[i]
         avg = tmp$avg[i]
         sd = tmp$sd[i]
         x = xx
         y = my_dgamma(xx tmp$avg[i] tmp$sd[i]))
})
         
tmp_2 %&gt;% 
  ggplot(aes(x ymax = y ymin = 0)) +
  geom_ribbon(fill = &quot;grey&quot;) + 
  facet_grid(candidate~. switch = &quot;y&quot;) +
  scale_x_continuous(breaks = seq(0 75 25) position = &quot;top&quot;
                     label = paste0(seq(0 75 25) &quot;%&quot;)) +
  geom_abline(intercept = 0 slope = 0) +
  xlab(&quot;&quot;) + ylab(&quot;&quot;) + 
  theme_minimal() + 
  theme(panel.grid.major.y = element_blank() 
        panel.grid.minor.y = element_blank()
        axis.title.y = element_blank()
        axis.text.y = element_blank()
        axis.ticks.y = element_blank()
        strip.text.y = element_text(angle = 180 size = 11 vjust = 0.2)) + 
  geom_ribbon(data = tmp_3 mapping = aes(x = x ymax = y ymin = 0 fill = candidate) inherit.aes = FALSE show.legend = FALSE) +
  scale_fill_manual(values = c(&quot;#3cace4&quot; &quot;#fc5c34&quot; &quot;#fccc2c&quot;)) +
  geom_point(data = tmp mapping = aes(x = avg y = m) inherit.aes = FALSE) + 
  geom_text(data = tmp mapping = aes(x = avg y = m label = avg_txt) inherit.aes = FALSE hjust = 0 nudge_x = 1) 

"
2019,8,27,So You Want to Start a Podcast,https://simplystatistics.org/2019/08/27/so-you-want-to-start-a-podcast/,"Podcasting has gotten quite a bit easier over the past 10 years due in part to improvements to hardware and software. I wrote about both how I edit and record both of my podcasts about 2 years ago and while not much has changed since then I thought it might be helpful if I organized the information in a better way for people just starting out with a new podcast.

One frustrating problem that I find with podcasting is that the easy methods are indeed easy and the difficult methods are indeed difficult but the methods that are just above easy which other markets might label as “prosumer” or something like that are&hellip;kind of hard. One of the reasons is that once you start buying better hardware everything kind of snowballs because the hardware becomes more modular. So instead of just using your phone headphones to record you might buy a microphone that connects to a stand that connects to a USB interface using an XLR cable that connects to your computer. Similarly on the software side there’s really not much out there that’s free. As a result of both phenomena costs start to go up pretty quickly as soon as you step up just a little bit.

I can’t do anything about costs but I thought I could help a little bit on sorting out what’s out there and what’s genuinely valuable. There are two versions here: the free and easy plan if you’re just starting out and the next level up which is basically what I use.

The three things I’ll cover here that you need for podcasting are:


Hardware - this includes all recording equipment like microphones stands cables etc.
Recording Software - Unless you live in a recording booth you’ll need some software for your computer (which I assume you have!)
Editing Software - the more complicated your podcast gets the more you’ll need to edit (beyond just trimming the beginning and end of the audio files)
Hosting - Unless you plan on running your own server (which is an option but I don’t recommend it) you’ll need someone to host your audio files.


Free and Easy

There are in fact ways to podcast for free and many people stay at this level for a long time because the quality is acceptable and cost is zero. If you want to just get started quickly here’s what you can do:


Hardware - just use the headphones/microphone that came with your mobile phone.
Recording Software - If you are doing a podcast by yourself you can just use whatever app your phone has to record things like voice memos. On your computer there should be a built-in app that just lets you record sound through the headphones.
Editing Software - For editing I recommend either not editing (simpler!) or using something like Audacity to just trim the beginning and the end.
Hosting - SoundCloud offers free hosting for up to 3 hours of content. This is plenty for just starting out and seeing if you like it but you will likely use it up.


If you are working with a partner it gets a little more complicated and there are some additional notes on the recording software. My go-to recommendation for recording with a partner is to use Zencastr. Zencastr has a free plan that lets you record high-quality audio for a max of 2 people. (If you need to record more than 2 people you can’t use the free option.) The nice thing about Zencastr is that it uses WebRTC to record directly off your microphone so you don’t need to worry too much about the quality of your internet connection. What you get is separate audio files one for each speaker that are synched together. Occasionally there are some synching glitches but usually it works out. The files are automatically uploaded to a Dropbox account so you’ll need one of those. Because Zencastr automatically goes to MP3 format the files are relatively small. Also if you have a guest who is less familiar with audio hardware/software you can just send them a link that they can click on and they’re recording.

Note that even if your partner is sitting right next to you it’s often simpler to just go to separate spaces and record “remotely”. The primary benefit of doing this is that you can cleanly record separate/independent audio tracks. This can be useful in the editing process.

If you prefer an all-in-one solution there are services like Cast and Anchor that offer recording hosting and distribution. Cast only has a free 1-month trial and so you have to pay eventually. Anchor appears to be free (I’ve never used it) but it was recently purchased by Spotify so it’s not immediately clear to me if anything will change. My guess is they’ll likely stay free because they want as many people making podcasts as possible. Anchor didn’t exist when I started podcasting but if it had I might have used it first. But it always makes me a little nervous when I can’t figure out how a company makes money.

To summarize here’s the “free and easy” workflow that I recommend:


Record your podcast using Zencastr (especially if you have a partner) which then puts audio files on Dropbox
Trim beginning/ending of audio file with Audacity
Upload audio to SoundCloud and add episode metadata


And here are the pros and cons:

Pros


It’s free


Cons


Audio quality is acceptable but not great. Earbud type microphones are not designed for high quality and you can usually tell when someone has used them to record. Given that podcasts are all about audio it’s hard for me to trade off audio quality.
Hosting limitations mean you can only get a few episodes up. But that’s a problem for down the road right?
Editing is generally a third-order issue but there is one scenario where it can be critical&mdash;when you have a bad internet connection. Bad internet connections can introduce delays and cross-talk. These problems can be mitigated when editing (I give an example here) but only with better software.


Beyond Free

Beyond the free workflow there are a number of upgrades that you can make and you can easily start spending a lot of money. But the only real upgrade that I think you need to make is to buy a good microphone. Surprisingly this does not need to cost much money. The best podcasting microphone for the money out there is the Audio Technica ATR2100 USB micrphone. This is the microphone that Elizabeth uses on the The Effort Report and Hilary uses on Not So Standard Deviations. As of this writing it’s \$65 on Amazon but I’ve seen it for as low as \$40. The benefits of this microphone are:


The audio quality is high
It isolates vocal audio really well and doesn’t pick up a lot of background audio (good for noisy rooms like my office).
It connects directly to a computer via USB so you don’t need to buy a separate USB interface.
It’s cheap


The problem with getting “better” (i.e. more expensive) microphones as that they tend to be more sensitive which means they pick up more high-frequency background noise. Professional microphones are designed for you to be working in a sound-proof recording studio environment in which you want to pick up as much sound as possible. But podcasting in general tends to take place wherever. So you want a microphone that will only pick up your voice right in front of it. Technically you lose a little quality this way but it’s equally annoying to have a lot of background noise.

Now that you’ve got a microphone you need to stick it somewhere. While you can always just hold the microphone I’d recommend an adjustable stand of some sort. Desk stands like this one are nice because they’re adjustable but they do require you to have a semi-permanent office where you can just keep it. The main point here is that podcasting requires you to sit still and talk for a while and you don’t want to be uncomfortable while you’re doing it.

The last upgrade you’ll likely need to make is the hosting provider. SoundCloud itself offers an unlimited plan but I don’t recommend it as it’s not really designed for podcasting. I use Libsyn which has a $5 a month plan that should be enough for a monthly podcast. They also provide some decent analytics that you can download and read into R. What I like about Libsyn is that they do one job and they do it really well. I give them money and they provide me a service in return. How simple is that?

That’s it for now. I’m happy to make more recommendations regarding software and hardware (feel free to tweet me @rdpeng) but I think what I’ve got here should get you 99% of the way there."
2019,8,23,Predicting sediment and nutrient concentrations from high-frequency water-quality data,https://robjhyndman.com/publications/water-quality/,Water-quality monitoring in rivers often focuses on the concentrations of sediments and nutrients constituents that can smother biota and cause eutrophication. However the physical and economic constraints of manual sampling prohibit data collection at the frequency required to adequately capture the variation in concentrations through time. Here we developed models to predict total suspended solids (TSS) and oxidized nitrogen (NOx) concentrations based on high-frequency time series of turbidity conductivity and river level data from in situ sensors in rivers flowing into the Great Barrier Reef lagoon.
2019,8,21,Forecasting is not prophecy: dealing with high-dimensional probabilistic forecasts in practice,https://robjhyndman.com/seminars/isi_prophecy/,Invited talk for ISI-WSC 2019 in Kuala Lumpur Many large organizations need to forecast huge numbers of related time series every week. Manufacturing companies forecast product demand to plan their supply chains; call centres forecast call volume to inform staff scheduling; technology companies forecast web traffic to maintain service levels; energy companies forecast electricity demand to prevent blackouts. In each case what is required is a high-dimensional probabilistic forecast describing multivariate quantiles of the uncertain future not a vector of point forecasts.
2019,8,17,High-dimensional time series analysis,https://robjhyndman.com/seminars/isi2019workshop/,"Venue Sasana Kijang Bank Negara Malaysia Kuala Lumpur
Presenters  Rob J Hyndman Mitchell O&rsquo;Hara-Wild  Course description It is becoming increasingly common for organizations to collect huge amounts of data over time and existing time series analysis tools are not always suitable to handle the scale and type of data collected. In this workshop we will look at some new methods that have been developed to handle the analysis of large collections of time series."
2019,8,10,The Size of the Global Master Data Management Market,https://www.mdmgeek.com/2019/08/10/the-size-of-the-global-master-data-management-market/,"Master Data Management is a growing market. Every analyst has a way to estimate how large it is. Are they accurate in their estimation? What should we know?
The post The Size of the Global Master Data Management Market appeared first on MDMgeek."
2019,7,23,The data deluge means no reasonable expectation of privacy - now what?,https://simplystatistics.org/2019/07/23/the-data-deluge-means-no-reasonable-expectation-of-privacy-no-what/,"Today a couple of different things reminded me about something that I suppose many people are talking about but has been on my mind as well.

The idea is that many of our societies social norms are based on the reasonable expectation of privacy. But the reasonable expectation of privacy is increasingly a thing of the past. Three types of data I&rsquo;ve been thinking about are:


Obviously identifying data: Data like cellphone GPS traces and public social media posts are obviously information that is indentifiable and reduce privacy.
Data that can be inferred from public data: We can also now infer a lot about people given the data that is public. For example a couple of years ago I challenged the students in my advanced data science class to predict the Gail score - one of the most widely used measures of breast cancer risk  - using only the information available from a person&rsquo;s public Facebook profile. While not all of the information was available a good fraction of it was. This is an example of something you might not think that posting pictures of your family your birthday celebrations and family life events could enable. I was reminded of this when hearing about this paper that claims to be able to deidentify up to 99.98\% of Americans using only 15 pieces of demographic information.
Data other people share about us: The stories around the capture of the Golden Gate Killer using genealogy data make it clear that even when you personally don&rsquo;t share your data someone else may be sharing it for you. The same can be said of photos of you that were tagged on Facebook even if you aren&rsquo;t on the platform.


I don&rsquo;t think these types of data are going to magically disappear. So like a lot of other people I&rsquo;ve been wondering how we should individually and as a society adapt to the world where privacy is no longer an expectation."
2019,7,19,More datasets for teaching data science: The expanded dslabs package,https://simplystatistics.org/2019/07/19/more-datasets-for-teaching-data-science-the-expanded-dslabs-package/,"
Introduction
We have expanded the dslabs package which we previously introduced as a package containing realistic interesting and approachable datasets that can be used in introductory data science courses.
This release adds 7 new datasets on climate change astronomy life expectancy and breast cancer diagnosis. They are used in improved problem sets and new projects within the HarvardX Data Science Professional Certificate Program which teaches beginning R programming data visualization data wrangling statistics and machine learning for students with no prior coding background.
You can install the dslabs package from CRAN:
install.packages(&quot;dslabs&quot;)
If you already have the package installed you can add the new datasets by updating the package:
update.packages(&quot;dslabs&quot;)
You can load the package into your workspace normally:
library(dslabs)
Let’s preview these new datasets! To code along use the following libraries and options:
# install packages if necessary
if(!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;)
if(!require(&quot;ggrepel&quot;)) install.packages(&quot;ggrepel&quot;)
if(!require(&quot;matrixStats&quot;)) install.packages(&quot;matrixStats&quot;)


# load libraries
library(tidyverse)
library(ggrepel)
library(matrixStats)

# set colorblind-friendly color palette
colorblind_palette &lt;- c(&quot;black&quot; &quot;#E69F00&quot; &quot;#56B4E9&quot; &quot;#009E73&quot;
                        &quot;#CC79A7&quot; &quot;#F0E442&quot; &quot;#0072B2&quot; &quot;#D55E00&quot;)


Climate change
Three datasets related to climate change are used to teach data visualization and data wrangling. These data produce clear plots that demonstrate an increase in temperature greenhouse gas levels and carbon emissions from 800000 years ago to modern times. Students can create their own impactful visualizations with real atmospheric and ice core measurements.

Modern temperature anomaly and carbon dioxide data: temp_carbon
The temp_carbon dataset includes annual global temperature anomaly measurements in degrees Celsius relative to the 20th century mean temperature from 1880-2018. The temperature anomalies over land and over ocean are reported also. In addition it includes annual carbon emissions (in millions of metric tons) from 1751-2014. Temperature anomalies are from NOAA and carbon emissions are from Boden et al. 2017 via CDIAC.
data(temp_carbon)

# line plot of annual global land and ocean temperature anomalies since 1880
temp_carbon %&gt;%
    select(Year = year Global = temp_anomaly Land = land_anomaly Ocean = ocean_anomaly) %&gt;%
    gather(Region Temp_anomaly Global:Ocean) %&gt;%
    ggplot(aes(Year Temp_anomaly col = Region)) +
    geom_line(size = 1) +
    geom_hline(aes(yintercept = 0) col = colorblind_palette[8] lty = 2) +
    geom_label(aes(x = 2005 y = -.08) col = colorblind_palette[8] 
               label = &quot;20th century mean&quot; size = 4) +
    ylab(&quot;Temperature anomaly (degrees C)&quot;) +
    xlim(c(1880 2018)) +
    scale_color_manual(values = colorblind_palette) +
    ggtitle(&quot;Temperature anomaly relative to 20th century mean 1880-2018&quot;)



Greenhouse gas concentrations over 2000 years: greenhouse_gases
The greenhouse_gases data frame contains carbon dioxide (\(\mbox{CO}_2\) ppm) methane (\(\mbox{CO}_2\) ppb) and nitrous oxide (\(\mbox{N}_2\mbox{O}\) ppb) concentrations every 20 years from 0-2000 CE. The data are a subset of ice core measurements from MacFarling Meure et al. 2006 via NOAA. There is a clear increase in all 3 gases starting around the time of the Industrial Revolution.
data(greenhouse_gases)

# line plots of atmospheric concentrations of the three major greenhouse gases since 0 CE
greenhouse_gases %&gt;%
    ggplot(aes(year concentration)) +
    geom_line() +
    facet_grid(gas ~ . scales = &quot;free&quot;) +
    xlab(&quot;Year&quot;) +
    ylab(&quot;Concentration (CH4/N2O ppb CO2 ppm)&quot;) +
    ggtitle(&quot;Atmospheric greenhouse gas concentration by year 0-2000 CE&quot;)

Compare this pattern with manmade carbon emissions since 1751 from temp_carbon which have risen in a similar way:
# line plot of anthropogenic carbon emissions over 250+ years
temp_carbon %&gt;%
    ggplot(aes(year carbon_emissions)) +
    geom_line() +
    xlab(&quot;Year&quot;) +
    ylab(&quot;Carbon emissions (metric tons)&quot;) +
    ggtitle(&quot;Annual global carbon emissions 1751-2014&quot;)



Carbon dioxide levels over the last 800000 years historic_co2
A common argument against the existence of anthropogenic climate change is that the Earth naturally undergoes cycles of warming and cooling governed by natural changes beyond human control. \(\mbox{CO}_2\) levels from ice cores and modern atmospheric measurements at the Mauna Loa observatory demonstrate that the speed and magnitude of natural variations in greenhouse gases pale in comparison to the rapid changes in modern industrial times. While the planet has been hotter and had higher \(\mbox{CO}_2\) levels in the distant past (data not shown) the current unprecedented rate of change leaves little time for planetary systems to adapt.
data(historic_co2)

# line plot of atmospheric CO2 concentration over 800K years colored by data source
historic_co2 %&gt;%
    ggplot(aes(year co2 col = source)) +
    geom_line() +
    ylab(&quot;CO2 (ppm)&quot;) +
    scale_color_manual(values = colorblind_palette[7:8]) +
    ggtitle(&quot;Atmospheric CO2 concentration -800000 BCE to today&quot;)




Properties of stars for making an H-R diagram: stars
In astronomy stars are classified by several key features including temperature spectral class (color) and luminosity (brightness). A common plot for demonstrating the different groups of stars and their propreties is the Hertzsprung-Russell diagram or H-R diagram. The stars data frame compiles information for making an H-R diagram with about approximately 100 named stars including their temperature spectral class and magnitude (which is inversely proportional to luminosity).
The H-R diagram has the hottest brightest stars in the upper left and coldest dimmest stars in the lower right. Main sequence stars are along the main diagonal while giants are in the upper right and dwarfs are in the lower left. Several aspects of data visualization can be practiced with these data.
data(stars)

# H-R diagram color-coded by spectral class
stars %&gt;%
    mutate(type = factor(type levels = c(&quot;O&quot; &quot;B&quot; &quot;DB&quot; &quot;A&quot; &quot;DA&quot; &quot;DF&quot; &quot;F&quot; &quot;G&quot; &quot;K&quot; &quot;M&quot;))
           star = ifelse(star %in% c(&quot;Sun&quot; &quot;Polaris&quot; &quot;Betelgeuse&quot; &quot;Deneb&quot;
                                     &quot;Regulus&quot; &quot;*SiriusB&quot; &quot;Alnitak&quot; &quot;*ProximaCentauri&quot;)
                         as.character(star) NA)) %&gt;%
    ggplot(aes(log10(temp) magnitude col = type)) +
    geom_point() +
    geom_label_repel(aes(label = star)) +
    scale_x_reverse() +
    scale_y_reverse() +
    xlab(&quot;Temperature (log10 degrees K)&quot;) +
    ylab(&quot;Magnitude&quot;) +
    labs(color = &quot;Spectral class&quot;) +
    ggtitle(&quot;H-R diagram of selected stars&quot;)
## Warning: Removed 88 rows containing missing values (geom_label_repel).



United States period life tables: death_prob
Obtained from the US Social Security Administration the 2015 period life table lists the probability of death within one year at every age and for both sexes. These values are commonly used to calculate life insurance premiums. They can be used for exercises on probability and random variables. For example the premiums can be calculated with a similar approach to that used for interest rates in this case study on The Big Short in Rafael Irizarry’s Introduction to Data Science textbook.


Brexit polling data: brexit_polls
brexit_polls contains vote percentages and spreads from the six months prior to the Brexit EU membership referendum in 2016 compiled from Wikipedia. These can be used to practice a variety of inference and modeling concepts including confidence intervals p-values hierarchical models and forecasting.
data(brexit_polls)

# plot of Brexit referendum polling spread between &quot;Remain&quot; and &quot;Leave&quot; over time
brexit_polls %&gt;%
    ggplot(aes(enddate spread color = poll_type)) +
    geom_hline(aes(yintercept = -.038 color = &quot;Actual spread&quot;)) +
    geom_smooth(method = &quot;loess&quot; span = 0.4) +
    geom_point() +
    scale_color_manual(values = colorblind_palette[1:3]) +
    xlab(&quot;Poll end date (2016)&quot;) +
    ylab(&quot;Spread (Proportion Remain - Proportion Leave)&quot;) +
    labs(color = &quot;Poll type&quot;) +
    ggtitle(&quot;Spread of Brexit referendum online and telephone polls&quot;)



Breast cancer diagnosis prediction: brca
This is the Breast Cancer Wisconsin (Diagnostic) Dataset a classic machine learning dataset that allows classification of breast lesion biopsies as malignant or benign based on cell nucleus characteristics extracted from digitized images of fine needle aspirate cytology slides. The data are appropriate for principal component analysis and a variety of machine learning algorithms. Models can be trained to a predictive accuracy of over 95%.
# scale x values
x_centered &lt;- sweep(brca$x 2 colMeans(brca$x))
x_scaled &lt;- sweep(x_centered 2 colSds(brca$x) FUN = &quot;/&quot;)

# principal component analysis
pca &lt;- prcomp(x_scaled) 

# scatterplot of PC2 versus PC1 with an ellipse to show the cluster regions
data.frame(pca$x[1:2] type = ifelse(brca$y == &quot;B&quot; &quot;Benign&quot; &quot;Malignant&quot;)) %&gt;%
    ggplot(aes(PC1 PC2 color = type)) +
    geom_point() +
    stat_ellipse() +
    ggtitle(&quot;PCA separates breast biospies into benign and malignant clusters&quot;)



Conclusion
We hope that these additional datasets make the dslabs package even more useful for teaching data science through real-world case studies and motivating examples.
Are you an R programming novice but want to learn how to do all of this and more? Check out the Data Science Professional Certificate Program from HarvardX on edX taught by Rafael Irizarry!
"
2019,7,16,"AI, ML, NN and DL: a visual explanation",http://themainstreamseer.blogspot.com/2019/07/ai-ml-nn-and-dl-visual-explanation.html,There appears to be a lot of confusion between the terms Artificial Intelligence (AI) Machine Learning (ML) Neural Networks (NN) and Deep Learning (DL).&nbsp; Based on research from various popular blogs and articles here is my attempt at a simple visual explanation:
2019,7,14,Israeli AI Week - call for talks ending Aug 1st,https://bickson.blogspot.com/2019/07/israeli-ai-week-call-for-talks-ending.html,My friend Assaf Araki from Intel invited me to give a hand organizing the Israeli AI Week. We are looking for talks from innovative companies and machine learning researchers.The&nbsp;AI-Week&nbsp;in Tel Aviv is a nationwide community event organized by representatives from the Academia Government Industry and NGOs.&nbsp;&nbsp;AI&nbsp;week&nbsp;will bring together ~2000&nbsp;AI&nbsp;researchers and practitioners from Israel and around the globe. The event will take place in Tel Aviv university campus during November 17th&nbsp;to November 20th&nbsp;and will include one day of hands on workshops two conference days with +100 speakers a two days of data hackathon and multiple additional events.&nbsp;The goals of&nbsp;AI&nbsp;Week&nbsp;are to increase the synergy among the local&nbsp;AI&nbsp;eco system and with leading global innovators while exposing Israel&nbsp;AI&nbsp;innovation to the world.&nbsp; This event will include cutting edge sessions delivered by leading researchers and industry innovators from Israel and around the world.&nbsp;The Israel&nbsp;AI&nbsp;week&nbsp;founding members include Tel Aviv University Startup Nation Central Israel Innovation Authority &amp; Intel. We are adding more and more partners into this community effort.Among our steering committee members are &nbsp;Prof. Amnon Shashua&nbsp;(SVP Intel President and CEO of Mobileye &amp; Professor at the Hebrew University)&nbsp;Major Gen. (Res.) Professor Isaac Ben-Israel(Chairman of Israel Space Agency Chairman of Israel National Council for R&amp;D &amp; Professor at Tel-Aviv University)&nbsp;Prof. Eugene Kandel&nbsp;(CEO of Start-Up Nation Central &amp; Professor at the Hebrew University) and&nbsp;Aharon Aharon&nbsp;(CEO of Israel Innovation Authority).
2019,6,23,The Research Process,http://themainstreamseer.blogspot.com/2019/06/the-research-process.html,To answer interesting questions you need data.You begin with an observation that you want to understand including anecdotal observations.&nbsp; For example a certain website layout attracts more visitors to our web page than a different website layout.&nbsp; From your observations you generate explanations or theories of those observations from which you can make predictions or hypothesis.&nbsp; To test your hypothesis or predictions you need data.So you collect relevant data (and to do that you need to identify things that can be measured) and then you analyze those data.&nbsp; The analysis of your data may support your theory or give you cause to modify the theory.As such the processes of data collection and analysis and generating theories are intrinsically linked: theories lead to data collection / analysis and data collection / analysis informs theories.&nbsp; The research process is summarized below:(adapted from Discovering Statistics using R by Andy Field et al)
2019,6,19,A feast of time series tools,https://robjhyndman.com/seminars/isf-feasts/,Presentation at the International Symposium on Forecasting Thessaloniki Greece and at useR!2019 Toulouse France. Modern time series are often high-dimensional and observed at high frequency but most existing R packages for time series are designed to handle low-dimensional and low frequency data such as annual monthly and quarterly data. The feasts package is part of new collection of tidyverts packages designed for modern time series analysis using the tidyverse framework and structures.
2019,6,19,Advancing forecasting research and practice,https://robjhyndman.com/seminars/isf-panel-2019/,Panel discussion at the International Symposium on Forecasting Thessaloniki Greece Topic: Advancing forecasting research and practice: the contribution of the Institute and its journals Speakers:  Robert Fildes. Director Centre for Marketing Analytics and Forecasting Lancaster University Spyros Makridakis. Professor University of Nicosia Gael Martin. Professor of Econometrics Monash University Esther Ruiz. Catedrático de Universidad Universidad Carlos III de Madrid Rob Hyndman. Professor of Statistics Monash University  Brief: The Institute and the two journals dedicated to forecasting (International Journal of Forecasting Journal of Forecasting) were set up starting in 1981 to act as a central focus for research into all the wide variety of methods and forecasting practice.
2019,6,14,Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization,https://robjhyndman.com/publications/mint/,Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series a constraint we refer to as &ldquo;coherence&rdquo;. Forecast reconciliation is the process of adjusting forecasts to make them coherent. The reconciliation algorithm proposed by Hyndman et al. (2011) is based on a generalized least squares estimator that requires an estimate of the covariance matrix of the coherency errors (i.
2019,6,8,Quantification of energy savings from energy conservation measures in buildings using machine learning,https://robjhyndman.com/publications/energy-savings/,"This paper demonstrates how machine learning is used to measure energy savings from energy conservation measures (ECMs); in particular ECMs with a low expected saving.
We develop a model that predict energy consumption in buildings on an hourly level. The model is trained on energy data from the main meter before the ECMs took place. The model is then used to predict energy consumption after the ECMs.
The difference between the prediction (the estimated energy consumption in the building given no ECMs) and the actual usage is the estimated savings."
2019,5,29,Research quality data and research quality databases,https://simplystatistics.org/2019/05/29/research-quality-data-and-research-quality-databases/,"When you are doing data science you are doing research. You want to use data to answer a question identify a new pattern improve a current product or come up with a new product. The common factor underlying each of these tasks is that you want to use the data to answer a question that you haven’t answered before. The most effective process we have come up for getting those answers is the scientific research process. That is why the key word in data science is not data it is science.

No matter where you are doing data science - in academia in a non-profit or in a company - you are doing research. The data is the substrate you use to get the answers you care about. The first step most people take when using data is to collect the data and store it. This is a data engineering problem and is a necessary first step before you can do data science. But the state and quality of the data you have can make a huge amount of difference in how fast and accurately you can get answers. If the data is structured for analysis - if it is research quality  - then it makes getting answers dramatically faster.

A common analogy says that data is the new oil. Using this analogy pulling the data from all of the different available sources is like mining and extracting the oil. Putting it in a data lake or warehouse is like storing the crude oil for use in different products. In this analogy research is like getting the cars to go using the oil. Crude oil extracted from the ground can be used for a lot of different products but to make it really useful for cars you need to refine the oil into gas. Creating research quality data is the way that you refine and structure data to make it conducive to doing science. It means that the data is no longer as general purpose but it means you can use it much much more efficiently for the purpose you care about - getting answers to your questions.

Research quality data is data that:


Is summarized the right amount
Is formatted to work with the tools you are going to use
Is easy to manipulate and use
Is valid and accurately reflects the underlying data collection
Has potential biases clearly documented.
Combines all the relevant data types you need to answer questions


Let’s use an example to make this concrete. Suppose that you want to analyze data from an electronic health record. You want to do this to identify new potential efficiencies find new therapies and understand variation in prescribing within your medical system. The data that you have collected is in the form of billing records. They might be stored in a large database for a health system where each record looks something like this:



An example electronic health record. Source: http://healthdesignchallenge.com/

These data are collected incidentally during the health process and are designed for billing not for research. Often they contain information about what treatments patients received and were billed for but they might not include information on the health of the patient and whether they had any health complications or relapses they weren’t billed for.

These data are great but they aren’t research grade. They aren’t summarized in any meaningful way can’t be manipulated with visualization or machine learning tools are unwieldy and contain a lot of information we don’t need are subject to all sorts of strange sampling biases and aren’t merged with any of the health outcome data you might care about.

So let’s talk about how we would turn this pile of crude data into research quality data.



Turning raw data into research quality data.

Summarizing the data the right amount

To know how to summarize the data we need to know what are the most common types of questions we want to answer and what resolution we need to answer them. A good idea is to summarize things at the finest unit of analysis you think you will need - it is always easier to aggregate than disaggregate at the analysis level. So we might summarize at the patient and visit level. This would give us a data set where everything is indexed by patient and visit. If we want to answer something at a clinic physician or hospital level we can always aggregate there.

We also need to choose what to quantify. We might record for each visit the date prescriptions with standardized codes tests and other metrics. Depending on the application we may store the free text of the physician notes as a text string - for potential later processing into specific tokens or words. Or if we already have a system for aggregating physicians notes we could apply it at this stage.

Is formatted to work with the tools you are going to use

Research quality data is organized so the most frequent tasks can be completed quickly and without large amounts of data processing and reformatting. Each data analytic tool has different requirements on the type of data you need to input. For example many statistical modeling tools use “tidy data” so you might store the summarized data in a single tidy data set or a set of tidy data tables linked by a common set of indicators. Some software (for example in the analysis of human genomic data) require inputs in different formats - say as a set of objects in the R programming language. Others like software to fit a convolutional neural network to a set of images might require a set of image files organized in a directory in a particular way along with a metadata file providing information about each set of images. Or we might need to one hot encode categories that need to be classified.

In the case of our EHR data we might store everything in a set of tidy tables that can be used to quickly correlate different measurements. If we are going to integrate imaging lab reports and other documents we might store those in different formats to make integration with downstream tools easier.

Is easy to manipulate and use

This seems like it is just a re-hash of formatting the data to work with the tools you care about but there are some subtle nuances. For example if you have a huge amount of data (petabyes of images for example) you might not want to do research on all of those data at once. It will be inefficient and expensive. So you might use sampling to get a smaller data set for your research quality data that is easier to use and manipulate. The data will also be easier to use if they are (a) stored in an easy to access database with security systems well documented (b) have a data dictionary that makes it clear what the data are and where they come from or &copy; have a clear set of tutorials on how to perform common tasks on the data.

In our EHR example you might include a data dictionary that describes the dates of the data pull the types of data pulled the type of processing performed and pointers to the scripts that pulled the data.

Is valid and accurately reflects the underlying data collection

Data can be invalid for a whole host of reasons. The data could be incorrectly formatted input with error could change over time could be mislabeled and more. All of these problems can occur on the original data pull or over time. Data can also be out of date as new data becomes available.

The research quality database should include only data that has been checked validated cleaned and QA’d so that it reflects the real state of the world. This process is not a one time effort but an ongoing set of code scripts and processes that ensure the data you use for research are as accurate as possible.

In the EHR example there would be a series of data pulls code to perform checks and comparisons to additional data sources to validate the values levels variables and other components of the research quality database.

Has potential biases clearly documented

A research quality data set is by definition a derived data set. So there is a danger that problems with the data will be glossed over since it has been processed and easy to use. To avoid this problem there has to be documentation on where the data came from what happened to them during processing and any potential problems with the data.

With our EHR example this could include issues about how patients come into the system what procedures can be billed (or not) what data was ignored in the research quality database what are the time periods the data were collected and more.

Combines all the relevant data types you need to answer questions

One big difference between a research quality data set/database and a raw database or even a general purpose tidy data set is that it merges all of the relevant data you need to answer specific questions even if they come from distinct sources. Research quality data pulls together and makes easy to access all the information you need to answer your questions. This could still be in the form of a relational database - but the databases organization is driven by the research question rather than driven by other purposes.

For example EHR data may already be stored in a relational database. But it is stored in a way that makes it easy to understand billing and patient flow in a clinic. To answer a research question you might need to combine the billing data with patient outcome data and prescription fulfillment data all processed and indexed so they are either already merged or can be easily merged.

Why do this?

So why build a research quality data set? It sure seems like a lot of work (and it is!). The reason is that this work will always be done one way or the other. If you don&rsquo;t invest in making a research quality data set up front you will do it as a thousand papercuts over time. Each time you need to answer a new question or try a different model you&rsquo;ll be slowed down by the friction of identifying creating and checking a new cleaned up data set. On the one hand this amortizes the work over the course of many projects. But by doing it piecemeal you also dramatically increase the chance of an error in processing reduce answer time slow down the research process and make the investment for any individual project much higher.

Problem Forward Data Science

If you want help planning or building a research quality data set or database we can help at Problem Forward Data Science. Get in touch here: https://problemforward.typeform.com/to/L4h89P"
2019,5,26,Poll position: statistics and the Australian federal election,https://robjhyndman.com/hyndsight/poll-position/,"One of the few people in Australia who did not write off a possible Coalition win at the recent federal election was Peter Ellis. We’ve invited him to come and give a talk about making sense of opinion polls and the Australian federal election on Friday this week at Monash University. Visitors are welcome. Here are the details.
11am 31 May 2019. Room G03 Learning and Teaching Building 19 Ancora Imparo Way Clayton Campus Monash University"
2019,5,23,Allen Institue Opens an Israeli Branch,https://bickson.blogspot.com/2019/05/allen-institue-opens-israeli-branch.html,This Tuesday Prof. Oren Etsioni announced on his keynote talk at the Data Science Summit that the Seattle Allen Institute opens an Israeli Branch:Prof. Yoav Goldberg from Bar Ilan is heading the research effort with focus on NLP.
2019,5,20,I co-founded a company! Meet Problem Forward Data Science,https://simplystatistics.org/2019/05/20/i-co-founded-a-company-meet-problem-forward-data-science/,"I have some exciting news about something I&rsquo;ve been working on for the last year or so. I started a company! It&rsquo;s called Problem Forward data science.  I&rsquo;m pumped about this new startup for a lot of reasons.


My co-founder is one of my families closest friends Jamie McGovern who has more than 2 decades of experience in the consulting world and who I&rsquo;ve known for 15 years.
We are creating a cool new model of &ldquo;data scientist as a service&rdquo; (more on that below)
We have a problem forward not solution backward approach to data science that grew out of the Hopkins philosophy of data science.
We are headquartered in East Baltimore and are creating awesome new tech jobs in a place where they haven&rsquo;t been historically.


Problem Forward Not Solution Backward

We have always had a &ldquo;problem forward not solution backward&rdquo; approach to statistics machine learning and data here at Simply Stats. This has grown out of the Johns Hopkins Biostatistics philosophy of starting with the public health or medical problem you care about and working back to the statistical models software and tools you need to solve it.

This idea is so important to us it is in the name of the company. When we work with people our first goal is to find out the problems and questions that they genuinely care about then work backward to figure out how to solve them. We don&rsquo;t come in with a particular predetermined algorithm or strategy. One of the first questions we ask people isn&rsquo;t about data at all it is:


What question do you wish you could answer about your business (ignoring if you have the data or not to answer it yet)?


My favorite example of this is Moneyball. This is one of the classic stories about how the Oakland A&rsquo;s used data to gain a unique advantage. But one of the key messages about this story that often gets missed is that the data weren&rsquo;t unique to the A&rsquo;s! Everyone had the same data the A&rsquo;s just started with a problem that they needed to solve. They needed to find a unique way to win games that wasn&rsquo;t as expensive. Then they moved forward to looking at the data and realized that on base percentage was cheaper than home runs. So the A&rsquo;s used a &ldquo;problem forward not solution backward&rdquo; approach to data analysis.

Using this approach we have worked with companies with a wide variety of needs. Our main capabilities are in data strategy data cleaning and research quality database generation modeling and machine learning and data views through dashboards reports and presentations.



Data Scientist as a Service

There are a huge number of data science platform companies out there. Some of them are producing awesome tools but as any serious data analyst will tell you we are years from automating real data science. We are only very recently seeing formal definitions of what success of a data analysis even means! So it isn&rsquo;t surprising when general purpose platforms like IBM Watson struggle with specific problems - the problem isn&rsquo;t specified clearly enough for a platform to solve it yet..

The reason there are so many platforms is that its easy to sell the &ldquo;cool&rdquo; part of the problem - say building an AI to classify images or drive a car. But often the deeper problem is (a) figuring out what you even want to or can say with a set of data set (b) collecting a set of disorganized data &copy; getting buy in from groups with different motivations and data sets (d) organizing ugly data from different sources or finding new data you might need and (e) putting your answers in context. These problems are more like &ldquo;glue&rdquo; that comes between each of the platforms. We have a phrase we like to use:


To solve your data problem you need a person not a platform


So we have set up a &ldquo;platform&rdquo; that lets you scale up and down the number team members you have to solve data problems just like you would scale up and down the number of servers or tools that you use on AWS.



This means if you are an early stage startup we can help you scale data science before you can afford to hire a whole team. Even if you are a non-profit or a small academic group we can scale up or down to suit your needs. And if you are a big company we can provide utility data science for projects with tight deadlines.

Working with friends and building East Baltimore

The thing that gets me most excited about this new adventure is working with my really close friend Jamie. It&rsquo;s been huge for me to learn about the ins and outs of starting and running a business with someone who has decades of experience in the consulting industry.

It&rsquo;s also exciting to be able to headquarter the company right in East Baltimore and to work to upskill and develop talent here in a neighborhood I care about.

Like what you hear? Get in touch

If you are looking for data science work we&rsquo;d love to hear from you! Whether you are an academic a non-profit a small startup or a big business our utility model means we can work with you.

If you are interested in working with us contact us here:

https://problemforward.typeform.com/to/L4h89P"
2019,5,17,You are what you vote,https://robjhyndman.com/hyndsight/election-conversation/,I’ve tried my hand at writing for the wider public with an article for The Conversation based on my paper with Di Cook and Jeremy Forbes on “Spatial modelling of the two-party preferred vote in Australian federal elections: 2001-2016”. With the next Australian election taking place tomorrow we thought it was timely to put out a publicly accessible version of our analysis.
2019,5,10,ICLR 2019 Thoughts,http://www.machinedlearnings.com/2019/05/iclr-2019-thoughts.html,ICLR 2019 was reminiscent of the early NeurIPS days (sans skiing): a single track of talks vibrant poster sessions and a large mid-day break.  The Tuesday morning talks were on climate change modeling proteins generating music and modeling the visual cortex.  Except for climate change these were all hot topics at NeurIPS in the late 1990s.  History doesn't repeat but it does rhyme.My favorite talk was by Pierre-Yves Oudeyer whose research in curiosity based learning spans both human subjects and robotics.  Pierre's presentation was an entertaining tour de force of cognitive science and I highly recommend watching the video (starts about 9 minutes 30 seconds).  These ideas have extensively influenced the reinforcement learning community: the well-known Achilles' Heel of reinforcement learning is sample complexity and recently practitioners have attacked it inspired by ideas from curiosity based learning (e.g. the Burda et. al. poster at the conference).   Furthermore the view &ldquo;exploration is for building world models&rdquo; is reflected in recent theoretical results in contextual decision processes.The strangest moment for me at the conference was seeing the GLUE poster.  Apparently with the latency of conference review and publication GLUE is just now being presented.  Of course it is already obsolete so the presenters had another poster about their new dataset called SuperGLUE.  Things are moving so quickly that the former &ldquo;fast path&rdquo; of conference proceedings is now noticeably behind.Here's some stuff that caught my attention:Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach: A couple years ago Zhang et. al. stunned the community by demonstrating convnets can fit Imagenet labels to randomly generated images destroying the common belief that convnets generalized well due to capacity control.  Here Zhou et. al. show that an MDL-style generalization bound applies i.e. networks whose representation can be compressed after training have tighter deviation bounds.  This is a (training) data-dependent bound and they seal the argument by noting networks trained on randomized training data do not compress as well.Episodic Curiousity through Reachability: One of many curiosity-based exploration posters Savinov et. al. propose a combination of a memory and something akin to a policy cover with promising results.  Also cool: the poster includes QR codes which trigger videos of agents learning to move via different algorithms.Supervised Policy Update for Deep Reinforcement Learning: Vuong et. al. presented a plausible improvement over TRPO and PPO by convexifying the constrained policy optimization.
2019,4,29,Generative and Analytical Models for Data Analysis,https://simplystatistics.org/2019/04/29/generative-and-analytical-models-for-data-analysis/,"Describing how a data analysis is created is a topic of keen interest to me and there are a few different ways to think about it. Two different ways of thinking about data analysis are what I call the “generative” approach and the “analytical” approach. Another more informal way that I like to think about these approaches is as the “biological” model and the “physician” model. Reading through the literature on the process of data analysis I’ve noticed that many seem to focus on the former rather than the latter and I think that presents an opportunity for new and interesting work.

Generative Model

The generative approach to thinking about data analysis focuses on the process by which an analysis is created. Developing an understanding of the decisions that are made to move from step one to step two to step three etc. can help us recreate or reconstruct a data analysis. While reconstruction may not exactly be the goal of studying data analysis in this manner having a better understanding of the process can open doors with respect to improving the process.

A key feature of the data analytic process is that it typically takes place inside the data analyst’s head making it impossible to directly observe. Measurements can be taken by asking analysts what they were thinking at a given time but that can be subject to a variety of measurement errors as with any data that depend on a subject’s recall. In some situations partial information is available for example if the analyst writes down the thinking process through a series of reports or if a team is involved and there is a record of communication about the process. From this type of information it is possible to gather a reasonable picture of “how things happen” and to describe the process for generating a data analysis.

This model is useful for understanding the “biological process” i.e. the underlying mechanisms for how data analyses are created sometimes referred to as “statistical thinking”. There is no doubt that this process has inherent interest for both teaching purposes and for understanding applied work. But there is a key ingredient that is lacking and I will talk about that more below.

Analytical Model

A second approach to thinking about data analysis ignores the underlying processes that serve to generate the data analysis and instead looks at the observable outputs of the analysis. Such outputs might be an R markdown document a PDF report or even a slide deck (Stephanie Hicks and I refer to this as the analytic container). The advantage of this approach is that the analytic outputs are real and can be directly observed. Of course what an analyst puts into a report or a slide deck typically only represents a fraction of what might have been produced in the course of a full data analysis. However it’s worth noting that the elements placed in the report are the cumulative result of all the decisions made through the course of a data analysis.

I’ve used music theory as an analogy for data analysis many times before mostly because&hellip;it’s all I know but also because it really works! When we listen to or examine a piece of music we have essentially no knowledge of how that music came to be. We can no longer interview Mozart or Beethoven about how they wrote their music. And yet we are still able to do a few important things:


Analyze and Theorize. We can analyze the music that we hear (and their written representation if available) and talk about how different pieces of music differ from each other or share similarities. We might develop a sense of what is commonly done by a given composer or across many composers and evaluate what outputs are more successful or less successful. It’s even possible to draw connections between different kinds of music separated by centuries. None of this requires knowledge of the underlying processes.
Give Feedback. When students are learning to compose music an essential part of that training is the play the music in front of others. The audience can then give feedback about what worked and what didn’t. Occasionally someone might ask “What were you thinking?” but for the most part that isn’t necessary. If something is truly broken it’s sometimes possible to prescribe some corrective action (e.g. “make this a C chord instead of a D chord”).


There are even two whole podcasts dedicated to analyzing music&mdash;Sticky Notes and Switched on Pop&mdash;and they generally do not interview the artists involved (this would be particularly hard for Sticky Notes). By contrast the Song Exploder podcast takes a more “generative approach” by having the artist talk about the creative process.

I referred to this analytical model for data analysis as the “physician” approach because it mirrors in a basic sense the problem that a physician confronts. When a patient arrives there is a set of symptoms and the patient’s own report/history. Based on that information the physician has to prescribe a course of action (usually to collect more data). There is often little detailed understanding of the biological processes underlying a disease but they physician may have a wealth of personal experience as well as a literature of clinical trials comparing various treatments from which to draw. In human medicine knowledge of biological processes is critical for designing new interventions but may not play as large a role in prescribing specific treatments.

When I see a data analysis as a teacher a peer reviewer or just a colleague down the hall it is usually my job to give feedback in a timely manner. In such situations there usually isn’t time for extensive interviews about the development process of the analysis even though that might in fact be useful. Rather I need to make a judgment based on the observed outputs and perhaps some brief follow-up questions. To the extent that I can provide feedback that I think will improve the quality of the analysis it is because I have a sense of what makes for a successful analysis.

The Missing Ingredient

Stephanie Hicks and I have discussed what are the elements of a data analysis as well as what might be the principles that guide the development of an analysis. In a new paper we describe and characterize the success of a data analysis based on a matching of principles between the analyst and the audience. This is something I have touched on previously both in this blog and on my podcast with Hilary Parker but in a generally more hand-wavey fashion. Developing a more formal model as Stephanie and I have done here has been useful and has provided some additional insights.

For both the generative model and the analytical model of data analysis the missing ingredient was a clear definition of what made a data analysis successful. The other side of that coin of course is knowing when a data analysis has failed. The analytical approach is useful because it allows us to separate the analysis from the analyst and to categorize analyses according to their observed features. But the categorization is “unordered” unless we have some notion of success. Without a definition of success we are unable to formally criticize analyses and explain our reasoning in a logical manner.

The generative approach is useful because it reveals potential targets of intervention especially from a teaching perspective in order to improve data analysis (just like understanding a biological process). However without a concrete definition of success we don’t have a target to strive for and we do not know how to intervene in order to make genuine improvement. In other words there is no outcome on which we can “train our model” for data analysis.

I mentioned above that there is a lot of focus on developing the generative model for data analysis but comparatively little work developing the analytical model. Yet both models are fundamental to improving the quality of data analyses and learning from previous work. I think this presents an important opportunity for statisticians data scientists and others to study how we can characterize data analyses based on observed outputs and how we can draw connections between analyses."
2019,4,25,Onsite Interactive Training Session for Data Analysts/Data Scientists,https://www.deep-data-mining.com/2019/04/onsite-interactive-training-session-for.html,"Dr. Zhou is offering a brand new service: a half-day (3 hours) interactive training session to help a company's data analysts/data scientists improve their skills and productivity. The format of the&nbsp;interactive training session&nbsp;is as follows:1.&nbsp;Dr. Zhou&nbsp;first gives a&nbsp;1.5 hours&nbsp;presentation&nbsp;to describe successful cases of using data analytics to solve business problems share best practices and talk about challenges.2. For the remaining 1.5 hours the audience asks questions and is actively engaged in discussion&nbsp;with Dr. Zhou.&nbsp; This the best part!&nbsp; As one of the scientists said ""Dr. Zhou has helped me solve an issue that I had struggled with&nbsp;for years"".The training session is designed for all data analysts/data scientists. Dr. Zhou shares his battle-tested strategies and best practices that are useful for them regardless what specific analytics tools or programming languages they use.On April 23 2019 Dr. Zhou delivered the interactive training session to one of the top three property insurance company located in Boston. It was extremely well received. The picture attached shows that I am making the presentation (in addition to people in the room there are more people joining the meeting through the phone.)&nbsp;The following are the testimonials&nbsp;from two persons who took the course.""We have learnt a lot from Dr.Zhou'"" Gang Xu Director Data Science at Lincoln Financial Group""Dr. Zhou gave us a great overview of procedures of doing a solid predictive analysis and illustrated real life AI consulting business cases. Dr. Zhou is really experienced in the AI space and his presentation was very well received by data scientists from Lincoln Financial Group. I would highly recommend any data science group to have Dr. Zhou sharing his experiences.&nbsp;""-&nbsp;&nbsp;Dr. Hao Zhou&nbsp;Principal Analyst&nbsp;&nbsp;Data Science at Lincoln Financial GroupThe following are testimonials about my previous talks and training activities.""It was a fortune to have Jay come to our computer science department to share his experience in solving business problems with predictive analytics on February 28 2017. What Jay had presented in his 3 talks each lasting for 1 hour in different topics of data mining was totally impressive and beyond our wildest expectation. Having built competition-winning predictive models for some of the biggest companies and produced hundreds of millions of dollars’ savings Jay shared the secret of his success with students and faculty without reservation. His strong presentations were such an inspiration for our computer science students and faculty and his methodology was innovative and powerful  even for very seasoned data scientists among the audience. Jay thank you so much for your hard work preparing and delivering these presentations! "" - Dr. Wei Ding Professor at University of Massachusetts Boston""Jay is more than just a coder he is a great trainer and a good presenter of theoretical data mining concepts so that they can be understood by most. ""-James Lukenbill Director of IT Project Management OptumBio of Dr.Jiang ZhouDr. Jiang Zhou has two decades of experience building predictive models across industries including telecommunication banking insurance  and smart city. These solutions have resulted in over $200 million savings for clients. Dr. Zhou has been involved in three real world competitions to build best predictive models i.e. a customer credit risk model for a top three cell phone company a bank card fraud detection model for a top 15 bank and a direct sales model for a marketing company. Dr. Zhou's models have won all these competitions. He has founded/co-founded data analytics companies including Business Data Miners Smart Credit and AI Strike. Previously he was a chief statistician at Lightbridge a vice president at Citizens Bank and a consulting member of technical staff at Oracle. Dr. Zhou is the author of an award-wining blog on data analytics https://www.deep-data-mining.com/The normal price for the training service is $6500.  If your company is interested in the service please contact Dr. Zhou at&nbsp;jay.zhou@deep-data-mining.com&nbsp;"
2019,4,23,"Translations of ""Forecasting: principles and practice""",https://robjhyndman.com/hyndsight/fpptranslations/,"There are now translations of my forecasting textbook (coauthored with George Athanasopoulos) into Chinese and Korean.
The Chinese translation was produced by a team led by Professor Yanfei Kang (Beihang University) and Professor Feng Li (Central University of Finance and Economics). The following students were also involved: Cheng Fan Liu Yu Long Xiaoyu Wang Xiaoqian Zeng Jiayue Zhang Bohan and Zhu Shuaidong.
The Korean translation was produced by Dr Daniel Young Ho Kim."
2019,4,22,Revealing high-frequency trading provision of liquidity with visualization,https://robjhyndman.com/publications/hft-liquidity/,Liquidity is crucial for successful financial markets. It ensures that all investors are able to buy and sell assets quickly at a fair price. High Frequency Traders (HFTs) utilize sophisticated algorithms operating with extreme speed and are frequently cited as liquidity providers. The objective of this paper is to investigate the liquidity provision of a number of HFTs to determine their effects on aggregate marketplace liquidity. We consider a large data set collected from the Australian Securities Exchange throughout 2013 providing a near complete picture of all trading activity.
2019,4,17,"Tukey, Design Thinking, and Better Questions",https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/,"Roughly once a year I read John Tukey’s paper “The Future of Data Analysis” originally published in 1962 in the Annals of Mathematical Statistics. I’ve been doing this for the past 17 years each time hoping to really understand what it was he was talking about. Thankfully each time I read it I seem to get something new out of it. For example in 2017 I wrote a whole talk around some of the basic ideas.
Well it’s that time of year again and I’ve been doing some reading.
Probably the most famous line from this paper is

Far better an approximate answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise.

The underlying idea in this sentence arises in at least two ways in Tukey’s paper. First is his warning that statisticians should not be called upon to produce the “right” answers. He argues that the idea that statistics is a “monolithic authoritarian structure designed to produce the ‘official’ results” presents a “real danger to data analysis”. Second Tukey criticizes the idea that much of statistical practice centers around optimizing statistical methods around precise (and inadequate) criteria. One can feel free to identify a method that minimizes mean squared error but that should not be viewed as the goal of data analysis.
But that got me thinking—what is the ultimate goal of data analysis? In 64 pages of writing I’ve found it difficult to identify a sentence or two where Tukey describes the ultimate goal why it is we’re bothering to analyze all this data. It occurred to me in this year’s reading of the paper that maybe the reason Tukey’s writing about data analysis is often so confusing to me is because his goal is actually quite different from that of the rest of us.

More Questions Better Questions
Most of the time in data analysis we are trying to answer a question with data. I don’t think it’s controversial to say that but maybe that’s the wrong approach? Or maybe that’s what we’re not trying to do at first. Maybe what we spend most of our time doing is figuring out a better question.
Hilary Parker and I have discussed at length the idea of design thinking on our podcast. One of the fundamental ideas from design thinking involves identifying the problem. It’s the first “diamond” in the “double diamond” approach to design.
Tukey describes the first three steps in a data analysis as:

Recognition of problem
One technique used
Competing techniques used

In other words try one approach then try a bunch of other approaches! You might be thinking why not just try the best approach (or perhaps the right approach) and save yourself all that work? Well that’s the kind of path you go down when you’re trying to answer the question. Stop doing that! There are two reasons why you should stop thinking about answering the question:

You’re probably asking the wrong question anyway so don’t take yourself too seriously;
The “best” approach is only defined as “best” according to some arbitrary criterion that probably isn’t suitable for your problem/question.

After thinking about all this I was inspired to draw the following diagram.


Strength of Evidence vs. Quality of Question

The goal in this picture is to get to the upper right corner where you have a high quality question and very strong evidence. In my experience most people assume that they are starting in the bottom right corner where the quality of the question is at its highest. In that case the only thing left to do is to choose the optimal procedure so that you can squeeze as much information out of your data. The reality is that we almost always start in the bottom left corner with a vague and poorly defined question and a similarly vague sense of what procedure to use. In that case what’s a data scientist to do?
In my view the most useful thing a data scientist can do is to devote serious effort towards improving the quality and sharpness of the question being asked. On the diagram the goal is to move us as much as possible to the right hand side. Along the way we will look at data we will consider things outside the data like context resources and subject matter expertise and we will try a bunch of different procedures (some optimal some less so).
Ultimately we will develop some of idea of what the data tell us but more importantly we will have a better sense of what kinds of questions we can ask of the data and what kinds of questions we actually want to have answered. In other words we can learn more about ourselves by looking at the data.


Exploring the Data
It would seem that the message here is that the goal of data analysis is to explore the data. In other words data analysis is exploratory data analysis. Maybe this shouldn’t be so surprising given that Tukey wrote the book on exploratory data analysis. In this paper at least he essentially dismisses other goals as overly optimistic or not really meaningful.
For the most part I agree with that sentiment in the sense that looking for “the answer” in a single set of data is going to result in disappointment. At best you will accumulate evidence that will point you in a new and promising direction. Then you can iterate perhaps by collecting new data or by asking different questions. At worst you will conclude that you’ve “figured it out” and then be shocked when someone else looking at another dataset concludes something completely different. In light of this discussions about p-values and statistical significance are very much beside the point.
The following is from the very opening of Tukey’s book *Exploratory Data Analysis:

It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it

(Note that the all caps are originally his!) Given this it’s not too surprising that Tukey seems to equate exploratory data analysis with essentially all of data analysis.


Better Questions
There’s one story that for me totally captures the spirit of exploratory data analysis. Legend has it that Tukey once asked a student what were the benefits of the median polish technique a technique he invented to analyze two-way tabular data. The student dutifully answered that the benefit of the technique is that it provided summaries of the rows and columns via the row- and column-medians. In other words like any good statistical technique it summarized the data by reducing it in some way. Tukey fired back saying that this was incorrect—the benefit was that the technique created more data. That “more data” was the residuals that are leftover in the table itself after running the median polish. It is the residuals that really let you learn about the data discover whether there is anything unusual whether your question is well-formulated and how you might move on to the next step. So in the end you got row medians column medians and residuals i.e. more data.
If a good exploratory technique gives you more data then maybe good exploratory data analysis gives you more questions or better questions. More refined more focused and with a sharper point. The benefit of developing a sharper question is that it has a greater potential to provide discriminating information. With a vague question the best you can hope for is a vague answer that may not lead to any useful decisions. Exploratory data analysis (or maybe just data analysis) gives you the tools that let the data guide you towards a better question.
"
2019,4,1,Interview with Abhi Datta,https://simplystatistics.org/2019/04/01/interview-with-abhi-datta/,"Editor’s note: This is the next in our series of interviews with early career statisticians and data scientists. Today we are talking to Abhi Datta about his work in large scale spatial analysis and his interest in soccer! Follow him on Twitter at @datta_science. If you have recommendations of an (early career) person in academics or industry you would like to see promoted reach out to Jeff (@jtleek) on Twitter!

SS: Do you consider yourself a statistician biostatistician data scientist or something else?

AD: That is a difficult question for me as I enjoy working on theory methods and data analysis and have co-authored diverse papers ranging from theoretical expositions to being primarily centered around a complex data analysis. My research interests also span a wide range of areas. A lot of my work on spatial statistics is driven by applications in environmental health and air pollution. Another significant area of my research is developing Bayesian models for epidemiological applications using survey data.

I would say what I enjoy most is developing statistical methodology motivated by a complex application where current methods fall short applying the method for analysis of the motivating data and trying to see if it is possible to establish some guarantees about the method through a combination of theoretical studies and empirical experiments that will help to generalize applicability of the method for other datasets. Of course not all projects involve all the steps but that is my ideal workflow. Not sure what that classifies me as.

SS: How did you get into statistics? What was your path to ending up at Hopkins?

AD: I was born and grew up in Kolkata India. I had the option of going for engineering medical or statistics undergrad. I chose statistics persuaded by my appreciation for mathematics and the reputation of the statistics program at Indian Statistical Institute (ISI) Kolkata. I completed my undergrad (BStat) and Masters (MStat) in Statistics from ISI and I’m thankful I made that choice as those 5 years at ISI played a pivotal role in my life. Besides getting rigorous training in the foundations of statistics most importantly I met my wife Dr. Debashree Ray at ISI.

After my Masters I had a brief stint in the finance industry working for 2 years at Morgan Stanley (in Mumbai and then in New York City) before I joined the PhD program at the Division of Biostatistics at University of Minnesota (UMN) in 2012 where Debashree was pursuing her PhD in Biostatistics. I had initially planned to work in Statistical Genetics as I had done a research project in that area in my Master’s. However I explored other research areas in my first year and ended up working on spatial statistics under the supervision of my advisor Dr. Sudipto Banerjee and on high-dimensional data with my co-advisorDr. Hui Zou from the Department of Statistics in Minnesota. I graduated from Minnesota in 2016 and joined Hopkins Biostat as an Assistant Professor in the Fall of 2016.

SS: You work on large scale spatio-temporal modeling - how do you speed up computations for the bootstrap when the data are very large?

AD: A main computational roadblock in spatio-temporal statistics is working with very big covariance matrices that strain memory and computing resources typically available in personal computers. Previously I have developed nearest neighbor Gaussian Processes (NNGP) &ndash; a Bayesian hierarchical model for inference in massive geospatial datasets. One issue with hierarchical Bayesian models is their reliance on long sequential MCMC runs. Bootstrap unlike MCMC can be implemented in an embarrassingly parallel fashion. However for geospatial data all observations are correlated across space prohibiting direct resampling for bootstrap.

In a recent work with my student Arkajyoti Saha we proposed a semi-parametric bootstrap for inference on large spatial covariance matrices. We use sparse Cholesky factors of spatial covariance matrices to approximately decorrelate the data before resampling for bootstrap. Arkajyoti has implemented this in an R-package BRISC: Bootstrap for rapid inference on spatial covariances. BRISC is extremely fast and at the time of publication to my knowledge it was the only R-package that offered inference on all the spatial covariance parameters without using MCMC. The package can also be used simply for super-fast estimation and prediction in geo-statistics.

SS: You have a cool paper on mapping local and global trait variation in plant distributions how did you get involved in that collaboration? Does your modeling have implications for people studying the impacts of climate change?

AD: In my final year of PhD at UMN I was awarded the Inter-Disciplinary Doctoral Fellowship &ndash;  a fantastic initiative by the graduate school at UMN providing research and travel funding and office space to work with an inter-disciplinary team of researchers on a collaborative project. In  my IDF mentored by Dr. Arindam Banerjee and Dr. Peter Reich I worked with a group of climate modelers ecologists and computer scientists from several institutions on a project whose eventual goal is to improve carbon projections from climate models.

The paper you mention was aimed at improving the global characterization of plant traits (measurements). This is important as plant trait values are critical inputs to climate model. Even the largest plant trait database TRY offers poor geographical coverage with little or no data across many large geographical regions. We used the fast NNGP approach I had been developing in my PhD to spatially gap-fill the plant trait data to create a global map of important plant traits with proper uncertainty quantification. The collaboration was a great learning experience for me on how to conduct a complex data analysis and how to communicate with scientists.

Currently we are looking at ways to incorporate the uncertainty quantified trait values as inputs to Earth System Models (ESMs) – the land component of climate models. We hope that replacing single trait values with entire trait distributions as inputs to these models will help to better propagate the uncertainty and improve the final model projections.

SS: What project has you most excited at the moment?

AD: There are two. I have been working with Dr. Scott Zeger on a project lead by Dr. Agbessi Amouzou in the Department of International Health at Hopkins aiming to estimate the cause-specific fractions (CSMF) of child mortality in Mozambique using family questionnaire data (verbal autopsy). Verbal autopsies are often used as a surrogate to full autopsy in many countries and there exists software that use these questionnaire data to predict a cause for every death. However these software are usually trained on some standard training data and yield inaccurate predictions in local context. This problem is a special case of transfer learning where a model trained using data representing a standard population offers poor predictive accuracy when specific populations are of interest. We have developed a general approach for transfer learning of classifiers that uses the predictions from these verbal autopsy software and limited full autopsy data from the local population to provide improved estimates of cause-specific mortality fractions. The approach is very general and offers a parsimonious model-based solution to transfer learning and can be used in any other classification-based application.

The second project involves creating high-resolution space-time maps of particulate matter (PM2.5) in Baltimore. Currently a network of low-cost air pollution monitors is being deployed in Baltimore that promises to offer air pollution measurements at a much higher geospatial resolution than what is provided by EPA’s sparse regulatory monitoring network. I was awarded a Bloomberg American Health Initiative Spark award for working with Dr. Kirsten Koehler in the Department of Environmental Health and Engineering to combine the low-cost network data the sparse EPA data and other land-use covariates to create uncertainty quantified maps of PM2.5 at an unprecedented spatial resolution. We have just started analyzing the first two months of data and I’m really looking forward to help create the end-product and understand how PM2.5 levels vary across the different neighborhoods in Baltimore.

SS: You have an interest in soccer and spatio temporal models have played an increasing role in soccer analytics. Have you thought about using your statistics skills to study soccer or do you try to avoid mixing professional work and being a fan?

AD: Yes I’m an avid soccer fan. I have travelled to Brazil in 2014 and Russia in 2018 to watch live games in the world cups. It also unfortunately means that I set my alarm to earlier times on weekends than on weekdays as the European league games start pretty early in US time.

However until recent times I’ve been largely ignorant of applications of spatio-temporal statistics in soccer analytics. I just finished teaching a Spatial Statistics course and one of the students presented a fascinating work he has done on predicting player’s scoring abilities using spatial statistics. I certainly plan to read more literature on this and maybe one day can contribute. Till then I remain a fan."
2019,3,28,Operationalize Trusted AI with IBM Watson OpenScale,http://themainstreamseer.blogspot.com/2019/03/operationalize-trusted-ai-with-ibm.html,
2019,3,20,Developing good research habits,https://robjhyndman.com/seminars/research_habits2019/,Presentation for the 2019 honours and masters students Magic button for library access to papers  Drag this Monash proxy link to your bookmarks.  Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template
2019,3,9,Detection of cybersecurity attacks through analysis of web browsing activities using principal component analysis,https://robjhyndman.com/publications/nids-anomalies/,Organizations such as government departments and financial institutions provide online service facilities accessible via an increasing number of internet connected devices which make their operational environment vulnerable to cyber attacks. Consequently there is a need to have mechanisms in place to detect cyber security attacks in a timely manner. A variety of Network Intrusion Detection Systems (NIDS) have been proposed and can be categorized into signature-based NIDS and anomaly-based NIDS. The signature-based NIDS which identify the misuse through scanning the activity signature against the list of known attack activities are criticized for their inability to identify new attacks (never-before-seen attacks).
2019,3,8,RL will disrupt OR,http://www.machinedlearnings.com/2019/03/rl-will-disrupt-or.html,"Operations research (OR) is in the initial stages of a revolution driven by reinforcement learning (RL).When I was at eHarmony years ago we used classical OR techniques to drive the matchmaking process.  Machine learning played a critical role but was limited to specfiying parameters to the OR solver.  In essence machine learning was to used to estimate the value function and the OR solver then produced a policy.  OR has historically focused on highly tractable specializations of convex optimization.  In an age of scarce compute this made perfect sense: indeed at that time eHarmony was pushing the limits of what was possible using high-end commercial OR solvers.  However compute is less scarce now: in predictive modeling convex optimization has been spectacularly superseded by non-convex techniques (aka “deep learning”).  A similar revolution is unfolding in OR.  The recipe is: develop a generative model of the problem (aka a “simulator”) and then directly optimize a policy on simulated data using RL techniques.All models are wrong but some models are useful.  At first blush it seems implausible that using advanced RL techniques on a crude approximation of the real world would yield substantial benefits.  However traditional OR techniques also preform extremely aggressive optimization (to machine precision (!)) on an approximation of the real world.  OR models are useful despite typically making tremendous simplifications such as replacing all random variables with their expected values (or in more sophisticated setups high probability bounds).  The simplifications for the RL techniques involve the assumptions in the generative model such as a particular parametric model for probability of an airplane service event.  Early research results suggest that for some economically important problems relatively crude simulators coupled with RL techniques can induce superior policies to those developed using traditional OR techniques.  Furthermore simulators admit expressing increasingly refined approximations of reality without the constraints imposed by classical OR formulations. Reaction time is a factor in this so please pay attention.Reaction time.&nbsp;You should almost never take seriously anyone's explanation for why something works.  Nonetheless I'll give you my intution as to why RL will eventually dominate OR.&ldquo;Classical OR techniques are optimized to try to avoid bad events ballistically whereas RL trained policies are optimized to react to events as they occur.&rdquo;  If this is true then it doesn't matter if the simulation exactly gets the probability of tail events right as long as they are all present in the simulation and somewhat rare because the ""use of remediation actions"" portion of the learned policy will be conditioned on events as they actually occur in practice.  (If the events are not rare then getting the coocurrence statistics right could matter.)If this explanation has merit then the upside to RL will be large for scenarios where classic OR optimization is frequently re-run in order to react to new events because RL will have the “reaction time advantage”."
2019,3,7,From Hype to Reality – Powering the AI-Driven Future of Insurance at Insurance AI and Analytics USA - by Ira Sopic,https://www.deep-data-mining.com/2019/03/from-hype-to-reality-powering-ai-driven.html,With 2018 witnessing unprecedented advances in the investment and deployment of artificial intelligence within the insurance industry Insurance Nexus is delighted to announce that the Insurance AI and Analytics USA Summit will return to Chicago for a sixth time in 2019 welcoming more than 450 senior attendees to the Renaissance Chicago Downtown Hotel May 2-3.Featuring an agenda designed to tackle the biggest challenges and opportunities in AI and advanced analytics Insurance AI and Analytics USA is a must-attend for any analytics underwriting claims or marketing innovators seeking to both achieve efficient and seamless operations and deliver valuable and relevant products and experiences.&nbsp; “It’s impossible to open a magazine without seeing hype about analytics changing every aspect of your life” says Will Dubyak VP Analytics for Product Development &amp; Innovation USAA. “The Insurance AI &amp; Analytics USA Summit is the optimal place to cut through the noise hear the latest thinking from industry leaders in analytics and compare best practices with your colleagues”Across three in-depth tracks more than 40 expert speakers from leading North American carriers will explore and discuss the latest strategies and approaches being deployed to maximize the impact of AI machine learning and advanced analytics across the insurance value chain. Featuring a whole session dedicated to case studies the practical retelling of success stories will ensure attendees discover how and where technological innovations are having the biggest impacts on insurance and walk away with a holistic roadmap for success. Confirmed speakers so far include Tilia Tanner Global Head of Analytics AIG Eugene Wen VP of Group Advanced Analytics Manulife and Jerry Gupta SVP Digital Analyst Catalyst SwissRe as well as:&nbsp; &nbsp; &nbsp; &nbsp;Thomas Sheffield SVP and Head of Specialty Claims QBE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Glenn Fung Chief Research Scientist AI and Machine Learning Research Director American Family Insurance &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Laurie Pierman Vice President Claim Operations Amerisure Insurance&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Michiko Kurahashi Chief Marketing Officer AXIS Capital        Attendees to Insurance AI and Analytics USA will also become part of a truly international insurance community with over 25 hours of networking and interactive discussions aplenty. In addition our ‘Open Design Workshops’ will see attendees attempt to live-solve industry challenges giving insight into how peers and competitors alike approach a challenge and how their own methods might be improved.“At QBE we’ve spent a great deal of time figuring out how we can strategically deploy artificial intelligence in practical use cases to drive immediate value for business” states Ted Stuckey Managing Director QBE Ventures. “We’re excited to share some of our experience at the Insurance AI and Analytics Conference in Chicago on May 2-3!”In short however you are seeking to leverage AI Insurance AI and Analytics USA is the event for you. Don’t miss this unparalleled opportunity. Join us in making 2019 the year AI insurance changes forever. Ira Sopic
2019,3,7,Upcoming data science events in Israel,https://bickson.blogspot.com/2019/03/upcoming-data-science-events-in-israel.html,My friend Assaf Araki is organizing the AI Week&nbsp;Nov 17-21 at Tel Aviv University. It is going to be the first national AI event with around 2000 participants. Dr. Ben Lorica chief scientist of O'Reilly will be one of the speakers along with Prof. Amnon Shashua CEO of MobileEye.Another interesting event is the AI Data Science Summit - May 21 in Jerusalem.&nbsp; It is a follow up event organized by Avner Algom of our European Data Science Summit. Prof. Oren Etzioni from the Allen Institute for AI is one of the keynote speakers.
2019,3,3,Time Series Data Library,https://robjhyndman.com/tsdl/,The Time Series Data Library is no longer hosted on this website. You can get the data from the tsdl R package.
2019,2,21,Post-docs in wind and solar power forecasting,https://robjhyndman.com/hyndsight/wind-solar-jobs/,"We currently have two postdoc opportunities together with an industry partner in the field of wind and solar power forecasting (full time Level B). They are suitable for recently graduated PhD students that can start between now and June-July.
The opportunities are as follows:
Wind power forecasting:  1 year contract Good programming skills in R and/or Python Solid background in Machine Learning and/or Statistics Background in time series forecasting desirable   Solar power forecasting:  6 months contract Good programming skills in R and/or Python Solid background in Machine Learning and/or Statistics Data will be cloud coverage data from sky cams so some image processing background is necessary Background in time series forecasting desirable  Please contact Christoph Bergmeir if you are interested."
2019,2,19,Hacking Deep Learning (Bar Ilan) Workshop Videos,https://bickson.blogspot.com/2019/02/hacking-deep-learning-bar-ilan-workshop.html,Hacking Deep Learning (Bar Ilan) Workshop Videos now online. Thanks for my friend Prof. Yossi Keshet for organizing and inviting me!One notable talk which is unfortunately missing from the videos is of Prof. Adi Shamir described in this paper. The work analysis how many pixels one should change to confuse a deep learning based classifier. The result is surprising - only a few! A related describe work is this.
2019,2,17,UAI 2019 is coming to Tel Aviv,https://bickson.blogspot.com/2019/02/uai-2019-is-coming-to-tel-aviv.html,I got this from my friend Nick:This year Tel Aviv will host&nbsp;UAI&nbsp;http://auai.org/uai2019/&nbsp;(July 22-25). Several students have been able to attend the conference and present their work thanks to the generosity&nbsp;of your institutions. We hope that you will continue to support us this year as well. You can find more information about sponsorship packages here&nbsp;http://auai.org/uai2019/sponsorships.php&nbsp;.RegardsNikolaos Vasiloglou&nbsp;UAI&nbsp;2019 Sponsorship ChairFeel free to reach out to Nick in case you are interested in sponsoring the event.
2019,2,8,How to make a self-service BI tools deployment less painful,https://athena-solutions.com/how-to-make-a-self-service-bi-tools-deployment-less-painful/,Why does self-service BI have to hurt so much? In theory it should be the answer to all your problems. But so many enterprises end up […]
2019,2,6,A framework for automated anomaly detection in high frequency water-quality data from in situ sensors,https://robjhyndman.com/publications/water-quality-2/,River water-quality monitoring is increasingly conducted using automated in situ sensors enabling timelier identification of unexpected values. However anomalies caused by technical issues confound these data while the volume and velocity of data prevent manual detection. We present a framework for automated anomaly detection in high-frequency water-quality data from in situ sensors using turbidity conductivity and river level data. After identifying end-user needs and defining anomalies we ranked their importance and selected suitable detection methods.
2019,2,4,Satellite imagery and remote sensing puzzles,http://themainstreamseer.blogspot.com/2019/02/satellite-imagery-and-remote-sensing.html,If you are looking for a fun way to experience satellite imagery and learn more about remote sensing check out Earth Image Puzzles here.Here is a solved jigsaw puzzle of SouthEastern PA:Enjoy!
2019,2,2,Chart of Accounts Domain in Master Data Management,https://www.mdmgeek.com/2019/02/02/chart-of-accounts-domain-in-master-data-management/,"A chart of accounts (COA) is a list of accounts used by an organization to define each class of items for which money is spent or received. It is used to organize the finances and segregate expenditures revenue assets and liabilities to give everyone a better understanding of the financial health of the organization. There [&#8230;]
The post Chart of Accounts Domain in Master Data Management appeared first on MDMgeek."
2019,1,25,Advice to PhD applicants,https://robjhyndman.com/hyndsight/phdapplicants/,"For students who are interested in doing a PhD at Monash under my supervision.
First check that you satisfy the following criteria:
 You must have completed a degree in statistics that involved some research component (e.g. an honours or masters thesis). A degree in computer science mathematics or econometrics might be acceptable if it contained a substantial amount of statistics. A degree in any other field is not sufficient background to work with me."
2019,1,25,Feature-based forecasting algorithms for large collections of time series,https://robjhyndman.com/seminars/acems-fforma/,"Talk given at ACEMS workshop on &ldquo;Statistical Methods for the Analysis of High-Dimensional and Massive Data Sets&rdquo; I will discuss two algorithms used in forecasting large collections of diverse time series. Each of these algorithms uses a meta-learning approach with vectors of features computed from time series to guide the way the forecasts are computed.
In FFORMS (Feature-based FORecast Model Selection) we use a random forest classifier to identify the best forecasting method using only time series features."
2019,1,18,forecast 8.5,https://robjhyndman.com/hyndsight/forecast85/,"The latest minor release of the forecast package has now been approved on CRAN and should be available in the next day or so.
Version 8.5 contains the following new features
 Updated tsCV() to handle exogenous regressors. Reimplemented naive() snaive() rwf() for substantial speed improvements. Added support for passing arguments to auto.arima() unit root tests. Improved auto.arima() stepwise search algorithm (some neighbouring models were missed previously).  We haven’t done a major release for two years and there is unlikely to be another one now."
2019,1,15,About Dr. Zhou's Oracle SQL for Data Science Course,https://www.deep-data-mining.com/2018/07/about-dr-zhous-oracle-sql-for-data.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} On January 31 2017 I was invited by Prof. Wei Ding at the Department of Computer Science University of Massachusetts Boston and gave 3 talks about my data science projects across different industries. These talks are extremely well received. The following is what Prof. Ding says about my talks. ""It was a fortune to have Jay come to our computer science department to share his experience in solving business problems with predictive analytics on February 28 2017. What Jay had presented in his 3 talks each lasting for 1 hour in different topics of data mining was totally impressive and beyond our wildest expectation. Having built competition-winning predictive models for some of the biggest companies and produced hundreds of millions of dollars’ savings Jay shared the secret of his success with students and faculty without reservation. His strong presentations were such an inspiration for our computer science students and faculty and his methodology was innovative and powerful even for very seasoned data scientists among the audience. Jay thank you so much for your hard work preparing and delivering these presentations!"" -Prof. Ding Wei Department of Computer Science University of Massachusetts Boston    The audience are particularly amazed by how I come up with solutions using Oracle SQL environment. To share my expertise I create the online course Oracle SQL for Data Science to show how to perform common data science tasks using Oracle SQL and the benefits for doing that. I let Charlie BergerSenior Director of Product Management Machine Learning AI and Cognitive Analytics at Oracle know about my course and he told me ""Your course is amazing.""document.getElementById(""thinkific-product-embed"") ||  document.write('');Enroll the Course!"
2019,1,15,Deep Learning World,https://www.deep-data-mining.com/2019/01/predictive-analytics-world-for-industry.html,The premier conference covering thecommercial deployment of deep learningDeepLearning World&nbsp;is the premier conference covering the commercial deployment of deep learning. The event’s mission is to foster breakthroughs in the value-driven operationalization of established deep learning methods. DLW runs parallel to the established&nbsp;PredictiveAnalytics World for Industry 4.0&nbsp;at the same venue. Combo passes are available.How to turn Deep Tech into Broad ApplicationThe hype is over: deep learning enters the “trough of disillusionment”. Companies are realizing that not every business problem requires the deep learning hammer. Of course there are use cases that are best solved with artificial neural networks: image speech and text recognition; anomaly detection and predictive maintenance on sensor data; complex data synthesis and sampling; reinforcement and sparse learning; and many more applications show the potential of artificial intelligence for real-world business scenarios. At the Deep Learning World conference data science experts present projects that went beyond experimentation and prototyping and showcase solutions that created economic value for the company. The case study sessions will focus on how it worked and what didn’t work while the deep dive sessions will explain topics such as RNN CNN LSTM transfer learning and further in analytical and technical detail. Meet the European deep learning community in May in Munich and learn from well-known industry leaders!Deep-data-mining.com blog readers receive 15% discount with code: DDMPAWDLW
2019,1,15,Predictive Analytics World for Industry 4.0,https://www.deep-data-mining.com/2019/01/predictiveanalytics-world-for-industry-4.html,6-7 May 2019 – MunichPredictive Analytics World is the leading vendor independent conference for applied machine learning for industry&nbsp;4.0.Business users decision makers and experts in predictive analytics will meet on 6-7 May 2019 in Munich to discover and discuss the&nbsp;latest trends and technologies in machine &amp; deep learning for the era of Internet of Things and artificial intelligence.Putting Machine Intelligence into ProductionSmart Factory Smart Supply Chain Smart Grid Smart Transport: artificial intelligence promises an intelligent and fully automated future but reality is: most machines most vehicles and most grids lack sensors and even where sensors do exist they might not be connected to the Internet of Things. Many companies invested in their infrastructure and are experimenting with prototypes e.g. for predictive maintenance dynamic replenishment route optimization and more but even if they succeeded in delivering a proof of concept they face the challenge to deploy their predictive model into production and scale their analytics solution to company wide adoption. The issues are not merely analytical but a combination of technical organisational judicial and economic details. At the Predictive Analytics World for Industry 4.0 experienced data scientists and business decision makers from a wide variety of industries will meet for two days to demonstrate and to discuss dozens of real-world case studies from well-known industry leaders. In addition predictive analytics experts will explore new methods and tools in special deep dive sessions in detail. Finally the Predictive Analytics World is accompanied by the Deep Learning World conference which focuses on the industry and business application of neural networks. Take the chance learn from the experts and meet your industry peers in Munich in May!&nbsp;Hot topics on the 2019 Agenda:Predictive Maintenance &amp;      LogisticsAnomaly Detection &amp; Root Cause      AnalysisFault Prediction &amp; Failure      DetectionRisk Management &amp; PreventionRoute &amp; Stock OptimizationIndustry &amp; Supply Chain AnalyticsImage &amp; Video RecognitionInternet of Things &amp; Smart      DevicesStream Mining &amp; Edge AnalyticsMachine ~ Ensemble ~ &amp; Deep      LearningProcess Mining &amp; Network AnalysesMining Open &amp; Earth Observation      DataEdge Analytics &amp; Federated      Learning… and many more related topicsPredictiveAnalytics World 4.0 will be co-located with&nbsp;Deep Learning World the premier conference covering the commercial deployment of deep learning in 2019. Deep-data-mining.com blog readers receive 15% discount with code:&nbsp;DDMPAWDLW
2019,1,10,Alibaba acquires Data Artisans?,https://bickson.blogspot.com/2019/01/alibaba-acquires-data-artisans.html,Data Artisans is the company behind Apache Flink - the European answer to Apache Spark.According to this news article&nbsp;Alibaba acquires Data Artisans.I wrote back in 2014 on Apache Flink project.
2019,1,9,The world of languages,http://themainstreamseer.blogspot.com/2019/01/the-world-of-languages.html, Courtesy of: Visual Capitalist  
2019,1,8,Analytics & AI in Travel North America,https://www.deep-data-mining.com/2019/01/analytics-ai-in-travel-north-america.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Analytics & AI in Travel North America launched by EyeForTravel  will take place on March 14-15 at the Hilton Parc 55 Hotel San Francisco USA. There will be over 350 senior data analytics pricing product development and digital marketing experts from the world’s leading travel companies the event will explore the strategies for brands to address the biggest opportunity right now – how to conquer hyper-personalization.  Confirmed speakers include Hilton’s SVP of Analytics Google’s head of AI global product partnerships Expedia’s Head of Platform – Loyalty Wyndham Hotel Group’s Vice President of Global Revenue Management Operations and Sales Carlson Wagonlit’s Principal Data Scientist and many more. Attendees can expect to explore insights into the following: • Harnessing AI and Data to Transform your Loyalty Strategy: Discover how weaving AI into your business capturing preference data and delivering a truly personalized service will give you the edge in winning loyal customers from your competition• Overcoming Pricing Peril with Personalized & Real-Time Revenue Generation Tactics: Make the shift to real-time pricing on an individual level Nail down the use-cases of how to overcome this forecast like a pro and optimize direct revenue• Getting Up Close and Personal with the Customer and Capitalize on Every Channel: Use AI to fuel CRM and CS to bring customer data to life at every touchpoint use the rich and famous on social to avoid brand erosion and secure market share.• Immersing Yourself in an AI-driven Predictive Future to Seize New Profits: It all comes down to being predictive if you want to turn new profits. Deliver AI-led futures in your company for more efficient internal mechanics and travel customer-centricity• Driving Real-Time Hyper-Personalization to Move Your Profit Needle: Delve into new levels of granularity become the Amazon of travel and deliver the perfect travel itinerary every time for unstoppable loyalty• Seizing Voice AR and VR Makes You Grab that Conversion: Be part of the lucky few that benefits from voice enabled search drive direct bookings and use AR and VR to give your customer the confidence to convert• Dominating Direct Bookings Through A Mastery of Mobile: Create an AI-enabled mobile product that drives direct bookings focus on UI and UX that screams out loyalty and bolster your bottom line• Outclassing your Competition with Total RM and Surge Ancillary Sales: Build state-of-the-art infrastructure that supports ancillary revenue and squeeze every ounce of profit from all revenue streams  Please check out the following icon for more information.  
2019,1,3,How to Deploy Artificial Intelligence in the Industrial World,https://prateekvjoshi.com/2019/01/03/how-to-deploy-artificial-intelligence-in-the-industrial-world/,Industrial companies want their assets to generate more revenue without investing further in infrastructure upgrades. It makes sense because the upgrade is extremely expensive! They need to leverage existing systems to achieve this. Large assets usually come with sensors that &#8230; Continue reading &#8594;
2019,1,1,Macroeconomic forecasting for Australia using a large number of predictors,https://robjhyndman.com/publications/ausmacrofcast/,A popular approach to forecasting macroeconomic variables is to utilize a large number of predictors. Several regularization and shrinkage methods can be used to exploit such high-dimensional datasets and have been shown to improve forecast accuracy for the US economy. To assess whether similar results hold for economies with different characteristics an Australian dataset containing observations on 151 aggregate and disaggregate economic series as well as 185 international variables is introduced.
2018,12,22,Network for early career researchers in forecasting,https://robjhyndman.com/hyndsight/iif-ecr-network/,"The International Institute of Forecasters has established interest group sections devoted to specialized domains of forecasting. One of the first such sections will be for early career researchers.
So if you are a PhD student post-doc or otherwise a relatively junior researcher working in forecasting this is for you! The first events will be during the ISF in Thessaloniki in June 2019 including the following:
 ECR welcoming event. A meet and greet event prior to the ISF welcome reception."
2018,12,20,Could Not Connect to Amazon RDS Oracle Database From Car Dealer WiFi,https://www.deep-data-mining.com/2018/12/could-not-connect-to-amazon-rds-oracle.html,"pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I am trying to connect to my Oracle database on Amazon RDS at a car dealer while my car is in service. My laptop is connecting to the public WiFi. When I try to connect to the Oracle server I got ""Error Message = IO Error: The Network Adapter could not establish the connection"".I realized the issue is caused by the new ip address not included Amazon Security Group inbound rules. I find my ip address. Then I log onto Amazon AWS console and find the security group associated with the DB instance. After I add a inbound rule 64.188.5.xxx/32 I am able to connect to the DB immediately. "
2018,12,20,Statistically Manufactured Personal Data,https://www.deep-data-mining.com/2018/12/statistically-manufactured-personal-data.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} To avoid the trouble of dealing with personal data when we test our analytics processes I have created mock personal data that closely reflect American population from statistical point of view. The largest data set has 1 million records with variables including first name last name sex date of birth social security number address phone number and email. The values of these variables are produced to be as realistic as possible to real American population. They represents about 0.33% of population in the United States.  These observations about the data 1 million mock personal data records are very close to the real statistics of the population in USA. 1.The top 4 states that have the most people are: California(138223 persons %13.82) Texas(99217 persons %9.92) Florida(69640 persons %6.96) and  New York(49979 persons %5). These are close to the real distribution of the population in  USA. 2. The female are 51% and the male are 49%. 3. Top 3 last names are Smith(10800 persons %1.08) Williams(8000 persons %.8) and  Jones(6900 persons %.69). 4. Top 3 female first names are Ava(4707 persons %.93) Olivia(4508 persons %.89) and Isabella(4311 persons %.85) and top 3 male first names are Noah(5075 persons %1.03) Elijah(4736 persons %.96) and Liam(4434 persons %.9). 5. The following table shows distributions of persons by age for both sexes. Women live longer than men.                         Female           MaleAge Group        #        %       #  %    .Under 5 years 34603 6.81% 35656 7.25%   .5 to 9 years 34707 6.83% 34010 6.92%   .10 to 14 years 30192 5.94% 33013 6.72%   .15 to 19 years 34361 6.76% 32689 6.65%   .20 to 24 years 32512 6.39% 36647 7.45%   .25 to 29 years 35626 7.01% 37278 7.58%   .30 to 34 years 34344 6.76% 31977 6.50%   .35 to 39 years 33325 6.55% 31927 6.49%   .40 to 44 years 33332 6.56% 34456 7.01%   .45 to 49 years 35070 6.90% 35443 7.21%   .50 to 54 years 37321 7.34% 34876 7.09%   .55 to 59 years 31623 6.22% 31315 6.37%   .60 to 64 years 28801 5.67% 24218 4.93%   .65 to 69 years 20999 4.13% 19881 4.04%   .70 to 74 years 16617 3.27% 14065 2.86%   .75 to 79 years 13520 2.66% 10272 2.09%   .80 to 84 years 10693 2.10% 7983 1.62%   .85 years and over 10754 2.12% 5894 1.20%You may download a small file with 100 records free here. Free Download. Files with 5k 50K 250K and 1 million records are available for purchase at https://www.datamanufacturer.com.                File Name   Description   Price   Buy       dm_mock_person_100.csv   100 mock personal data records. CSV format.            free   Free Download       dm_mock_person_5k.csv   5K mock personal data records. About 0.7M bytes. CSV format.            $2.95   &nbsp;       dm_mock_person_50k.csv   50K mock personal data records. About 7M bytes. CSV format.            $7.95   &nbsp;       dm_mock_person_250k.csv   250K mock personal data records. About 35M bytes. CSV format.            $9.95   &nbsp;       dm_mock_person_1m.csv   1 million mock personal data records. About 140M bytes. CSV format.            $39.95   &nbsp;    
2018,12,19,Generate Random String in Oracle,https://www.deep-data-mining.com/2018/12/generate-random-string-in-oracle.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} The following query generate random email address. SQL> select dbms_random.string('l' 8)||'@'||dbms_random.string('l' 7)||'.com'         email from dual;EMAIL------------------------------------------------------------------------------------irslsxrf@wltikyv.comThe first parameter 'l' means string will be created in lower cases. 
2018,12,18,Find the Most Frequent Values,https://www.deep-data-mining.com/2018/12/find-most-frequent-values.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue}  To find the most frequent values we can use STATS_MODE function. The following query shows areacode in state Missouri. SQL> select areacode from T_PHONE_AREA where state='Missouri' order by 1;  AREACODE----------       314       314       314       314       314       314       314       314       314       314       314       314       417       417       573       573       573       636       636       636       636       636       636       660       816       816       816       816       816       816       816       816       816       81634 rows selected.In the following query stats_mode(areacode) returns the areacode 314 that is the most frequent value. SQL> select stats_mode(areacode) from T_PHONE_AREA where state='Missouri';STATS_MODE(AREACODE)--------------------                 314
2018,12,18,Why doesn't auto.arima() return the model with the lowest AICc value?,https://robjhyndman.com/hyndsight/badarima/,"This question seems to come up frequently on crossvalidated.com or in my inbox.
 I have this time series however it yields different results when I use the auto.arima and Arima functions.
 library(forecast) xd &lt;- ts(c(23786 25955 54373 21561 14552 13284 12714 11821 15445 21307 17228 20007 23065 32811 43147 15127 13497 12224 11412 11888 1421018978 15782 17216 16417 22861 42616 17057 9741 10503 7170 10686 9762 15773 15280 13212 14784 26104 29947) frequency = 12 start=c(20141) end=c(20173)) fit1 &lt;- auto."
2018,12,17,Remove the Last Word From a String Using Oracle SQL,https://www.deep-data-mining.com/2018/12/remove-last-word-from-string-using.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I use the following query to remove the last word of a sentence. with tbl as (select 'Boston city' as name  from dual)select  name substr(name 1 instr(name' '-1)-1 ) simple_name  from tbl;NAME        SIMPLE_NAME----------- -----------Boston city Boston     
2018,12,17,Find Out Table Columns That Are Indexed,https://www.deep-data-mining.com/2018/12/find-out-table-columns-that-are-indexed.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I use the following query to find out columns that are indexed for a table. select index_name column_name from USER_IND_COLUMNS where table_name='MYTABLE'
2018,12,17,Oracle Function Based Index is Handy,https://www.deep-data-mining.com/2018/12/oracle-function-based-index-is-handy.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} In table t_my_table column name is in lower case. However I want to join this table with another table where names are in upper cases. I create a function based index. create index t_my_tablei on t_my_table(upper(name));That way I don't to create another column or table that contains upper(name) and create index on it. When I join the two tables based on upper(a.name) = b.name function based index upper(a.name) is used and it is fast. select a.* b.* from t_my_table a my_another_table b where upper(a.name) = b.name;
2018,12,16,Amazon RDS Oracle Instance Running Out of Disc Space,https://www.deep-data-mining.com/2018/12/amazon-rds-oracle-instance-running-out.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} My Oracle database instance on Amazon RDS runs out of disc space. I add more of them by modifying the instance and add extra disc space.  This is the link to instructions.
2018,12,15,Roulette Wheel Selection Using SQL,https://www.deep-data-mining.com/2018/12/roulette-wheel-selection-using-sql.html,"Roulette wheel selection is a very useful algorithm found in many applications such as Genetic Algorithm(GA). In GA solutions with higher fitness values are given larger probabilities of being selected to produce children just like natural evolution. I implemented an Oracle SQL version of the Roulette wheel selection algorithm. The first step is to calculate for each record the cumulative value for the variable that the selection will be based on such as fitness function probability or other. I used sum() over(order by) analytics function. Make sure the ""order by"" is using a unique key so that the cumulative value is also unique.  create table tbl as select id num sum(num) over(order by id) as cum_count  from t_mydata; The following is the roulette wheel selection scripts.  create table t_rw(sel number);declare  mx number;  rnd  number;  x number;begin   select max(cum_count) into mx from tbl;  for i in 1..10000 loop     execute immediate 'select ora_hash(:1:2) from dual '          into rnd using i mx;     select min(cum_count) into x from tbl where cum_count >= rnd;     insert into t_rw(sel) values(x);     end loop;end;create view v_selected as select a.* from tbl a t_rw b where a.cum_count=b.sel;In the above scripts ora_hash() generates a uniformly distributed random number between 0 and maximum cum_count. The selected cum_count is inserted into t_rw. The final result is the view v_selected which is based on the inner join of table tbl and t_rw."
2018,12,12,Using ggplot2 for functional time series,https://robjhyndman.com/hyndsight/ftsviz/,"This week I’ve been attending the Functional Data and Beyond workshop at the Matrix centre in Creswick.
I spoke yesterday about using ggplot2 for functional data graphics rather than the custom-built plotting functionality available in the many functional data packages including my own rainbow package written with Hanlin Shang.
It is a much more powerful and flexible way to work so I thought it would be useful to share some examples."
2018,12,11,Data visualization for functional time series,https://robjhyndman.com/seminars/ftsviz/,Presentation to MATRIX Workshop on Functional Data Analysis and Beyond Any good data analysis begins with a careful graphical exploration of the observed data. For functional time series data this area of statistical analysis has been largely neglected. I will look at the tools that are available such as rainbow plots and functional box plots and propose several new tools including functional ACF plots functional season plots calendar plots and embedded pairwise distance plots.
2018,12,10,Forecasting large collections of time series,http://www.bzst.com/2018/12/forecasting-large-collections-of-time.html,"With the recent launch of Amazon Forecast I can no longer procrastinate writing about forecasting ""at scale""!

Quantitative forecasting of time series has been used (and taught) for decades with..."
2018,12,9,Seasonal functional autoregressive models,https://robjhyndman.com/seminars/sfar/,Presentation to ACEMS/MATRIX Conference on Functional Data Analysis Functional autoregressive models have been widely used in functional time series analysis but no attention has been given to handling seasonality within this framework. I will discuss a proposed seasonal functional autoregressive model and explore some of its statistical properties including stationarity conditions and limiting behaviour. I will also look at methods for estimation and prediction of seasonal functional autoregressive time series of order one.
2018,12,5,High-dimensional time series analysis,https://robjhyndman.com/seminars/highdimts/,"Presentation to the Australasian Actuarial Education and Research Symposium It is becoming increasingly common for organizations to collect huge amounts of data over time. Traditional time series methods are not well suited to this new paradigm. I will review some new tools that have been developed to analyse large collections of time series including visualization anomaly detection and forecasting.
Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations."
2018,12,4,Generate Serial Number for Existing Table,https://www.deep-data-mining.com/2018/12/generate-serial-number-for-existing.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} Table T_REVIEW has column id as the unique key and dt as the timestamp. I want to add a serial number to the table based on the order by dt and id. In the following scripts I use window function row_number to generate the serial number and update the table. alter table t_review add(seq number);update t_review a set seq= (with tbl as (select id row_number() over(order by dt id) rnk from t_review)select rnk from tbl b where b.id=a.id);
2018,12,4,Incrementally Add New Records to Table,https://www.deep-data-mining.com/2018/12/incrementally-add-new-records-to-table.html,pre{border: 2px solid #666; padding: 10px; background-color: lightgray;} pre.code {border: 2px solid #666; padding: 10px; background-color: lightyellow;} pre.out {border: 2px solid #666; padding: 10px; background-color: lightgray;} div.code {color:blue} I have a table t_review that stores historical records including key sid and timestamp dt. Every day more records come into table t_new. I use the following scripts to add those new records identified by sid in t_new to t_review. begininsert into t_review(sid dt) select sid sysdate from  (select sid from t_new minus select sid from t_review);commit;end;
2018,11,30,Forecasting competitions,https://robjhyndman.com/seminars/forecasting-competitions/,Presentation for the Monash Executive MBA students
2018,11,28,M4 Forecasting Conference,https://robjhyndman.com/hyndsight/m4conference/,Following the highly successful M4 Forecasting Competition there will be a conference held on 10-11 December at Tribeca Rooftop New York to discuss the results. The conference will elaborate on the findings of the M4 Competition with prominent speakers from leading business firms (Amazon Uber Google Microsoft SAS and ProLogistica) and top universities. Nassim Nicholas Taleb will deliver a keynote address about uncertainty in forecasting and elaborate on his claims that &ldquo;tail risks are much worse now than in 2007&rdquo; while Spyros Makridakis will discuss how organizations can benefit by improving the accuracy of their predictions and assessing uncertainty realistically.
2018,11,26,Feature-based time series analysis,https://robjhyndman.com/seminars/monash-fbtsa/,Presentation to the Monash international workshop on time series and panel data It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed. I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series.
2018,10,16,Writing for Researchers,https://robjhyndman.com/seminars/writing/,"ACEMS Mentoring Seminar
Researchers need to write a lot: theses papers reviewer reports responses to reviewer reports grant applications industry reports media releases blog posts and more. We will look at some of the tools available to help researchers with these writing tasks and how to structure different types of writing depending on the audience and purpose.
Slides References  Mendeley Zotero Paperpile Google Scholar  LaTeX and Rmarkdown  TeXlive Rmarkdown Rmarkdown thesis template Rmarkdown templates for papers  Text Editors  SublimeText Atom TeXstudio RStudio  Version control  Happy git with R Github"
2018,9,18,Forecasting the future of the power industry: What can you learn from smart meter data?,https://robjhyndman.com/seminars/monash-masterclass-2018/,"Monash Masterclass Smart electricity meters are providing very rich data in their third year of deployment in Victoria. We now have the potential to see how millions of households and businesses consume high-frequency electricity. But what does this type of data tell us? And how should we use it?
I will discuss how to:
 visualise data from smart electricity meters; identify common household consumption patterns and anomalies; consider the methods for forecasting demand for household electricity; use information from smart electricity meters to monitor changes in the power industry and anticipate consumption trends."
2018,9,18,Tableau vs. Power BI vs. Qlik: How the BI rivals stack up,https://athena-solutions.com/tableau-vs-power-bi-vs-qlik-how-the-bi-rivals-stack-up/,Tableau Power BI and Qlik Sense offer similar self-service BI functionality consultant Rick Sherman says. But he points to some of their strengths and weaknesses. Purchasing […]
2018,8,26,MeDaScIn 2018,https://robjhyndman.com/hyndsight/medascin-2018/,"The annual Melbourne Data Science Initiative (or MeDaScIn pronounced medicine) is on again next month (24-27 September) with lots of tutorials and the annual datathon.
This year there will be a &ldquo;Forecasting with R&rdquo; workshop (25 September) led my two of my Monash colleagues &ndash; George Athanasopoulos and Elena Sanina.
Another great tutorial will be led by Steph Kovalchik (from Tennis Australia) on sports analytics with R (24 September).
For the full list of tutorials see the MeDaScIn website."
2018,8,22,Developing good research habits,https://robjhyndman.com/seminars/research_habits2018/,"Presentation for the 2018 honours students
Slides Links  Mendeley Zotero Paperpile Google Scholar Rmarkdown Happy git with R Rmarkdown thesis template"
2018,8,9,Crude oil price forecasting based on internet concern using an extreme learning machine,https://robjhyndman.com/publications/crude-oil-price/,The growing internet concern (IC) over the crude oil market and related events influences market trading thus creating further instability within the oil market itself. We propose a modeling framework for analyzing the effects of IC on the oil market and for predicting the price volatility of crude oil’s futures market. This novel approach decomposes the original time series into intrinsic modes at different time scales using bivariate empirical mode decomposition (BEMD).
2018,8,6,National Science Week Melbourne Mathematics Activities,https://robjhyndman.com/hyndsight/natsciweek2018/,"Next week is National Science Week and there are a few mathematics activities happening around Melbourne that are being sponsored by ACEMS.

 Elsewhere in Melbourne: Mon 13 Aug 2018 6:00pm - 7:30pm
Public Talk: Is this your card?
Location: University of Melbourne
Speakers: Anthony Mays &amp; Jen Palisse
Pick a card any card! The immortal phrase of the magician. In this talk we&rsquo;ll look at some great card tricks that have simple maths behind them."
2018,8,3,Saving ts objects as csv files,https://robjhyndman.com/hyndsight/ts2csv/,Occasionally R might not be the tool you want to use (hard to believe but apparently that happens). Then you may need to export some data from R via a csv file. When the data is stored as a ts object the time index can easily get lost. So I wrote a little function to make this easier using the tsibble package to do almost all of the work in looking after the time index.
2018,7,26,ACEMS Forecasting Workshop,https://robjhyndman.com/seminars/acemsforecasting2018/,"Date: 26 July 2018
Location: University of Melbourne
This page is for people enrolled in my ACEMS half-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the fpp2 package and its dependencies.
Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed.
Reference ![Online textbook on forecasting](https://otexts."
2018,7,22,Producing a map with 5 lines of code,http://themainstreamseer.blogspot.com/2018/07/producing-map-in-5-lines-of-code.html,"Over the past year I have been exploring the geospatial capabilities of various R packages.&nbsp; Today I want to share the most basic of geospatial capabilities which is producing a map.&nbsp; Using R you can do this in just 5 lines of code.Let's produce a map of Boston Massachusetts.&nbsp; Boston has a longitude of -71.0588801 and a latitude of 42.3600825.&nbsp; Since we list x and y coordinates in order (i.e. we list y after x) we list longitude (the horizontal coordinate) before latitude (the vertical coordinate).&nbsp; Let's create our map!Code line 1:install.packages(""ggmap"")This command installs the ggmap package in your R environment.Code line 2:library(ggmap)This command loads the ggmap package in your R environment.Code line 3:boston &lt;- c(lon = -71.0588801 lat = 42.3600825)This line creates a variable called ""boston"" and assigns the lon and lat coordinates in it.Code line 4:boston_map &lt;- get_map(boston zoom = 13 scale = 1)This line creates a variable called ""boston_map"" and assigns the get_map command to it.Code line 5:ggmap(boston_map)This line generates the map.And that's it!&nbsp; If you reproduce these five lines in your R environment you will generate a map that looks like this:"
2018,7,15,UseR!2018 talks,https://robjhyndman.com/hyndsight/user2018-talks/,"All talks from useR!2018 held in Brisbane last week are now available on YouTube.
Links to talks from members of my research team are given below."
2018,7,14,Seasonal decomposition of short time series,https://robjhyndman.com/hyndsight/tslm-decomposition/,"Many users have tried to do a seasonal decomposition with a short time series and hit the error “Series has less than two periods”.
The problem is that the usual methods of decomposition (e.g. decompose and stl) estimate seasonality using at least as many degrees of freedom as there are seasonal periods. So you need at least two observations per seasonal period to be able to distinguish seasonality from noise."
2018,7,13,Tidy forecasting in R,https://robjhyndman.com/seminars/user-fable/,"Presentation at the useR 2018 conference in Brisbane Australia The forecast package in R is widely used and provides good tools for monthly quarterly and annual time series. But it is not so well-developed for daily and sub-daily data and it does not interact easily with modern tidy packages such as dplyr purrr and tidyr.
I will describe our plans and progress in developing a collection of packages to provide tidy tools for time series and forecasting which will interact seamlessly with tidyverse packages and provide functions to handle time series at any frequency."
2018,7,6,Bivariate smoothing of mortality surfaces with cohort and period ridges,https://robjhyndman.com/publications/mortality-smoothing/,Mortality rates typically vary smoothly over age and time. Exceptions occur due to events such as wars and epidemics which create ridges in the age-period surface of mortality rates in a particular year or for cohorts born in a particular year. We propose a new practical method for modelling the age-period surface of mortality rates. Our approach uses $L_1$ regularization with bivariate smoothing and allows for age-varying period and cohort ridges in the otherwise smooth surface.
2018,6,26,"Hello GDPR, It’s Been a Month. How Are You Doing?",https://www.mdmgeek.com/2018/06/26/hello-gdpr-happy-birthday/,"It is about a month since General Data Protection Regulation(GDPR) came into effect across the European Union. It’s the most critical data privacy law thus far an 88-page monster translated into 26 different languages. When we summarize those pages GDPR on privacy requires companies to: Clearly state how they’re collecting and storing data about EU [&#8230;]
The post Hello GDPR It’s Been a Month. How Are You Doing? appeared first on MDMgeek."
2018,6,25,Forecasting: principles and practice (NYC),https://robjhyndman.com/seminars/nyc2018/,"Date: 25-27 June 2018
Location: eBay 625 6th Ave New York NY 10011 USA.
This page is for people enrolled in my NYC 3-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the following packages and their dependencies:
 fpp2 readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2018,6,24,A forecast ensemble benchmark,https://robjhyndman.com/hyndsight/benchmark-combination/,"Forecasting benchmarks are very important when testing new forecasting methods to see how well they perform against some simple alternatives. Every week I get sent papers proposing new forecasting methods that fail to do better than even the simplest benchmark. They are rejected without review.
Typical benchmarks include the naïve method (especially for finance and economic data) the seasonal naïve method (for seasonal data) an automatically selected ETS model and an automatically selected ARIMA model."
2018,6,21,Feature-based time series analysis,https://robjhyndman.com/seminars/nyc-fbtsa/,"Presentation to the New York R Meetup It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available means that new time series visualisation methods are needed.
I will demonstrate an approach to this problem using a vector of features on each time series measuring characteristics of the series."
2018,6,20,How mature is your advanced analytics program?,https://datamakesworld.com/2018/06/20/how-mature-is-your-advanced-analytics-program/,Advanced analytics is being deployed in a range of use cases across business units and industries especially as data types and volume increase. One of the top use cases for advanced analytics we see at TDWI is predictive analytics to understand customer or operational behavior. Statistical as well as machine learning models are used to &#8230; Continue reading How mature is your advanced analytics&#160;program?
2018,6,20,Modern Data Architecture—Avoiding  the ‘One-Tool-Fits-All Trap,https://athena-solutions.com/modern-data-architecture-avoiding-the-one-tool-fits-all-trap/,A modern data architecture for business intelligence and analytics has to support structured unstructured and semi-structured sources as well as hybrid integration and also meet the […]
2018,6,19,Tidy forecasting in R,https://robjhyndman.com/seminars/isf-fable/,"Presentation at the International Symposium on Forecasting Boulder USA. The forecast package in R is widely used and provides good tools for monthly quarterly and annual time series. But it is not so well-developed for daily and sub-daily data and it does not interact easily with modern tidy packages such as dplyr purrr and tidyr.
I will describe our plans and progress in developing a collection of packages to provide tidy tools for time series and forecasting which will interact seamlessly with tidyverse packages and provide functions to handle time series at any frequency."
2018,6,18,5 Best Practices for Building a program to become data-driven,https://datamakesworld.com/2018/06/18/5-best-practices-for-building-a-program-to-become-data-driven/,There are organizational and technology components critical for a business to succeed in becoming data-driven. On the organizational side a key component to succeeding with data and analytics is to create a culture that supports these efforts. Companies that succeed are typically goal-driven transparent empowering and collaborative. They have strong leadership that believes in data &#8230; Continue reading 5 Best Practices for Building a program to become&#160;data-driven
2018,6,15,Data Science Book: Everybody Lies,http://www.dataminingblog.com/data-science-book-everybody-lies/,Seth Stephens-Davidowitz has written a very entertaining book on big data and how it can be used to understand Humankind. The main idea of Seth is that Google searches is the most powerful source of information to understand what people really think about. Seth argues that the main advantage of Big Data is our ability [&#8230;]
2018,6,12,The Predictive Analytics Conundrum,https://datamakesworld.com/2018/06/12/the-predictive-analytics-conundrum/,&#160; TDWI research indicates that if users stuck to their plans around predictive analytics adoption would be at 75–80% versus the 35–40% we currently see. &#160; Predictive analytics is on the cusp of widespread adoption. Many organizations are excited to make use of the power of predictive analytics (including machine learning) because they understand the &#8230; Continue reading The Predictive Analytics&#160;Conundrum
2018,6,12,Statistics – the rules of the game,https://justindomke.wordpress.com/2018/06/12/statistics-the-rules-of-the-game/,What is statistics about really? It&#8217;s easy to go through a class and get the impression that it&#8217;s about manipulating intimidating formulas. But what&#8217;s the goal of them? Why did people invent them? If you zoom out the big picture is more conceptual than mathematical. Statistics has a crazy grasping ambition: it wants to tell &#8230; Continue reading Statistics – the rules of the&#160;game &#8594;
2018,6,11,You guessed it – women still earn less in IT than men,https://datamakesworld.com/2018/06/11/you-guessed-it-women-still-earn-less-in-it-than-men/,Since joining TDWI about six years ago I have been following and reporting on our salary survey. The report quantifies and interprets the compensation roles responsibilities skills and experience of individual BI and IT professionals. It also provides detailed profiles of the 10 most common BI and data warehousing roles examining age gender education salary &#8230; Continue reading You guessed it &#8211; women still earn less in IT than&#160;men
2018,6,6,Meta-learning how to forecast time series,https://robjhyndman.com/publications/fforms/,A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting.
2018,5,28,Data Science Book: Profit Driven Business Analytics,http://www.dataminingblog.com/data-science-book-profit-driven-business-analytics/,Verbeke Baesens and Bravo have written a data science book focusing on profit. Instead of the typical statistical or programming point of view Profit Driven Business Analytics has a self-proclaimed value-centric perspective. This means the book approaches each topic with a focus on profit costs and ROI. Each data science subject is briefly explained and [&#8230;]
2018,5,11,"Data Science Workshop, EPFL, June 4-6th",http://www.dataminingblog.com/data-science-workshop-epfl-june-4-6th/,What do Swisscom Expedia Cisco Google Frontiers and Bühler have in common? They will present use cases at the Data Science Workshop EPFL June 4-6th. Industry Talks &#8220;The 2018 IEEE Data Science Workshop is a new workshop that aims to bring together researchers in academia and industry to share the most recent and exciting advances [&#8230;]
2018,4,29,IBM SPSS and Entity Analytics at work,http://themainstreamseer.blogspot.com/2018/04/ibm-spss-and-entity-analytics-at-work.html,
2018,4,27,Registration for eRum 2018 closes in two days!,https://www.r-statistics.com/2018/04/registration-for-erum-2018-closes-in-two-days/,"Why I&#8217;m going to eRum this year instead of useR! I have attended the useR! conferences every year now for the past 9 years and loved it! However this year I&#8217;m saddened that I won&#8217;t be able to go. This is because this year the conference will be held in Australia and going there would require &#8230; Continue reading ""Registration for eRum 2018 closes in two days!""
The post Registration for eRum 2018 closes in two days! first appeared on R-statistics blog."
2018,4,24,R 3.5.0 is released! (major release with many new features),https://www.r-statistics.com/2018/04/r-3-5-0-is-released-major-release-with-many-new-features/,"R 3.5.0 (codename &#8220;Joy in Playing&#8221;) was released yesterday. You can get the latest binaries version from here. (or the .tar.gz source code from here). This is a major release with many new features and bug fixes the full list is provided below. Upgrading R on Windows and Mac If you are using Windows you can easily upgrade to the &#8230; Continue reading ""R 3.5.0 is released! (major release with many new features)""
The post R 3.5.0 is released! (major release with many new features) first appeared on R-statistics blog."
2018,4,23,Forecasting in NYC: 25-27 June 2018,https://robjhyndman.com/hyndsight/forecasting-nyc-2018/,"In late June I will be in New York to teach my 3-day workshop on Forecasting using R. Tickets are available at Eventbrite.
This is the first time I&rsquo;ve taught this workshop in the US having previously run it in the Netherlands and Australia. It will be based on the 2nd edition of my book &ldquo;Forecasting: Principles and Practice&rdquo; with George Athanasopoulos. All participants will get a print version of the book."
2018,4,23,Upcoming talks: May-July 2018,https://robjhyndman.com/hyndsight/upcoming-talks-2018/,"First semester teaching is nearly finished and that means conference season for me. Here are some talks I&rsquo;m giving in the next two months. Click the links for more details.
 Melbourne Australia. 28 May: Panel discussion: Forecasting models the uncertainties and associated risk Boulder Colorado USA. 17-20 June: International Symposium on Forecasting. I&rsquo;ll be talking about &ldquo;Tidy forecasting in R&rdquo;. New York USA. 21 June: Feature-based time series analysis. New York Open Statistical Programming Meetup eBay NYC."
2018,4,21,Testing Senzing's Entity Resolution Workbench,http://themainstreamseer.blogspot.com/2018/04/testing-senzings-entity-resolution.html,"I have the great honor of knowing ex-IBM Fellow Jeff Jonas the co-Founder CEO and Chief Scientist of Senzing.&nbsp; Apart from being exceptionally talented Jeff is also an amazing human being who is always willing to help others.&nbsp; I have personally been the beneficiary of his generosity and continue to benefit from his counsel every day.&nbsp; Jeff is one of the main reasons why I have chosen to follow a technical career path at IBM.Jeff left IBM in 2016 to start a new venture called Senzing.&nbsp; Senzing has built the first real-time AI software product for Entity Resolution (ER) a space that Jeff is the world's #1 expert in.&nbsp; Senzing's new offering has huge implications in the post-GDPR world and has the potential to increase trust in Blockchain networks.&nbsp; Jeff recently gave a keynote at the IBM Think conference where he described what Senzing does and its potential applications (including as part of IBM Blockchain).&nbsp; I strongly recommend watching it. When I spoke with Jeff yesterday he asked that I give Senzing's ER workbench a try and provide feedback.&nbsp; So that is what I did earlier today.&nbsp; Here are my first impressions.Questions for JeffCurrently Senzing only runs on Windows.&nbsp; When will it be offered on other operating systems (especially MacOS)?Why do I need to download the workbench?&nbsp; Can I not just have a Cloud based version?Getting startedI found the workbench very easy to use.&nbsp; The instructions were clear and the steps to get from start to finish were intuitive.I uploaded a csv file of all my Google contacts.&nbsp; I could not believe I had 2451 contacts in my Google contact list!&nbsp; Clearly I have a lot of spring cleaning to do.The csv file upload process was straightforward and quick.&nbsp; On that point though the workbench currently only works with csv files.&nbsp; Any plans to directly connect to other data sources?The ER process is very quick.&nbsp; After uploading your data ER is a one-click process.&nbsp; Very cool.The user interface could use an upgrade.Results of the ER processThe workbench identified 32 duplicates 4 possible duplicates and 6 possibly related entities.The results had a lot more detail than Google contact's duplicate function providesInterestingly of the 6 possibly related entries entities 5 and 6 both related to my wife.&nbsp; I was a little surprised that the workbench did not merge them both and give me 5 possibly related entities instead of 6Apart from this it was really interesting to see how the workbench linked different entities&nbsp;Single Search FunctionThe Single Search Function (SSF) is very cool.&nbsp; I only tried it with the name field since it was the most intuitive one for me to try it with.One potential bug(?) I noticed is that you have to type full name of a contact in order for the SSF to work correctly.&nbsp; Partial name (just first or last name) searches resulted in (0 results found) errors.Also I wish there was an option to merge the various contacts.&nbsp; While this may not be the focus of the workbench sometimes it is useful if you want to clean up an address book.&nbsp; For example in Google contacts after it displays the duplicates it gives you the option of merging all contacts.&nbsp; That gives the process a logical end point IMHO.Compared to Google contacts' duplicates functionIt is probably unfair to compare Senzing's workbench to Google contacts' duplicates function but I did it and I might as well write about it.Google contacts identified 8 duplicates (Senzing identified 32 duplicates 4 possible duplicates and 6 possibly related entities).The results were not nearly as sophisticated as those from Senzing in terms of the information provided.Also Google got several duplicates wrong.&nbsp; Some were clearly not related.&nbsp; For example for one contact I had an old phone number and a new phone number saved.&nbsp; Even though the person who now owned the ""old"" phone number was clearly different from my friend (based on a Google update they had posted about where they were and a new photograph) Google suggested they might be the same person.&nbsp; Senzing did not.Give it a try yourselfOverall I really enjoyed taking Senzing's ER workbench for a test ride.&nbsp; You can too.&nbsp; Go watch Jeff's IBM Think keynote to get a password to download it and take it for a spin!"
2018,4,14,forecast v8.3 now on CRAN,https://robjhyndman.com/hyndsight/forecast83/,"The latest version of the forecast package for R is now on CRAN. This is the version used in the 2nd edition of my forecasting textbook with George Athanasopoulos. So readers should now be able to replicate all examples in the book using only CRAN packages.
A few new features of the forecast package may be of interest. A more complete Changelog is also available.
mstl() handles multiple seasonality STL decomposition was designed to handle a single type of seasonality but modern data often involves several seasonal periods (e."
2018,4,11,A brief history of time series forecasting competitions,https://robjhyndman.com/hyndsight/forecasting-competitions/,"Prediction competitions are now so widespread that it is often forgotten how controversial they were when first held and how influential they have been over the years.
To keep this exercise manageable I will restrict attention to time series forecasting competitions &mdash; where only the history of the data is available when producing forecasts.
Nottingham studies The earliest non-trivial study of time series forecast accuracy was probably by David Reid as part of his PhD at the University of Nottingham (1969)."
2018,4,10,No Omnichannel Experiences? You Are Dead to Me,https://www.mdmgeek.com/2018/04/09/no-omnichannel-experiences-means-death/,"Harsh words? But I have a feeling I am not alone telling this when it comes to omnichannel experiences. Born in the early 80s I fall into a category called  Xennials &#8211; a micro-generation born on the cusp of the two generations between 1977 and 1985. This article has a lot of details and says as [&#8230;]
The post No Omnichannel Experiences? You Are Dead to Me appeared first on MDMgeek."
2018,4,9,High dimensional time series analysis,https://robjhyndman.com/seminars/abs-hdts/,"Presentation for the Australian Bureau of Statistics I will provide an overview of some of my recent research on methods to deal with high-dimensional time series.
1. Visualizing many time series
It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods."
2018,3,26,R package for M4 Forecasting Competition,https://robjhyndman.com/hyndsight/m4comp2018/,"The M4 forecasting competition is well under-way and a few of my PhD students have been working on submissions.
Pablo Montero-Manso Carla Netto and Thiyanga Talagala have made an R package containing all of the data (100000 time series) which should make it substantially easier for other contestants to load the data into R in order to compute forecasts.
Grab the package from this github repository.
For more details about the M4 competition see this post or go to the M4 website."
2018,3,23,Research++: what you should know about being a researcher but probably don't,https://robjhyndman.com/seminars/research++/,"Presentation for my research group about aspects of being a researcher.
Some of this material has appeared as blog posts at some time.
Slides Links  Paperpile My Google Scholar profile How to Write a Lot: A Practical Guide to Productive Academic Writing Monash University authorship policy Antarctica Journal of Mathematics"
2018,3,19,Creating Value with Big Data Analytics (book review),http://www.dataminingblog.com/creating-value-with-big-data-analytics-book-review/,Verhoef Kooge and Walk have written a detailed and technical book on the application of data analytics to Marketing. While not stated in the title the subtitle makes it clear: the book is dedicated to people in Marketing and Sales. The strong academic background of the authors is transparent in the book which is full [&#8230;]
2018,3,13,IJF Tao Hong Award 2018,https://robjhyndman.com/hyndsight/ijf-hong-nominations-2018/,"Every two years the International Journal of Forecasting awards a prize to the best paper on energy forecasting. The prize is generously funded by Professor Tao Hong. This year we will award the prize to a paper published in the IJF during the period 2015-2016. The prize will be US$1000 plus an engraved plaque. The award committee is Rob J Hyndman Pierre Pinson and James Mitchell.
Nominations are invited from any reader of the IJF."
2018,3,6,pytorch PU learning trick,http://www.machinedlearnings.com/2018/03/pytorch-pu-learning-trick.html,"I'm often using positive-unlabeled learning nowadays.  In particular for observational dialog modeling next utterance classification is a standard technique for training and evaluating models.  In this setup the observed continuation of the conversation is considered a positive (since a human said it it is presumed a reasonable thing to say at that point in the conversation) and other randomly chosen utterances are treated as unlabeled (they might be reasonable things to say at that point in the conversation).Suppose you have a model whose final layer is a dot product between a vector produced only from context and a vector produced only from response.  I use models of this form as &ldquo;level 1&rdquo; models because they facilitate precomputation of a fast serving index but note the following trick will not apply to architectures like bidirectional attention.  Anyway for these models  you can be more efficient during training by drawing the negatives from the same mini-batch.  This is a well-known trick but I couldn't find anybody talking about how to do this explicitly in pytorch.  Structure your model to have a leftforward and a rightforward like this:class MyModel(nn.Module):...    def forward(leftinput rightinput):        leftvec = self.leftforward(leftinput)        rightvec = self.rightforward(rightinput)        return torch.mul(leftvec rightvec).sum(dim=1 keepdim=True)At training time compute the leftforward and rightforward for your mini-batch distinctly:...criterion = BatchPULoss()model = MyModel()...leftvec = model.leftforward(batch.leftinput)rightvec = model.rightforward(batch.rightinput)(loss preds) = criterion.fortraining(leftvectors rightvectors)loss.backward()# ""preds"" contains the highest score right for each left # so for instance calculate ""mini-batch precision at 1""gold_labels = torch.arange(0 batch.batch_size).long().cuda()n_correct += (preds.data == gold_labels).sum()...Finally use this loss:import torchclass BatchPULoss():    def __init__(self):      self.loss = torch.nn.CrossEntropyLoss()    def fortraining(self left right):      outer = torch.mm(left right.t())      labels = torch.autograd.Variable(torch.arange(0outer.shape[0]).long().cuda()                                        requires_grad=False)      loss = self.loss(outer labels)      _ preds = torch.max(outer dim=1)      return (loss preds)    def __call__(self *args **kwargs):      return self.loss(*args **kwargs)At training time you call the fortraining method but if you have fixed distractors for evaluation you can also call it directly just like CrossEntropyLoss."
2018,3,1,"Watson Analytics, SPSS Modeler and Esri ArcGIS",http://themainstreamseer.blogspot.com/2018/03/watson-analytics-spss-modeler-and-esri.html,
2018,2,6,3 Best Practices for Becoming More Self-Sufficient with Self-Service Analytics,https://datamakesworld.com/2018/02/06/3-best-practices-for-becoming-more-self-sufficient-with-self-service-analytics/,My colleague Dave Stodder and I recently lead a Webinar in conjunction with our best practices report about becoming a data-driven organization.  Audience questions included several about self-service. In particular attendees were interested in how to make self-service more accessible to managers and leaders in their organization. Let me set the stage for self-service analytics &#8230; Continue reading 3 Best Practices for Becoming More Self-Sufficient with Self-Service&#160;Analytics
2018,2,4,Data Ethics Regulation: Two key updates in 2018,http://www.bzst.com/2018/02/data-ethics-regulation-two-key-updates.html,This year two important new regulations will be impacting research with human subjects: the EU's General Data Protection Regulation (GDPR) which kicks in May 2018 and the USA's updated Common...
2018,2,4,Exploring the sources of uncertainty: why does bagging for time series forecasting work?,https://robjhyndman.com/publications/bagging-uncertainty/,In a recent study Bergmeir Hyndman and Benítez (2016) successfully employed a bootstrap aggregation (bagging) technique for improving the performance of exponential smoothing. Each series is Box-Cox transformed and decomposed by Seasonal and Trend decomposition using Loess (STL); then bootstrapping is applied on the remainder series before the trend and seasonality are added back and the transformation reversed to create bootstrapped versions of the series. Subsequently they apply automatic exponential smoothing on the original series and the bootstrapped versions of the series with the final forecast being the equal-weight combination across all forecasts.
2018,2,1,Visualizing big energy data,https://robjhyndman.com/publications/visualizing-big-energy-data/,Visualization is a crucial component of data analysis. It is always a good idea to plot the data before fitting any models making any predictions or drawing any conclusions. As sensors of the electric grid are collecting large volumes of data from various sources power industry professionals are facing the challenge of visualizing such data in a timely fashion. In this article we demonstrate several data visualization solutions for big energy data through three case studies involving smart meter data phasor measurement unit (PMU) data and probabilistic forecasts respectively.
2018,1,15,Advanced Analytics: What’s Ahead for 2018,https://datamakesworld.com/2018/01/15/advanced-analytics-whats-ahead-for-2018/,2018 will be the year of the three As:  AI Automation and Advancing analytics skills In 2017 advanced analytics maintained its momentum in the enterprise. Open source technologies such R and Python gained ground.  Other technologies such as machine learning continued to pique interest.  Use of the cloud become more mainstream. TDWI expects these technologies &#8230; Continue reading Advanced Analytics: What’s Ahead for&#160;2018
2018,1,12,Three Organizational Best Practices for Becoming Data-Driven,https://datamakesworld.com/2018/01/12/three-organizational-best-practices-for-becoming-data-driven/,Dave Stodder and I just finished writing our 4Q Best Practices Report on “What it Takes to Be Data-Driven: Technologies and Practices for Becoming a Smarter Organization.”  What struck me in analyzing the data for the report is that although organizations have embraced BI and analytics they still have a journey in front of them &#8230; Continue reading Three Organizational Best Practices for Becoming&#160;Data-Driven
2018,1,1,A note on the validity of cross-validation for evaluating autoregressive time series prediction,https://robjhyndman.com/publications/cv-time-series/,One of the most widely used standard procedures for model evaluation in classification and regression is $K$-fold cross-validation (CV). However when it comes to time series forecasting because of the inherent serial correlation and potential non-stationarity of the data its application is not straightforward and often omitted by practitioners in favour of an out-of-sample (OOS) evaluation. In this paper we show that in the case of a purely autoregressive model the use of standard $K$-fold CV is possible as long as the models considered have uncorrelated errors.
2017,12,31,Thank You,https://www.svds.com/thankyou/,"We thank our customers partners investors friends and family for their support over the years. And most importantly we thank our employees for their hard work and dedication to building a great company!
The post Thank You appeared first on Silicon Valley Data Science."
2017,12,28,"For BI, you must know the data integration process",https://athena-solutions.com/know-data-integration/,Understanding the data integration process is central to self-service BI and data architecture design consultant Rick Sherman says in an end-of-year look at data management trends. […]
2017,12,28,Happy Holidays from SVDS,https://www.svds.com/happy-holidays-svds/,"Happy holidays from SVDS! We wish you peace prosperity and happiness this season and in the year ahead.
The post Happy Holidays from SVDS appeared first on Silicon Valley Data Science."
2017,12,25,Election polls: description vs. prediction,http://www.bzst.com/2017/12/election-polls-description-vs-prediction.html,My papers To Explain or To Predict and Predictive Analytics in Information Systems Research contrast the process and uses of predictive modeling and causal-explanatory modeling. I briefly mentioned...
2017,12,22,M4 Forecasting Competition update,https://robjhyndman.com/hyndsight/m4competition/,"The official guidelines for the M4 competition have now been published and there have been several developments since my last post on this.
  There is now a prize for prediction interval accuracy using a scaled version of the Mean Interval Score. If the $100(1-\alpha)$% prediction interval for time $t$ is given by $[L_{t}U_{t}]$ for $t=1\dotsh$ then the MIS is defined as $$\frac{1}{h}\sum_{t=1}^{h} \left[ (U_t-L_t) + \frac{2}{\alpha}(L_t-Y_t)1(Y_t &lt; L_t) + \frac{2}{\alpha}(Y_t-U_t)1(Y_t &gt; U_t) \right] $$ where $Y_t$ is the observation at time $t$."
2017,12,21,Crossing the Development to Production Divide,https://www.svds.com/tbt-crossing-development-production-divide/,"In this post we'll give an overview of obstacles we’ve faced (you may be able to relate) and talk about solutions to overcome these obstacles.
The post Crossing the Development to Production Divide appeared first on Silicon Valley Data Science."
2017,12,15,NIPS Conversation AI Workshop,http://www.machinedlearnings.com/2017/12/nips-conversation-ai-workshop.html,"I only attended NIPS for the Conversation AI workshop so my thoughts are limited to that.  I really liked the subtitle of the workshop: ""today's practice and tomorrow's potential.""  Since I'm on a product team trying to build chatbots that are actually effective it struck me as exactly the right tone.Several presentations were related to the Alexa prize.  When reading these papers keep in mind that contestants were subject to extreme sample complexity constraints.  Semifinalists had circa 500 on-policy dialogs and finalists less than 10 times more.  This is because 1) the Alexa chat function is not the primary purpose of the device so not all end users participated and 2) they had to distribute the chats to all contestants.The result of sample complexity constraints is a &ldquo;bias against variance&rdquo; as I've discussed before.  In the Alexa prize that meant the winners had the architecture of &ldquo;learned mixture over mostly hand-specified substrategies.&rdquo;  In other words the (scarce) on-policy data was limited to adjusting the mixture weights.  (The MILA team had substrategies that were trained unsupervised on forum data but it looks like the other substrategies were providing most of the benefit.)  Sample complexity constraints are pervasive in dialog but nonetheless the conditions of the contest were more extreme than what I encounter in practice so if you find yourself with more on-policy data consider more aggressive usage.Speaking of sample complexity constraints we have found pre-training representations on MT tasks a la CoVE is extremely effective in practice for multiple tasks.  We are now playing with ELMo-style pre-training using language modeling as the pre-training task (very promising: no parallel corpus needed!).Another sample complexity related theme I noticed at the workshop was the use of functional role dynamics.  Roughly speaking this is modeling the structure of the dialog independent of the topic.  Once topics are abstracted the sample complexity of learning what are reasonably structured conversations seems low.  Didericksen et. al. combined a purely structural L1 model with a simple topically-sensitive L2 (tf-idf) to build a retrieval based dialog simulator.  Analogously for their Alexa prize submission Serban et. al. learned a dialog simulator from observational data which utilized only functional role and sentiment information and then applied Q-learning: this was more effective than off-policy reinforce with respect to some metrics.Overall the workshop gave me enough optimism to continue plugging away despite the underwhelming performance of current dialog systems."
2017,12,13,Probabilistic outlier detection and visualization of smart metre data,https://robjhyndman.com/seminars/nzsa2017/,"Talk given at meeting of New Zealand Statistical Association and International Association for Statistical Computing (11-14 December 2017) Auckland New Zealand.
It is always a good idea to plot your data before fitting any models making any predictions or drawing any conclusions. But how do you actually plot data on thousands of smart meters each comprising thousands of observations over time? We cannot simply produce time plots of the demand recorded at each meter due to the sheer volume of data involved."
2017,12,9,Apple shares Turi Create open source framework,https://bickson.blogspot.com/2017/12/apple-shares-turi-create-open-source.html,It is very exciting that after many years of hard work we have finally released our machine learning framework as open source! The announcement made yesterday at NIPS by Prof. Carlos Guestrin:And here is our github link: https://github.com/apple/turicreate
2017,12,8,R 3.4.3 is released (a bug-fix release),https://www.r-statistics.com/2017/12/r-3-4-3-is-released-a-bug-fix-release/,"R 3.4.3 (codename &#8220;Kite-Eating Tree&#8221;) was released last week. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned by David Smith R 3.4.3 is primarily a bug-fix release: It fixes an issue with incorrect time zones on MacOS High Sierra and some issues with handling Unicode characters. (Incidentally representing international and &#8230; Continue reading ""R 3.4.3 is released (a bug-fix release)""
The post R 3.4.3 is released (a bug-fix release) first appeared on R-statistics blog."
2017,12,7,Q&A: On Being Data-Driven,https://www.svds.com/qa-data-driven/,"The best way to spread data-driven thinking through an organization is by proving that you can use data to solve a real business problem.
The post Q&#038;A: On Being Data-Driven appeared first on Silicon Valley Data Science."
2017,12,5,Data Science for Managers: May 2018,https://robjhyndman.com/hyndsight/data-science-for-managers/,"For the last few years I have been involved with running a 3-day short course on &ldquo;Data Science for Managers&rdquo;. We have run it twice each year since 2015 and it continues to prove very popular. We have some awesome presenters including Monash University professors Di Cook Geoff Webb and Kim Marriott as well as several very experienced data scientists working in industry.
The next course will be held on 8-10 May 2018."
2017,12,5,Trends in Indigenous mortality and life expectancy 2001-2015,https://robjhyndman.com/publications/aihw2017/,The Enhanced Mortality Database (EMD) was developed in 2010 by the Australian Institute of Health and Welfare to explore the feasibility of creating an ongoing enhanced mortality data set that allows analysis of key mortality indicators including life expectancy and causes of death to assist with monitoring ‘Closing the Gap’ health targets. The method involves using data linkage to enhance the identification of Aboriginal and Torres Strait Islander people in death registrations.
2017,12,5,Chasing Fermat’s Last Theorem,https://prateekvjoshi.com/2017/12/05/chasing-fermats-last-theorem/,Fermat&#8217;s Last Theorem is one of the most celebrated problems in mathematics. It is a problem in the field of Number Theory that&#8217;s very simple to understand yet extremely difficult to solve. In fact it&#8217;s so difficult that it remained &#8230; Continue reading &#8594;
2017,11,30,Managing Uncertainty,https://www.svds.com/managing-uncertainty/,"Being data-driven is the best way to manage uncertainty—but achieving that is about far more than bringing a bunch of numbers to your latest meeting.
The post Managing Uncertainty appeared first on Silicon Valley Data Science."
2017,11,29,Some new time series packages,https://robjhyndman.com/hyndsight/tspackages/,This week I have finished preliminary versions of two new R packages for time series analysis. The first (tscompdata) contains several large collections of time series that have been used in forecasting competitions; the second (tsfeatures) is designed to compute features from univariate time series data. For now both are only on github. I will probably submit them to CRAN after they’ve been tested by a few more people.
2017,11,21,Machine Learning – Next Big Step in Master Data Management,https://www.mdmgeek.com/2017/11/21/machine-learning-master-data-management/,"Over last 12 years I have seen Master Data Management help companies automate and improve data. It has helped companies take a strategic approach to managing data by removing processes that were mainly left manual and time-consuming for years. We have seen an exponential increase in volume and variety of data in last 5-6 years. [&#8230;]
The post Machine Learning &#8211; Next Big Step in Master Data Management appeared first on MDMgeek."
2017,11,21,Analyzing Sentiment in Caltrain Tweets,https://www.svds.com/analyzing-sentiment-caltrain-tweets/,"As a first step to using Twitter activity as one of the data sources for train prediction we start with a simple question: How do Twitter users currently feel about Caltrain?
The post Analyzing Sentiment in Caltrain Tweets appeared first on Silicon Valley Data Science."
2017,11,20,M4 Forecasting Competition: response from Spyros Makridakis,https://robjhyndman.com/hyndsight/m4comp-response/,"Following my post on the M4 competition yesterday Spyros Makridakis sent me these comments for posting here.
I would like to thank Rob my friend and co-author for his insightful remarks concerning the upcoming M4 competition. As Rob says the two of us have talked a great deal about competitions and I certainly agree with him about the “ideal” forecasting competition. In this reply I will explain why I have deviated from the “ideal” mostly for practical reasons and to ensure higher participation."
2017,11,19,M4 Forecasting Competition,https://robjhyndman.com/hyndsight/m4comp/,"The &ldquo;M&rdquo; competitions organized by Spyros Makridakis have had an enormous influence on the field of forecasting. They focused attention on what models produced good forecasts rather than on the mathematical properties of those models. For that Spyros deserves congratulations for changing the landscape of forecasting research through this series of competitions.
Makridakis &amp; Hibon (JRSSA 1979) was the first serious attempt at a large empirical evaluation of forecast methods."
2017,11,18,2017 Beijing Workshop on Forecasting,https://robjhyndman.com/seminars/beijing2017/,"Location: Central University of Finance and Economics Beijing China.
Slides  Forecast Accuracy and Evaluation Automatic Forecasting Algorithms Hierarchical Forecasting Probabilistic Hierarchical Forecasting  Textbook 
Relevant papers  Rob J Hyndman Anne B Koehler Ralph D Snyder Simone Grose (2002) A state space framework for automatic forecasting using exponential smoothing methods. International Journal of Forecasting 18(3) 439-454. Rob J Hyndman Anne B Koehler (2006) Another look at measures of forecast accuracy."
2017,11,16,A Divergence Bound For Hybrids of MCMC and Variational Inference and …,https://justindomke.wordpress.com/2017/11/16/a-divergence-bound-for-hybrids-of-mcmc-and-variational-inference-and/,At ICML I recently published a paper that I somehow decided to title &#8220;A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI&#8221;. This paper gives one framework for building &#8220;hybrid&#8221; algorithms between Markov chain Monte Carlo (MCMC) and Variational inference (VI) algorithms. Then it gives an &#8230; Continue reading A Divergence Bound For Hybrids of MCMC and Variational Inference and&#160;&#8230; &#8594;
2017,11,16,Learning from Imbalanced Classes,https://www.svds.com/tbt-learning-imbalanced-classes/,"For this month's Throwback Thursday a post that provides insight and concrete advice on how to tackle imbalanced data. 
The post Learning from Imbalanced Classes appeared first on Silicon Valley Data Science."
2017,11,14,Come and work with me,https://robjhyndman.com/hyndsight/acems-postdoc/,"I have funding for a new post-doctoral research fellow on a 2-year contract to work with me and Professor Kate Smith-Miles on analysing large collections of time series data. We are particularly seeking someone with a PhD in computational statistics or statistical machine learning.
Desirable characteristics:
 Experience with time series data. Experience with R package development. Familiarity with reproducible research practices (e.g. git rmarkdown etc). A background in machine learning or computational statistics."
2017,11,14,Visualization of the 1854 London Cholera Outbreak,http://themainstreamseer.blogspot.com/2017/11/visualization-of-1854-london-cholera.html,This post attempts to visualize the 1854 London Cholera Outbreak based on data collected by Dr. John Snow and provided in the HistData R package. Dr. Snow was able to identify that cholera was a water borne disease by visualizing his data in 1854 and was able to bring the Cholera outbreak to an end. This dataset and analysis speaks to power of geospatial data and its importance in decision making.
2017,11,13,"Govern first, then ask many questions",https://athena-solutions.com/govern-first-then-ask-many-questions/,Join me for this webinar hosted by Information Management Live. Thursday November 16 2017 1 PM ET/10 AM PT The path to fruitful analytics starts with good […]
2017,11,12,Data Analytics for Internal Audit,http://www.dataminingblog.com/data-analytics-for-internal-audit/,This is a guest post from Marcel Baumgartner Data Analytics Expert at Nestlé S.A. Introduction Large publicly listed companies not only have external auditors who check the books but often also a large community of internal auditors. These collaborators provide the company with a sufficient level of assurance in terms of adherence to internal and [&#8230;]
2017,11,9,Self-Service BI on DM Radio,https://athena-solutions.com/self-service-bi-on-dm-radio/,Join me on DM Radio DM Radio / Leave IT Alone &#8211; The Vast Value of Self-Service DATE: November 9 2017 TIME: 3 PM Eastern / […]
2017,11,9,Exploring the Possibilities of Artificial Intelligence,https://www.svds.com/exploring-the-possibilities-of-artificial-intelligence/,"In this interview Paco Nathan discusses making life more livable AI fears and more.
The post Exploring the Possibilities of Artificial Intelligence appeared first on Silicon Valley Data Science."
2017,11,7,Dissecting Diophantine Equations,https://prateekvjoshi.com/2017/11/07/dissecting-diophantine-equations/,The word Diophantine refers to Diophantus the third century mathematician from Alexandria. Numbers are the fundamental building blocks of mathematics so he started thinking about problems for which we need integer solutions. For example how many horses do we need &#8230; Continue reading &#8594;
2017,11,6,"Statistical test for ""no difference""",http://www.bzst.com/2017/11/statistical-test-for-no-difference.html,"To most researchers and practitioners using statistical inference the popular hypothesis testing universe consists of two hypotheses:

H0 is the null hypothesis of ""zero effect""

H1 is the..."
2017,11,2,Self-Organizing Maps Tutorial,https://algobeans.com/2017/11/02/self-organizing-map/,Visualize large datasets and identify potential clusters with this special breed of neural networks that uses neurons to learn the intrinsic shape of your data.
2017,11,2,Merging Data Science and Business,https://www.svds.com/merging-data-science-and-business/,"Business leaders cannot afford to ignore their organization&#8217;s data—rather that data should be used to make informed decisions. In this post Principal Data Scientist Tom Fawcett and Professor of Data Science Foster Provost discuss how businesses can make the most of their analytical teams. Tom and Foster are the authors of Data Science for Business. What aspect [&#8230;]
The post Merging Data Science and Business appeared first on Silicon Valley Data Science."
2017,11,1,2017 Beijing Workshop on Forecasting,https://robjhyndman.com/hyndsight/beijing-2017/,"Later this month I&rsquo;m speaking at the 2017 Beijing Workshop on Forecasting to be held on Saturday 18 November at the Central University of Finance and Economics.
I&rsquo;m giving four talks as part of the workshop. Other speakers are Junni Zhang Lei Song Hui Bu Feng Li and Yanfei Kang.
Full program details are available online."
2017,11,1,High dimensional time series analysis,https://robjhyndman.com/seminars/acems-hdts/,Keynote presentation for ACEMS retreat.
2017,10,30,heatmaply: an R package for creating interactive cluster heatmaps for online publishing,https://www.r-statistics.com/2017/10/heatmaply-an-r-package-for-creating-interactive-cluster-heatmaps-for-online-publishing/,"This post on the heatmaply package is based on my recent paper from the journal bioinformatics (a link to a stable DOI). The paper was published just last week and since it is released as CC-BY I am permitted (and delighted) to republish it here in full. My co-authors for this paper are Jonathan Sidi Alan O&#8217;Callaghan and Carson Sievert. Summary: heatmaply is an R &#8230; Continue reading ""heatmaply: an R package for creating interactive cluster heatmaps for online publishing""
The post heatmaply: an R package for creating interactive cluster heatmaps for online publishing first appeared on R-statistics blog."
2017,10,27,What caused the Challenger disaster?,http://themainstreamseer.blogspot.com/2017/10/what-caused-challenger-disaster.html,The motivation for this blog is to examine the reasons behind the explosion of the USA Space Shuttle Challenger on 28 January 1986. The night before the launch a decision had to be made regarding launch safety and engineers recommended that the launch be postponed in the event the temperature at launch was below freezing as this adversely impacted the integrity of O-rings a key component holding in field joints. The engineers advice was ignored and disaster ensued. Let's dive in!
2017,10,26,Dataversity 2017,https://athena-solutions.com/dataversity-2017/,I’m excited to share with you that I’ll be presenting two unique sessions at DATAVERSITY’s Data Architecture Summit in Chicago IL on November 13-16 2017.  I’m also […]
2017,10,26,Handling Small Files in MapR-FS,https://www.svds.com/handling-small-files-in-mapr-fs/,"In this post we will discuss how dealing with small files is different if you are using MapR-FS rather than the traditional HDFS installation.
The post Handling Small Files in MapR-FS appeared first on Silicon Valley Data Science."
2017,10,21,The academic tip: What is Deep Learning?,http://www.dataminingblog.com/the-academic-tip-what-is-deep-learning/,This is a guest post from Jacques Zuber Data Science Teacher at HEIG-VD. The commonly called deep learning or hierarchical learning is now a popular trend in machine learning. Recently during the Swiss Analytics Meeting Prof. Dr. Sven F. Crone presented how we can use deep learning in the industry in a forecasting perspective (beer [&#8230;]
2017,10,18,"Interview of Jerome Berthier, Head of BI and Big Data at ELCA",http://www.dataminingblog.com/interview-of-jerome-berthier-head-of-bi-and-big-data-at-elca/,Data Mining Research (DMR): Can you tell us who you are and how you came to the field of Data Science? Jerome Berthier (JB): My name is Jerome Berthier I am an engineer in Computer Science and I have an MBA in management. After 10 years working in different roles for an IT provider (developer [&#8230;]
2017,10,15,Regression in R,http://themainstreamseer.blogspot.com/2017/10/regression-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:This notebook covers:Fitting and interpreting linear models;Evaluating model assumptions; andSelecting among competing models.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest notebook: Regression in R https://t.co/HDYFzTAFPr #rstats #DataScience #ibmaot #Statistics #Stats #dsx #Bluemix h/t @kabacoff pic.twitter.com/LxKc9HkBC0— Venky Rao (@VRaoRao) October 15 2017 
2017,10,12,Will Data Scientists be Replaced by Machines?,http://www.dataminingblog.com/will-data-scientists-be-replaced-by-machines/,Data Science automation is a hot topic recently with several articles about it[1]. Most of them discuss the so-called &#8220;automation&#8221; tools[2]. Too often editors claim that their tools can automate the Data Science process. This provides the feeling that combining these tools with a Big Data architecture can solve any business problems. The misconception comes [&#8230;]
2017,10,12,Analysing sub-daily time series data,https://robjhyndman.com/seminars/subdaily-2017/,"Talk given by me Earo Wang and Mitchell O&rsquo;Hara-Wild at the Melbourne Users of R Network meeting.
Slides   

 Packages"
2017,10,9,Understanding Catalan’s Conjecture,https://prateekvjoshi.com/2017/10/09/understanding-catalans-conjecture/,Numbers are deceptively simple. We use them everyday in a variety of contexts. It&#8217;s beautiful how there are so many mysteries hidden inside these numbers. Catalan&#8217;s Conjecture is a mathematical problem that&#8217;s really easy to understand but extremely difficult to &#8230; Continue reading &#8594;
2017,10,7,Coefficient of Alienation,http://themainstreamseer.blogspot.com/2017/10/coefficient-of-alienation.html,"If you thought the coefficient of alienation referred to the hostility I receive from my family as I update my blog on a Saturday afternoon I would not fault you too much. &nbsp;However this is a blog about predictive analytics which is based on Statistics. &nbsp;So let's keep that in mind as we understand what the ""Coefficient of Alienation"" means.Apart from being one of the coolest sounding Statistical terms the Coefficient of Alienation measures the proportion of variation in the outcome not “explained” by the variables on the right-hand side of a simple linear regression (ordinary least squares) equation.The Coefficient of Alienation is also known as the Coefficient of Non-Determination since the formula for calculating it is:where:is the Coefficient of Determination.And now before my personal (and non-Statistical) Coefficient of Alienation reaches the point of no return I will bring this post to an end."
2017,9,30,Homoscedasticity and heteroscedasticity,http://themainstreamseer.blogspot.com/2017/09/homoscedasticity-and-heteroscedasticity.html,"Homoscedasticity and heteroscedasticity - two of the scariest sounding terms in all of Statistics! &nbsp;So what do they mean?When one calculates the variance or standard deviation of a dataset of random variables one assumes that the variance is constant across the entire population. &nbsp;This assumption is homoscedasticity. &nbsp;The opposite of this assumption is heteroscedasticity.In other words a collection of random variables is heteroscedastic if there are sub-populations within the dataset that have different variances from others (source: https://en.wikipedia.org/wiki/Heteroscedasticity). &nbsp;Another way of describing homoscedasticity is constant variance and another way of describing heteroscedasticity is variable variance.Jeremy J Taylor&nbsp;in his blog&nbsp;provides a great example of a distribution that is heteroscedastic. &nbsp;In his example the independent variable is ""age"" and the predictor variable is ""income"". &nbsp;The example discusses how incomes start to vary more as age increases (as some people earn more than others as they grow older). &nbsp;You can read his blog on this topic here."
2017,9,29,R 3.4.2 is released (with several bug fixes and a few performance improvements),https://www.r-statistics.com/2017/09/r-3-4-2-is-released-with-several-bug-fixes-and-a-few-performance-improvements/,"R 3.4.2 (codename &#8220;Short Summer&#8221;) was released yesterday. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned by David Smith R 3.4.2 includes a performance improvement for names: c() and unlist() are now more efficient in constructing the names(.) of their return value thanks to a proposal by Suharto Anggono. (PR#17284) The full list of bug &#8230; Continue reading ""R 3.4.2 is released (with several bug fixes and a few performance improvements)""
The post R 3.4.2 is released (with several bug fixes and a few performance improvements) first appeared on R-statistics blog."
2017,9,27,Standard Deviation versus Absolute Mean Deviation,http://themainstreamseer.blogspot.com/2017/09/standard-deviation-versus-absolute-mean.html,One of the first things that any student of statistics learns is 2 popular measures of descriptive statistics: mean and standard deviation.Has the approach to calculating Standard Deviation ever got you wondering about the need to square the distances from the mean in order to remove negatives instead of just using the average of the absolute values to eliminate negatives? &nbsp;Well you are certainly not alone.As it turns out squaring the distances from the mean and then calculating their square root to arrive at the Standard Deviation of a distribution is more as a result of convention than anything else. &nbsp;In fact there is a measure called the Absolute Mean Deviation that does not take the squared distances from the mean to eliminate negative values. &nbsp;Instead it just takes the absolute values of the differences from the mean and calculates the average of the sum of those values to determine deviation from the mean.The convention of course is to use Standard Deviation in most cases instead of Absolute Mean Deviation and therefore it is much more popular as a descriptive statistic than Absolute Mean Deviation. &nbsp;Here is an interesting article that discusses the difference between the two approaches and identifies situations where using the Absolute Mean Deviation may be advantageous.
2017,9,26,Forecasting: principles and practice (UWA),https://robjhyndman.com/seminars/uwa2017/,"Date: 26-28 September 2017
Location: The University Club University of Western Australia
This page is for people enrolled in my UWA 3-day workshop.
Prerequisites Please bring your own laptop with a recent version of R and RStudio installed along with the following packages and their dependencies:
 fpp2 readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,9,21,High-dimensional time series,https://robjhyndman.com/seminars/high-dimensional-time-series/,Presentation for ACEMS Monash node giving an overview of my current major research interests.
2017,9,13,Basic Statistics in R,http://themainstreamseer.blogspot.com/2017/09/basic-statistics-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:This notebook covers:Descriptive statisticsFrequency and contingency tablesCorrelations and covariancest-tests; andNonparametric statistics.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest R notebook: Basic #Statistics in R https://t.co/b3NmhNXI5X #DataScience #dsx #IBM #Bluemix #ibmaot #rstats h/t @kabacoff pic.twitter.com/AsIhr51Q5l— Venky Rao (@VRaoRao) September 13 2017
2017,9,13,Adding a .RData file to DSX in 5 easy steps,http://themainstreamseer.blogspot.com/2017/09/adding-rdata-file-in-5-easy-steps.html,I created a tutorial to show how users can add a .RData file to an R Jupyter Notebook in IBM's Data Science Experience (DSX) in 5 easy steps.My latest #R #notebook: Add a .RData file to a DSX R Notebook in 5 steps https://t.co/uznXwZWKSv #dsx #IBM #DataScience #rstats #ibmaot pic.twitter.com/plKuTwYDwt&mdash; Venky Rao (@VRaoRao) September 13 2017
2017,9,8,Prof. Joseph Keshet from BIU fools deep learning,https://bickson.blogspot.com/2017/09/prof-joseph-keshet-from-biu-fools-deep.html,My friend Joseph (Yossi) Keshet have recently released work for fooling deep learning systems. His work got a lot of attendion including MIT Technology Review&nbsp;and the New Scientist. Nice work!!
2017,9,8,Dataiku raised 28M$,https://bickson.blogspot.com/2017/09/dataiku-raised-28m.html,According to VentureBeat Dataiku just raised 28M$. Dataiku has a web based platform for data science.Here is my personal connection. Strangely last time I wed a couple I was wearing their t-shirt.Unrelated I just learned from my colleague Brian that Cloudera just acquired Fast Forward Labs which is the company behind Hilary Mason. I visited Hilary in her offices a couple of years ago and learned they had an interesting consulting models of sharing periodical tech reports for educating data scientists to become more proficient. Congrats Hilary!
2017,9,7,Looking for a new research assistant,https://robjhyndman.com/hyndsight/ra-wanted/,"I&rsquo;m currently looking for a new research assistant to help (primarily) with some modelling and R coding as part of a project on forecasting mobile phone sales. The position is likely to last for about 6&ndash;9 months and will be casual.
 Requirements    Based in Melbourne. I&rsquo;d rather not communicate remotely.    Able to work at least 20 hours per week. Some of that can be from home if necessary but you do need to be at Monash University (Clayton campus) at least some of the time."
2017,9,7,rOpenSci OzUnconference coming to Melbourne,https://robjhyndman.com/hyndsight/ozunconf2017/,"For a second year running there will be another rOpenSci OzUnconference in Australia. This one will be held in Melbourne on 26-27 October 2017.
Unlike regular conferences there are no talks and there is no pre-determined agenda. It brings together scientists developers and open data enthusiasts from academia industry government and non-profit to get together for a few days to work on R-related projects. The agenda is mostly decided during the conference itself and involves participants dividing into small groups to work on the projects of most interest to them."
2017,9,5,My videos for “Business Analytics using Data Mining” now publicly available!,http://www.bzst.com/2017/09/my-videos-for-business-analytics-using.html,Five years ago in 2012 I decided to experiment in improving my teaching by creating a flipped classroom (and semi-MOOC) for my course “Business Analytics Using Data Mining” (BADM) at the Indian...
2017,9,5,Deepgram - Audio Search with Deep Learning,https://bickson.blogspot.com/2017/09/deepgram-audio-search-with-deep-learning.html,A very interesting podcast by Sam Charrington who is interviewing Scott Stephenson from DeepGram. DeepGram is using deep learning activations for creating indexes that allows to search text in voice recordings.DeepGram have released Kur which is a high level abstraction of deep learning framework to allow quickly defining network layouts. But still writing the target persona is researchers with deep learning knowledge.A related Israeli startup is AudioBurst. &nbsp;They claim to use AI for indexing but it is not clear what they actually do. Another Israeli startup is Verbit. They seem to transcribe audio with humans going over the preliminary result.In addition my friend Yishay Carmiel is working on importing parts of Kaldi to TensorFLow. A recent Google developer blog post describes this effort. Yishay is leading a spinoff of Spoken called IntelligentWire who is also searching audio files using deep learning.Overall it seems that search in audio files using deep learning is getting hotter!
2017,9,4,Basic graphs in R,http://themainstreamseer.blogspot.com/2017/09/basic-graphs-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook provides a tutorial on:Bar box and dot plotsPie and fan chartsHistograms and kernel density plots.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts. My latest #R #notebook: Basic Graphs in R https://t.co/o7j97GwEUL #DataScience #dsx #IBM #ibmaot h/t @kabacoff pic.twitter.com/MBfZQgg4Y0&mdash; Venky Rao (@VRaoRao) September 4 2017
2017,9,2,Advanced Data Preparation in R,http://themainstreamseer.blogspot.com/2017/09/advanced-data-preparation-in-r.html,My&nbsp;latest publicly available R notebook&nbsp;created in&nbsp;IBM's Data Science Experience&nbsp;is&nbsp;here! &nbsp;This notebook addresses some advanced features available in R focusing on Data Preparation.I hope you enjoy this&nbsp;notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest #R #notebook: Advanced Data Preparation in R https://t.co/7Dvc9nCPK0 #DataScience #dsx #ibmaot #IBM h/t @kabacoff pic.twitter.com/cNpP45vpoR— Venky Rao (@VRaoRao) September 2 2017
2017,9,1,Data Science Book Review: Statistics Done Wrong,http://www.dataminingblog.com/data-science-book-review-statistics-done-wrong/,If you read this blog you are very likely to be involved in any kind of data collection manipulation or analysis. When not performed wisely your analysis will lead you to incorrect conclusions. Alex Reinhart in his book Statistics Done Wrong has listed several concepts that are key when analysing data such as statistical power [&#8230;]
2017,8,31,Finding distinct rows of a tibble,https://robjhyndman.com/hyndsight/distinct/,"I’ve been using R or its predecessors for about 30 years so I tend to I know a lot about R but I don’t necessarily know how to use modern R tools. Lately I’ve been teaching my students the tidyverse approach to data analysis which means that I need to unlearn some old approaches and to re-learn them using new tools. But old dogs and new tricks…
Yesterday I was teaching a class where I needed to extract some rows of a data set."
2017,8,28,Engine bleed air: a primer,http://themainstreamseer.blogspot.com/2017/08/engine-bleed-air-primer.html,"Use of Bleed Air in Aircraft Pneumatic Systems: A Primer(taken from Chapter 6 on Pneumatic Systems from the 3rd Edition of the book “Aircraft Systems” by Ian Moir and Allan Seabridge)The use of aircraft engines as a source of high pressure high temperature air can be understood by examining the characteristics of the turbofan engine.&nbsp; Modern engines “bypass” a significant portion of the mass flow past the engine and increasingly a small portion of the mass flow passes through the engine core or gas generation section.&nbsp; The ratio of bypass air to engine core air is called the bypass ratio and this can easily exceed 10:1 for the very latest civil engines; much higher than the 4 or 5:1 ratio for the previous generation.The characteristics of a modern turbofan engine are shown in figure 6.1.&nbsp; This shows the pressure (in psi) and the temperature (in degree centigrade) at various points throughout the engine for three conditions: ground idle take off power and in the cruise condition.It can be seen that in the least stressful condition – ground idle – the engine is in a state of equilibrium but that even at this low level the compressor air pressure is 50 psi and the temperature is 180 oC.&nbsp; At take-off conditions the compressed air soars to 410 psi / 540 oC.&nbsp; In the cruise condition the compressor air is at 150 psi / 400 oC.&nbsp; The engine is therefore a source of high pressure and high temperature air that can be “bled” for the engine to perform various functions around the aircraft.&nbsp; The fact that there are such considerable variations in air pressure and temperature for various engine conditions places an imposing control task upon the pneumatic system.&nbsp; Also the variations in engine characteristics between similarly rated engines of different manufacturers poses additional design constraints.&nbsp; Some aircraft such as the Boeing 777 offer three engine choices Pratt &amp; Whitney General Electric and Rolls-Royce and each of these engines has to be separately matched to the aircraft systems the loads of which may differ as a result of operator specified configurations.As well as the main aircraft engines the Auxiliary Power Unit (APU) is also a source of high pressure bleed air.&nbsp; The APU is in itself a small turbojet engine designed more from the viewpoint of an energy and power generator than a thrust provider which is the case for main engines.&nbsp; The APU is primarily designed to provide electrical and pneumatic power by a shaft driven generator and compressor.&nbsp; The APU is therefore able to provide an independent source of electrical power and compressed air while the aircraft is on the ground although it can be used as a backup provider of power while airborne.&nbsp; Some aircraft designs are actively considering the use of in-flight operable APUs to assist in in-flight engine re-lighting and to relieve the engines of offtake load in certain areas of the flight envelope.It is also usual for the aircraft to be designed to accept high pressure air from a ground power cart for aircraft engine starting.The three sources of pneumatic power provide the muscle or means by which the pneumatic is able to satisfy the aircraft demands.&nbsp; In a simplified form the pneumatic system may be represented by the interrelationships shown in figure 6.2 below:This simplified drawing – the ground air power source is omitted – shows how the aircraft High Pressure (HP) air sources provide bleed air which forms the primary source for the three major aircraft air related systems:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ice protection: the provision of hot air to provide anti icing of engine nacelles and the wing tailplane or fin leading edges; or to dislodge ice that has formed on the surfaces·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ECS and cooling: the provision of the main air source for environmental temperature control and cooling·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pressurization: the provision of a means by which the aircraft may be pressurized giving the crew and passengers a more comfortable operating environment.A simplified representation of this relationship is shown in figure 6.3.&nbsp; This example shows a twin-engine configuration typical of many business jets and regional jet transport aircraft.Bleed air from the engines is passed through a Pressure-Reducing Shut-Off Valve (PRSOV) which serves the function of controlling and when required shutting off the engine bleed air supply.&nbsp; Air downstream of the PRSOV may be used in a number of ways:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; By means of a cross flow Shut-Off Valve (SOV) the system may supply air to the opposite side of the aircraft during engine start or if the opposite engine is inoperative for any reason·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A SOV from the APU may be used to isolate the APU air supply·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SOVs provide isolation as appropriate to the left and right air conditioning packs and pressurization systems·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Additional SOVs provide the means by which the supply to left and right wing anti-icing systems may be shut off in the event that these functions are not requiredThis is a simplified model of the use of engine bleed air in pneumatic systems (ATA Chapter 36).&nbsp; A more comprehensive list of those aircraft systems with which bleed air is associated are listed as follows&nbsp; with the accompanying civil ATA chapter classification:·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Air conditioning (ATA Chapter 21)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cargo compartment heating (ATA Chapter 21)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wing and engine anti-icing (ATA Chapter 30)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Engine start (ATA Chapter 80)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thrust reverser (ATA Chapter 78)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hydraulic reservoir pressurization (ATA Chapter 29)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Rain repellent nozzles – aircraft windscreen (ATA Chapter 30)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Water tank pressurization and toilet waste (ATA Chapter 38)·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Air driven hydraulic pump (ADP) (ATA Chapter 29)     96     Normal  0          false  false  false    EN-US  X-NONE  X-NONE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /* Style Definitions */ table.MsoNormalTable  {mso-style-name:""Table Normal"";  mso-tstyle-rowband-size:0;  mso-tstyle-colband-size:0;  mso-style-noshow:yes;  mso-style-priority:99;  mso-style-parent:"""";  mso-padding-alt:0in 5.4pt 0in 5.4pt;  mso-para-margin:0in;  mso-para-margin-bottom:.0001pt;  mso-pagination:widow-orphan;  font-size:12.0pt;  font-family:Calibri;  mso-ascii-font-family:Calibri;  mso-ascii-theme-font:minor-latin;  mso-hansi-font-family:Calibri;  mso-hansi-theme-font:minor-latin;}                                                                                                       "
2017,8,25,Biggish time series data,https://robjhyndman.com/seminars/biggish-time-series-data/,Informal presentation for a UNSW research group giving an overview of some of my research interests over the last few years.
2017,8,25,Optimal forecast reconcilation,https://robjhyndman.com/seminars/unsw2017/,"Talk given at UNSW
Abstract
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2017,8,24,Data Preparation in R,http://themainstreamseer.blogspot.com/2017/08/data-preparation-in-r.html,My latest publicly available R notebook created in IBM's Data Science Experience is here! &nbsp;This notebook focuses on the basics of one of the most important aspects of Data Science: Data Preparation!I hope you enjoy this notebook. &nbsp;Please feel free to share and let me know your thoughts.My latest #R #notebook: Data Preparation in R https://t.co/5yXpG5DHFY #DataScience #dsx #ibmaot #IBM h/t @kabacoff pic.twitter.com/42j6hMRFaF— Venky Rao (@VRaoRao) August 24 2017 
2017,8,19,What is Empirical Risk Minimization,https://prateekvjoshi.com/2017/08/19/what-is-empirical-risk-minimization/,Even though it has an ornate name the underlying concept is actually quite simple and intuitive. The concept of Empirical Risk Minimization becomes relevant in the world of supervised learning. The actual goal of supervised learning is to find a &#8230; Continue reading &#8594;
2017,8,11,Visualizing and forecasting big time series data,https://robjhyndman.com/seminars/icml2017/,"ICML Time Series Workshop
Sydney Australia"
2017,8,11,ICML 2017 Thoughts,http://www.machinedlearnings.com/2017/08/icml-2017-thoughts.html,"ICML 2017 has just ended.  While Sydney is remote for those in Europe and North America the conference centeris a wonderful venue (with good coffee!) and the city is a lot of fun.  Everything went smoothly and the organizers did a great job.You can get a list of papers that I liked from my Twitter feed so instead I'd like to discuss some broad themes I sensed.Multitask regularization to mitigate sample complexity in RL.  Both in video games and in dialog it is useful to add extra (auxiliary) tasks in order to accelerate learning.Leveraging knowledge and memory.  Our current models are powerful function approximators but in NLP especially we need to go beyond ""the current example"" in order exhibit competence.Gradient descent as inference.  Whether it's inpainting with a GAN or BLUE score maximization with an RNN gradient descent is an unreasonably good inference algorithm.Careful initialization is important.  I suppose traditional optimization people would say ""of course"" but we're starting to appreciate the importance of good initialization for deep learning.  In particular start close to linear with eigenvalues close to 1. (Balduzzi et. al.  Poole et. al.)Convolutions are as good as and faster than recurrent models for NLP.  Nice work out of Facebook on causal convolutions for seq2seq.  This aligns with my personal experience: we use convolutional NLP models in production for computational performance reasons.Neural networks are overparameterized.  They can be made much sparser without losing accuracy (Molchanov et. al. Lobacheva et. al.).maluuba had the best party.  Woot!Finally I kept thinking the papers are all &ldquo;old&rdquo;.  While there were lots of papers I was seeing for the first time it nonetheless felt like the results were all dated because I've become addicted to &ldquo;fresh results&rdquo; on arxiv."
2017,7,30,Forecasting workshop in Perth,https://robjhyndman.com/hyndsight/forecasting-workshop-perth/,"On 26-28 September 2017 I will be running my 3-day workshop in Perth on &ldquo;Forecasting: principles and practice&rdquo; based on my book of the same name.
Topics to be covered include seasonality and trends exponential smoothing ARIMA modelling dynamic regression and state space models as well as forecast accuracy methods and forecast evaluation techniques such as cross-validation.
Workshop participants are expected to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,7,27,Some misc news,https://bickson.blogspot.com/2017/07/some-misc-news.html,I just learned my postdoc roommate Yisong Yue from Caltech released a new interesting paper: Factorized Variational Autoencoders for Modeling Audience Reactions to Movies: a joint work with Disney Research published @ CVPR 2017.Another interesting paper: Accelerating Innovation Through Analogy Mining just received the best paper award at KDD 2017. The paper is by Dafna Shahaf who studied with me at CMU and her student Tom Hope.An earlier related work of Dafna is a paper for identifying humor in cartoon captions.Misha Bilenko formerly from M$ released an source for gradient boosting. It seems to compete with XGBoost with the claim that it supports categorical variables as well. (In GraphLab Create we had an extended XGBoost with categorical variable support).
2017,7,26,Rational Inexuberance,http://www.machinedlearnings.com/2017/07/rational-inexuberance.html,"Recently Yoav Goldberg had a famous blog rant.  I appreciate his concern because the situation is game-theoretically dangerous: any individual researcher receives a benefit for aggressively positioning their work (as early as possible) but the field as a whole risks another AI winter as rhetoric and reality become increasingly divergent.  Yoav's solution is to incorporate public shaming in order to align local incentives with aggregate outcomes (c.f. reward shaping).I feel there is a better way as exemplified by a recent paper by Jia and Liang.  In this paper the authors corrupt the SQUAD dataset with distractor sentences which have no effect on human performance but which radically degrade the performance of the systems on the leaderboard.  This reminds me of work by Paperno et. al. on a paragraph completion task which humans perform with high skill and for which all state of the art NLP approaches fail miserably.  Both of these works clearly indicate that our current automatic systems only bear a superficial (albeit economically valuable) resemblance to humans.This approach to honest self-assessment of our capabilities is not only more scholarly but also more productive as it provides concrete tasks to consider.  At minimum this will result in improved technological artifacts.  Furthermore iterating this kind of goal-setting-and-goal-solving procedure many many times might eventually lead to something worthy of the moniker Artificial Intelligence.(You might argue that the Yoav Goldberg strategy is more entertaining but the high from the Yoav Goldberg way is a ""quick hit"" whereas having a hard task to think about has a lot of ""replay value"".)"
2017,7,25,Azure SQL Data Warehouse turns up the heat,https://athena-solutions.com/azure-sql-data-warehouse-turns-up-the-heat/,Rick Sherman is quoted extensively in this new TechTarget article: Azure SQL Data Warehouse turns up the heat expands processing power Abstract: Spirited competition is under way among […]
2017,7,18,Layman’s Guide to A/B Testing,https://algobeans.com/2017/07/19/laymans-guide-to-ab-testing/,A/B tests help you decide between two options A and B. Read this step-by-step guide on conducting your own A/B test to make the right decisions.
2017,7,17,"Tiered Architectures, Counterfactual Learning, and Sample Complexity",http://www.machinedlearnings.com/2017/07/tiered-architectures-counterfactual.html,I'm on a product team now and once again I find myself working on a tiered architecture: an &ldquo;L1&rdquo; model selects some candidates which are passed to an &ldquo;L2&rdquo; model which reranks and filters the candidates which are passed to an &ldquo;L3&rdquo; etc.  The motivation for this is typically computational e.g. you can index a DSSM model pretty easily but indexing a BIDAF model is more challenging.  However I think there are potential sample complexity benefits as well.I worry about sample complexity in counterfactual setups because I think it is the likely next source for AI winter.  Reinforcement learning takes a tremendous amount of data to converge which is why all the spectacular results from the media are in simulated environments self-play scenarios discrete optimization of a sub-component within a fully supervised setting or other situations where there is essentially infinite data.  In real life data is limited.So when I read Deep Reinforcement Learning in Large Discrete Action Spaces by Dulac-Arnold et. al. I noticed that the primary motivation was computational but figured another (more important?) benefit might be statistical.  Tiered architectures cannot overcome worst-case sample complexity bounds but I think in practice they are a good strategy for counterfactual setups.Tiered architectures admit semi-supervised approaches because an L1 model can often be initialized using unsupervised techniques (e.g. word embeddings sentence embeddings inverted indicies with tf-idf).  Learning the L2 model utilizing this L1 model only has a sample complexity based upon the number of candidates produced by the L1 model rather than the total number of candidates.  Of course learning the L1 still has a sample complexity based upon the total number of candidates but if the unsupervised initialization is good then it is ok that the L1 learns slowly.  Furthermore in practice the L1 hypothesis class is simpler (because of computational reasons) which mitigates the sample complexity.There was a workshop called ``coarse-to-fine inference'' at NIPS 2017 which presumably explored these ideas but I didn't attend it and their website is down.  Hopefully there will be another one I will attend!
2017,7,14,Using data to tackle poverty,https://robjhyndman.com/seminars/poverty2017/,"Talk to Year 11 students as part of the Monash Scholars Knowledge Summit on &ldquo;Ending Global Poverty&rdquo;.
Introductory remarks    Hans Rosling: New insights on poverty"
2017,7,12,IJF Best Paper Award 2017,https://robjhyndman.com/hyndsight/ijf-best-paper-award-2017/,"At the recent International Symposium on Forecasting I announced the awards for the best paper published in the International Journal of Forecasting in the period 2014&ndash;2015.
We make an award every two years to the best paper(s) published in the journal. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The selected papers are selected by vote of the editorial board. The best paper wins an engraved bronze plaque and US$1000."
2017,7,11,R 3.4.1 is released – with some Windows related bug-fixes,https://www.r-statistics.com/2017/07/r-3-4-1-is-released-with-some-windows-related-bug-fixes/,"R 3.4.1 (codename &#8220;Single Candle&#8221;) was released several days ago. You can get the latest binaries version from here. (or the .tar.gz source code from here). As mentioned last week by David Smith R 3.4.1 includes several Windows related bug fixed: including an issue sometimes encountered when attempting to install packages on Windows and problems displaying functions including Unicode characters &#8230; Continue reading ""R 3.4.1 is released &#8211; with some Windows related bug-fixes""
The post R 3.4.1 is released – with some Windows related bug-fixes first appeared on R-statistics blog."
2017,7,9,IIF Tao Hong Award 2016,https://robjhyndman.com/hyndsight/iif-hong-award-2016/,"A generous donation from Professor Tao Hong has funded this new award for papers on energy forecasting published in the International Journal of Forecasting. The award for 2016 is for papers published within 2013&ndash;2014. Next year we will award a paper published in 2015&ndash;2016 and we will make the award every two years after that.
The award decision was made by a committee consisting of Professors Pierre Pinson James Mitchell and Rob J Hyndman."
2017,7,8,Measuring the Stability of Machine Learning Algorithms,https://prateekvjoshi.com/2017/07/08/measuring-the-stability-of-machine-learning-algorithms/,When you think of a machine learning algorithm the first metric that comes to mind is its accuracy. A lot of research is centered on developing algorithms that are accurate and can predict the outcome with a high degree of &#8230; Continue reading &#8594;
2017,7,7,IIF Fellow Ralph Snyder,https://robjhyndman.com/hyndsight/ralph-snyder/,"
At the International Symposium on Forecasting last week my friend and colleague Ralph Snyder was made a Fellow of the International Institute of Forecasters."
2017,6,28,A Firm Foundation for Modernized BI & Analytics,https://athena-solutions.com/foundation-for-modernization/,You wouldn’t modernize a house without fixing its rickety foundation first. Similarly your BI and analytics environment needs a firm foundation before you can update it. […]
2017,6,26,Hex stickers for the forecast package,https://robjhyndman.com/hyndsight/forecast-stickers/,I&rsquo;ve caved in to the hex sticker craze and produced some hex stickers for the forecast package for R. If you attend a workshop I teach I&rsquo;ll give you one.
2017,6,22,Probabilistic outlier detection and visualization of smart metre data,https://robjhyndman.com/seminars/isea2017/,"Talk to be given at International Symposium on Energy Analytics (22-23 June 2017) Cairns Australia.
Standard time series analysis and visualization tools fail on smart metre data due to the sheer volume of available data (both in the time dimension and due to the large numbers of smart metres providing data). In addition smart metre data is often messy with missing observations periods where some metres are offline periods of relatively constant low level energy usage occasional days of unusually high demand and so on."
2017,6,21,Why I'm not celebrating the 2016 impact factors,https://robjhyndman.com/hyndsight/2016-impact-factors/,"Once every year the journal citation reports are released including journal impact factors. This year the International Journal of Forecasting 2-year impact factor has increased to 2.642 which is the highest it has been in the journal’s history and puts the journal higher than such notable titles as Journal of the American Statistical Association and just below Management Science.
The 2-year impact factor is the average number of citations for articles published in the previous 2 years."
2017,6,18,Data Science Book Review: Superforecasting,http://www.dataminingblog.com/2437-2/,Superforecasting &#8211; by Tetlock and Gartner &#8211; explains the huge study performed by Tetlock about the ability of people to predict future events (mainly geo-political). The closed questions (i.e. choose between yes/no) are far from real numbers you will predict in business forecasting. Tetlock discusses skills that have been identified as driving accurate forecasts. The [&#8230;]
2017,6,2,Coherent Probabilistic Forecasts for Hierarchical Time Series,https://robjhyndman.com/publications/probhts/,Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Although forecasts can be produced independently for each series in the hierarchy typically this does not lead to coherent forecasts — the property that forecasts add up appropriately across the hierarchy. State-of-the-art hierarchical forecasting methods usually reconcile these independently generated forecasts to satisfy the aggregation constraints. A fundamental limitation of prior research is that it has looked only at the problem of forecasting the mean of each time series.
2017,6,1,My new DataCamp course: Forecasting Using R,https://robjhyndman.com/hyndsight/datacamp/,"For the past few months I’ve been working on a new DataCamp course teaching Forecasting using R. I’m delighted that it is now available for anyone to do.
Course blurb Forecasting involves making predictions about the future. It is required in many situations such as deciding whether to build another power generation plant in the next ten years requires forecasts of future demand; scheduling staff in a call center next week requires forecasts of call volumes; stocking an inventory requires forecasts of stock requirements."
2017,5,29,Time Series in R: Forecasting and Visualisation,https://robjhyndman.com/seminars/forecasting-medascin/,"This is a one-day workshop given as part of the Melbourne Data Science Week.
Date: 29 May 2017
Presenters: Rob J Hyndman and Earo Wang
Location: KPMG Tower Two Collins Square 727 Collins St Melbourne
Prerequisites Please bring your own laptop with a recent version of R installed along with the following packages and their dependencies:
 devtools fpp2 knitr plotly shiny tidyverse  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2017,5,25,Prediction intervals for NNETAR models,https://robjhyndman.com/hyndsight/nnetar-prediction-intervals/,"The nnetar function in the forecast package for R fits a neural network model to a time series with lagged values of the time series as inputs (and possibly some other exogenous inputs). So it is a nonlinear autogressive model and it is not possible to analytically derive prediction intervals. Therefore we use simulation.
Suppose we fit a NNETAR model to the famous Canadian lynx data:
library(forecast) set.seed(2015) (fit &lt;- nnetar(lynx lambda=0."
2017,5,24,The second and third best features of LyX you aren’t using.,https://justindomke.wordpress.com/2017/05/24/the-second-and-third-best-features-of-lyx-you-arent-using/,LyX is a WYSIWYG editor for latex files. It&#8217;s a little bit clunky to use at first and isn&#8217;t perfect (thank you open source developers&#8211; I&#8217;m not ungrateful!) but after becoming familiar with it it&#8217;s probably the single piece of software that has most improved my productivity. I like it so much I use it &#8230; Continue reading The second and third best features of LyX you aren&#8217;t&#160;using. &#8594;
2017,5,23,ISI Karl Pearson Prize for 2017,https://robjhyndman.com/hyndsight/kpprize2017/,"Recently I was privileged to sit on the committee that selects the winner of the Karl Pearson Prize. KP was of course an early mathematical statistician famous for many commonly-used statistical methods and tools including histograms the correlation coefficient the method of moments p-values the chi-squared test and principal components analysis. He is also infamous for his highly racist views support for eugenics anti-semitism and for refusing a knighthood.
All that aside the job of the committee was to select an English-language article or book published in the last 30 years that has made a stand-alone research contribution and which has had major influence on one or more of statistical theory statistical methodology statistical practice and application."
2017,5,14,Ergodicity In The World Of IoT,https://prateekvjoshi.com/2017/05/14/ergodicity-in-the-world-of-iot/,Ergodicity is one of the most important concepts in statistics. More importantly it has a lot of real world applications. In this case it&#8217;s applicable to the staggering number of internet connected devices in the world of Internet of Things (IoT). &#8230; Continue reading &#8594;
2017,5,13,Forecasting with temporal hierarchies,https://robjhyndman.com/publications/temporal-hierarchies/,This paper introduces the concept of Temporal Hierarchies for time series forecasting. A temporal hierarchy can be constructed for any time series by means of non-overlapping temporal aggregation. Predictions constructed at all aggregation levels are combined with the proposed framework to result in temporally reconciled accurate and robust forecasts. The implied combination mitigates modelling uncertainty while the reconciled nature of the forecasts results in a unified prediction that supports aligned decisions at different planning horizons: from short-term operational up to long-term strategic planning.
2017,5,9,Party Data Model in Master Data Management,https://www.mdmgeek.com/2017/05/08/party-data-model-in-master-data-management/,"Many MDM initiatives center around customer data. A standard definition used in the industry is “Party” and “Party Domain” is a shared phrase used amongst MDM practitioners. Data model design around party domain is a critical area to address during MDM. In this blog I share my observations and suggest best practices. Party usually has [&#8230;]
The post Party Data Model in Master Data Management appeared first on MDMgeek."
2017,5,4,Handgun acquisitions in California after two mass shootings,https://robjhyndman.com/hyndsight/handguns/,My new paper (Studdert et al 2017) on &ldquo;Handgun acquisitions in California after two mass shootings&rdquo; has been attracting some press.  Here are some of the news items I&rsquo;ve found:
2017,5,3,Monthly seasonality,https://robjhyndman.com/hyndsight/monthly-seasonality/,"I regularly get asked why I don&rsquo;t consider monthly seasonality in my models for daily or sub-daily time series. For example this recent comment on my post on seasonal periods or this comment on my post on daily data. The fact is I&rsquo;ve never seen a time series with monthly seasonality although that does not mean it does not exist.
Monthly seasonality is defined as a regular pattern that recurs every month in data that is observed more frequently than monthly."
2017,5,2,Handgun acquisitions in California after two mass shootings,https://robjhyndman.com/publications/handguns/,"Background Mass shootings are common in the United States. They are the most visible form of firearm violence. Their effect on personal decisions to purchase firearms is not well understood.
Objective To determine changes in handgun acquisition patterns after the mass shootings in Newtown Connecticut in 2012 and San Bernardino California in 2015.
Design Time-series analysis using seasonal autoregressive integrated moving-average (SARIMA) models.
Setting California.
Population Adults who acquired handguns between 2007 and 2016."
2017,4,30,Converting to blogdown,https://robjhyndman.com/hyndsight/blogdown/,"This website has gone through several major updates over the years. It began in 1993 as some handcrafted html files transitioned to Joomla and later to Wordpress. Then it slowly grew into a collection of ten connected Wordpress installations that became increasingly difficult to maintain and rather slow.
So I&rsquo;ve now converted the entire site to Blogdown/Hugo. Nearly 700 pages of wordpress content have been translated to markdown. I decided to drop a few parts of the site notably the pages for my 1998 forecasting textbook."
2017,4,25,Grouped functional time series forecasting: an application to age-specific mortality rates,https://robjhyndman.com/publications/grouped-functional-time-series-forecasting-an-application-to-age-specific-mortality-rates/,Age-specific mortality rates are often disaggregated by different attributes such as sex state and ethnicity. Forecasting age-specific mortality rates at the national and sub-national levels plays an important role in developing social policy. However independent forecasts of age-specific mortality rates at the sub-national levels may not add up to the forecasts at the national level. To address this issue we consider the problem of reconciling age-specific mortality rate forecasts from the viewpoint of grouped univariate time series forecasting methods (Hyndman et al 2011) and extend these methods to functional time series forecasting where age is considered as a continuum.
2017,4,24,R 3.4.0 is released – with new speed upgrades and bug-fixes,https://www.r-statistics.com/2017/04/r-3-4-0-is-released-with-new-speed-upgrades-and-bug-fixes/,"R 3.4.0 (codename &#8220;You Stupid Darkness&#8221;) was released 3 days ago. You can get the latest binaries version from here. (or the .tar.gz source code from here). The full list of bug fixes and new features is provided below. As mentioned two months ago by David Smith R 3.4.0 indicates several major changes aimed at improving the performance of R in &#8230; Continue reading ""R 3.4.0 is released &#8211; with new speed upgrades and bug-fixes""
The post R 3.4.0 is released – with new speed upgrades and bug-fixes first appeared on R-statistics blog."
2017,4,22,You deserve better than two-sided finite differences,https://justindomke.wordpress.com/2017/04/22/you-deserve-better-than-two-sided-finite-differences/,In calc 101 the derivative is derived as . So if you want to estimate a derivative an easy way to do so would be to just pick some small and estimate: This can work OK. Let&#8217;s look at an example of trying to calculate the derivative of  using a range of different What&#8217;s &#8230; Continue reading You deserve better than two-sided finite&#160;differences &#8594;
2017,4,18,Follow-up Forecasting Forum,https://robjhyndman.com/seminars/eindhoven2017/,Follow up to my October 2016 3-day workshop in Eindhoven Netherlands
2017,4,4,Time Series Analysis with Generalized Additive Models,https://algobeans.com/2017/04/04/laymans-tutorial-time-series-analysis/,Whenever you spot a trend plotted against time you would be looking at a time series. The de facto choice for studying financial market performance and weather forecasts time series are one of the most pervasive analysis techniques because of its inextricable relation to time - we are always interested to foretell the future.
2017,4,4,Software for honours students,https://robjhyndman.com/hyndsight/software-for-honours-students/,"I spoke to our new crop of honours students this morning. Here are my slides example files and links.
 Managing References  Mendeley Zotero Paperpile  Data analysis and computation  Download R Download Rstudio Online R tutorial R packages for time series R packages for econometrics R packages for finance  Writing your thesis LaTeX  Windows: Download MikTeX Mac OSX: Download MacTeX Linux: Check your usual software source for TeXLive; otherwise install TeX Live directly."
2017,4,4,Software for honours students,https://robjhyndman.com/seminars/2017-04-04-software-for-honours-students/,"I spoke to our new crop of honours students this morning. Here are my slides example files and links.
 Managing References  Mendeley Zotero Paperpile  Data analysis and computation  Download R Download Rstudio Online R tutorial R packages for time series R packages for econometrics R packages for finance  Writing your thesis LaTeX  Windows: Download MikTeX Mac OSX: Download MacTeX Linux: Check your usual software source for TeXLive; otherwise install TeX Live directly."
2017,3,30,Monash Rmarkdown templates on github,https://robjhyndman.com/hyndsight/monash-rmarkdown-templates-on-github/,"Rmarkdown templates for staff and students in my department are now available on github.
For a PhD thesis fork the repository MonashThesis.
For an Honours thesis fork the repository MonashHonoursThesis.
For beamer slides with a Monash Business School theme use the binb package.
For other templates install the R package MonashEBSTemplates R package. This provides templates for
 working papers exams letters reports"
2017,3,28,A note on upper bounds for forecast-value-added relative to naïve forecasts,https://robjhyndman.com/publications/fvanaive/,In forecast value added analysis the accuracy of relatively sophisticated forecasting methods is compared to that of naïve 1 forecasts to see whether the extra costs and effort of implementing them are justified. In this note we derive a ratio that indicates the upper bound of a forecasting method’s accuracy relative to naïve 1 forecasts when the mean squared error is used to measure one-period-ahead accuracy. The ratio is applicable when a series is stationary or when its first differences are stationary.
2017,3,28,shinyHeatmaply – a shiny app for creating interactive cluster heatmaps,https://www.r-statistics.com/2017/03/shinyheatmaply-a-shiny-app-for-creating-interactive-cluster-heatmaps/,"My friend Jonathan Sidi and I (Tal Galili) are pleased to announce the release of shinyHeatmaply (0.1.0): a new Shiny application (and Shiny gadget) for creating interactive cluster heatmaps. shinyHeatmaply is based on the heatmaply R package which strives to make it easy as possible to create interactive cluster heatmaps. The app introduces a functionality that saves to disk a self &#8230; Continue reading ""shinyHeatmaply &#8211; a shiny app for creating interactive cluster heatmaps""
The post shinyHeatmaply – a shiny app for creating interactive cluster heatmaps first appeared on R-statistics blog."
2017,3,25,Why now is the time for dialog,http://www.machinedlearnings.com/2017/03/why-now-is-time-for-dialog.html,I'm working on a task-oriented dialog product and things are going surprisingly well from a business standpoint.  It turns out that existing techniques are sufficient to substitute some portion of commercial dialog interactions from human to machine mediated with tremendous associated cost savings which exceed the cost of developing the automatic systems.  Here's the thing that is puzzling: the surplus is so large that as far as I can tell it would have been viable to do this 10 years ago with then-current techniques.  All the new fancy AI stuff helps but only to improve the margins.  So how come these businesses didn't appear 10 years ago?I suspect the answer is that a format shift has occurred away from physical transactions and voice mediated interactions to digital transactions and chat mediated interactions.  The movement away from voice is very important: if we had to try and do this using ASR even today it probably wouldn't work.  Fortunately today you chat with your cable company rather than talking to them.  That shift was motivated by cost savings: a human agent can handle multiple concurrent chat sessions more easily than multiple concurrent voice conversations.  However it requires most of your customers to have a computer smartphone or other device rather than an old-school telephone.  The continuing dominance of e-commerce over physical stores is also a factor (RIP Sears).  In e-commerce human salespersons increasingly assist customers in transactions via live chat interfaces.  Once again what starts as a more effective way of deploying human resources becomes the vector by which automation increasingly handles the workload.The end game here is that the number of people employed in retail goes down but that their compensation goes up.  That is because the machines will increasingly handle the routine aspects of these domains leaving only the long tail of extremely idiosyncratic issues for the humans to resolve.  Handling these non-routine issues will require more skill and experience and therefore demand higher compensation (also an increasing part of the job will be to structure the torso of non-routine issues into something that the machines can handle routinely i.e. teaching the machines to handle more; this is analogous to programming and will also demand higher compensation).
2017,3,21,Probabilistic energy forecasting for smart grids and buildings,https://robjhyndman.com/seminars/melbuni-smartgrids/,"Energy Systems Integration in smart buildings communities and microgrids
The University of Melbourne Melbourne Australia
21-22 March 2017
Organised by The University of Melbourne and the International Institute for Energy Systems Integration (iiESI)"
2017,3,18,Academic phishing,https://robjhyndman.com/hyndsight/academic-phishing/,"Invitations to write for bogus journals and speak at bogus conferences keep rolling in. Here is one I received today.
 Dear Dr. Rob J. Hyndman It is our great pleasure to welcome you to join in Part 2: Knowledge Economy Symposium of GCKE-2017 which will be held in Qingdao China during September 19-21 2017. And we cordially invite you to propose a Speech on your recent research of Corrigendum to: “Hierarchical forecasts for Australian domestic tourism” [International Journal of Forecasting 25 (2009) 146–166]&hellip; ."
2017,3,14,Data mining algorithms: how many dummies?,http://www.bzst.com/2017/03/data-mining-algorithms-how-many-dummies.html,"There's lots of posts on ""k-NN for Dummies"". This one is about ""Dummies for k-NN""

Categorical predictor variables are very common. Those who've taken a Statistics course covering linear (or..."
2017,3,7,Follow-up forecasting forum in Eindhoven,https://robjhyndman.com/hyndsight/eindhoven2/,"Last October I gave a 3-day masterclass on &ldquo;Forecasting with R&rdquo; in Eindhoven Netherlands. There is a follow-up event planned for Tuesday 18 April 2017. It is particularly designed for people who attended the 3-day class but if anyone else wants to attend they would be welcome.
Please register here if you want to attend.The preliminary schedule is as follows.
10.00 -- 11.00  New developments in forecasting using R   forecast v8."
2017,3,7,WOMBAT MeDaScIn 2017,https://robjhyndman.com/hyndsight/wombat-medascin-2017/,"Last year we had WOMBAT (Workshop Organized by the Monash Business Analytics Team) at the zoo and MeDaScIn (Melbourne Data Science Initiative) in the city.
This year we are combining forces to hold WOMBAT MeDaScIn 2017.
There will be four days of tutorials (Monday 29 May to Thursday 1 June) and the main conference on Friday 2 June. We have an impressive range of local and international presenters including Yihui Xie (author of Rmarkdown Knitr Bookdown Blogdown and more) Di Cook (data visualization guru) Stephanie Kovalchik (Data Scientist at Tennis Australia) Amy Shi-Nash (Head of Data Science at Commonwealth Bank of Australia) Graham Williams (Director of Data Science at Microsoft) and many more."
2017,3,7,R 3.3.3 is released!,https://www.r-statistics.com/2017/03/r-3-3-3-is-released/,"R 3.3.3 (codename &#8220;Another Canoe&#8221;) was released yesterday You can get the latest binaries version from here. (or the .tar.gz source code from here). The full list of bug fixes and new features is provided below. A quick summary by David Smith: R 3.3.3 fixes an issue related to attempting to use download.file on sites that automatically redirect from http &#8230; Continue reading ""R 3.3.3 is released!""
The post R 3.3.3 is released! first appeared on R-statistics blog."
2017,3,1,IJF Best Paper Award 2014-2015,https://robjhyndman.com/hyndsight/ijf-nominations1415/,Every two years we award a prize for the best paper published in the International Journal of Forecasting. It is now time to identify the best paper published in the IJF during 2014 and 2015. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The prize is US$1000 plus an engraved plaque. I will present the prize at the ISF in Cairns in late June.
2017,2,28,forecast 8.0,https://robjhyndman.com/hyndsight/forecast8/,"In what is now a roughly annual event the forecast package has been updated on CRAN with a new version this time 8.0.
A few of the more important new features are described below.
Check residuals A common task when building forecasting models is to check that the residuals satisfy some assumptions (that they are uncorrelated normally distributed etc.). The new function checkresiduals makes this very easy: it produces a time plot an ACF a histogram with super-imposed normal curve and does a Ljung-Box test on the residuals with appropriate number of lags and degrees of freedom."
2017,2,27,Invited sessions at ISF2017,https://robjhyndman.com/hyndsight/invited-sessions-at-isf2017/,We are still looking for a few more invited sessions for the International Symposium on Forecasting to be held in Cairns Australia 25-28 June 2017.An invited session consists of 3 or 4 talks around a specific forecasting theme. You are allowed to be one of the speakers in a session you organize (although it is not necessary). Therefore if you know what you are planning to speak about all you need to do is find 2 or 3 other speakers who will speak on a related topic and invite them.
2017,2,21,Cauchy Sequences In The Real World,https://prateekvjoshi.com/2017/02/21/cauchy-sequences-in-the-real-world/,Sequences occur everywhere in our daily life. Some of the examples include sensor data stock market quotes speech signals and many more. A sequence is a collection of elements where each element is indexed. Repetitions are allowed in this case which &#8230; Continue reading &#8594;
2017,2,16,Software Engineering vs Machine Learning Concepts,http://www.machinedlearnings.com/2017/02/software-engineering-vs-machine.html,Not all core concepts from software engineering translate into the machine learning universe.  Here are some differences I've noticed.Divide and Conquer A key technique in software engineering is to break a problem down into simpler subproblems solve those subproblems and then compose them into a solution to the original problem.  Arguably this is the entire job recursively applied until the solution can be expressed in a single line in whatever programming language is being used.  The canonical pedagogical example is the Tower of Hanoi.Unfortunately in machine learning we never exactly solve a problem.  At best we approximately solve a problem.  This is where the technique needs modification: in software engineering the subproblem solutions are exact but in machine learning errors compound and the aggregate result can be complete rubbish.  In addition apparently paradoxical situations can arise where a component is &ldquo;improved&rdquo; in isolation yet aggregate system performance degrades when this &ldquo;improvement&rdquo; is deployed (e.g. due to the pattern of errors now being unexpected by downstream components even if they are less frequent).Does this mean we are doomed to think holistically (which doesn't sound scalable to large problems)?  No but it means you have to be defensive about subproblem decomposition.  The best strategy when feasible is to train the system end-to-end i.e. optimize all components (and the composition strategy) together rather than in isolation.  Often this is not feasible so another alternative (inspired by Bayesian ideas) is to have each component report some kind of confidence or variance along with the output in order to facilitate downstream processing and integration.In practice when systems get to a particular scope there needs to be decomposition in order to divide the work up amongst many people.  The fact that this doesn't work right now in machine learning is a problem as elegantly described by Leon Bottou in his ICML 2015 invited talk.Speaking of another concept that Leon discussed $\ldots$Correctness In software engineering an algorithm can be proven correct in the sense that given particular assumptions about the input certain properties will be true when the algorithm terminates.  In (supervised) machine learning the only guarantee we really have is that if the training set is an iid sample from a particular distribution then performance on another iid sample from the same distribution will be close to that on the training set and not too far from optimal.Consequently anyone who practice machine learning for a living has an experimental mindset.  Often times I am asked whether option A or option B is better and most of the time my answer is &ldquo;I don't know let's try both and see what happens.&rdquo;  Maybe the most important thing that people in machine learning know is how to assess a model in such a way that is predictive of generalization.  Even that is a &ldquo;feel&rdquo; thing: identifying and preventing leakage between training and validation sets (e.g. by stratified and temporal sampling) is something you learn by screwing up a few times; ditto for counterfactual loops.  Kaggle is great for learning about the former but the latter seems to require making mistakes on a closed-loop system to really appreciate.Experimental &ldquo;correctness&rdquo; is much weaker than the guarantees from other software and there are many ways for things to go badly.  For example in my experience it is always temporary: models go stale it just always seems to happen.  Ergo you need to plan to be continually (hence automatically) retraining models.Reuse This one is interesting.  Reuse is the key to leverage in traditional software engineering: it's not just more productive to reuse other code but every line of code you write yourself is an opportunity to inject defects.  Thus reuse not only allows you to move faster but also make less mistakes: in return you must pay the price of learning how to operate a piece of software written by others (when done well this price has been lowered through good organization documentation and community support).  Some aspects of machine learning exhibit exactly the same tradeoff.  For instance if you are writing your own deep learning toolkit recognize that you are having fun.  There's nothing wrong with having fun and pedagogical activities are arguably better than playing video games all day.  However if you are trying to get something done you should absolutely attempt to reuse as much technology as you can which means you should be using a standard toolkit.  You will move faster and make less mistakes once you learn how to operate the standard toolkit.Machine learning toolkits are &ldquo;traditional software&rdquo; however and are designed to be reused.  What about model reuse?  That can be good as well but the caveats about decomposition above still apply.  So maybe you use a model which produces features from a user profile as inputs to your model.  Fine but you should version the model you depend upon and not blindly upgrade without assessment or retraining.  Reusing the internals of another model is especially dangerous as most machine learning models are not identifiable i.e. have various internal symmetries which are not determined by the training procedure.  Couple an embedding to a tree for instance and when the next version of the embedding is a rotation of the previous one you can watch your performance go to crap immediately.Basically model reuse creates strong coupling between components which can be problematic if one component is changed.  Testing I find the role of software testing in machine learning to be the trickiest issue of all.  Without a doubt testing is necessary but the challenge in using something like property-based testing is that the concept that is being captured by the machine learning component is not easily characterized by properties (otherwise you would write it using non-ml software techniques).  To the extent there are some properties that the ml component should exhibit you can test for these but unless you incorporate these into the learning procedure itself (e.g. via parameter tying or data augmentation) you are likely to have some violations of the property that are not necessarily indicative of defects.Having a &ldquo;extra-test&rdquo; data set of with minimal acceptable quality is a good idea: this could be easy examples that &ldquo;any reasonable model&rdquo; should get correct.  There's also self-consistency: at Yahoo they used to ship models with a set of input-output pairs that were computed with the model when it was put together and if the loaded model didn't reproduce the pairs the model load was cancelled.  (That should never happen right?  Surprise!  Maybe you are featurizing the inputs using a library with a different version or something.)  Monitoring the metrics (proxy and true) of deployed models is also good for detecting problems.  If the proxy metric (i.e. the thing on which you actually trained your model and estimated generalization performance) is going south the inputs to your model are changing somehow (e.g. nonstationary environment change in feature extraction pipeline); but if the proxy metric is stable while the true metric is going south the problem might be in how the outputs of your model are being leveraged.Unfortunately what I find is many software systems with machine learning components are tested in a way that would make traditional software engineers cringe: we look at the output to see if it is reasonable.  Crazy!  As machine learning becomes a more pervasive part of software engineering this state of affairs must change.
2017,2,15,Forecasters: bring your family to Cairns,https://robjhyndman.com/hyndsight/isf-social/,"Courtesy of Tourism Tropical North Queensland   We know Australia is a long way to come for many forecasters so we are making it easy for you to bring your families along to the International Symposium on Forecasting and have a vacation at the same time.
During the International Symposium on Forecasting there will be a social program organized for family and friends. Monday 26 June Official Partners Tour: Kuranda Train &amp; Skyrail."
2017,2,15,The Australian Macro Database,https://robjhyndman.com/hyndsight/ausmacrodata/,AusMacroData is a new website that encourages and facilitates the use of quantitative publicly available Australian macroeconomic data. The Australian Macro Database hosted at ausmacrodata.org provides a user-friendly front end for searching among over 40000 economic variables and is loosely based on similar international sites such as the Federal Reserve Economic Database (FRED). In total data on 40304 variables are available for download from AusMacroData. The majority of variables are sourced from the Australian Bureau of Statistics (ABS) and include data on national accounts balance of payments and trade housing and finance labour force consumer price indices.
2017,2,14,The Australian Macro Database: An online resource for macroeconomic research in Australia,https://robjhyndman.com/publications/ausmacrodata/,A website that encourages and facilities the use of quantitative publicly available Australian macroeconomic data is introduced. The Australian Macro Database hosted at ausmacrodata.org provides a user friendly front end for searching among over 40000 economic variables sourced from the Australian Bureau of Statistics and the Reserve Bank of Australia. The search box tags and categories used to facilitate data retrieval are described in detail. Known issues with the website and future plans are discussed in the conclusion.
2017,2,13,Data Science Summit Europe 2017,https://bickson.blogspot.com/2017/02/data-science-summit-europe-2017.html,The 3rd Data Science Summit Europe is coming! This year I am not involved in the organization she it should probably be a better event :-) Save the date - May 29 2017 in Jerusalem. The date was picked just after O'Reilly Strata London 2017 thus the conference will attract many speakers and guests from abroad.The keynote speaker is my friend Dr. Ben Lorica chief scientist of O'Reilly Media and the content organizer for O'Reilly Strata and O'Reilly AI conferences.Hope to see you there!
2017,2,2,Forecasting practitioner talks at ISF 2017,https://robjhyndman.com/hyndsight/isf2017-practitioners/,The International Symposium on Forecasting is a little unusual for an academic conference in that it has always had a strong presence of forecasters working in business and industry as well as academic forecasters mostly at universities. We value the combination and interaction as it helps the academics understand the sorts of problems facing forecasters in practice and it helps practitioners stay abreast of new methods and developments coming out of forecasting research.
2017,1,31,Associations between outdoor fungal spores and childhood and adolescent asthma hospitalisations,https://robjhyndman.com/publications/jaci2016/,"Background: Childhood asthma is a significant public health problem and severe exacerbation can result in diminished quality of life and hospitalisation.
Objective: To examine the contribution of outdoor fungi to childhood and adolescent asthma hospitalisations
Methods: The Melbourne Air Pollen Children and Adolescent (MAPCAH) study is a case-crossover study of 644 children and adolescents (aged 2-17 years) hospitalised for asthma between September 2009 and December 2011. MAPCAH collected individual data on human rhinovirus (HRV) infection and fungal sensitisation; and daily counts of ambient concentrations of fungal spores pollen and air pollutants."
2017,1,28,Reinforcement Learning and Language Support,http://www.machinedlearnings.com/2017/01/reinforcement-learning-and-language.html,What is the right way to specify a program that learns from experience?  Existing general-purpose programming languages are designed to facilitate the specification of any piece of software.  So we can just use these programming languages for reinforcement learning right?  Sort of.Abstractions matterAn analogy with high performance serving might be helpful.  An early influential page on high performance serving (the C10K problem by Dan Kegel) outlines several I/O strategies.  I've tried many of them.  One strategy is event-driven programming where a core event loop monitors file descriptors for events and then dispatches handlers.  This style yields high performance servers but is difficult to program and sensitive to programmer error.  In addition to fault isolation issues (if all event are running in the same address space) this style is sensitive to whenever any event handler takes too long to execute (e.g. hidden blocking I/O calls computationally intensive operations etc.).  In contrast thread-based programming allowed you to pretend that you were the only handler running.  It was less computationally efficient and still had fault isolation issues but it was easier to reason about.  (Subsequently I started getting into Erlang because it essentially tried to bake user-space threading with fault isolation into the language which was even better.)I don't know what the state-of-the-art is in high performance serving now I'm a bit out of that game.  The main point is that all programming languages are not created equal in that they create different cognitive burdens on the programmer and different computational burdens at runtime.  I could use an existing language (at that time C++) in one of two ways (cooperative scheduling vs. pre-emptive scheduling) or I could use a different language (Erlang) that was designed to mitigate the tradeoff.Imperative specification with automatic credit assignmentAs previously stated the difference between the programs we'd like to specify now versus the ones specified in the past is that we want our programs to be able to learn from experience.  As with high-performance serving we'd like to balance the cognitive burden on the programmer with the computational burden imposed at runtime (also possibly the statistical burden imposed at runtime; computational burdens correspond to resources such as time or space whereas the statistical burden corresponds to data resources).Within the current &ldquo;AI summer&rdquo; one idea that become popular is automatic differentiation.  Full AD means that essentially any language construct can be used to define a function and the computation to compute the gradient of the function with respect to the input is provided &ldquo;for free.&rdquo;  A language equipped with AD which is computing a (sub-)differentiable function can learn from experience in the sense of moving closer to a local optimum of a loss function.  Deep learning toolkits implement AD to various degrees with some frameworks (e.g. Chainer) aggressively pursuing the idea of allowing arbitrary language constructs when specifying the forward computation.The ability to use arbitrary language constructs becomes increasingly important as inference becomes more complicated.  Simple inference (e.g. classification or ranking) is easy to reason about but beyond that it quickly becomes a major source of defects to 1) specify how the output of a machine learning model is used to synthesize a complete system and 2) specify how the data obtained from running that complete system is used to update the model.The problem is clearly visible in the field of structured prediction.  &ldquo;Structured prediction&rdquo; of course is a somewhat ridiculous term analogous to the term &ldquo;nonlinear systems analysis&rdquo;; in both cases a simpler version of the problem was solved initially (classification and linear systems analysis respectively) and then an umbrella term was created for everything else.  Nonetheless Hal Daume has a good definition of structured prediction which is making multiple predictions on a single example and experiencing a joint (in the decisions) loss.  (He also has a Haiku version of this definition.)Because inference in structured prediction is complicated the ideas of imperative specification and automated credit assignment were essentially reinvented for structured prediction.  The technique is outlined in an Arxiv paper by Chang et. al. but fans of Chainer will recognize this as the analog of &ldquo;define-by-run&rdquo; for structured prediction.  (Note the optimization strategy here is not gradient descent at least not on the forward computation but rather something like a policy gradient method which translates to a discrete credit assignment problem over the predictions made by the forward computation.)One way to view episodic RL is structured prediction with bandit feedback: structured prediction is fully observed analogous to supervised learning in that it is possible to compute the loss of any sequence of decisions given a particular input.  In reinforcement learning you have bandit feedback i.e. you only learn about the loss associated with the sequence of decisions actually taken.  While this isn't the only way to view episodic RL it does facilitate connecting with some of the ideas of the paper mentioned in the previous paragraph.A Motivating ExampleHere's an example which will hopefully clarify things.  Suppose we want to build an interactive question-answering system in which users pose questions and then the system can optionally ask a (clarifying) question to the user or else deliver an answer.  We can view this as an episodic RL problem where the user statements are observations system questions are actions system answers are more actions and the episode ends as soon as we deliver an answer.What I'd like to do is specify the computation something like this pseudo-python:def interactive_qa_episode():  uq = get_user_question()  qapairs = []  sysaction = get_next_system_action(uq qapairs)  while (sysaction.is_question):    ua = get_user_answer(sysaction.utterance)    qapairs.append((sysactionua))    sysaction = get_next_system_action(uq qapairs)  deliverAnswer(sysaction.utterance)It is pretty clear what is going on here: we get a user question conditionally ask questions and then deliver an answer.  Before the advent of machine learning an implementer of such a system would attempt to fill out the unspecified functions above: in particular get_next_system_action is tricky to hand specify.  What we would like to do is learn this function instead.It would be nice to use decorators to achieve this.  First to learn we need some idea of doing better or worse so assume after delivering an answer there is some way to decide how satisfied the user is with the session (which ceterus perebus should be monotonically decreasing with the number of questions asked to encourage expediency):@episodicRLdef interactive_qa_episode():  uq = get_user_question()  qapairs = []  sysaction = get_next_system_action(uq qapairs)  while (sysaction.is_question):    ua = get_user_answer(sysaction.utterance)    qapairs.append((sysactionua))    sysaction = get_next_system_action(uq qapairs)# this next line is the only change to the original function  reward = deliverAnswer(sysaction.utterance) All too easy!  Pseudo-code is so productive.  We can even imagine updating reward multiple times with the decorator keeping track of the reward deltas for improved credit assignment.Now some magic metaprogramming kicks in and converts this into a model being trained with an RL algorithm (e.g. a value iteration method such as q-learning or a policy iteration method such as bandit LOLS).  Or does it?  We still haven't said which functions are to be learned and which are hand-specified.  The default will be hand-specified so we will decorate one function.@learnedFunctiondef get_next_system_action(uq qapairs):  ...Now we get into some thorny issues.  We need to specify this functions ultimately in terms of a parameterized model like a neural network; we'll have to say what the initial representation is that is computed from variables like uq and qapairs; and we'll have to say how the output of the model is mapped onto an actual decision.  Just to keep moving let's assume there is a fixed small set of system questions and system answers.action_table = [ ... ] # list containing action mapping@learnedFunctiondef get_next_system_action(uq qapairs):  not_allowed_action_ids = [ sysa.action_id for (sysa _) in qapairs ]  action_id = categorical_choice(uq: uq                                 qapairs: qapairs                                 not_allowed_action_ids: not_allowed_action_ids                                 tag: 'nextsystemaction')  return action_table[action_id]categorical_choice is the representation of a forced choice from one of a set of possibilities.  For small action spaces this could be directly implemented as an output per action but for large action spaces this might be implemented via action embeddings with an information-retrieval style cascading pipeline.Great right?  Well some problems remain. The best model structure (i.e. policy class) for the choice requires some specification by the programmer e.g. a convolutional text network vs. an iterated attention architecture.  Ideally this specification is distinct from the specification of inference so that many modeling ideas can be tried.  That's the purpose of the tag argument to join with a separate specification of the learning parameters.  (If not provided sane default tags could be generated during compilation.) As indicated in the previous post bootstrapping is everything.  So an initial implementation of get_next_system_action needs to be provided.  Maybe this reduces to providing an initial setting of the underlying model but maybe it doesn't depending upon the initialization scenario.  Note if initialization is done via simulation or off-policy learning from historical data these could be supported by facilitating the mockup of the I/O functions get_user_question and get_user_answer.  Another common scenario is that a not-learned function is provided as a reference policy with which the learned function should compete.Can't I do this with Chainer already? Sort of.  If you use a particular RL algorithm definitely.  For instance q-learning reduces reinforcement learning to regression so if you code that inline you get something Chainer could handle.  However the goal is to specify inference without leaking details about the learning algorithm so I'd rather not code that inline.  An alternative is to compile to Chainer akin to cfront in the early days of c++.  Ultimately however I would hope to have a different compilation strategy.  There's more at stake than just implementing the learning algorithm: there are all the issues mentioned in my previous post that have convinced me that the implementation should be able to leverage a reinforcement learning service.
2017,1,27,Simulating from a specified seasonal ARIMA model,https://robjhyndman.com/hyndsight/simulating-from-a-specified-seasonal-arima-model/,"From my email today
 You use an illustration of a seasonal arima model:
  ARIMA(111)(111)4
  I would like to simulate data from this process then fit a model… but I am unable to find any information as to how this can be conducted… if I set phi1 Phi1 theta1 and Theta1 it would be reassuring that for large n the parameters returned by Arima(fooorder=c(111)seasonal=c(111)) are in agreement…"
2017,1,27,Deep learning for cancer research,https://bickson.blogspot.com/2017/01/deep-learning-for-cancer-research.html,A recent interesting news from Stanford regarding identification of skin cancer using deep leaning for images.A different project featured by NVIDIA is using deep learning for breast cancer research where they claim that the error went down 85%.Unrelated I heard today about Grail who raised 100M$ for cancer detection&nbsp;in blood tests. Grail raised money from Amazon Google and Microsoft (Bill Gates). &nbsp;Looking at their career page they are also looking for deep learning researchers.Another interesting company is Zebra Medical Research which shares medical data with researchers in return for a fraction of future revenues.Following this blog post publication my friend Assaf Araki from Intel sent me a reminder for Intel's cloud cancer research initiative. Broad MIT institute joined last year.
2017,1,25,Amazing deep learning visualization,https://bickson.blogspot.com/2017/01/amazing-deep-learning-visualization.html,I found this amazing deep learning visualization:&nbsp;http://playground.tensorflow.org/The tool is written by Daniel Smilkov and Shan Carter from Google Brain's team.It is a great tool to understand using small examples the network operation.The tool promised:It took me 5 minutes to find a configuration which breaks it ! :-)Here it is:Note that both Test loss and training loss are NaN. I am waiting for the fix. (I see this error was already reported)
2017,1,22,Reinforcement Learning as a Service,http://www.machinedlearnings.com/2017/01/reinforcement-learning-as-service.html,I've been integrating reinforcement learning into an actual product for the last 6 months and therefore I'm developing an appreciation for what are likely to be common problems.  In particular I'm now sold on the idea of reinforcement learning as a service of which the decision service from MSR-NY is an early example (limited to contextual bandits at the moment but incorporating key system insights).Service not algorithm Supervised learning is essentially observational: some data has been collected and subsequently algorithms are run on it.  (Online supervised learning doesn't necessarily work this way but mostly online techniques have been used for computational reasons after data collection.)  In contrast counterfactual learning is very difficult do to observationally.  Diverse fields such as economics political science and epidemiology all attempt to make counterfactual conclusions using observational data essentially because this is the only data available (at an affordable cost).  When testing a new medicine however the standard is to run a controlled experiment because with control over the data collection more complicated conclusions can be made with higher confidence.Analogously reinforcement learning is best done &ldquo;in the loop&rdquo; with the algorithm controlling the collection of data which is used for learning.  Because of this a pure library implementation of a reinforcement learning algorithm is unnatural because of the requisite state management.  For example rewards occur after actions are taken and these need to be ultimately associated with each other for learning.  (One of my first jobs was at a sponsored search company called Overture and maintaining the search-click join was the full time job of a dozen engineers: note this was merely an immediate join for a singleton session!)Ergo packaging reinforcement learning as a service makes more sense.  This facilitates distributed coordination of the model updates the serving (exploration) distribution and the data collection.  This scenario is a natural win for cloud computing providers.  However in practice there needs to be an offline client mode (e.g. for mobile and IOT applications); furthermore this capability would be utilized even in a pure datacenter environment because of low latency decision requirements.  (More generally there would probably be a &ldquo;tiered learning&rdquo; architecture analogous to the tiered storage architectures utilized in cloud computing platforms.  Brendan McMahan has been thinking along these lines under the rubric of federated learning.)Bootstrapping is everything It is amazing how clarifying it is to try and solve and actual problem.  I now appreciate that reinforcement learning has been oversold a bit.  In particular the sample complexity requirements for reinforcement learning are quite high.  (That's fancy talk for saying it takes a lot of data to converge.)  When you are working in a simulated environment that's not such a concern because you have the equivalent of infinite training data so we see dramatic results in simulated environments.  When reinforcement learning is done on live traffic with real users you have less data than you think because you always start with a test fraction of data and you don't get more until you are better (catch 22).  So I actually spend a lot of my time developing initial serving policies unfortunately somewhat idiosyncratically: imitation learning can be great with the right data assets but heuristic strategies are also important.  I suspect initialization via not-smartly-initialized-RL in a simulated environment is another possibility (in dialog simulators aren't so good so I haven't leveraged this strategy yet).This creates some design questions for RL as a service.   Assuming there is an initial serving policy how do I specify it?  In the decision service you pass in the action that the initial serving policy would take which is fine for contextual bandits but for a multi-step epoch this could be cumbersome because the initial serving policy needs to maintain state.  It would make sense for the service to make it easier to manage this. How does the service help me put together the initial serving policy?  Considering my experience so far here are some possible ways to develop an initial serving policy: An arbritrary program (``heuristic'').  Sometimes this is the easiest way to cold start or this might be the current ``champion'' system. Imitation learning.  Assumes suitable data assets are available. Off-policy learning from historical data.  This can be better than imitation learning if the historical policy was suitably randomized (e.g. the exhaust of previous invocations of RL as a service). Boostrapping via simulation.  In dialog this doesn't seem viable but if a good simulator is available (e.g. robotics and game engines?) this could be great.  Furthermore this would involve direct reuse of the platform albeit on generated data.Language is the UI of programming I think ideas from credit-assignment compilation would not only address the question of how to specify the initial policy but also provide the most natural interface for utilizing RL a service.  I'll do another post exploring that.
2017,1,19,TensorFlow to support Keras API,https://bickson.blogspot.com/2017/01/tensorflow-to-support-keras-api.html,I found this interesting blog post&nbsp;by Rachel Thomas. My favorite quote:Using TensorFlow makes me feel like I’m not smart enough to use TensorFlow; whereas using Keras makes me feel like neural networks are easier than I realized.&nbsp;This is because TensorFlow’s API is verbose and confusing and because Keras has the most thoughtfully designed expressive API I’ve ever experienced. I was too embarrassed to publicly criticize TensorFlow after my first few frustrating interactions with it. It felt so clunky and unnatural but surely this was my failing. However Keras and Theano confirm my suspicions that tensors and neural networks don’t have to be so painful.&nbsp;
2017,1,19,Pipeline.io - production environment to serve TensorFlow models,https://bickson.blogspot.com/2017/01/pipelineio-production-environment-to.html,"I recently stumbled upon pipeline.io - an open source production environment to serve TensorFlow deep learning models. By looking into Giuhub activity plots&nbsp;I see the Chris Fregly&nbsp;is the main force behind it. Pipeline.io is trying to solve the major headache around scoring and maintaining ML models in production.Here us their general architecture diagram:Here is a talk by Chris:&nbsp;Alternative related systems are seldon.io prediction.io (sold to SalesForce) sense.io (sold to Cloudera) Domino Data Labs and probably some others I forgot :-)BTW Chris will be giving a talk at AI by the bay conference (March 6-8 in San Francisco). The conference looks pretty interesting.&nbsp;And here is a note I got from Chris following my initial blog post:Thanks for the mention Danny! Love your work.Here's an updated video:Here's the jupyter notebook that powers the entire demo:&nbsp;I asked Chris which streaming applications he has in mind and this is what I got:We've got a number of streaming-related Github issues (features) in the works:  here are the some relevant projects that are in the works:  - working with the Subscriber-Growth Team @ Netflix to replace their existing multi-armed bandit Spark-Streaming-based data pipeline to select the best model to increase signups. we're using Kafka + Kafka Streams + Spark + Cassandra (they love Cassandra!) + Jupyter/Zeppelin Notebooks in both Python/Scala.  - working with the Platform Team @ Twilio to quickly detect application logs that potentially violate Privacy Policies. this is already an issue outside the US but quickly becoming an issue here in the US. we're using Kafka + custom Kafka Input Readers for Tensorflow + Tensorflow to train the models (batch) and score every log line (real-time).  - working with a super-large Oil &amp; Gas company out of Houston/Oslo (stupid NDA's) to continuously train deploy and compare scikit-learn and Spark ML models on live data in parallel - all from a Jupyter notebook.   - working with PagerDuty to predict potential outages based on their new ""Event"" stream which includes code deploys configuration changes etc. we're using Kafka + the new Spark 2.0 Structure Streaming. &nbsp;What are the main benefits of piepline.io vs. other systems?  - the overall goal as you can probably figure out is to give data scientists the ""freedom and responsibility"" (hello Netflix Culture Deck!) to iterate quickly without depending on production engineers or an ops group.  - this is a life style that i really embraced while at Netflix. with proper tooling anyone (devs data scientists etc) should be able to deploy scale and rollback their own code or model artifacts.  - we're providing the platform for this ML/AI-focused freedom and responsibility!  - you pointed out a few of our key competitors/cooperators like seldon.io. i have a list of about 20 more that i keep an eye on each and every day. i'm in close talks with all of them.   - we're looking to partner with guys like Domino Data Labs who have a weak deployment story.   - and we're constantly sharing experience and code with seldon.io and hydrosphere.io and others.  - we're super performance-focused as well. we have a couple efforts going on including PMML optimization native code generation etc.  - also super-focused on metrics and monitoring - including production-deployment dashboards targeted to data scientists.  - i feel like our main competitors are actually the cloud providers. they're the ones that keep me awake. one of our underlying themes is to reverse engineer Google and AWS's Cloud ML APIs.&nbsp;&nbsp;"
2017,1,18,Dynamic Algorithm Selection for Pareto Optimal Set Approximation,https://robjhyndman.com/publications/dynamic-pareto-approximation/,This paper presents a meta-algorithm for approximating the Pareto optimal set of costly black-box multiobjective optimization problems given a limited number of objective function evaluations. The key idea is to switch among different algorithms during the optimization search based on the predicted performance of each algorithm at the time. Algorithm performance is modeled using a machine learning technique based on the available information. The predicted best algorithm is then selected to run for a limited number of evaluations.
2017,1,17,CryptoNets: scoring deep learning on encrypted data,https://bickson.blogspot.com/2017/01/cryptonets-scoring-deep-learning-on.html,Last week I attended &nbsp;an interesting lecture by Ran Gilad Bachrach from MSR. Ran presented CryptoNets who was reported in ICML 2016. CryptoNets allows to score trained deep learning models on encrypted data. They use homomorphic encryption&nbsp;a well known mechanism which allows computing encrypted products and sums. So the main trick is to limit the neural net operations to include only sums and products. To overcome this problem CryptoNet is using the square function as the only non-linear operation supported (vs. sigmoids ReLU etc.)On the up side CryptoNets reports 99% accuracy on MNIST data which is the toy example everyone is using for deep learning. On the downside you can not train a network but just score on new test data. Scoring is quite slow - around 5 minutes although you can batch up to a few thousands scoring operations together at the same batch. Due to increasing complexity of the represented numbers the technique is also limited to a certain number of network layers.I believe that in the coming few years additional research effort will be invested for trying to tackle the training of neural networks on private data without revealing the data contents.Anyone who is interested in reading about other primitives who may be used for performing similar computation is welcome to take a look at my paper: D. Bickson D. Dolev G. Bezman and B. Pinkas  Secure Multi-party Peer-to-Peer Numerical Computation. Proceedings of the 8th IEEE Peer-to-Peer Computing (P2P'08) Sept. 2008 Aachen Germany - where we use both homomorphic encryption but also Shamir Secret Sharing to compute a similar distributed computation (in terms of sums and products).
2017,1,13,Generating Text via Adversarial Training,http://www.machinedlearnings.com/2017/01/generating-text-via-adversarial-training.html,There was a really cute paper at the GAN workshop this year Generating Text via Adversarial Training by Zhang Gan and Carin.  In particular they make a couple of unusual choices that appear important.  (Warning: if you are not familiar with GANs this post will not make a lot of sense.)They use a convolutional neural network (CNN) as a discriminator rather than an RNN.  In retrospect this seems like a good choice e.g. Tong Zhang has been crushing it in text classification with CNNs.  CNNs are a bit easier to train than RNNs so the net result is a powerful discriminator with a relatively easy optimization problem associated with it.They use a smooth approximation to the LSTM output in their generator but actually this kind of trick appears everywhere so isn't so remarkable in isolation.They use a pure moment matching criterion for the saddle point optimization (estimated over a mini-batch).  GANs started with a pointwise discrimination loss and more recent work has augmented this loss with moment matching style penalties but here the saddle point optimization is pure moment matching.  (So technically the discriminator isn't a discriminator.  They actually refer to it as discriminator or encoder interchangeably in the text this explains why.)They are very smart about initialization.  In particular the discriminator is pre-trained to distinguish between a true sentence and the same sentence with two words swapped in position.  (During initialization the discriminator is trained using a pointwise classification loss).  This is interesting because swapping two words preserves many of the $n$-gram statistics of the input i.e. many of the convolutional filters will compute the exact same value.  (I've had good luck recently using permuted sentences as negatives for other models now I'm going to try swapping two words.)They update the generator more frequently than the discriminator which is counter to the standard folklore which says you want the discriminator to move faster than the generator.  Perhaps this is because the CNN optimization problem is much easier than the LSTM one; the use of a purely moment matching loss might also be relevant.The old complaint about neural network papers was that you couldn't replicate them.  Nowadays it is often easier to replicate neural network papers than other papers because you can just fork their code on github and run the experiment.  However I still find it difficult to ascertain the relative importance of the various choices that were made.  For the choices enumerated above: what is the sensitivity of the final result to these choices?  Hard to say but I've started to assume the sensitivity is high because when I have tried to tweak a result after replicating it it usually goes to shit.  (I haven't tried to replicate this particular result yet.)Anyway this paper has some cool ideas and hopefully it can be extended to generating realistic-looking dialog.
2017,1,12,Visualising forecasting algorithm performance using time series instance spaces,https://robjhyndman.com/publications/ts-feature-space/,It is common practice to evaluate the strength of forecasting methods using collections of well-studied time series datasets such as the M3 data. But how diverse are these time series how challenging and do they enable us to study the unique strengths and weaknesses of different forecasting methods? In this paper we propose a visualisation method for a collection of time series that enables a time series to be represented as a point in a 2-dimensional instance space.
2017,1,11,IJF Tao Hong Award for the best paper in energy forecasting 2013-2014,https://robjhyndman.com/hyndsight/ijf-hong-award/,"Professor Tao Hong has generously funded a new prize for the best IJF paper on energy forecasting to be awarded every two years. The first award will be for papers published in the International Journal of Forecasting during the period 2013-2014. The prize will be US$1000 plus an engraved plaque. The award committee is Rob J Hyndman Pierre Pinson and James Mitchell.
Nominations are invited from any reader of the IJF."
2016,12,27,"Ethics, logistic regression, and 0-1 loss",https://lingpipe-blog.com/2016/12/27/ethics-logistic-regression-and-0-1-loss/,Andrew Gelman and David Madigan wrote a paper on why 0-1 loss is so problematic: Gelman and Madigan. 2015. How is Ethics Like Logistic Regression? Chance 28(12). This is related to the issue of whether one should be training on an artificial gold standard. Suppose we have a bunch of annotators and we don&#8217;t have [&#8230;]
2016,12,22,Key challenges in online experiments: where are the statisticians?,http://www.bzst.com/2016/12/key-challenges-in-online-experiments.html,Randomized experiments (or randomized controlled trials RCT) are a powerful tool for testing causal relationships. Their main principle is random assignment where subjects or items are assigned...
2016,12,17,On the Sustainability of Open Industrial Research,http://www.machinedlearnings.com/2016/12/on-sustainability-of-open-industrial.html,I'm glad OpenAI exists: the more science the better!  Having said that there was a strange happenstance at NIPS this year.  OpenAI released OpenAI universe which is their second big release of a platform for measuring and training counterfactual learning algorithms.  This is the kind of behaviour you would expect from an organization which is promoting the general advancement of AI without consideration of financial gain.  At the same time Google Facebook and Microsoft all announced analogous platforms.  Nobody blinked an eyelash at the fact that three for-profit organizations were tripping over themselves to give away basic research technologies.A naive train of thought says that basic research is a public good subject to the free-rider problem and therefore will be underfunded by for-profit organizations.  If you think this is a strawman position you haven't heard of the Cisco model for innovation.  When this article was written:&hellip;Cisco has no &ldquo;pure&rdquo; blue-sky research organization.  Rather when Cisco invests research dollars it has a specific product in mind.  The company relies on acquisitions to take the place of pure research &hellip;Articles like that used to worry me alot.  So why (apparently) is this time different?Factor 1: Labor Market ScarcityInformal discussions with my colleagues generally end up at this explanation template.  Specific surface forms include:&ldquo;You can't recruit the best people without good public research.&rdquo;  Facially I think this statement is true but the logic is somewhat circular.  You certainly can't recruit the best researchers without good public research but why do you want them in the first place?  So is the statement more like &ldquo;With good public research you can recruit the best people and then convince them to do some non-public research.&rdquo; (?) Alot of grad students do seem to graduate and then &ldquo;disappear&rdquo; so there is probably some truth to this.&ldquo;The best people want to publish: it's a perk that you are paying them.&rdquo; Definitely getting public recognition for your work is rewarding and it makes total sense for knowledge workers to want to balance financial capital and social capital.  Public displays of competence are transferable to a new gig for instance.  But this line of thought assumes that public research is a cost for employers that they chose to pay in lieu of e.g. higher salaries.I not only suspect this factor is only part of the picture: I strongly hope that it is only part of the picture.  Because if it is the whole picture as soon as the labor market softens privately funded public research will experience a big pullback which would suck.Factor 2: Positive ExternalitiesThis argument is: &ldquo;researchers improve the productivity of those nearby such that it is worth paying them just to hang out.&rdquo; In this line of thinking even a few weeks lead time on the latest ideas plus the chance to talk in person with thought leaders in order to explain the nuances of the latest approaches is worth their entire salary.   There is some truth to this e.g. Geoffrey Hinton performed some magic for the speech team here back in the day.  The problem I have with this picture is that in practice it can be easier to communicate and collaborate with somebody across the planet than with somebody downstairs.  It's also really hard to measure so if I had to convince the board of directors to fund a research division based upon this I think I would fail.This is another favorite argument that comes up in conversation by the way.  It's funny to hear people characterize the current situation as &ldquo; we're scarce and totally awesome.&rdquo;  As Douglas Adams points out there is little benefit to having a sense of perspective.Factor 3: Quality AssuranceThe idea here is basically &ldquo;contributing to the public research discussion ensures the high quality of ideas within the organization.&rdquo;  The key word here is contributing as the alternative strategy is something more akin to free-riding e.g. sending employees to conferences to attend but not contribute.There is definite value in preparing ideas for public consumption.  Writing the related work section of a paper is often an enlightening experience although honestly it tends to happen after the work has been done rather than before.  Before is more like a vague sense that there is no good solution to whatever the problem is hopefully informed by a general sense of where the state-of-the-art is.  Writing the experiment section in my experience is more of a mixed bag: you often need to dock with a standard metric or benchmark task that seems at best idiosyncratic and at worst unrelated to the thrust of your work and therefore forcing particular hacks to get over the finish line.  (Maybe this is why everybody is investing so heavily in defining the next generation of benchmark tasks.)The funny thing is most of the preceeding benefits occur during the preparation for publication.  Plausibly at that point you could throw the paper away and still experience the benefits (should we call these &ldquo;the arxiv benefits&rdquo;?).  Running the reviewer gauntlet is a way of measuring whether you are doing quality work but it is a noisy signal.  Quality peer feedback can suggest improvements and new directions but is a scarce resource.  Philanthropic organizations that want to advance science should attack this scarcity e.g. by funding high quality dedicated reviewers or inventing a new model for peer feedback.I don't find this factor very compelling as a rationale for funding basic research i.e. if I were the head of a research department arguing for funding from the board of directors I wouldn't heavily leverage this line of attack.  Truth is less important than perception here and I think the accounting department would rather test the quality of their ideas in the marketplace of products.Factor 4: MarketingA company can use their basic research accolades as a public display of the fitness and excellence of their products.  The big players definitely make sure their research achievements are discussed in high profile publications such as the New York Times.  However this mostly feels like an afterthought to me.  What seems to happen is that researchers are making their choices on what to investigate some of it ends up being newsworthy and another part of the organization has dedicated individuals whose job it is to identify and promote newsworthy research.  IBM is the big exception e.g. Watson going after Jeopardy.  This is arguably sustainable (IBM has been at it for a while) but it creates activity that looks like big pushes around specific sensational goals rather than distribution of basic research tools and techniques.  In other words it doesn't look like what was happening at this year's NIPS.Factor 5: MonopoliesI find this explanation agreeable: that technology has created more natural monopolies and natural monopolies fund research c.f. Bell Labs and Xerox PARC.  All market positions are subject to disruption and erosion but Microsoft Google and Facebook all have large competitive moats in their respective areas (OS search and social) so they are currently funding public basic research.  This factor predicts that as Amazon's competitive moats in retail (and cloud computing) widen they will engage in more public basic research something we have seen recently.For AI (née machine learning) in particular the key monopoly is data (which derives from customer relationships).  Arguably the big tech giants would love for AI technologies to be commodities because they would then be in the best position to exploit such technologies due to their existing customer relationships.  Conversely if a privately discovered disruptive AI technology were to emerge it would be one of the &ldquo;majors&rdquo; being disrupted by a start-up.  So the major companies get both benefits and insurance from a vibrant public research ecosystem around AI.Nonetheless a largish company with a decent defensive moat might look at the current level of public research activity and say &ldquo;hey good enough let's free ride.&rdquo; (Not explicitly perhaps but implicitly).  Imagine you are in charge of Apple or Salesforce what do you do?  I don't see a clear &ldquo;right answer&rdquo; although both companies appear to be moving in the direction of more open basic research.Factor 6: Firms are IrrationalTech firms are ruled by founder-emperors whose personal predilections can decide policies such as whether you can bring a dog to work.  The existence of a research department with a large budget in practice can be similarly motivated.  All the above factors are partially true but difficult to measure so it comes down to a judgement call and as long as a company is kicking ass deference for the founder(s) will be extreme.  If this factor is important however then when the company hits a rough patch or experiences a transition at the top things can go south quickly.  There have been examples of that in the last 10 years for sure.
2016,12,16,Dialogue Workshop Recap,http://www.machinedlearnings.com/2016/12/dialogue-workshop-recap.html,Most of the speakers have sent me their slides which can be found on the schedule page.  Overall the workshop was fun and enlightening.  Here are some major themes that I picked up upon.Evaluation There is no magic bullet but check out Helen's slides for a nicely organized discussion of metrics.  Many different strategies were on display in the workshop:Milica Gasic utilized crowdsourcing for some of her experiments.  She also indicated the incentives of crowdsourcing can lead to unnatural participant behaviours.Nina Dethlefs used a combination of objective (BLEU) and subjective (“naturalness”) evaluation.Vlad Serban has been a proponent of next utterance classification as a useful intrinsic metric.Antoine Bordes (and the other FAIR folks) are heavily leveraging simulation and engineered tasks.Jason Williams used imitation metrics (from hand labeled dialogs) as well as simulation.As Helen points out computing metrics from customer behaviour is probably the gold standard for industrial task-oriented systems but this is a scarce resource.  (Even within the company that has the customer relationship by the way: at my current gig they will not let me flight something without demonstrating limited negative customer experience impact.)Those who have been around longer than I have experienced several waves of enthusiasm and pessimism regarding simulation for dialogue.  Overall I think the takeaway is that simulation can be useful tool as long as one is cognizant of the limitations.Antoine quickly adapted his talk to Nina's with a fun slide that said “Yes Nina we are bringing simulation back.”  The FAIR strategy is something like this: “Here are some engineered dialog tasks that appear to require certain capabilities to perform well such as multi-hop reasoning interaction with a knowledge base long-term memory etc.  At the moment we have no system that can achieve 100% accuracy on these engineered tasks so we will use these tasks to drive research into architectures and optimization strategies.  We also monitor performance other external tasks (e.g. DSTC) to see if our learning generalizes beyond the engineered task set.”  Sounds reasonable.Personally as a result of the workshop I'm going to invest more heavily in simulators in the near-term.Leveraging Linguistics Fernando Pereira had the killer comment about how linguistics is a descriptive theory which need not have explicit correspondence to implementation: &ldquo;when Mercury goes around the Sun it is not running General Relativity.&rdquo;  Nonetheless linguistics seems important not only for describing what behaviours a competent system must capture but also for motivating and inspiring what kinds of automata we need to achieve it.  Augmenting or generating data sets seems like a natural way to leverage lingustics.  As an example in the workshop I learned that 4 year old native English speakers are sensitive to proper vs. improper word order given simple sentences containing some nonsense words (but with morphological clues such as capitalization and -ed suffix).  Consequently I'm trying a next utterance classification run on a large dialog dataset where some of the negative examples are token-permuted versions of the true continuation to see if this changes anything.Raquel Fernandez's talk focused on adult-child language interactions and I couldn't help but think about potential relevance to training artificial systems.  In fact current dialog systems are acting like the parent (i.e. the expert) e.g. by suggesting reformulations to the user.  But this laughable because our systems are stupid: shouldn't we be acting like the child?The most extreme use of linguistics was the talk by Eshghi and Kalatzis where they develop a custom incremental semantic parser for dialog and then use the resulting logical forms to drive the entire dialog process.  Once the parser is built the amount of training data required is extremely minimal but the parser is presumably built from looking at a large number of dialogs.Nina Dethlefs discussed some promising experiments with AMR.  I've been scared of AMR personally.  First it is very expensive to get the annotations.  However if that were the only problem we could imagine a human-genome-style push to generate a large number of them.  The bigger problem is the relatively poor inter-annotator agreement (it was just Nina and her students so they could come to agreement via side communication).  Nonetheless I could imagine a dialog system which is designed and built using a small number of prototypical semantic structures.  It might seem a bit artificial and constrained but so does the graphical user interface with the current canonical set of UX elements which users learn to productivity interact with.Angeliki Lazaridou's talk reminded me that communication is fundamentally a cooperative game which explains why arguing on the internet is a waste of time. Neural Networks: Game Changer?  I asked variants of the following question to every panel: &ldquo;what problems have neural networks mitigated and what problems remain stubbornly unaddressed.&rdquo;  This was essentially the content of Marco Baroni's talk.  Overall I would say: there's enthusiasm now that we are no longer afraid of non-convex loss functions (along these lines check out Julien Perez's slides).However we currently have only vague ideas on how to realize the competencies that are apparently required for high quality dialog.  I say apparently because the history of AI is full of practitioners assuming sufficient capabilities are necessary for some task and recent advances in machine translation suggest that savant-parrots might be able to do surprisingly well.  In fact during the discussion period there was some frustration that heuristic hand-coded strategies are still superior to machine learning based approaches with the anticipation that this may continue to be true for the Alexa prize.  I'm positive about the existence of superior heuristics however: not only do they provide a source of inspiration and ideas for data-driven approaches but learning methods that combine imitation learning and reinforcement learning should be able to beneficially exploit them.Entity Annotation Consider the apparently simple and ubiquitous feature engineering strategy: add additional sparse indicator features which indicate semantic equivalence of tokens or token sequences.  So maybe “windows 10” and “windows anniversary edition” both get the same feature.  Jason Williams indicated his system is greatly improved by this but he's trying to learn from $O(10)$ labeled dialogues so I nodded.  Antoine Bordes indicated this helps on some bAbI dialog tasks but those tasks only have $O(1000)$ dialogues so again I nodded.  Then Vlad Serban indicated this helps for next utterance classification on the Ubuntu Dialog Corpus.  At this point I thought &ldquo;wait that's $O(10^5)$ dialogs.&rdquo;Apparently knowing a turtle and a tortoise are the same thing is tricky.In practice I'm ok with manual feature engineering: it's how I paid the rent during the linear era.  But now I wonder: does it take much more data to infer such equivalences?  Will we never infer this no matter how much data given our current architectures?Spelling The speakers were roughly evenly split between “dialog” and “dialogue”.  I prefer the latter as it has more panache.
2016,12,15,Neural networks for graphs,https://bickson.blogspot.com/2016/12/neural-networks-for-graphs.html,I met at Thomas Kipf&nbsp;from University of Amsterdam at NIPS 2016 and he pointed out some interesting blog post he wrote regarding neural networks for graph analytics.
2016,12,12,What is going on?,https://robjhyndman.com/hyndsight/ijf-reject/,"I seem to be getting an increasing number of submissions where the author has clearly not bothered to actually check that the paper was submitted correctly. Here is a rejection letter I wrote today.
 Dear xxxxx
  I am writing concerning manuscript #INTFOR_16xxxxx entitled &ldquo;xxxxxxxxxxxxxxxx&rdquo; which you submitted to the International Journal of Forecasting.
  Thank you for this submission but as it consists entirely of the IJF author guidelines it is not suitable for publication in the IJF."
2016,12,12,NIPS 2016 Reflections,http://www.machinedlearnings.com/2016/12/nips-2016-reflections.html,It was a great conference.  The organizers had to break with tradition to accommodate the rapid growth in submissions and attendance but despite my nostalgia I feel the changes were beneficial.  In particular leveraging parallel tracks and eliminating poster spotlights allowed for more presentations while ending the day before midnight and the generous space allocation per poster really improved the poster session.  The workshop organizers apparently thought of everything in advance: I didn't experience any hiccups (although we only had one microphone so I got a fair bit of exercise during discussion periods).Here are some high-level themes I picked up on.Openness.  Two years ago Amazon started opening up their research and they are now a major presence at the conference.  This year at NIPS Apple announced they would be opening up their research practices.  Clearly companies are finding it in their best interests to fund open basic research which runs counter to folk-economic reasoning that basic research appears to be a pure public good and therefore will not be funded privately due to the free-rider problem.  A real economist would presumably say that is simplistic undergraduate thinking.  Still I wonder to what extent are companies being irrational?  Conversely what real-world aspects of basic research are not well modeled as a public good?  I would love for an economist to come to NIPS to give an invited talk on this issue.  Simulation.  A major theme I noticed at the conference was the use of simulated environments.  One reason was articulated by Yann LeCun during his opening keynote: (paraphrasing) ``simulation is a plausible strategy for mitigating the high sample complexity of reinforcement learning.''  But another reason is scientific methodology: for counterfactual scenarios simulated environments are the analog of datasets in that they allow for a common metric reproducible experimentation and democratization of innovation.  Simulators are of course not new and have had waves of enthusiasm and pessimism in the past and there are a lot of pitfalls which basically boil down to overfitting the simulator (both in a micro sense of getting a bad model but also in a macro sense of focusing scientific attention on irrelevant aspects of a problem).  Hopefully we can learn from the past and be cognizant of the dangers.  There's more than a blog post worth of content to say about this but here are two things I heard at the dialog workshop along these lines: first Jason Williams suggested that relative performance conclusions based upon simulation can be safe but that absolute performance conclusions are suspect; and second Antoine Bordes advocated for using an ensemble of realizable simulated problems with dashboard scoring (i.e. multiple problems for which perfect performance is possible which exercise apparently different capabilities and for which there is currently no single approach that is known to handle all the problems).Without question simulators are proliferating.  I noticed the following discussed at the conference this year: GVGAI  CommAI-env  Project Malmo  OpenAI universe  DeepMind Lab and I probably missed some others.By the way the alternatives to simulation aren't perfect either: some of the discussion in the dialogue workshop was about how the incentives of crowdsourcing induces unnatural behaviour in participants of crowdsourced dialogue experiments.GANs The frenzy of GAN research activity from other conferences (such as ICLR) colonized NIPS in a big way this year. This is related to simulation albeit more towards the mitigating-sample-complexity theme than the scientific-methodology theme.  The quirks of getting the optimization to work are being worked out which should enable some interesting improvements in RL in the near-term (in addition to many nifty pictures).  Unfortunately for NLU tasks generating text from GANs is currently not as mature as generating sounds or images but there were some posters addressing this.Interpretable Models The idea that model should be able to &ldquo;explain itself&rdquo; is very popular in industry but this is the first time I have seen interpretability receive significant attention at NIPS. Impending EU regulations have certainly increased interest in the subject.  But there are other reasons as well: as Irina Rish pointed out in her invited talk on (essentially) mindreading recent advances in representation learning could better facilitate scientific inquiry if the representations were more interpretable.Papers I noticedWould you trust a single reviewer on yelp?  I wouldn't.  Therefore I think we need some way to crowdsource what people thought were good papers from the conference.  I'm just one jet-lagged person with two eyeballs (btw use bigger font people!  it gets harder to see the screen every year &hellip;) plus everything comes out on arxiv first so if I read it already I don't even notice it at the conference.  That makes this list weird but here you go.Generating Text via Adversarial Training GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution and Adversarial Evaluation of Dialogue Models.   I'm interested in techniques that are relevant to simulating or evaluating dialogue systems.Building Machines That Learn and Think Like People.  The talk was great so I want to dig into the paper.   The talk explored how humans are leveraging lots of priors that we probably want to build into our systems with some specific observations resulting in actionable research directions.  (This appears relevant to dialog since this line of research might explain the pseudo-intelligibility of statements like &ldquo; the blorf flazzed the peezul.&rdquo;)Learning values across many orders of magnitude.  At first blush this might appear to be optimization minutae but this problem is pervasive in counterfactual setups; and I'm a big fan of scale invariance as a useful prior.Reward Augmented Maximum Likelihood for Neural Structured Prediction.  If you squint this reads as another way to use a world model to mitigate the sample complexity of reinforcement learning (e.g. what if edit distance was just the initial model of the reward?).Safe and Efficient Off-Policy Reinforcement Learning.  This is an important setting.  The particular adjustment is reminiscent of a previously proposed estimator in this area but nonetheless this is interesting.Also this paper was not at the conference as far as I know but I found out about it during the coffee break and it's totally awesome:Understanding deep learning requires rethinking generalization.  TL;DR: convnets can shatter the standard image training sets when the pixels are permuted or even randomized!  Of course generalization is poor in this case but it indicates they are way more flexible than their &ldquo;local pixel statistics composition&rdquo; architecture suggests.  So why do they work so well?
2016,12,7,"Exploring the influence of short-term temperature patterns on temperature-related mortality: a case-study of Melbourne, Australia",https://robjhyndman.com/publications/temperature-mortality/,"Background Several studies have identified the association between ambient temperature and mortality; however several features of temperature behavior and their impacts on health remain unresolved.
We obtain daily counts of nonaccidental all-cause mortality data in the elderly (65 + years) and corresponding meteorological data for Melbourne Australia during 1999 to 2006. We then characterize the temporal behavior of ambient temperature development by quantifying the rates of temperature change during periods designated by pre-specified windows ranging from 1 to 30 days."
2016,12,6,Sneaking up on Bayesian Inference (A fable in four acts),https://justindomke.wordpress.com/2016/12/06/sneaking-up-on-bayesian-inference-a-fable-in-four-acts/,Act 1: Magical Monkeys Two monkeys Alfred () and Betty () live in a parallel universe with two kinds of blocks green () and yellow (). Alfred likes green blocks and Betty prefers the yellow blocks. One day a Wizard decides to give one of the monkeys the magical power to send one block over &#8230; Continue reading Sneaking up on Bayesian Inference (A fable in four&#160;acts) &#8594;
2016,12,6,Properties of Interpretability,https://shapeofdata.wordpress.com/2016/12/05/properties-of-interpretability/,In my last two posts I wrote about model interpretability with the goal of trying to understanding what it means and how to measure it. In the first post I described the disconnect between our mental models and algorithmic models &#8230; Continue reading &#8594;
2016,12,5,Cross-validation for time series,https://robjhyndman.com/hyndsight/tscv/,I&rsquo;ve added a couple of new functions to the forecast package for R which implement two types of cross-validation for time series.K-fold cross-validation for autoregression The first is regular k-fold cross-validation for autoregressive models. Although cross-validation is sometimes not valid for time series models it does work for autoregressions which includes many machine learning approaches to time series. The theoretical background is provided in Bergmeir Hyndman and Koo (2015). So cross-validation can be applied to any model where the predictors are lagged values of the response variable.
2016,12,3,Learning Methods for Dialog Workshop at NIPS This Saturday,http://www.machinedlearnings.com/2016/12/learning-methods-for-dialog-workshop-at.html,The schedule for the workshop has been finalized and I'm pretty excited.  We managed to convince some seasoned researchers in dialog who don't normally attend NIPS to give invited talks.  We're also devoting some time to &ldquo;Building Complete Systems&rdquo; because it's easy to focus on the trees instead of the forest especially when the tree is something really interesting like a neural network trained on a bunch of GPUs.  But don't worry there's plenty of &ldquo;NIPS red meat&rdquo; in the schedule as well.See you on Saturday!
2016,12,3,How to Write an Academic Teaching Statement,http://yyue.blogspot.com/2016/12/how-to-write-academic-teaching-statement.html,A while back I wrote a post on how to write an academic research statement.  This is a follow-up post on how to write an academic teaching statement and contains my thoughts on what makes for a good teaching statement when applying to computer science departments in US research universities.Like I said about research statements the teaching statement is not the most important part of your application package.  In fact for research universities the teaching statement is probably the least important part.  Nonetheless there are pitfalls that should be avoided because a bad teaching statement can hurt your application.  Being an ineffective teacher is grounds for not getting tenure at many schools and schools don't like to hire faculty that they don't think can get tenure.For reference here is my teaching statement from when I went on the job market in Fall 2012.  I want to emphasize a few points:(1) Do not blather on and on about how much you LOVE teaching. No one wants to read this just like no one wants to read in your grad school application about how you've loved computers since you were three years old.  Stuff like this lacks substance and makes you seem immature.(2) Keep it short. No one wants to read a long teaching statement and long teaching statements are usually just fluffed up anyways.  Just like for the research statement optimize the wording to be as concise as possible.  I'd recommend keeping the teaching statement to one page.(3) List the courses and topics you can teach. Some faculty search committees like to explicitly see what courses you can teach so this is an important thing to mention in the teaching statement.  I typically like to list this at the very end of the teaching statement. I would recommend being somewhat open-minded about what courses you can teach.  You can always negotiate these things later.(4) Briefly list your credentials. I typically like to do this at the very beginning. You don't need anything flashy here but you just can't count on your recommendation letters to talk about all of your teaching and mentorship experience.(5) Have a teaching philosophy. No one is expecting you to know exactly how you'd like to teach your courses but not having an explicit teaching philosophy of some kind can be an indicator of complete lack of preparation.  For reference this part took up the 2nd paragraph of my teaching statement.  One strategy is to connect this part to prior teaching and mentoring experience where you can talk about what teaching styles of have been successful for you.(6) Say something meaningful about outreach and interdisciplinary education. Computer science and data science are quickly becoming vital enabling disciplines for virtually all scientific and engineering disciplines.  Recognize this and have some kind of plan for how you want to contribute to it or why it matters to you.
2016,11,30,Measuring The Memory Of Time Series Data,https://prateekvjoshi.com/2016/11/30/measuring-the-memory-of-time-series-data/,Time series data has memory. It remembers what happened in the past and avenge any wrongdoings! Can you believe it? Okay the avenging part may not be true but it definitely remembers the past. The &#8220;memory&#8221; refers to how strongly the &#8230; Continue reading &#8594;
2016,11,28,Invited sessions at the International Symposium on Forecasting,https://robjhyndman.com/hyndsight/isf-invited-sessions/,"We are currently calling for invited session proposals for the ISF to be held in Cairns Australia in June 2017.
An invited session consists of 3 or 4 talks around a specific forecasting theme. You are allowed to be one of the speakers in a session you organize (although it is not necessary). So if you know what you are planning to speak about all you need to do is find 2 or 3 other speakers who will speak on something related and invite them to join you."
2016,11,20,Fintech in 2017: Risk management takes center stage,https://practicalanalytics.wordpress.com/2016/11/20/fintech-in-2017-risk-management-takes-center-stage/,Regulation is an inherent aspect of banking. Emerging technologies such as cognitive analytics big data machine learning and predictive analytics offer a lot of options for companies to manage their regulatory compliance risk.
2016,11,17,Goals of Interpretability,https://shapeofdata.wordpress.com/2016/11/17/goals-of-interpretability/,In my last post I looked at the gap that arises when we delegate parts of our thought processes to algorithmic models rather than incorporating the rules they identify directly into our mental models like we do with traditional statistics. I &#8230; Continue reading &#8594;
2016,11,6,We're still hiring at Monash,https://robjhyndman.com/hyndsight/still-hiring-at-monash/,We have another position available this time for a lecturer (equivalent to an assistant professor tenure track in the US). The department covers a wide range of areas in statistics and econometrics but for this position we are looking for someone with expertise in at least one of business analytics data science actuarial science computational statistics and machine learning. Applicants who have recently completed a PhD or expect to do so in the next 6 months are welcome to apply.
2016,11,3,Artificial Neural Networks Introduction (Part II),https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/,In the 2nd part of our tutorial on artificial neural networks we cover 3 techniques to improve prediction accuracy: distortion mini-batch gradient descent and dropout.
2016,10,26,Interacting with ML Models,https://shapeofdata.wordpress.com/2016/10/26/interacting-with-ml-models/,The main difference between data analysis today compared with a decade or two ago is the way that we interact with it. Previously the role of statistics was primarily to extend our mental models by discovering new correlations and causal &#8230; Continue reading &#8594;
2016,10,24,Experimenting with quantified self: two months hooked up to a fitness band,http://www.bzst.com/2016/10/experimenting-with-quantitative-self.html,It's one thing to collect and analyze behavioral big data (BBD) and another to understand what it means to be the subject of that data. To really understand. Yes we're all aware that our social...
2016,10,24,Q&A: predictive analytics,https://robjhyndman.com/hyndsight/qa-predictive-analytics/,"A major news outlet interviewed me on predictive analytics. Here were my responses. Data mining is not just for tech companies in fact it can be especially useful for industries which are not typically thought of to be &lsquo;innovative&rsquo; such as agriculture. What are some of the main industries that you think benefit from predictive analysis?
 Any industry that collects data can use data mining and statistical modelling.
Agriculture is becoming a heavy user of data science methods with data being collected on every aspect of crop or livestock health and development."
2016,10,23,Q&A time,https://robjhyndman.com/hyndsight/qa-time/,"Someone sent me some questions by email and I decided to answer some of them here. How important is it that I know and understand the underlying mathematical framework to forecasting methods? I understand conceptually how most of them work but I feel as if I may benefit from truly understanding the math.
 The main benefit of understanding the mathematics behind different forecasting models is to be able to adapt the models when they don&rsquo;t work well for your data."
2016,10,22,Tourism forecasting competition data as an R package,https://robjhyndman.com/hyndsight/tcomp/,"The data used in the tourism forecasting competition discussed in Athanasopoulos et al (2011) have been made available in the Tcomp package for R. The objects are of the same format as for Mcomp package containing data from the M1 and M3 competitions.
Thanks to Peter Ellis for putting the package together. He has also produced a nice blog post about it."
2016,10,19,Forecasting Using R (Eindhoven),https://robjhyndman.com/seminars/eindhoven/,"Date: 19-21 October 2016
Location: Eurandom Metaforum Building MF11-12 Department of Mathematics and Computer Science TU Eindhoven the Netherlands
Prerequisites Please bring your own laptop with a recent version of R installed along with the following packages and their dependencies:
 fpp ggplot2 magrittr readxl thief knitr  Participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed."
2016,10,14,GEFCom2017: Hierarchical Probabilistic Load Forecasting,https://robjhyndman.com/hyndsight/gefcom2017/,"After the great success of the previous two energy forecasting competitions we have run (GEFCom2012 and GEFCom2014) we are holding another one this time focused on hierarchical probabilistic load forecasting. Check out all the details over on Tao Hong&rsquo;s blog.
The previous GEFComs have led to some major advances in forecasting methodology available via IJF papers by the winning teams. I expect similar developments to arise out of this competition. Winners get to present their work in Cairns Australia at ISEA2017."
2016,10,13,Reconciling forecasts: the hts and thief packages,https://robjhyndman.com/seminars/erum2016/,eRum2016 Poznań Poland.
2016,10,12,"Come to Melbourne, even if not to Monash",https://robjhyndman.com/hyndsight/unimelb-prof-datascience/,The University of Melbourne is advertising for a &ldquo;Professor in Statistics (Data Science)&quot;. Melbourne (the city) is fast becoming a vibrant centre for data science and applied statistics with more than 4700 people signed up for the Data Science Meetup Group a thriving start-up scene the group at Monash Business School (including Di Cook and me) and the Monash Centre for Data Science (including Geoff Webb and Wray Buntine). Not to mention that Melbourne is a wonderful place to live having won the &ldquo;World&rsquo;s most liveable city&rdquo; award from the Economist for the last 6 years in a row.
2016,10,5,Hadley Wickham Master R Developer course coming to Melbourne,https://robjhyndman.com/hyndsight/hadley-course-dec-2016/,"Hadley Wickham&rsquo;s popular R developer course is coming to Melbourne on 12-13 December 2016. Bookings can be made via Eventbrite.Hadley of course is the developer of the wonderful tidyverse set of R packages including ggplot2 dplyr tidyr readr purrr tibble and many more. He is the author of several books including the new &ldquo;R for Data Science&rdquo; he is the chief scientist at RStudio and a fellow cocktail enthusiast.
From the course blurb:"
2016,9,29,SoCal ML Symposium,http://yyue.blogspot.com/2016/09/socal-ml-symposium.html,Julian McAuley and I are organizing the Southern California Machine Learning Symposium on Friday November 18 at Caltech!http://dolcit.cms.caltech.edu/scmls/** CPF Deadline is October 4th!!The SoCal ML Symposium brings together students and faculty to promote machine learning in the Southern California region. The workshop serves as a forum for researchers from a variety of fields working on machine learning to share and discuss their latest findings.Topics to be covered at the symposium include but are not limited to:+ Machine learning with graphs social networks and structured data.+ Active learning reinforcement learning crowdsourcing.+ Learning with images and natural language.+ Learning with high-dimensional data.+ Neural networks deep learning and graphical models.+ Learning dynamic and streaming data.+ Applications to interesting new domains.+ Addressing each of these issues at scale.The majority of the workshop will be focused on student contributions in the form of contributed talks and posters.We invite submissions in the form of 1-2 page extended absracts to be presented as posters and oral presentations at the symposium. Submissions may be made on our easychair page:https://easychair.org/conferences/?conf=scmls16A $500 first-prize and a $250 runner-up prize sponsored by Google Research will be awarded for the best student presentations.Timeline:Oct 4: Abstract submissionOct 14: NotificationNov 11: Registration deadlineNov 18: SymposiumFor more details including submission and registration instructions visit our symposium webpage:http://dolcit.cms.caltech.edu/scmls/and please help distribute our flyer:http://dolcit.cms.caltech.edu/scmls/scmls.pdf
2016,9,28,"Call for forecasting workshops in Cairns, Australia",https://robjhyndman.com/hyndsight/cairns-workshops/,"The 37th annual International Symposium on Forecasting will be held in Cairns Australia from 25-28 June 2017. We plan to hold some workshops on Sunday 25 June before the main conference.We are currently calling for workshop proposals. Proposals can be for a half-day or full-day workshop. As usual there will be a Practitioner Track at the conference which generally leads to excellent attendance at the workshops.
Though June sounds far away planning for the conference is well underway!"
2016,9,27,Eindhoven seminar on time series visualization,https://robjhyndman.com/hyndsight/eindhoven-seminar/,"I&rsquo;m currently in the Netherlands for a few weeks and I&rsquo;ll be giving a seminar at the Data Science Centre in Eindhoven next Wednesday afternoon on &ldquo;Visualization of big time series data&rdquo;. Details follow.Date: 5 October 2016 Time: 12.30-13.30 Venue: Filmhuis De Zwarte Doos 2 Den Dolech Eindhoven
Registration is required but free. Please book here.
Abstract:
It is becoming increasingly common for organizations to collect very large amounts of data over time."
2016,9,20,NIPS dialogue workshop,http://www.machinedlearnings.com/2016/09/nips-dialogue-workshop.html,I'm co-organizing a workshop on dialogue at NIPS 2016.  NIPS is not a traditional forum for dialogue research but there are increasing number of people (like myself!) in machine learning who are becoming interested in dialogue so the time seemed right.  From a personal perspective dialogue is interesting because 1) it smells like AI 2) recent advances in (deep learning) NLP techniques suggest the problem is more tractable and 3) corporate interest means both money and data will be plentiful.  Honestly the first point is very important: it was impossible to explain to my kids the minutiae on which I previously worked whereas now I can show them videos like this.  However there are a lot of issues in dialogue that aren't going to be demolished merely by using a flexible hypothesis class so I felt the need to educate myself about the activities of veteran dialogue researchers and the best way to ensure that was to organize a workshop and invite some of them.Hopefully you'll join the conversation.
2016,9,15,Forecasting large collections of related time series,https://robjhyndman.com/seminars/augsburg2016/,"Keynote talk given at the German Statistical Week Augsburg.
Abstract
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2016,9,15,What Is Pareto Optimality,https://prateekvjoshi.com/2016/09/15/what-is-pareto-optimality/,Let&#8217;s consider a business deal where there are multiple parties negotiating the terms. In such a situation it&#8217;s usually not possible for every single party to get everything it wants. They need to optimize their demands so that everyone comes out &#8230; Continue reading &#8594;
2016,9,14,k-Nearest Neighbors & Anomaly Detection Tutorial,https://algobeans.com/2016/09/14/k-nearest-neighbors-anomaly-detection-tutorial/,Do you know what gives red and white wine their colors? Use k-NN to discover the chemical make-up that defines typical types of wines as well as to detect atypical ones.
2016,9,14,Forecast intervals for aggregates,https://robjhyndman.com/hyndsight/forecast-intervals-for-aggregates/,"A common problem is to forecast the aggregate of several time periods of data using a model fitted to the disaggregated data. For example you may have monthly data but wish to forecast the total for the next year. Or you may have weekly data and want to forecast the total for the next four weeks.
If the point forecasts are means then adding them up will give a good estimate of the total."
2016,9,9,R package forecast v7.2 now on CRAN,https://robjhyndman.com/hyndsight/forecast-v7-2/,"I’ve pushed a minor update to the forecast package to CRAN. Some highlights are listed here.
Plotting time series with ggplot2 You can now facet a time series plot like this:
library(forecast) library(ggplot2) lungDeaths &lt;- cbind(mdeaths fdeaths) autoplot(lungDeaths facets=TRUE) So autoplot.mts now behaves similarly to plot.mts
 Multi-step fitted values The fitted function has a new argument h to allow computation of in-sample fitted values of more than one-step-ahead."
2016,9,1,R packages for forecast combinations,https://robjhyndman.com/hyndsight/forecast-combinations/,"It has been well-known since at least 1969 when Bates and Granger wrote their famous paper on “The Combination of Forecasts” that combining forecasts often leads to better forecast accuracy.
So it is helpful to have a couple of new R packages which do just that: opera and forecastHybrid.
opera Opera stands for “Online Prediction by ExpeRt Aggregation”. It was written by Pierre Gaillard and Yannig Goude and Pierre provides a nice introduction in the vignette."
2016,8,31,Sponsorship for the Cairns forecasting conference,https://robjhyndman.com/hyndsight/isf2017-sponsorship/,"Regular readers will know that the International Symposium on Forecasting is coming to Australia in June 2017. This is the leading international forecasting conference and one I&rsquo;ve attended every year for the past 17 years.
It will be held in Cairns Australia &mdash; one of the most beautiful locations in the country (and there is some stiff competition!) and right next to the Great Barrier Reef. Some further information is available on our website (still in progress)."
2016,8,28,Rmarkdown template for a Monash working paper,https://robjhyndman.com/hyndsight/rmarkdown-template/,"This is only directly relevant to my Monash students and colleagues but the same idea might be useful for adapting to other institutions.
Some recent changes in the rmarkdown and bookdown packages mean that it is now possible to produce working papers in exactly the same format as we previously used with LaTeX.Just install the MonashEBSTemplates package from github. You also need a recent version of LaTeX.
Then from within RStudio create a new document by selecting &ldquo;Rmarkdown&rdquo; &ldquo;From Template&rdquo; and select &ldquo;Monash EBS Working Paper&rdquo;."
2016,8,25,Random Forest Tutorial: Predicting Crime in San Francisco,https://algobeans.com/2016/08/25/random-forest-tutorial/,Learn how random forests an ensemble of decision trees can help predict where and when a crime will happen in San Francisco California.
2016,8,22,The thief package for R: Temporal HIErarchical Forecasting,https://robjhyndman.com/hyndsight/thief/,"I have a new R package available to do temporal hierarchical forecasting based on my paper with George Athanasopoulos Nikolaos Kourentzes and Fotios Petropoulos. (Guess the odd guy out there!)
It is called “thief” - an acronym for Temporal HIErarchical Forecasting. The idea is to take a seasonal time series and compute all possible temporal aggregations that result in an integer number of observations per year. For example a quarterly time series is aggregated to biannual and annual; while a monthly time series is aggregated to 2-monthly quarterly 4-monthly biannual and annual."
2016,8,18,"""Forecasting with R"" short course in Eindhoven",https://robjhyndman.com/hyndsight/eindhoven-course/,"I will be giving my 3-day short-course/workshop on &ldquo;Forecasting with R&rdquo; in Eindhoven (Netherlands) from 19-21 October.
Details at https://www.win.tue.nl/~adriemel/shortcourse.html
Register here"
2016,8,10,Tourism time series repository,https://robjhyndman.com/hyndsight/tourism-time-series/,"A few years ago I wrote a paper with George Athanasopoulos and others about a tourism forecasting competition. We originally made the data available as an online supplement to the paper but that has unfortunately since disappeared although the paper itself is still available.
So I am posting the data here in case anyone wants to use it for replicating our results or for other research purposes. The data are split into monthly quarterly and yearly data."
2016,7,27,Decision Trees Tutorial,https://algobeans.com/2016/07/27/decision-trees-tutorial/,Decision trees can be used to identify customer profiles or to predict who will resign. Using the Titanic dataset learn about its advantages and pitfalls as well as better alternatives.
2016,7,8,Update on dialogue progress,http://www.machinedlearnings.com/2016/07/update-on-dialogue-progress.html,In a recent blog post I discussed two ideas for moving dialogue forward; both ideas are related to the need to democratize access to the data required to evaluate a dialog system.  It turns out both ideas have already been advanced to some degree: Having computers &ldquo;talk&rdquo; to each other instead of with people: Marco Beroni is on it. Creating an open platform for online assessment: Maxine Eskenazi is on it.This is good to see.
2016,7,5,ICML 2016 Thoughts,http://www.machinedlearnings.com/2016/07/icml-2016-thoughts.html,ICML is too big for me to ``review'' it per se but I can provide a myopic perspective.The heavy hitting topics were Deep Learning Reinforcement Learning and Optimization; but there was a heavy tail of topics receiving attention.  It felt like deep learning was less dominant this year; but the success of deep learning has led to multiple application specific alternative venues (e.g. CVPR EMNLP) and ICLR is also a prestigious venue; so deep learning at ICML this year was heavyweight in either the more theoretical or multimodal works.  Arguably reinforcement learning and optimization both should partially count towards deep learning's footprint; reinforcement learning has been this way for at least a year but optimization has recently developed more interest in non-convex problems especially the kind that are empirically tractable in deep learning (sometimes although seemingly innocuous architecture changes can spoil the pudding; I suppose one dream of the optimization community would be the identification of a larger-than-convex class of problems which are still tractable to provide guidance).Here are some papers I liked:Strongly-Typed Recurrent Neural NetworksThe off-putting title makes sense if you are into type theory or if you've ever been a professional Haskell programmer and have had to figure out wtf a monad is.  tl;dr: if you put units of measurement on the various components of a recurrent neural network you'll discover that you are adding apples and oranges.  T-LSTM a modification of the standard LSTM to fix the problem behaves similarly empirically; but is amenable to analysis.  Theorem 1 was the nice part for me: the modified architectures are shown to compute temporal convolutions with dynamic pooling.  Could type consistency provide a useful prior on architectures?  That'd be a welcome development.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing and Dynamic Memory Networks for Visual and Textual Question AnsweringMore titles I'm not over the moon about: everybody seems to be equating &ldquo;memory&rdquo; = &ldquo;attention over current example substructure&rdquo;.  If you ask for the layperson's definition they would say that memory is about stuff you can't see at the moment (note: Jason started this particular abuse of terminology with End-to-End Memory Networks).  Pedantry aside undeniably these iterated attention architectures have become the state of the art in question-answering style problems and the baseline to beat.  Note since the next step in iterated attention is to incorporate previously seen and stored examples the use of the term &ldquo;memory&rdquo; will soon become less objectionable.From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification This is an alternative to the softmax layer (&ldquo;link function&rdquo;) used as the last layer of a neural network.  Softmax maps $\mathbb{R}^n$ onto the (interior of the) simplex whereas sparsemax projects onto the simplex.  One big difference is that sparsemax can &ldquo;hit the corners&rdquo; i.e. zero out some components.  Empirically the differences in aggregate task performance when swapping softmax with sparsemax are modest and attributable to the selection pressures on experimental sections. So why care?  Attention mechanisms are often implemented with softmax and it is plausible that a truly sparse attention mechanism might scale better (either computationally or statistically) to larger problems (such as those involving actual memory c.f. previous paragraph). Guided Cost Learning: Deep Inverse Optimal Control via Policy OptimizationI find Inverse RL unintuitive: didn't Vapnik say not to introduce difficult intermediate problems?  Nonetheless it seems to work well.  Perhaps requiring the learned policy to be &ldquo;rational&rdquo; under some cost function is a useful prior which mitigates sample complexity?  I'm not sure I have to noodle on it.  In the meantime cool videos of robots doing the dishes!Dueling Network Architectures for Deep Reinforcement Learning.Best paper so I'm not adding any value by pointing it out to you.  However after reading it meditate on why learning two things is better than learning one.  Then re-read the discussion section.  Then meditate on whether a similar variance isolation trick applies to your current problem.From the workshops some fun stuff I heard:Gerald Tesauro dusted off his old Neurogammon code ran it on a more powerful computer (his current laptop) and got much better results.  Unfortunately we cannot conclude that NVIDIA will solve AI for us if we wait long enough.  In 2 player games or in simulated environments more generally computational power equates to more data resources because you can simulate more.  In the real world we have sample complexity constraints: you have to perform actual actions to get actual rewards.  However in the same way that cars and planes are faster than people because they have unfair energetic advantages (we are 100W machines; airplanes are much higher) I think &ldquo;superhuman AI&rdquo; should it come about will be because of sample complexity advantages i.e. a distributed collection of robots that can perform more actions and experience more rewards (and remember and share all of them with each other).  So really Boston Dynamics not NVIDIA is the key to the singularity.  (In the meantime &hellip; buy my vitamins!)Ben Recht talked about the virtues of random hyperparameter optimization and an acceleration technique that looks like a cooler version of sub-linear debugging.  This style in my experience works.Leon Bottou pointed out that first order methods are now within constant factors of optimal convergence with the corollary that any putative improvement has to be extremely cheap to compute since it can only yield a constant factor.  He also presented a plausible improvement on batch normalization in the same talk.
2016,6,26,Accelerating progress in dialogue,http://www.machinedlearnings.com/2016/06/accelerating-progress-in-dialogue.html,In machine learning assessment isn't everything: it's the only thing.  That's the lesson from Imagenet (a labeled data set) and the Arcade Learning Environment (a simulation environment).  A simulator is the partial feedback analog of a labeled data set: something that lets any researcher assess the value of any policy.  Like data sets when simulators are publicly available and the associated task is well designed useful scientific innovation can proceed rapidly.In dialogue systems partial feedback problems abound: anyone who has ever unsuccessfully tried to get a job has considered the counterfactual: &ldquo;what if I had said something different?&rdquo;  Such questions are difficult to answer using offline data yet anybody trying to offline assess a dialogue system has to come up with some scheme for doing so and there are pitfalls.Online evaluation has different problems.  In isolation it is ideal; but for the scientific community at large it is problematic.  For example Honglak Lee has convinced the registrar of his school to allow him to deploy a live chat system for recommending course registrations.  This is a brilliant move on his part analogous to getting access to a particle accelerator in the 1940s: he'll be in a position to discover interesting stuff first.  But he can't share this resource broadly because 1) there are a finite number of chats and 2) the registrar presumably wants to ensure a quality experience.  Similar concerns underpin the recent explosion of interest in dialogue systems in the tech sector: companies with access to live dialogues are aware of the competitive moat this creates and they need to be careful in the treatment of their customers.That's fine and I like getting a paycheck but: how fast would reinforcement learning be advancing if the Arcade Learning Environment was only available at the University of Alberta?So here are some ideas.First we could have agents talk with each other to solve a task without any humans involved.  Perhaps this would lead to the same rapid progress that has been observed in 2 player games.  Arguably we might learn more about ants than people from such a line of research.  However with the humans out of the loop we could use simulated environments and democratize assessment.  Possibly we could discover something interesting about what it takes to learn to repeatedly communicate information to cooperate with another agent.Second we could make a platform that democratizes access to an online oracle.  Since online assessment is a scarce resource it would have to cost something but imagine: suppose we decide task foo is important.  We create a standard training program to create skilled crowdsource workers plus standard HITs which constitute the task quality control procedures etc.  Then we try as hard as possible to amortize these fixed costs across all researchers by letting anyone assess any model in the framework paying only the marginal costs of the oracle.  Finally instead of just doing this for task foo we try to make it easy for researchers to create new tasks as well.  To some degree the crowdsourcing industry does this already (for paying clients); and certainly researchers have been leveraging crowdsourcing extensively.  The question is how we can make it easier to 1) come up with reliable benchmark tasks that leverage online assessment and then 2) provide online access for every researcher at minimum cost.  Merely creating a data set from the crowdsourced task is not sufficient as it leads to the issues of offline evaluation.Of course it would be great for the previous paragraph if the task was not crowdsourced but some natural interactive task that is happening all the time at such large volume that the main issue is democratizing access.  One could imagine e.g. training on all transcripts of car talk and building a dialogue app that tries to diagnose car problems.  If it didn't totally suck people would not have to be paid to use it and it could support some level of online assessment for free.  Bootstrapping that however would itself be a major achievement.
2016,6,25,A new machine learning podcast,https://bickson.blogspot.com/2016/06/a-new-machine-learning-podcast.html,A new machine earning podcast by Sam Charrington. It tracks down the weekly machine learning news and summarizes the major events. Highly recommended!
2016,6,23,4th Large Scale Recommender Systems workshop - deadline extended,https://bickson.blogspot.com/2016/06/large-scale-recommender-systems.html,We have extended the deadline of our Large Scale Recommender Systems workshop to June 30. This is the 4th year we are organizing this workshop as part of ACM Recsys 2016. Anyone with novel work in the area of applied recommender systems is welcome to submit a talk proposal.
2016,6,20,Exploring time series collections used for forecast evaluation,https://robjhyndman.com/seminars/isf2016/,"International Symposium on Forecasting. Santander Spain
It is common practice to evaluate the strength of forecasting methods using collections of well-studied time series datasets such as the M3 data. But how diverse are these time series how challenging and do they enable us to study the unique strengths and weaknesses of different forecasting methods? In this paper we propose a visualisation method for a collection of time series that enables a time series to be represented as a point in a 2-dimensional feature space."
2016,6,19,GraphLab Create healthcare use case,https://bickson.blogspot.com/2016/06/graphlab-create-healthcare-use-case.html,A nice blog post from Mark Pinches our Manchester evangelist who is working with John Snow &nbsp;Labs. It shows how to use GraphLab Create for slicing dicing and aggregations of healthcare data.
2016,6,19,"Novomatic - comparing spark, pandas and Dato",https://bickson.blogspot.com/2016/06/novomatic-comparing-spark-pandas-and.html,Interesting slides from Bogdan Pirvu head data science @&nbsp;Novomatic (Austrian Gaming Industries). I met Bogdan at RecSys Vienna last fall and he got interested in GraphLab Create. In his talk Bogdan compares pandas Spark and Graphlab Create. Guess who is the winner?
2016,6,16,Prof. Alex Smola moves to Amazon,https://bickson.blogspot.com/2016/06/prof-alex-smola-moving-to-amazon.html,Here is the note he wrote on his blog. Alex will be heading Amazon Cloud ML effort. Definitely a great win for Amazon!If you like to hear Alex speaking about his recent research you should attend our Data Science Summit July 12-13 in SF. You are welcome to use discount code DSSfriend.
2016,6,14,Principal Component Analysis Tutorial,https://algobeans.com/2016/06/15/principal-component-analysis-tutorial/,You are exploring the nutritional content of food. How can food items be differentiated? How might they be classified? PCA derives underlying variables that help you slice your data for these insights.
2016,6,14,RSA fraud in social media report,https://bickson.blogspot.com/2016/06/rsa-fraud-in-social-media-report.html,An interesting report from RSA is found here. A lot of cool visualizations of social communities of fraudsters.
2016,6,10,The latest IJF issue with GEFCom2014 results,https://robjhyndman.com/hyndsight/ijf-gefcom2014/,"The latest issue of the IJF is a bumper issue with over 500 pages of forecasting insights.
The GEFCom2014 papers are included in a special section on probabilistic energy forecasting guest edited by Tao Hong and Pierre Pinson. This is a major milestone in energy forecasting research with the focus on probabilistic forecasting and forecast evaluation done using a quantile scoring method. Only a few years ago I was having to explain to energy professionals why you couldn&rsquo;t use a MAPE to evaluate a percentile forecast."
2016,6,4,LSTMs,https://shapeofdata.wordpress.com/2016/06/04/lstms/,In past posts I&#8217;ve described how Recurrent Neural Networks (RNNs) can be used to learn patterns in sequences of inputs and how the idea of unrolling can be used to train them. It turns out that there are some significant &#8230; Continue reading &#8594;
2016,6,3,2017 International Symposium on Energy Analytics,https://robjhyndman.com/hyndsight/isea2017/,"Predictive Energy Analytics in the Big Data World Cairns Australia June 22-23 2017 ISEA2017 
This will be a great conference and it is in a great location &mdash; Cairns Australia right by the Great Barrier Reef. Even better if you stay on you can attend the International Symposium on Forecasting which immediately follows the International Symposium on Energy Analytics.
So block out 22-28 June 2017 on your calendars so you can enjoy a tropical paradise in one of the most beautiful parts of Australia while attending two awesome conferences."
2016,6,1,Forecast v7 (part 2),https://robjhyndman.com/hyndsight/forecast7-part-2/,"As mentioned in my previous post on the forecast package v7 the most visible feature was the introduction of ggplot2 graphics. This post briefly summarizes the remaining new features of forecast v7.
library(forecast) library(ggplot2) tslm rewritten The tslm function is designed to fit linear models to time series data. It is intended to approximately mimic lm (and calls lm to do the estimation) but to package the output to remember the ts attributes."
2016,5,27,Explore Australian Elections Data with R,https://robjhyndman.com/hyndsight/eechidna/,This is a guest post by my colleague Professor Di Cook cross-posted from her Visiphilia blog. Di and I are two of the authors of the new eechidna package for R now on CRAN. The eechidna package has just been posted on CRAN in time for the longest election campaign in Australia since the 1950s. The next Federal election scheduled for 2nd July was announced a few weeks ago. A little before this a handful of academics met at the first ever Australian ROpenSci Unnconference at the Microsoft headquarters in Brisbane in an event primarily organised by students at the Queensland University of Technology.
2016,5,12,SSA helping you find a job,https://robjhyndman.com/hyndsight/ssa-jobs-board/,"One of the great services of the Statistical Society of Australia is an excellent jobs board advertising available jobs for statisticians data analysts data scientists etc. Jobs can be filtered by industry location and job function.
Today the SSA announced a new service to job seekers: CV/Resume Critique. As a job seeker registered on the SSA Job Board you now have the option to request a free confidential CV/resume evaluation from an expert and writer."
2016,5,10,Applied Machine Learning and AI @ FaceBook,https://practicalanalytics.wordpress.com/2016/05/10/data-science-cutting-edge-facebook/,“Facebook today cannot exist without AI. Every time you use Facebook or Instagram or Messenger you may not realize it but your experiences are being powered by AI.”  &#8212;  Joaquin Candela Facebook’s Applied ML team leader &#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212; &#8220;Machine learning is a core transformative way by which we are rethinking everything we are doing. Google Brain is [&#8230;]
2016,5,9,forecast v7 and ggplot2 graphics,https://robjhyndman.com/hyndsight/forecast7-ggplot2/,"Version 7 of the forecast package was released on CRAN about a month ago but I’m only just getting around to posting about the new features.
The most visible feature was the introduction of ggplot2 graphics. I first wrote the forecast package before ggplot2 existed and so only base graphics were available. But I figured it was time to modernize and use the nice features available from ggplot2. The following examples illustrate the main new graphical functionality."
2016,5,6,Automatic foRecasting using R,https://robjhyndman.com/seminars/automatic-forecasting-using-r/,Melbourne Data Science Initiative
2016,4,28,Bagging exponential smoothing methods using STL decomposition and Box-Cox transformation,https://robjhyndman.com/publications/bagging-ets/,Exponential smoothing is one of the most popular forecasting methods. We present a method for bootstrap aggregation (bagging) of exponential smoothing methods. The bagging uses a Box-Cox transformation followed by an STL decomposition to separate the time series into trend seasonal part and remainder. The remainder is then bootstrapped using a moving block bootstrap and a new series is assembled using this bootstrapped remainder. On the bootstrapped series an ensemble of exponential smoothing models is estimated.
2016,4,28,Rolling and Unrolling RNNs,https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/,A while back I discussed Recurrent Neural Networks (RNNs) a type of artificial neural network in which some of the connections between neurons point &#8220;backwards&#8221;. When a sequence of inputs is fed into such a network the backward arrows feed &#8230; Continue reading &#8594;
2016,4,27,Chief Data Officer Role & Challenges,https://practicalanalytics.wordpress.com/2016/04/27/chief-data-officer-challenges/,Big Data has been replaced by Machine Learning and AI as the next &#8220;must have&#8221; trend. Machine learning has caught the attention of venture capitalists. Chief Data Officers Chief Analytics Officers  Chief Data Science Officers and Chief Digital Officers are everywhere. The job is to leverage the latest in predictive analytics data science machine learning and multi-tenant cloud architecture to [&#8230;]
2016,4,26,Statistical software should remove *** notation for statistical significance,http://www.bzst.com/2016/04/statistical-software-should-remove.html,Now that the emotional storm following the American Statistical Association's statement on p-values is slowing down (is it? was there even a storm outside of the statistics area?) let's think about...
2016,4,13,Melbourne Data Science Initiative 2016,https://robjhyndman.com/hyndsight/medascin2016/,"In just over three weeks the inaugural MeDaScIn event will take place. This is an initiative to grow the talent pool of local data scientists and to promote Melbourne as a world city of excellence in Data Science.
The main event takes place on Friday 6th May with lots of interesting sounding titles and speakers from business and government. I&rsquo;m the only academic speaker on the program giving the closing talk on &ldquo;Automatic FoRecasting&rdquo;."
2016,4,9,Buy book,https://robjhyndman.com/unbelievable/buy-book/,"If you prefer a print or offline version you can buy a copy via one of the links below.
Buy a print copy via CreateSpace
Buy a print copy via Amazon
Buy an e-copy via Google Books"
2016,4,7,Thoughts on reviewing,http://www.machinedlearnings.com/2016/04/thoughts-on-reviewing.html,During ICML reviews I noticed that my personal take on reviewing is becoming increasingly distinct from my peers.  Personally I want to go to a conference and come away with renewed creativity and productivity.  Thus I like works that are thought provoking groundbreaking or particularly innovative; even if the execution is a bit off.  However I suspect most reviewers feel that accepting a paper is a validation of the quality and potential impact of the work.  There's no right answer here as far as I can tell.  Certainly great work should be accepted and presented but the problem is there really isn't that much of it per unit time.  Therefore like a producer on a Brittany Spears album we are faced with the problem of filling in the rest of the material.  The validation mindset leads to the bulk of accepted papers being extremely well executed marginal improvements.  It would be nice if the mix were tilted more towards the riskier novel papers.The validation mindset leads to reviews that are reminiscent of food critic reviews.  That might sound objectionable given that food quality is subjective and science is about objective truth: but the nips review experiment suggests that the ability of reviewers to objectively recognize the greatness of a paper is subjectively overrated.  Psychologists attempting to &ldquo;measure&rdquo; mental phenomena have struggled formally with the question of &ldquo;what is a measurement&rdquo; and lack of inter-rater reliability is a bad sign (also: test-retest reliability is important but it is unclear how to assess this as the reviewers will remember a paper).  So I wonder: how variable are the reviews among food critics for a good restaurant relative to submitted papers to a conference?  I honestly don't know the answer.What I do know is that while I want to be informed I also want to be inspired.  That's why I go to conferences.  I hope reviewers will keep this in mind when they read papers.
2016,3,28,Sample quantiles 20 years later,https://robjhyndman.com/hyndsight/sample-quantiles-20-years-later/,"Almost exactly 20 years ago I wrote a paper with Yanan Fan on how sample quantiles are computed in statistical software. It was cited 43 times in the first 10 years and 457 times in the next 10 years making it my third paper to receive 500+ citations.
So what happened in 2006 to suddenly increase the citations? I think it was a combination of things:  I wrote a new quantile() function (with Ivan Frohne) which made it into R core v2."
2016,3,28,Bibliography,https://robjhyndman.com/unbelievable/bibliography/,"Books and articles cited Angelakis A. N. and S. V. Spyridakis (1996). ‘The status of water resources in Minoan times: A preliminary study’. In: Diachronic Climatic Impacts on Water Resources: with emphasis on the Mediterranean region. Edited by A. N. Angelakis and A. S. Issar. Volume 36. NATO ASI Series. Berlin: Springer pp.161–191.
Archer G. L. (1982). Encyclopedia of Bible difficulties. Grand Rapids MI USA: Zondervan.
Bartholomew D. J. (2008). God chance and purpose: Can God have it both ways?"
2016,3,28,23. Welcome to the dark side,https://robjhyndman.com/unbelievable/ch23/,"This final collection of messages were from people who had already left the Christadelphian community in some cases many years before me. I know very few of these people but I appreciated their thoughtfulness in writing to me.
Message 31  I bet you have been getting the emails lately huh? Some pulling this way some pulling that way?
  I can tell from the comments on the blog that there is a contest going on for the control of your thoughts and mind."
2016,3,27,22. Me too,https://robjhyndman.com/unbelievable/ch22/,"Several people contacted me to say they were going through a similar process of deconversion. I think it was comforting for them (and for me) to know that the journey need not be a lonely one.
Message 26  I have similar questions but it is hard for me to break away. I think it would be liberating but not sure I can bear the abject grief of my parents. So do I continue to live a lie?"
2016,3,27,21. What now?,https://robjhyndman.com/unbelievable/ch21/,"A few of my correspondents wondered about my future. What would I do now I was no longer writing books and giving talks about the Bible? Would I start an atheist crusade with the same energy and enthusiasm I once gave to my preaching? Others wrote to me a few months after I had resigned asking how I was getting on.
Message 23  I know that you feel as though you have had an epiphany and that you can finally see the ‘truth’ clearly but from my very simple perspective it looks more like you have been blinded as a result of your intelligence … that you have had your nose in the books so long that you have forgotten to look up and see the evidence of God all around you."
2016,3,27,20. You must be ignorant,https://robjhyndman.com/unbelievable/ch20/,"A few parcels sent to me contained books and DVDs that were presumably supposed to convince me of my errors. I appreciate the care demonstrated by these gifts but they do assume that I am misinformed and ignorant about what I believe.
I find it rather insulting to be told that I gave up on a lifetime of faith and religious activity without really thinking through the issues and without reading widely."
2016,3,27,19. Was the pressure too much?,https://robjhyndman.com/unbelievable/ch19/,"A lot of people have assumed that something awful must have happened to me or that all the criticism directed at me has had some cumulative effect that has hurt my faith. It is not like that. I just don’t believe it and I did not want to spend the rest of my life pretending otherwise.
Message 17 I know you have taken a hammering for having opinions different to some of the more traditional ones and I have to say some of them I share … but having them shouldn’t have made you a target for people’s fear."
2016,3,27,18. I'm praying for you,https://robjhyndman.com/unbelievable/ch18/,Understandably a lot of my religious friends have said they are praying for me. I would have done the same when I was a believer. It was my instinctive response to any problem and even after I came to the view that God was probably imaginary I would still find myself occasionally wanting to pray when faced with difficulties. It is comforting to think that someone else might deal with the problems.
2016,3,27,17. You made me cry,https://robjhyndman.com/unbelievable/ch17/,"In contrast to the letters in the previous chapter most of the messages I received expressed sadness love and friendship for which I am deeply grateful.
I have very few comments on these. What can I say? I feel sad too. I have stayed in touch with most of these writers and they remain dear friends even though we no longer have a shared faith.
Message 4 Just read your latest blog post … firstly wanted to say that I love and respect you and always will regardless of what you call yourself now But secondly you made me cry today … and for that you need a clip over the ears!"
2016,3,27,16. Repent or die,https://robjhyndman.com/unbelievable/ch16/,"I have received several hundred emails letters and other messages as a result of my deconversion and resignation. Many of them were from friends some from people who had read my books or heard me speak and some were from people who knew me only as a professor of statistics but who were moved to send me their thoughts.
In this last section I have included a selection of these messages along with some retrospective reflections."
2016,3,27,15. I am not an axe-murderer,https://robjhyndman.com/unbelievable/ch15/,Christians often claim that their beliefs underpin their morality and those who do not believe in God or the Bible must therefore lack a moral compass. They assume that values and morals are a consequence of holding the true doctrines. Well I have not become an axe-murderer since becoming an unbeliever I have not suddenly become unfaithful to my wife nor have I begun torturing kittens I have not even started swearing.
2016,3,27,14. Did you hear about the guy...?,https://robjhyndman.com/unbelievable/ch14/,If the miracles of Jesus happened as reported then they would provide evidence for him having supernatural power. If he really walked across the Sea of Galilee provided food for thousands of people from five loaves and two fish and raised Lazarus from the dead then of course he is supernatural. If all those things are true then his claims to be the Son of God would need to be taken seriously.
2016,3,27,13. Drink the magic potion,https://robjhyndman.com/unbelievable/ch13/,"The Way of Life book (pp.9–10) gives two examples of laws from Leviticus and Deuteronomy that supposedly demonstrate an understanding of science that was not possible for the Israelites to have developed.
Food prohibitions The first example is that the Law of Moses prohibited eating seafood (Leviticus 11:4–810–12) which we now know can cause intestinal problems and food poisoning without careful preparation and refrigeration.
However it was not necessary to understand the science for the Israelites to have noticed that people who ate seafood often became ill."
2016,3,27,12. Does God make mistakes?,https://robjhyndman.com/unbelievable/ch12/,The contradictions game Three times I have debated skeptics of the Bible and each time they raised Bible contradictions. I knew they would and I was prepared. I am rather good at reconciling contradictions. Give me a long list of apparent contradictions and I can usually think of explanations. The difficulty is that the explanations are often contrived and can seem like a desperate attempt to explain away the problem. They satisfy the believers but unbelievers remain unconvinced.
2016,3,27,11. Artefacts and anachronisms,https://robjhyndman.com/unbelievable/ch11/,I have given nearly 50 talks on biblical archaeology. One was entitled “Bible skeletons and fingerprints” in which I described some recent archaeological finds that supported and illuminated the biblical record. I showed photographs of the ossuary of Caiaphas from the first century AD containing a skeleton of a 60 year old man — probably Caiaphas the high priest mentioned in the gospels. I covered the tiny clay seals that were found in Jerusalem from around 586BC.
2016,3,27,10. Evolving views on creation,https://robjhyndman.com/unbelievable/ch10/,Sunday School nonsense In my early teens I was taught in Sunday School that scientists had got it wrong — that the universe was actually a few thousand years old not the billions of years claimed by the “evolutionists”. I found that confusing. I was aware that the light from the nearest galaxy to our own took at least a few tens of thousands of years to get here and the light from many other galaxies took millions of years to arrive.
2016,3,27,9. Legend of the empty tomb,https://robjhyndman.com/unbelievable/ch9/,"The resurrection of Jesus is often cited as evidence of the Bible’s inspiration or at least of God’s existence. There is an obvious problem here — all we know about the resurrection of Jesus comes from the Bible so it cannot be used as evidence in support of itself. But leaving that difficulty aside let’s review what the Bible actually says about it.
Harmonizing the accounts The resurrection accounts are notoriously difficult to harmonize."
2016,3,27,8. The biblical crystal ball,https://robjhyndman.com/unbelievable/ch8/,"Forecasting is my area of expertise. I am Editor-in-Chief of the International Journal of Forecasting and I have written several textbooks on statistical forecasting. I spend many hours every week thinking about the probabilities associated with future events. So prophecy which is closely related to forecasting has always been of particular interest to me.
When I was a believer my favourite fulfilled prophecies were
 the return of Israel to their land; Daniel’s prophecies of four empires; the destruction of Tyre; and the crucifixion of Jesus."
2016,3,27,7. Thank God,https://robjhyndman.com/unbelievable/ch7/,"It is natural for believers to thank God when they survive a natural disaster or even when they avoid the effects of man-made tragedies. When the great tsunami of Boxing Day 2004 wiped out huge areas of Thailand Indonesia India and Sri Lanka many survivors thanked God for protecting them. Others wondered what God was doing while 230000 people died.
When the Australian Black Saturday bushfires in February 2009 killed 173 people many survivors thanked God for protecting them."
2016,3,27,6. What are the chances?,https://robjhyndman.com/unbelievable/ch6/,"One of the first things I have been asked by people who find out that I no longer believe is “What about answered prayers?”.
Prayer is not falsifiable when you already believe Prayer is not falsifiable if you begin by assuming that God exists and is listening. If someone prays that X occurs and it does then they say that the prayer has been answered. If X does not occur then they say the answer was no it wasn’t God’s will."
2016,3,27,5. Talking to myself,https://robjhyndman.com/unbelievable/ch5/,I tried hard to have an active and regular prayer time every morning for many many years. I would rise early and spend time jotting some thoughts in my prayer journal. Then I would offer a prayer based on what I had written. For a few months in 2002 when we lived in Canberra I would walk to the top of a nearby mountain each morning and surrounded by grazing kangaroos I would say my prayers as the sun rose over Black Mountain.
2016,3,26,4. What would convince you?,https://robjhyndman.com/unbelievable/ch4/,There is an old joke about a psychiatrist working with a patient who was convinced that he was dead. The psychiatrist says “So you think you’re dead. Do dead people bleed?” to which the patient replies “No of course not.” Then the psychiatrist takes a sharp needle and pricks the patient’s finger drawing a drop of blood. The shocked patient says “Well what do you know. Dead people do bleed!”
2016,3,26,3. Which is the better story?,https://robjhyndman.com/unbelievable/ch3/,The beautiful movie Life of Pi is a story about a boy (Pi) raised in India where his father owns a small zoo. He is deeply religious and simultaneously embraces Hinduism Christianity and Islam. At one level the book is about Pi’s deep faith in the face of his father’s atheist disapproval. When the family and zoo emigrate for Canada the ship founders in a storm and Pi finds himself on a lifeboat with a hyena a chimpanzee a zebra and a Bengal tiger for company.
2016,3,26,2. An end of faith,https://robjhyndman.com/unbelievable/ch2/,"This is the blog post in which I announced my loss of faith and my resignation from the Christadelphian community — 29 July 2013
This week I resigned from the Christadelphian faith after nearly 30 years as a member and having attended Christadelphian activities almost every week of my life.
I resigned because I no longer believe. I don’t believe the Bible is inspired by God. I am not even sure there is a God."
2016,3,26,1. No reason to believe,https://robjhyndman.com/unbelievable/ch1/,Two years after I started my blog I bumped into an old friend at church one Sunday. I hadn’t seen her for a couple of years and I asked after her family. She told me that Josh her oldest son had left the Christadelphian community and that he no longer believed in God. I decided to write to him to ask what had happened. After some correspondence Josh read my blog and responded with the following email (slightly edited):
2016,3,26,Preface,https://robjhyndman.com/unbelievable/preface/,"I was a Christadelphian1 for nearly 30 years from age 16 until I resigned in July 2013 when I no longer thought that there was sufficient evidence to warrant belief in the Bible or Christianity. This book explains why I changed.
Many of my Christadelphian friends have asked why I no longer believe and I want to give them the answer they deserve. After many years of speaking at Christadelphian meetings in Australia and in several other countries there are a lot of people who have heard me talk about my faith and will have wondered what has happened."
2016,3,24,A non-traditional definition of Big Data: Big is Relative,http://www.bzst.com/2016/03/a-non-traditional-definition-of-big.html,"I've noticed that in almost every talk or discussion that involves the term Big Data one of the first slides by the presenter or the first questions to be asked by the audience is ""what is Big..."
2016,3,23,Plotting overlapping prediction intervals,https://robjhyndman.com/hyndsight/overlappingpi/,"I often see figures with two sets of prediction intervals plotted on the same graph using different line types to distinguish them. The results are almost always unreadable. A better way to do this is to use semi-transparent shaded regions. Here is an example showing two sets of forecasts for the Nile River flow.
library(forecast) f1 = forecast(auto.arima(Nile lambda=0) h=20 level=95) f2 = forecast(ets(Nile) h=20 level=95) plot(f1 shadecol=rgb(001.4) flwd=1 main=&quot;Forecasts of Nile River flow&quot; xlab=&quot;Year&quot; ylab=&quot;Billions of cubic metres&quot;) polygon(c(time(f2$mean)rev(time(f2$mean))) c(f2$lowerrev(f2$upper)) col=rgb(100."
2016,3,21,"rOpenSci unconference in Brisbane, 21-22 April 2016",https://robjhyndman.com/hyndsight/ropensci2016/,"The first rOpenSci unconference in Australia will be held on Thursday and Friday (April 21-22) in Brisbane at the Microsoft Innovation Centre.
This event will bring together researchers developers data scientists and open data enthusiasts from industry government and university. The aim is to conceptualise and develop R-based tools that address current challenges in data science open science and reproducibility.
Past examples of the projects can here here and here. Also here."
2016,3,20,Robotic Process Automation + Analytics,https://practicalanalytics.wordpress.com/2016/03/20/robotic-process-automation-analytics/,&#8220;Looking to the future the next big step will be for the very concept of the “device” to fade away. Over time the computer itself—whatever its form factor—will be an intelligent assistant helping you through your day. We will move from mobile first to an AI first world.&#8221; &#8212; Sundar Pichai CEO Google A global oil [&#8230;]
2016,3,9,Omni-channel Retail Paradox: the Curse of Digital,https://practicalanalytics.wordpress.com/2016/03/09/the-retail-paradox-the-curse-of-digital/,Everyone knows that the retail industry is being transformed by digital analytics and big data. Winning requires continual data-driven experimentation and transformation.   Shortened time from idea-to-app is a constant challenge. Evidence of this &#8220;digital disruption&#8221; by category are mounting every day. Wal-Mart closes 269 stores as it retools portfolio to compete with online natives like [&#8230;]
2016,3,9,Monash Business Analytics Team Profile,https://robjhyndman.com/hyndsight/monash-insider/,Our research group been growing lately as you can see below! We were featured in the latest issue of the Monash newsletter The Insider. Check it out.
2016,3,4,Model variance for ARIMA models,https://robjhyndman.com/hyndsight/model-variance-for-arima-models/,"From today&rsquo;s email:
 I wanted to ask you about your R forecast package in particular the Arima() function. We are using this function to fit an ARIMAX model and produce model estimates and standard errors which in turn can be used to get p-values and later model forecasts. To double check our work we are also fitting the same model in SAS using PROC ARIMA and comparing model coefficients and output."
2016,3,3,Analytics and ML Use Case –  Robo-Advisors in Wealth Management,https://practicalanalytics.wordpress.com/2016/03/03/4917/,New data driven FinTech business models built on Hadoop Spark and Machine Learning are rapidly emerging and disrupting wealth management.  Here is my recent posting from disruptivedigital.wordpress.com about one such use case&#8230; Robo-Advisors. We are in the early stages of a generational shift in wealth management especially &#8220;plain vanilla&#8221; investing for the mass affluent and millennial segment.  Until recently you had [&#8230;]
2016,2,28,On sampling methods for costly multi-objective black-box optimization,https://robjhyndman.com/publications/sampling-multiobjective-optimization/,We investigate the impact of different sampling techniques on the performance of multi-objective optimization methods applied to costly black-box optimization problems. Such problems are often solved using an algorithm in which a surrogate model approximates the true objective function and provides predicted objective values at a lower cost. As the surrogate model is based on evaluations of a small number of points the quality of the initial sample can have a great effect on the overall effectiveness of the optimization.
2016,2,24,Omitting outliers,https://robjhyndman.com/hyndsight/omitting-outliers/,"Someone sent me this email today:
 One of my colleagues said that you once said/wrote that you had encountered very few real outliers in your work and that normally the &ldquo;outlier-looking&rdquo; data points were proper data points that should not have been treated as outliers. Have you discussed this in writing? If so I would love to read it.
 I don&rsquo;t think I&rsquo;ve ever said or written anything quite like that and I see lots of outliers in real data."
2016,2,24,Making data analysis easier: Hadley Wickham at WOMBAT2016,https://robjhyndman.com/hyndsight/wombat2016-hadley/,"Slides for Hadley&rsquo;s talk
The slides for all the other talks from the workshop are also now online at wombat2016.org"
2016,2,18,Big Data is Entering the Trough of Disillusionment,https://practicalanalytics.wordpress.com/2016/02/18/big-data-is-entering-the-trough-of-disillusionment/,Big data data lakes and becoming data-driven is no longer news or novel. Hype is not enough anymore. The challenge today for leaders in every enterprises is (a) how to monetize data? (b) how to create enterprise class platforms instead of sandboxes? Basically how can data analytics and insight drive digital operations and digital transformation? The paradox [&#8230;]
2016,2,18,Making forecasting easier: forecast v7 for R,https://robjhyndman.com/seminars/wombat-2016/,Talk given at WOMBAT 2016 conference
2016,2,17,Electricity price forecasting competition,https://robjhyndman.com/hyndsight/electricity-price-forecasting-competition/,The GEFCom competitions have been a great success in generating good research on forecasting methods for electricity demand and in enabling a comprehensive comparative evaluation of various methods. But they have only considered price forecasting in a simplified setting. So I&rsquo;m happy to see this challenge is being taken up as part of the European Energy Market Conference for 2016 to be held from 6-9 June at the University of Porto in Portugal.
2016,2,9,Big Data Evolving Landscape – 2016,https://practicalanalytics.wordpress.com/2016/02/09/big-data-evolving-landscape-2016/,In 2017 Big Data as a term is pretty much dead.   The market &#8211; VCs Startups and F500 companies &#8211; have stopped using Big Data as a term to describe their programs/projects and have moved to Machine Learning (ML) and AI. Everything Big Data is now AI. 2015 was a year of significant change [&#8230;]
2016,2,4,Forecasting uncertainty in electricity smart meter data by boosting additive quantile regression,https://robjhyndman.com/publications/smart-meter-quantiles/,Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared to traditional electricity data based on aggregated consumption smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand in order to undertake appropriate planning of generation and distribution. We propose a probabilistic forecasting method where a different quantile regression model is estimated for each quantile of the future distribution.
2016,1,31,Fast computation of reconciled forecasts for hierarchical and grouped time series,https://robjhyndman.com/publications/hgts/,We show that the least squares approach to reconciling hierarchical time series forecasts can be extended to much more general collections of time series with aggregation constraints. The constraints arise due to the need for forecasts of collections of time series to add up in the same way as the observed time series. We also show that the computations involved can be handled efficiently by exploiting the structure of the associated design matrix or by using sparse matrix routines.
2016,1,31,The Future has more Co-authors,http://www.machinedlearnings.com/2016/01/the-future-has-more-co-authors.html,Here's something to noodle on while you finalize your ICML submissions.Have you ever heard of Max Martin?  You probably haven't which is something considering he (currently) has 21 #1 hits in the United States.  Lennon (26) and McCartney (32) have more but Max Martin has the advantage of still being alive to catch up.  A phenomenal genius right?  Well yes but if you look at his material he always has co-authors usually several.  His process is highly collaborative as he manages a constellation of young songwriting talent which he nurtures like a good advisor does grad students and post-docs.  In the increasingly winner-take-all dynamics of pop music it's better to write a #1 song with 5 people then to write a #20 song by yourself. I think Machine Learning is headed in this direction.  Already in Physics pushing the envelope experimentally involves an astonishing number of co-authors.  Presumably Physics theory papers have fewer co-authors but since the standard model is too damn good in order to make real progress some amazingly difficult experimental work is required. Now consider an historic recent achievement: conquering Go.  That paper has 20 authors.  Nature papers are a big deal so presumably everybody is trying to attribute fairly and this leads to a long author list: nonetheless there is no denying that this achievement required many people working together with disparate skills.   I think the days where Hastie and Tibshirani can just crush it by themselves like Lennon and McCartney in their day are over.  People with the right theoretical ideas to move something forward in e.g. reinforcement learning are still going to need a small army of developers and systems experts to build the tools necessary.So here's some advice to any young aspiring academics out there envisioning a future Eureka moment alone at a white-board: if you want to be relevant pair up with as many talented people as you can.
2016,1,30,Bayesian rank selection in multivariate regression,https://robjhyndman.com/publications/bayesian-rank-selection-in-multivariate-regression/,Estimating the rank of the coefficient matrix is a major challenge in multivariate regression including vector autoregression (VAR). In this paper we develop a novel fully Bayesian approach that allows for rank estimation. The key to our approach is reparameterizing the coefficient matrix using its singular value decomposition and conducting Bayesian inference on the decomposed parameters. By implementing a stochastic search variable selection on the singular values of the coefficient matrix the ultimate selected rank can be identified as the number of nonzero singular values.
2016,1,28,What's your Hall number?,https://robjhyndman.com/hyndsight/whats-your-hall-number/,Today I attended the funeral of Peter Hall one of the finest mathematical statisticians ever to walk the earth and easily the best from Australia. One of the most remarkable things about Peter was his astonishing productivity with over 600 papers. As I sat in the audience I realised that many of the people there were probably coauthors of papers with Peter and I wondered how many statisticians in the world would have been his coauthors or second-degree co-authors.
2016,1,25,Probabilistic Energy Forecasting: Global Energy Forecasting Competition 2014 and Beyond,https://robjhyndman.com/publications/gefcom2014/,The energy industry has been going through a significant modernization process over the last decade. Its infrastructure is being upgraded rapidly. The supply demand and prices are becoming more volatile and less predictable than ever before. Even its business model is being challenged fundamentally. In this competitive and dynamic environment many decision-making processes rely on probabilistic forecasts to quantify the uncertain future. Although most of the papers in the energy forecasting literature focus on point or single-valued forecasts the research interest in probabilistic energy forecasting research has taken off rapidly in recent years.
2016,1,24,Long-term forecasts of age-specific participation rates with functional data models,https://robjhyndman.com/publications/participation-rates/,Many countries have implemented social programs providing long-term financial or in-kind entitlements. These programs often focus on specific age-groups and consequently their expenditure streams are subject to demographic change. Given the strains already existing on public budgets long-term forecasts are an increasingly important instrument to monitor the budgetary consequences of social programs. The expected development of the labour force is a key input to these forecasts. We suggest combining a functional data approach to age-profiles of labour market participation rates with information on education marital status and other exogenous variables to improve long-term forecasts of labour supply.
2016,1,20,ACEMS Business Analytics Prize 2016,https://robjhyndman.com/hyndsight/acems-prize-2016/,"We have established a new annual prize for research students at Monash University in the general area of business analytics funded by the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS). The rules of the award are listed below.
  The student must have submitted a paper to a high quality journal or refereed conference on some topic in the general area of business analytics computational statistics or data visualization."
2016,1,20,Continuous Bayes’ Theorem,https://shapeofdata.wordpress.com/2016/01/20/continuous-bayes-theorem/,Bayes&#8217; Rule is one of the fundamental Theorems of statistics but up until recently I have to admit I was never very impressed with it. Bayes&#8217; gives you a way of determining the probability that a given event will occur or &#8230; Continue reading &#8594;
2016,1,12,Attention: More Musings,http://www.machinedlearnings.com/2016/01/attention-more-musings.html,The attention model I posed last post is still reasonable but the comparison model is not.  (These revelations are the fallout of a fun conversation with myself Nikos and Sham Kakade.  Sham recently took a faculty position at the University of Washington which is my neck of the woods.)As a reminder the attention model is a binary classifier which takes matrix valued inputs $X \in \mathbb{R}^{d \times k}$ with $d$ features and $k$ columns weights (&ldquo;attends&rdquo;) to some columns more than others via parameter $v \in \mathbb{R}^d$ and then predicts with parameter $u \in \mathbb{R}^d$ \[\begin{aligned}\hat y &= \mathrm{sgn \;} \left( u^\top X z \right) \\z &= \frac{\exp \left( v^\top X_i \right)}{\sum_k \exp \left (v^\top X_k \right) }.\end{aligned}\]  I changed the notation slightly from my last post ($w \rightarrow u$) the reasons for which will be clear shortly.  In the previous post the comparison model was an unconstrained linear predictor on all columns \[\begin{aligned}\hat y &= \mathrm{sgn \;} \left( w^\top \mathrm{vec\} (X) \right)\end{aligned}\] with $w \in \mathbb{R}^{d k}$.  But this is not a good comparison model because the attention model in nonlinear in ways this model cannot achieve: apples and oranges really.This is easier to see with linear attention and a regression task.  A linear attention model weights each column according to $(v^\top X_i)$ e.g. $(v^\top X_i)$ is close to zero for &ldquo;background&rdquo; or &ldquo;irrelevant&rdquo; stuff and is appreciably nonzero for &ldquo;foreground&rdquo; or &ldquo;relevant&rdquo; stuff.  In that case \[\begin{aligned}\hat y &= u^\top X (v^\top X)^\top = \mathrm{tr} \left( X X^\top v u^\top \right)\end{aligned}\] (using properties of the trace) which looks like a rank-1 assumption on a full model \[\begin{aligned} \hat y &= \mathrm{tr} \left( X X^\top W \right) = \sum_{ijk} X_{ik} W_{ij} X_{jk} \\%&= \sum_i \left( X X^\top W \right)_{ii} = \sum_{ij} \left( X X^\top \right) _{ij} W_{ji} \\%&= \sum_{ijk} X_{ik} X_{jk} W_{ji} = \sum_{ijk} X_{ik} X_{jk} W_{ij}\end{aligned}\] where $W \in \mathbb{R}^{d \times d}$ and w.l.o.g. symmetric.  (Now hopefully the notation change makes sense: the letters $U$ and $V$ are often used for the left and right singular spaces of the SVD.)  The symmetry of $W$ confuses me because it suggests $u$ and $v$ are the same (but then the prediction is nonnegative?) so clearly more thinking is required.  However this gives a bit of insight and perhaps this leads to some known results about sample complexity.
2016,1,11,Farewell Peter Hall (1951-2016),https://robjhyndman.com/hyndsight/farewell-peter-hall-1951-2016/,Peter Hall passed away on Saturday after a long battle with illness over the last couple of years. No statistician will need reminding of Peter&rsquo;s extensive contributions to the field. He had over 500 published papers and had won every major award available many of them listed on his Wikipedia page.I spent a few months visiting Peter at ANU in 2002 and we wrote a couple of papers together (here and here).
2016,1,2,Data Science Positions for Sports Analytics,http://yyue.blogspot.com/2016/01/data-science-positions-for-sports.html,I want to give a plug for STATS LLC which is building a data science team and has several openings for data scientist positions.  For those who don't know STATS is sports data company that provides the tracking data for the National Basketball Association amongst other sports and leagues. STATS also recently acquired Prozone which provides tracking data for many professional soccer leagues around the world. Sports analytics is definitely entering an exciting phase due to the rapid growth of new data sources that offer far greater granularity than was possible before.  See e.g. these papers that analyze tracking data provided by STATS and Prozone.Patrick Lucey is the new Director of Data Science.  I previously worked with Patrick at Disney Research and I can vouch for him being a great collaborator with lots of fantastic ideas and unbounded enthusiasm for sports analytics research.
2015,12,31,Another look at forecast-accuracy metrics for intermittent demand,https://robjhyndman.com/publications/accuracy-intermittent-demand/,
2015,12,31,Measuring forecast accuracy,https://robjhyndman.com/publications/measuring-forecast-accuracy/,
2015,12,31,Thoughts on NIPS 2015 and OpenAI,http://yyue.blogspot.com/2015/12/thoughts-on-nips-2015-and-openai.html,"A few weeks ago I attended NIPS 2015 which turned out to be (by far) the largest machine learning conference ever. With nearly 4000 attendees the conference saw a roughly 50% increase from the previous year.  Much of this growth seems fueled by industry interest especially in topics such as deep learning and large scale learning.  Deep learning in particular seems to be all the rage these days at least in the public zeitgeist.  I think this is great for the field because this degree of interest will also percolate to the rest of machine learning more broadly.  There have been plenty of posts regarding NIPS already (see: Sebastien Bubeck Neil Lawrence John Langford Paul Mineiro and Hal Daume) with plenty of great pointers to interesting NIPS papers that I'll hopefully get around to reading soon.  On my end I didn't get a chance to see too many papers in part because I was helping presenting a poster during one poster session and a demo during another.  But I did very much enjoy many of the talks especially during the workshops.OpenAIPerhaps the biggest sensation at NIPS was the announcement of OpenAI which is a non-profit artificial intelligence research company with $1B in endowment donated by people such as Sam Altman Elon Musk Peter Thiel and others.  The core ideal of OpenAI is to promote open research in Artificial Intelligence.  For the most part not much is known about how OpenAI will operate (and from what I've gathered the people at OpenAI haven't fully decided on a strategy yet either).  One thing that I do know on good authority is that OpenAI will NOT be patenting their research.  Nonetheless there have already been many reactions to OpenAI from the usual ""robots will steal our jobs"" trope to nuanced concerns voiced by machine learning expert Neil Lawrence observing that open access to data is just as important as open access to research and systems.  I do very much agree with Neil's point and I think that one of the best things that OpenAI can do for the research community is to generate interesting new datasets and testbeds.  There have also been concerns voiced that the founding team is overwhelmingly deep learning people.  I don't think this is much of an issue at the moment because representation learning has been the biggest practical leap forward and giving broader access to learned representations is a great thing.  The announcement has even caught the attention of rationalists such as Scott Alexander who voiced concerns about whether AI research should be open at all for risk of losing control of the technology and potentially leading to the catastrophic results.  Scott's concern is a meta-concern about the current mentality of AI research being an arms race and institutions such as OpenAI not focusing on ""controlling"" access to AI that could become dangerous.  These meta-concerns are predicated on the assumptions that hard takeoff of AGI is a legitimate existential threat to humanity (which I agree with) and that existing institutions such as OpenAI could directly lead to that happening (which I strongly disagree with).  I realize that OpenAI ponders about human-level intelligence in their opening blog post but that's just a mission statement of sorts.  For instance Google while awesome has (thus far) fallen quite short of their mission to ""organize the world's information and make it universally accessible and useful"".  Likewise I don't expect OpenAI to succeed in their mission statement anytime soon.Most machine learning experts probably do take an overly myopic view of machine learning progress which is partly due to the aforementioned research arms race but also just due to how research works (i.e. it is REALLY hard to make tangible progress on something that you can't even begin to rigorously and precisely reason about).  However from what I've read rationalist non-experts conversely tend to phrase things in such imprecise terms that it's hard to have a substantive discussion between the two communities.  I imagine the ""truth"" such as it is is somewhere in the middle. Perhaps one should gather both camps together for a common discussion.What is definitely going to happen in the near term is that access to AI technologies will be an increasingly important competitive advantage moving forward.  And it's great that institutions such as OpenAI will help promote open access to those technologies.I am optimistic that the crew at OpenAI will explore alternative mechanisms in contrast to NSF-style funding of research and how places like the Allen Institute engages in research.  I think it'll be exciting to see what comes out of that process.  Hopefully OpenAI will also engage with places like the Future of Humanity Institute and maybe even create forums that bring together people like Stuart Russell Eric Horvitz Scott Alexander and Eliezer Yudkowsky.Cynthia Dwork on Universal Adaptive Data AnalysisCynthia Dwork gave a great talk on using differential privacy to guard against overfitting when re-using a validation set multiple times.  See this Science paper for more details.  The basic idea is that when you use your validation set to evaluate the performance of a model do so in a differentially private way so that you don't overfit to the idiosyncrasies of the validation set.  See for instance this paper describing an application to Kaggle-style competitions. This result demonstrates a great instance of (unexpected?) convergence between different areas of study: privacy-preserving computation and machine learning.Jerry Zhu on Machine TeachingJerry Zhu has been doing very interesting work on Machine Teaching which he talked about at the NIPS workshop on adaptive machine learning.  Roughly speaking machine teaching is the computational and statistical problem of how to select training examples to teach a learner as quickly as possible.  One can think of machine teaching as the converse of active learning where instead of the learner actively querying for training examples a teacher actively provides them.  Machine teaching has a wide range of applications but the one that I'm most interested in is when the learner is a human.  As models become necessarily more complex in the quest for predictive accuracy it is important that we devise methods to keep these models somehow interpretable to humans.  One way is to use a machine teaching approach to quickly show the human what concepts the trained model has learned.  For instance this approach would have  applications debugging complicated machine learning models.Rich Caruana on Interpretable Machine Learning for Health CareOn the flip side Rich Carauna talked about training models that are inherently interpretable by domain experts such as medical professionals.  Of course these models are only applicable in restricted domains such as when there is a ""sufficient"" set of hand-crafted features such that a generalized additive model can accurately capture the phenomenon of interest.  The approach was applied to two settings: predicting the risk of pneumonia and 30-day re-admission.   One interesting consequence of this study was that these interpretable models could be used to tease out biases in the data collection process.  For instance the model predicted that patients with asthma are at lower risk of dying from pneumonia.  Consulting with medical experts revealed that historically patients with asthma are more closely monitored for signs of pneumonia and so the disease is detected much earlier than for the general populace.  Nonetheless it's clear that one wouldn't want a predictive model to predict a lower risk of pneumonia for patients with asthma -- that was simply a consequence of how the historical data was collected.  See this paper for details.Zoubin Ghahramani on Probabilistic ModelsZoubin Ghahramani gave a keynote talk on probabilistic models.  During this deep learning craze it's important keep in mind that properly quantifying uncertainty is often a critical component as well.  We are rarely given perfect information and so we can rarely make perfect predictions.  In order to make informed decisions our models should make calibrated probabilities regarding so that we can properly weigh different tradeoffs.  Recall that one of the critical aspects of the Jeopardy! winning IBM Watson machine was being able to properly calibrate its own confidence in the right answer (or question).  Another point that Zoubin touched on was rational allocation of computational resources under uncertainty.  See also this great essay on the interplay between machine learning and statistics by Max Welling.Interesting PapersAs I mentioned earlier I didn't get a chance to check out too many posters but here are a few that I did see which I found quite interesting.Generalization in Adaptive Data Analysis and Holdout Reuseby Cynthia Dwork Vitaly Feldman Moritz Hardt Toniann Pitassi Omer Reingold Aaron RothThis paper generalizes previous work on adaptive data analysis by: 1) allow the query to the validation set be adaptive to the result of previous queries and 2) provide a more general definition of adaptive data analysis.Logarithmic Time Online Multiclass Prediction by Anna Choromanska John LangfordThis paper studies how to quickly construct multiclass classifiers whose running time is logarithmic in the number of classes.  This approach is especially useful for settings where the number of classes is enormous which is also known as Extreme Multiclass Classification.Spatial Transformer Networksby Max Jaderberg Karen Simonyan Andrew Zisserman Koray KavukcuogluThis paper studies how to incorporate more invariants into convolutional neural networks beyond just shift invariance.  The most obvious cases are being invariant to rotation and skew.  See also this post.Optimization as Estimation with Gaussian Processes in Bandit Settingsby Zi Wang Bolei Zhou Stefanie JegelkaA preliminary version of this paper was presented at the Women in Machine Learning Workshop at NIPS and will be formally published at AISTATS 2016.  This is a really wonderful paper that unifies to some extent two of the most popular views in Bayesian optimization: UCB-style bandit algorithms and probability of improvement (PI) algorithms.  One obvious future direction is to also unify with expected improvement (EI) algorithms as well.Fast Convergence of Regularized Learning in Gamesby Vasilis Syrgkanis Alekh Agarwal Haipeng Luo Robert E. SchapireThis paper won a best paper award at NIPS and analyzed the setting of learning in a repeated game.  Previous results showed a regret convergence of O(T-1/2) and this paper demonstrates an asymptotic improvement to O(T-3/4) for individual regret and O(T-1) for the sum of utilities. Data Generation as Sequential Decision Makingby Philip Bachman Doina PrecupThis paper takes the view of sampling from sequential generative models as sequential decision making.  For instance can we view sequential sampling as an Markov decision process?  In particular this paper focuses on the problem of data imputation or filling in missing values.  This style of research has been piquing my interest recently since it can offer the potential to dramatically speed up computation when sampling or prediction can be very computationally intensive.Sampling from Probabilistic Submodular Modelsby Alkis Gotovos S. Hamed Hassani Andreas KrauseAndreas's group has been working on a general class of probabilistic models called log-submodular and log-supermodular models. These models generalize models such as determinantal point processes. This paper studies how to do inference on these models via MCMC sampling and establish conditions for fast mixing.The Self-Normalized Estimator for Counterfactual Learningby Adith Swaminathan Thorsten JoachimsThis paper addresses a signficant limitation of previous work on counterfactual risk minimization which is overfitting to hypotheses that match or avoid the logged (bandit) training data which the authors call propensity overfitting.  The authors propose a new risk estimator which deals with this issue."
2015,12,30,Starting a career in data science,https://robjhyndman.com/hyndsight/starting-a-career-in-data-science/,"I received this email from one of my undergraduate students:
 I&rsquo;m writing to you asking for advice on how to start a career in Data Science. Other professions seem a bit more straight forward in that accountants for example simply look for Internships and ways into companies from there. From my understanding the nature of careers in data science seem to be on a project-to-project basis. I&rsquo;m not sure how to get my foot stuck in the door."
2015,12,20,Making data analysis easier,https://robjhyndman.com/hyndsight/wombat2016/,"Di Cook and I are organizing a workshop on &ldquo;Making data analysis easier&rdquo; for 18-19 February 2016.
We are calling it WOMBAT2016 which an acronym for Workshop Organized by the Monash Business Analytics Team. Appropriately it will be held at the Melbourne Zoo. Our plan is to make these workshops an annual event.
Some details are available on the workshop website. Key features are:
  Hadley Wickham is our keynote speaker."
2015,12,11,RStudio just keeps getting better,https://robjhyndman.com/hyndsight/rstudio-just-keeps-getting-better/,"RStudio has been a life-changer for the way I work and for how I teach data analysis. I still have a couple of minor frustrations with it but they are slowly disappearing as RStudio adds features.
I use dual monitors and I like to code on one monitor and have the console and plots on the other monitor. Otherwise I see too little context and long lines get wrapped making the code harder to read."
2015,12,9,Who's downloading the forecast package?,https://robjhyndman.com/hyndsight/fpp-downloads/,"The github page for the forecast package currently shows the following information 
Note the downloads figure: 264K/month. I know the package is popular but that seems crazy. Also the downloads figure on github only counts the downloads from the RStudio mirror and ignores downloads from the other 125 mirrors around the world.Here are the top ten downloaded packages from the last month:
library(cranlogs) cran_top_downloads(when=&#39;last-month&#39;) rank package count from to 1 zoo 308290 2015-11-09 2015-12-08 2 forecast 263797 2015-11-09 2015-12-08 3 Rcpp 260636 2015-11-09 2015-12-08 4 lmtest 258810 2015-11-09 2015-12-08 5 fpp 244989 2015-11-09 2015-12-08 6 expsmooth 244179 2015-11-09 2015-12-08 7 fma 243556 2015-11-09 2015-12-08 8 tseries 243172 2015-11-09 2015-12-08 9 stringi 199384 2015-11-09 2015-12-08 10 ggplot2 192072 2015-11-09 2015-12-08 OK that is very weird."
2015,12,7,Predictive analytics in the long term,http://www.bzst.com/2015/12/predictive-analytics-in-long-term.html,Ten years ago micro-level prediction the way we know it today was nearly absent in companies. MBAs learned about data analysis mostly in a requires statistics course which covered mostly...
2015,11,30,The TensorFlow perspective on neural networks,https://shapeofdata.wordpress.com/2015/11/30/the-tensorflow-perspective-on-neural-networks/,A few weeks ago Google announced that it was open sourcing an internal system called TensorFlow that allows one to build neural networks as well as other types of machine learning models. (Disclaimer: I work for Google.) Because TensorFlow is designed &#8230; Continue reading &#8594;
2015,11,29,The hidden benefits of open-source software,https://robjhyndman.com/hyndsight/oss-benefits/,"I&rsquo;ve been having discussions with colleagues and university administration about the best way for universities to manage home-grown software.
The traditional business model for software is that we build software and sell it to everyone willing to pay. Very often that leads to a software company spin-off that has little or nothing to do with the university that nurtured the development. Think MATLAB S-Plus Minitab SAS and SPSS all of which grew out of universities or research institutions."
2015,11,13,ODI looking for young postgrad statisticians,https://robjhyndman.com/hyndsight/odi-fellowships/,"The Overseas Development Institute Fellowship Scheme sends young postgraduate statisticians (and economists) to work in the public sectors of developing countries in Africa the Caribbean and the Pacific on two-year contracts. This is a great way to develop skills and gain experience working within a developing country&rsquo;s government. And you get to live in a fascinating place!
The application process for the 2016-2018 Fellowship Scheme is now open. Students are advised to apply before 17 December 2015 for a chance to be part of the ODI Fellowship Scheme."
2015,11,9,"Neural networks, linear transformations and word embeddings",https://shapeofdata.wordpress.com/2015/11/09/neural-networks-linear-transformations-and-word-embeddings/,In past posts I&#8217;ve described the geometry of artificial neural networks by thinking of the output from each neuron in the network as defining a probability density function on the space of input vectors. This is useful for understanding how &#8230; Continue reading &#8594;
2015,10,28,Piecewise linear trends,https://robjhyndman.com/hyndsight/piecewise-linear-trends/,"I prepared the following notes for a consulting client and I thought they might be of interest to some other people too.
Let \(y_t\) denote the value of the time series at time \(t\) and suppose we wish to fit a trend with correlated errors of the form \[ y_t = f(t) + n_t \] where \(f(t)\) represents the possibly nonlinear trend and \(n_t\) is an autocorrelated error process."
2015,10,24,Forecasting big time series data using R,https://robjhyndman.com/seminars/china2015/,Keynote address given at the Chinese R conference held in Nanchang Jianxi province. 24-25 October 2015.
2015,10,20,forecast package v6.2,https://robjhyndman.com/hyndsight/forecast6-2/,"It is a while since I last updated the CRAN version of the forecast package so I uploaded the latest version (6.2) today. The github version remains the most up-to-date version and is already two commits ahead of the CRAN version.
This update is mostly bug fixes and additional error traps. The full ChangeLog is listed below.  Many unit tests added using testthat.
  Fixed bug in ets() when very short seasonal series were passed in a data frame."
2015,10,20,Recurrent Neural Networks,https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks/,So far on this blog we&#8217;ve mostly looked at data in two forms &#8211; vectors in which each data point is defined by a fixed set of features and graphs in which each data point is defined by its connections &#8230; Continue reading &#8594;
2015,10,7,Stanford seminar,https://robjhyndman.com/hyndsight/stanford-seminar/,"I gave a seminar at Stanford today. Slides are below. It was definitely the most intimidating audience I&rsquo;ve faced with Jerome Friedman Trevor Hastie Brad Efron Persi Diaconis Susan Holmes David Donoho and John Chambers all present (and probably other famous names I&rsquo;ve missed).
I&rsquo;ll be giving essentially the same talk at UC Davis on Thursday."
2015,10,6,Optimal forecast reconciliation for big time series data,https://robjhyndman.com/seminars/optimal-forecast-reconciliation/,Seminar given at Stanford University on 6th October and University of California (Davis) on 8th October.
2015,10,5,Google workshop: Forecasting and visualizing big time series data,https://robjhyndman.com/seminars/google-oct-2015/,"Workshop for Google Mountain View California.
 Automatic algorithms for time series forecasting Optimal forecast reconciliation for big time series data Visualization of big time series data"
2015,9,25,Reproducibility in computational research,https://robjhyndman.com/hyndsight/reproducibility/,"Jane Frazier spoke at our research team meeting today on &ldquo;Reproducibility in computational research&rdquo;. We had a very stimulating and lively discussion about the issues involved. One interesting idea was that reproducibility is on a scale and we can all aim to move further along the scale towards making our own research more reproducible. For example
 Can you reproduce your results tomorrow on the same computer with the same software installed?"
2015,9,24,Chinese R conference,https://robjhyndman.com/hyndsight/chinese-r-conference/,"I will be speaking at the Chinese R conference in Nanchang to be held on 24-25 October on &ldquo;Forecasting Big Time Series Data using R&rdquo;.
Details (for those who can read Chinese) are at china-r.org."
2015,9,22,Upcoming talks in California,https://robjhyndman.com/hyndsight/upcoming-talks-in-california/,"I&rsquo;m back in California for the next couple of weeks and will give the following talk at Stanford and UC-Davis.
Optimal forecast reconciliation for big time series data Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2015,9,21,International Symposium on Forecasting: Spain 2016,https://robjhyndman.com/hyndsight/isf2016/,"June 19-22 2016 Santander Spain – Palace of La Magdalena
The International Symposium on Forecasting (ISF) is the premier forecasting conference attracting the world&rsquo;s leading forecasting researchers practitioners and students. Through a combination of keynote speaker presentations academic sessions workshops and social programs the ISF provides many excellent opportunities for networking learning and fun.
Speakers: Greg Allenby The Ohio State University USA Todd Clark Federal Reserve Bank of Cleveland USA José Duato Polytechnic University of Valencia Spain Robert Fildes Lancaster University United Kingdom Edward Leamer UCLA Anderson USA Henrik Madsen Technical University of Denmark Adrian Raftery University of Washington USA"
2015,9,15,Unbelievable,https://robjhyndman.com/hyndsight/unbelievable/,"This is a very different book from my usual areas of forecasting and statistics. It is a personal memoir describing my journey of deconversion from Christianity.
Until a few years ago I was regularly speaking at church conferences internationally and my books are still used in Bible classes and Sunday Schools around the world. I even helped establish an innovative new church which became a model for similar churches in other countries."
2015,9,15,IJF vol 31(4): Forecasting in telecommunications and ICT,https://robjhyndman.com/hyndsight/ijf-vol-314/,The last issue of the International Journal of Forecasting for 2015 has been released. This one contains the usual mix of topics plus a special section on Forecasting in telecommunications and ICT including a nice review article by Nigel Meade and Towhidul Islam. Enjoy!
2015,9,14,Algorithmic Dimensions,https://justindomke.wordpress.com/2015/09/14/algorithmic-dimensions/,There are many dimensions on which we might compare a machine learning or data mining algorithm. A few of the first that come to mind are: 1) Sample complexity convergence How much predictive power is the algorithm able to extract from a given number of examples? All else being equal if algorithm A with N &#8230; Continue reading Algorithmic Dimensions &#8594;
2015,9,13,Advice to other journal editors,https://robjhyndman.com/hyndsight/finding-reviewers/,"I get asked to review journal papers almost every day and I have to say no to almost all of them. I know it is hard to find reviewers but many of these requests indicate very lazy editors. So to all the editors out there looking for reviewers here is some advice.
  Never ask someone who is an editor for another journal. I am handling about 500 submissions per year for the International Journal of Forecasting and about 10 per year for the Journal of Statistical Software."
2015,9,7,Thoughts on KDD 2015,http://yyue.blogspot.com/2015/09/thoughts-on-kdd-2015.html,"Last month I attended KDD 2015 in beautiful Sydney Australia.  For those who don't know KDD is the premier international conference for applied machine learning & data mining and is often the venue for some of the most interesting data analysis research projects.  Despite concerns that KDD 2015 would be a let down after KDD 2014 was such a great success in New York City overall KDD 2015 was a fantastic conference with an excellent lineup of invited speakers and plenty of interesting papers.  Congratulations also to my PhD advisor Thorsten Joachims who not only did a great job as PC Co-Chair but also was the recipient of a Test of Time Award for his work on Optimizing Search Engines using Clickthrough Data.Data Science for ScienceOne of the biggest themes at KDD 2015 was applying data science to support the sciences which is something that's been on my mind a lot recently.  Hugh Durrant-White gave a great keynote on applying machine learning to discovery processes in geology and ecology.  One thing that jumped out of his talk was how challenging it is to develop models that are interpretable to domain experts.  This issue is ameliorated in his settings because he largely focused on spatial models which are easier to visualize and interpret.  Susan Athey gave another keynote on the interplay between machine learning and causal inference in policy evaluation which is an important issue for the sciences as well.  I must admit most of the talk went over my head but there was some interesting debate after the talk about whether causality should be the goal or rather just more ""robust"" correlations (whatever that might mean).I also really enjoyed the Data-Driven Science Panel where the debate got quite heated at times.  Two issues in particular  stood out.  First what should be the role of machine learning and data mining experts in the ecosystem of data-driven science?  One the one hand computer scientists have historically had a large impact by developing systems and platforms that abstract away low-level complexity and empower the end user to be more productive.  However how to achieve such a solution in a data-rich world is a much messier (or at least different) type of endeavor.  There are of course plenty of startups that address aspects of this problem but a genuine scalable solution for science remains elusive. A second issue that was raised was whether computational researchers have made much of a direct impact on the sciences.  The particular area raised by Tina Eliassi-Rad is the social sciences.   Machine learning and data mining have taken great interest in computational social science via studying large social networks.  However it is not clear to what extent computational researchers have directly made an impact to traditional social science fields.  Of course this issue is tied back to what the role of computational researchers should be.  On the one hand many social scientists do use tools made by computational people so the indirect impact is quite clear.  Does it really matter that there hasn't been much direct impact?Update on MOOCsDaphne Koller gave a great keynote on the state of MOOCs and Coursera in particular.  It seems that MOOCs nowadays are much smarter about their consumer base and have diversified the way they deliver content and measure success for a wide range of students.  For example people now understand much better the different needs of college aspirants (who use MOOCs to supplicant high school & college education) versus young professionals (who use MOOCs to get ahead in their careers) versus those seeking vocational skills (which is very popular in less developed countries).  One striking omission that was pointed out during the Q&A was that MOOCs have mostly abandoned the pre-college demographic especially before high school.  In retrospect this is not too surprising in large part due to the very different requirements for primary and secondary education across different states and school districts.  But it does put a damper on the current MOOC enthusiasm since many problems with education start much earlier than college.Lessons Learned from Large-Scale A/B TestingRon Kohavi gave a keynote on lessons learned from online A/B testing.  The most interesting aspect of his talk was just how well-tuned the existing systems are.  One symptom of a highly tuned system is that it becomes very difficult to intuit about whether certain modifications will increase or decrease the performance of the system (or have no effect). For example he gave the audience a number of questions to the audience such as: ""Does increasing the description of the sponsored advertisements lead to increased overall clicks on ads?""  Basically the audience could not guess better than random.  So the main lesson is to basically to follow the data and don't be to (emotionally) tied to your own intuitions when it comes to optimizing large complex industrial systems.Sports Analytics WorkshopI co-organized the 2nd workshop on Large-Scale Sports Analytics.  I tried to get more eSports into the workshop this year but alas fell a bit short.  Thorsten did give an interesting talk that used eSports data although the phenomenon he was studying was not specific to eSports.  In many ways eSports is an even better test bed for sports analytics than traditional sports because game replays track literally everything.Within the more traditional sports regimes it's clear that access to data remains a large bottleneck. Many professional leagues are hoarding their data like gold but sadly do not have the expertise leverage the data effectively.  The situation actually seems better in Europe where access to tracked soccer (sorry futbol) games are relatively common.  In the US it seems like the data is only available to a select few sports analytics companies such as Second Spectrum.  I'm hopeful that this situation will change in the near future as the various stake holders become more comfortable with the idea that it's not the raw data that has value but the processed artifacts built on top of that data.Interesting PapersThere were plenty of interesting research papers at KDD of which I'll just list a few that I particularly liked.  A Decision Tree Framework for Spatiotemporal Sequence Prediction by Taehwan Kim Yisong Yue Sarah Taylor and Iain MatthewsI'll start with a shameless piece of self-advertising.  In collaboration with Disney Research we trained a model to generate visual speech i.e. animate the lower face in response to audio or phonetic inputs.  See the demo video below:More details here.Inside Jokes: Identifying Humorous Cartoon Captionsby Dafna Shahaf Eric Horvitz and Robert MankoffProbably the most interesting application at KDD was on studying the anatomy of a joke.  While the results may not seem too surprising in retrospect (e.g. the punchline should be at the end of the joke) what was really cool was that the model could quantify if one joke was funnier than another joke (i.e. rank jokes).  Cinema Data Mining: The Smell of Fearby Jörg Wicker Nicolas Krauter Bettina Derstorff Christof Stönner Efstratios Bourtsoukidis Thomas Klüpfel Jonathan Williams and Stefan KramerThis was a cool paper that studied how the exhaled organic particles vary in response to different emotions.  The authors instrumented a movie theater's air circulation system with chemical sensors and found that the chemicals you exhale are indicative of various emotions such as fear or amusement.  The author repeatedly lamented the fact that they didn't do this for any erotic films and so they don't know what the cinematic chemical signature of arousal would look like. Who supported Obama in 2012? Ecological inference through distribution regressionby Seth Flaxman Yu-Xiang Wang and Alex SmolaThis paper presents a new solution to the ecological inference problem of inferring individual level preferences from aggregate data.  The primary data testbed were county-wise election outcomes and demographic data that reported at a different granularity or overlay.  The main issue is how to estimate e.g. female preference for one presidential candidate using just these kinds of aggregate data.Certifying and removing disparate impactby Michael Feldman Sorelle Friedler John Moeller Carlos Scheidegger and Suresh VenkatasubramanianMany people assume that because algorithms are ""objective"" then they can't be biased or discriminatory.  This assumption is invalid because the data or features themselves can be biased (cf. this interview with Cynthia Dwork).  The authors of this paper propose a way to detect & remove bias in machine learning models that is tailored to the US legal definition of bias.  The work is of course preliminary but this paper was arguably the most thought provoking of the entire conference.Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrierby Wenlei Xie David Bindel Alan Demers and Johannes GehrkeThis paper proposes a reduction approach to personalized PageRank that yields a computational boost by several orders of magnitude thus allowing for the first time personalized PageRank to be computed at interactive speeds.  This paper was also the recipient of the best paper award."
2015,9,4,Customer Journey Analytics and Data Science,https://practicalanalytics.wordpress.com/2015/09/04/customer-journey-analytics-and-data-science/,Where do customers abandon the shopping process? Is it the same in every geography? Audience of One&#8230;. Who are your fans versus haters in the marketplace? How do customers feel about your products? How engaged are customers with your brand versus your competitors’ brands across social media and web channels? Fortune 500 companies are making large investments [&#8230;]
2015,9,2,Mathematical annotations on R plots,https://robjhyndman.com/hyndsight/latex2exp/,"I’ve always struggled with using plotmath via the expression function in R for adding mathematical notation to axes or legends. For some reason the most obvious way to write something never seems to work for me and I end up using trial and error in a loop with far too many iterations.
So I am very happy to see the new latex2exp package available which translates LaTeX expressions into a form suitable for R graphs."
2015,8,24,New IJF editors,https://robjhyndman.com/publications/new-ijf-editors/,
2015,8,19,Categorical predictors: how many dummies to use in regression vs. k-nearest neighbors,http://www.bzst.com/2015/08/categorical-predictors-how-many-dummies.html,Recently I've had discussions with several instructors of data mining courses about a fact that is often left out of many books but is quite important: different treatment of dummy variables in...
2015,8,17,Machine learning bootcamp,https://robjhyndman.com/seminars/machine-learning-bootcamp/,"A talk on time series forecasting for the Monash University Machine Learning Bootcamp.
Demo R code"
2015,8,7,Statistical issues with using herbarium data for the estimation of invasion lag-phases,https://robjhyndman.com/publications/lagphase/,Current methods for using herbarium data as time series for example to estimate the length of the invasion lag phase often make assumptions that are both statistically and logically inappropriate. We present an alternative statistical approach estimating the lag phase based on annual rather than cumulative data a generalized linear model incorporating a log link for overall collection effort and piecewise linear splines. We demonstrate the method on two species representing good and poor data quality then apply it to two data sets comprising 448 species/region combinations.
2015,8,3,The bias-variance decomposition,https://robjhyndman.com/hyndsight/bias-variance/,"This week I am teaching my Business Analytics class about the bias-variance trade-off. For some reason the proof is not contained in either ESL or ISL even though it is quite simple. I also discovered that the proof currently provided on Wikipedia makes little sense in places.
So I wrote my own for the class. It is longer than necessary to ensure there are no jumps that might confuse students."
2015,7,21,Consumerization of BI: Data Visualization Competency Center,https://practicalanalytics.wordpress.com/2015/07/21/enabling-enterprise-data-mining-and-visualization/,What do users want? Self-service interactive analytics with all kinds of datasets with instant response times no waiting. Today there is a strong move towards &#8220;Consumerization of BI&#8221; as business users are demand the same speed and ease of use from their enterprise applications as their at-home software. Consumerization of BI (“data at your fingertips”) means: Help [&#8230;]
2015,7,16,Murphy diagrams in R,https://robjhyndman.com/hyndsight/murphy-diagrams/,"At the recent International Symposium on Forecasting held in Riverside California Tillman Gneiting gave a great talk on “Evaluating forecasts: why proper scoring rules and consistent scoring functions matter”. It will be the subject of an IJF invited paper in due course.
One of the things he talked about was the “Murphy diagram” for comparing forecasts as proposed in Ehm et al (2015). Here’s how it works for comparing mean forecasts."
2015,7,14,GPUs and Neural Networks,https://shapeofdata.wordpress.com/2015/07/14/gpus-and-neural-networks/,Artificial neural networks have been around for a long time &#8211; since either the 1940s or the 1950s depending on how you count. But they&#8217;ve only started to be used for practical applications such as image recognition in the last &#8230; Continue reading &#8594;
2015,7,1,Useful tutorials,https://robjhyndman.com/hyndsight/useful-tutorials/,"There are some tools that I use regularly and I would like my research students and post-docs to learn them too. Here are some great online tutorials that might help.
  ggplot tutorial from Winston Chang
  Writing an R package from Karl Broman
  Rmarkdown from RStudio
  Shiny from RStudio
  git/github guide from Karl Broman
  minimal make tutorial from Karl Broman"
2015,6,29,Exploring the feature space of large collections of time series,https://robjhyndman.com/seminars/banff2015/,"Work­shop on Fron­tiers in Func­tional Data Analy­sis Banff Canada.
It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods.
For example Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers."
2015,6,29,My Yahoo talk is now online,https://robjhyndman.com/hyndsight/yahoo2015/,Last week I gave a talk in the Yahoo! Big Thinkers series. The video of the talk is now online and embedded below.
2015,6,27,Keeping up to date with my research papers,https://robjhyndman.com/hyndsight/new-papers/,"Many people ask me to let them know when I write a new research paper. I can&rsquo;t do that as there are too many people involved and it is not scalable.
The solution is simple. Take your pick from the following options. Each is automatic and will let you know whenever I produce a new paper.
  Subscribe to the rss feed on my website using feedly or some other rss reader."
2015,6,26,"Exploring the boundaries of predictability: what can we forecast, and when should we give up?",https://robjhyndman.com/seminars/yahoo2015/,"Yahoo Big Thinkers Sunnyvale California
Friday 26 June 2015 3:00-4:00 pm Location: Yahoo Sunnyvale Campus and LIVE at labs.yahoo.com
Why is it that we can accurately forecast a solar eclipse in 1000 years time but we have no idea whether Yahoo&rsquo;s stock price will rise or fall tomorrow? Or why can we forecast electricity consumption next week with remarkable precision but we cannot forecast exchange rate fluctuations in the next hour?"
2015,6,24,Automatic algorithms for time series forecasting,https://robjhyndman.com/seminars/google2015/,"Google Mountain View California.
Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In these circumstances it is not feasible for time series models to be developed for each series by an experienced analyst. Instead an automatic forecasting algorithm is required."
2015,6,23,IJF best paper awards,https://robjhyndman.com/hyndsight/ijf-best-paper-awards/,"Today at the International Symposium on Forecasting I announced the awards for the best paper published in the International Journal of Forecasting in the period 2012-2013.
We make an award every two years to the best paper(s) published in the journal. There is always about 18 months delay after the publication period to allow time for reflection citations etc. The selected papers are selected by vote of the editorial board. The best paper wins an engraved bronze plaque and US$1000."
2015,6,22,MEFM: An R package for long-term probabilistic forecasting of electricity demand,https://robjhyndman.com/seminars/isf2015/,"International Symposium on Forecasting Riverside California
I will describe and demonstrate a new open-source R package that implements the Monash Electricity Forecasting Model a semi-parametric probabilistic approach to forecasting long-term electricity demand. The underlying model proposed in Hyndman and Fan (2010) is now widely used in practice particularly in Australia. The model has undergone many improvements and developments since it was first proposed and these have been incorporated in this R implementation."
2015,6,18,Probabilistic forecasting of peak electricity demand,https://robjhyndman.com/seminars/sce2015/,"Southern California Edison Rosemead California
Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage. It is also subject to some known calendar effects due to the time of day day of week time of year and public holidays."
2015,6,16,North American seminars: June 2015,https://robjhyndman.com/hyndsight/northamerica2015/,"For the next few weeks I am travelling in North America and will be giving the following talks.
  **19 June: **Southern California Edison Rosemead CA. &ldquo;Probabilistic forecasting of peak electricity demand&rdquo;.
  23 June: International Symposium on Forecasting Riverside CA. &ldquo;MEFM: An R package for long-term probabilistic forecasting of electricity demand&rdquo;.
  **25 June: **Google Mountain View CA. &ldquo;Automatic algorithms for time series forecasting&rdquo;.
  **26 June: **Yahoo Sunnyvale CA."
2015,6,10,Do human rhinovirus infections and food allergy modify grass pollen–induced asthma hospital admissions in children?,https://robjhyndman.com/publications/jaci2015/,
2015,6,4,Probabilistic time series forecasting with boosted additive models: an application to smart meter data,https://robjhyndman.com/publications/kdd2015/,A large body of the forecasting literature so far has been focused on forecasting the conditional mean of future observations. However there is an increasing need for generating the entire conditional distribution of future observations in order to effectively quantify the uncertainty in time series data. We present two different methods for probabilistic time series forecasting that allow the inclusion of a possibly large set of exogenous variables. One method is based on forecasting both the conditional mean and variance of the future distribution using a traditional regression approach.
2015,6,3,R vs Autobox vs ForecastPro vs ...,https://robjhyndman.com/hyndsight/show-me-the-evidence/,"Every now and then a commercial software vendor makes claims on social media about how their software is so much better than the forecast package for R but no details are provided.
There are lots of reasons why you might select a particular software solution and R isn&rsquo;t for everyone. But anyone claiming superiority should at least provide some evidence rather than make unsubstantiated claims.The M3 forecasting competition was organized by Spyros Makridakis and Michèle Hibon."
2015,6,1,Large-scale unusual time series detection,https://robjhyndman.com/publications/icdm2015/,"It is becoming increasingly common for organizations to collect very large amounts of data over time and to need to detect unusual or anomalous time series. For example Yahoo has banks of mail servers that are monitored over time. Many measurements on server performance are collected every hour for each of thousands of servers. We wish to identify servers that are behaving unusually.
We compute a vector of features on each time series measuring characteristics of the series."
2015,5,31,A new R package for detecting unusual time series,https://robjhyndman.com/hyndsight/anomalous/,"The anomalous package provides some tools to detect unusual time series in a large collection of time series. This is joint work with Earo Wang (an honours student at Monash) and Nikolay Laptev (from Yahoo Labs). Yahoo is interested in detecting unusual patterns in server metrics. The package is based on this paper with Earo and Nikolay.
The basic idea is to measure a range of features of the time series (such as strength of seasonality an index of spikiness first order autocorrelation etc."
2015,5,26,Visualization of big time series data,https://robjhyndman.com/seminars/big-time-series/,"Talk given to a joint meeting of the Statistical Society of Australia (Victorian branch) and the Melbourne Data Science Meetup Group.It is becoming increasingly common for organizations to collect very large amounts of data over time. Data visualization is essential for exploring and understanding structures and patterns and to identify unusual observations. However the sheer quantity of data available challenges current time series visualisation methods.
For example Yahoo has banks of mail servers that are monitored over time."
2015,5,22,Probabilistic forecasting of long-term peak electricity demand,https://robjhyndman.com/seminars/memsi/,Electricity demand forecasting plays an important role in long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage. It is also subject to some known calendar effects due to the time of day day of week time of year and public holidays.
2015,5,19,Modelling the participation function with a one-parameter family of cubic splines,https://robjhyndman.com/publications/cubicsplinesiti/,We suggest that a simple one-parameter family of cubic spline functions would serve quite adequately as models of the participation curve that is the key component of ITI calculations. This would remove the subjectivity associated with the use of two-parameter logistic functions and would allow all states to use the same method for ITI calculations.
2015,5,15,New in forecast 6.0,https://robjhyndman.com/hyndsight/forecast6/,"This week I uploaded a new version of the forecast package to CRAN. As there were a lot of changes I decided to increase the version number to 6.0.
The changes are all outlined in the ChangeLog file as usual. I will highlight some of the more important changes since v5.0 here.ETS One of the most used functions in the package is ets() and it provides a stock forecasting engine for many organizations."
2015,5,14,More changes to the IJF editorial board,https://robjhyndman.com/hyndsight/ijf-board/,The editorial board of the International Journal of Forecasting is going through a renewal process with several changes to the team of editors and the team of associate editors in the last few weeks.New Editors Graham Elliott has decided to step down from the IJF editorial board after many years of service. Graham is best known for his research on optimal forecast combination and forecasting under asymmetric and flexible loss functions.
2015,5,7,Nominations for IJF Best Paper 2012-2013,https://robjhyndman.com/hyndsight/ijf-nominations2/,The following papers have been nominated for the best paper published in the International Journal of Forecasting in 2012-2013. I have included an excerpt from the nomination in each case. The papers in bold have been short-listed for the award and the editorial board are currently voting on them. Bellotti T. &amp; Crook J. (2012). Loss given default models incorporating macroeconomic variables for credit cards. IJF 28(1) 171-182.  &gt;The first rule for the award of best paper should be that the paper clearly reflects the value of the new method/approach when compared to established alternatives in the particular problem context chosen by the researchers.
2015,4,22,Thinking big at Yahoo,https://robjhyndman.com/hyndsight/thinking-big-at-yahoo/,"I&rsquo;m speaking in the &ldquo;Yahoo Labs Big Thinkers&rdquo; series on Friday 26 June. I hope I can live up to the title!
My talk is on &ldquo;Exploring the boundaries of predictability: what can we forecast and when should we give up?&quot; Essentially I will start with some of the ideas in this post and then discuss the features of hard-to-forecast time series.
So if you&rsquo;re in the San Francisco Bay area please come along."
2015,4,22,Travelling Thilaksha,https://robjhyndman.com/hyndsight/travelling-thilaksha/,"One of my PhD students Thilaksha Tharanganie has been very successful in getting travel funding to attend conferences. She was the subject of a write-up in today&rsquo;s Monash News.
We encourage students to attend conferences and provide funding for them to attend one international conference and one local conference during their PhD candidature. Thilaksha was previously funded to attend last year&rsquo;s COMPSTAT in Geneva Switzerland and IMS conference in Sydney. Having exhausted local funding she has now convinced several other organizations to support her conference habit."
2015,4,10,Feeling the FPP love,https://robjhyndman.com/hyndsight/fpp-reviews/,It is now exactly 12 months since the print version of my forecasting textbook with George Athanasopoulos was released on Amazon.com. Although the book is freely available online it seems that a lot of people still like to buy print books.It&rsquo;s nice to see that it has been getting some good reviews. It is rated 4.6 stars on Amazon.com with 6 out of 8 reviewers giving it 5 stars (the 3 reviewers on Amazon.
2015,4,9,Paperpile makes me more productive,https://robjhyndman.com/hyndsight/paperpile/,"One of the first things I tell my new research students is to use a reference management system to help them keep track of the papers they read and to assist in creating bib files for their bibliography. Most of them use Mendeley one or two use Zotero. Both do a good job and both are free.
I use neither. I did use Mendeley for several years but it became slower and slower to sync as my reference collection grew."
2015,4,9,KDD 2015 Workshop on Large-Scale Sports Analytics,http://yyue.blogspot.com/2015/04/kdd-2015-workshop-on-large-scale-sports.html,We are pleased to announce that the KDD Workshop on Large-Scale Sports Analytics will be taking place in Sydney this year on August the 10th at KDD 2015. Similar to last year it will be a full day workshop consisting of invited speakers as well as poster sessions for submitted papers. A call for paper submissions is below. === Call for Submissions === When: August 10th 2015Where: Sydney AustraliaWebsite: http://large-scale-sports-analytics.org/Description: Virtually every aspect of sports analytics is now entering the “Big Data” phase and the interest in effectively mining modeling and learning from such data has also been correspondingly growing. Relevant data sources include detailed play-by-play game logs tracking data physiological sensor data to monitor the health of players social media and text-based content and video recordings of games.The objective of this workshop is to bring together researchers and analysts from academia and industry who work in sports analytics data mining and machine learning. We hope to enable meaningful discussions about state-of-the-art in sports analytics research and how it might be improved upon. We seek poster submissions (which can be both preliminary research as well as recently published work) on topics including but not limited to:* Spatiotemporal modeling* Video text and social media analysis* Feature selection and dimensionality reduction* Feature learning and latent factor models* Computational rationality* Real-time predictive modeling* Interactive analysis &amp; visualization tools* Sensor technology and reliability* Labeling and annotation of events/activities/tactics* Real-time/deployed analytical systems* Knowledge discovery of player/team/league behaviors* Game Theory* eSportsSubmission Details:Poster submissions should be extended abstracts no more than 4 pages in length (in KDD format do not need to be anonymous).  Extended abstracts should be submitted by June 5th 11:59 PM PDT. Details can be found at: http://www.large-scale-sports-analytics.org/Large-Scale-Sports-Analytics/Submissions.htmlImportant Dates:Submission - 5th June 2015 11:59 PM PDT Notification - 30th June 2015Workshop - 10th August 2015Organizers:Patrick Lucey (Disney Research) (patrick.lucey@disneyresearch.com)Yisong Yue (Caltech) (yyue@caltech.edu)Jenna Wiens (University of Michigan) (wiensj@umich.edu)Stuart Morgan (Australian Institute of Sport) (stuart.morgan@ausport.gov.au)
2015,4,8,Help,https://robjhyndman.com/hyndsight/help/,"This is not a help service for all your R and forecasting questions so please don&rsquo;t post questions in the comments or send them to me by email.
If you have questions about data analysis ask for help on crossvalidated.com.
If you have questions about R ask for help on stackoverflow.com.
If you think you have found a bug in one of my R packages report it on github as explained here."
2015,4,3,Discussion of “High-dimensional autocovariance matrices and optimal linear prediction”,https://robjhyndman.com/publications/mpcomments/,
2015,4,2,Does Gender Matter in BI Salaries?,https://datamakesworld.com/2015/04/02/does-gender-matter-in-bi-salaries/,As a female and a feminist who has worked in male-dominated fields for most of my career what immediately caught my eye when reading the most recent TDWI Salary Survey report was the on-going pay disparity between women and men in BI. According to TDWI research men continue to out-earn women in the BI field &#8230; Continue reading Does Gender Matter in BI&#160;Salaries?
2015,4,1,A new open source data set for detecting time series outliers,https://robjhyndman.com/hyndsight/yahoo-data/,Yahoo Labs has just released an interesting new data set useful for research on detecting anomalies (or outliers) in time series data. There are many contexts in which anomaly detection is important. For Yahoo the main use case is in detecting unusual traffic on Yahoo servers.The data set comprises real traffic to Yahoo services along with some synthetic data. There are 367 time series in the data set each of which contains between 741 and 1680 observations recorded at regular intervals.
2015,3,31,Change to the IJF editors,https://robjhyndman.com/publications/change-to-the-ijf-editors/,
2015,3,25,What to cite?,https://robjhyndman.com/hyndsight/what-to-cite/,"This question comes from a comment on another post:
 I&rsquo;ve seen authors citing as many references as possible to try to please potential referees. Many of those references are low quality papers though. Any general guidance about a typical length for the reference section?
 It depends on the subject and style of the paper. I&rsquo;ve written a paper with over 900 citations but that was a review of time series forecasting over a 25 year period and so it had to include a lot of references."
2015,3,18,Dark themes for writing,https://robjhyndman.com/hyndsight/dark-themes-for-writing/,"I spend much of my day sitting in front of a screen coding or writing. To limit the strain on my eyes I use a dark theme as much as possible. That is I write with light colored text on a dark background. I don&rsquo;t know why this is not the default in more software as it makes a big difference after a few hours of writing.
Most of the time I am writing using either Sublime Text RStudio or TeXstudio."
2015,3,12,Common reasons for rejection,https://robjhyndman.com/hyndsight/ijf-rejections/,Every week I reject some papers submitted to the International Journal of Forecasting without sending the papers off to associate editors or reviewers. Here are five of the most common reasons for rejection.1. Wrong Journal Submissions to the IJF should be about forecasting obviously. But we often get papers on econometrics or time series analysis or something else that is not forecasting. Even if a paper has some implications for forecasting if these are not discussed at all the paper is not within scope for the journal.
2015,3,2,Psychology journal bans statistical inference; knocks down server,http://www.bzst.com/2015/03/psychology-journal-bans-statistical.html,In its recent editorial the journal Basic and Applied Social Psychology announced that it will no longer accept papers that use classical statistical inference. No more p-values t-tests or even......
2015,2,23,Visualization and forecasting of big time series data,https://robjhyndman.com/seminars/big-time-series-data/,Talk given at the ACEMS Big data workshop QUT.
2015,2,22,Statistical modelling and analysis of big data,https://robjhyndman.com/hyndsight/bigdata2015/,"I&rsquo;m currently attending the one day workshop on this topic at QUT in Brisbane. This morning I spoke on &ldquo;Visualizing and forecasting big time series data&rdquo;. My slides are here.
OVERVIEW Big data is now endemic in business industry government environmental management medical science social research and so on. One of the commensurate challenges is how to effectively model and analyse these data.
This workshop will bring together national and international experts in statistical modelling and analysis of big data to share their experiences approaches and opinions about future directions in this field."
2015,2,9,Thanks Paul and welcome Dilek,https://robjhyndman.com/hyndsight/ijf-change-of-editors/,Today there is a change in editors at the International Journal of Forecasting. Paul Goodwin is retiring from the editorial board and Dilek Önkal is taking his place.Paul Goodwin was appointed as an associate editor in 1999 and as an editor in 2010. Paul is retiring from his position as Professor of Management Science at the University of Bath UK and has decided to also retire from the IJF editorial board.
2015,2,7,"Teaching spaces: ""Analytics in a Studio""",http://www.bzst.com/2015/02/teaching-spaces-analytics-in-studio.html,My first semester at NTHU has been a great learning experience. I introduced and taught two new courses in our new Business Analytics concentration (data mining and forecasting). Both courses met...
2015,2,6,Achieving Analytics Maturity:   3 Tips from the experts,https://datamakesworld.com/2015/02/06/achieving-analytics-maturity-3-tips-from-the-experts/,What does it take to achieve analytics maturity?  Earlier this week Dave Stodder and I hosted a webcast with a panel of vendor experts from Cloudera Microstrategy and Tableau.  These three companies are all sponsors of the Analytics Maturity Model; an analytics assessment tool that measures where your organization stands relative to its peers in &#8230; Continue reading Achieving Analytics Maturity:   3 Tips from the&#160;experts
2015,2,4,Standard error: a poem,https://robjhyndman.com/hyndsight/standard-error-poem/,"This poem was written by David Goddard from the Monash University Department of Epidemiology and Preventive Medicine. It is reproduced here with his permission. The poem won the inaugural Monash University poetry competition and will soon be published in an anthology of contemporary poetry.For those who like this sort of thing (as I do) there is a nice collection of statistical poetry here.
Standard error David Gordon Goddard
An inference that’s very often made –"
2015,2,2,IASC Data Analysis Competition 2015,https://robjhyndman.com/hyndsight/iasc-competition-2015/,The International Association for Statistical Computing (IASC) is holding a Data Analysis Competition. Winners will be invited to present their work at the Joint Meeting of IASC-ABE Satellite Conference for the 60th ISI WSC 2015 to be held at Atlântico Búzios Convention &amp; Resort in Búzios RJ Brazil (August 2-4 2015). They will also be invited to submit a manuscript for possible publication (following peer review) to IASC&rsquo;s official journal Computational Statistics &amp; Data Analysis.
2015,1,30,Next-Generation Analytics: Four Findings from TDWI’s Latest Best Practices Report,https://datamakesworld.com/2015/01/30/next-generation-analytics-four-findings-from-tdwis-latest-best-practices-report/,I recently completed TDWI’s latest Best Practices Report: Next Generation Analytics and Platforms for Business Success. Although the phrase &#8220;next-generation analytics and platforms&#8221; can evoke images of machine learning big data Hadoop and the Internet of things (IoT) most organizations are somewhere in between the technology vision and today’s reality of BI and dashboards. For &#8230; Continue reading Next-Generation Analytics: Four Findings from TDWI’s Latest Best Practices&#160;Report
2015,1,23,RSS feeds for statistics and related journals,https://robjhyndman.com/hyndsight/rss-feeds/,I&rsquo;ve now resurrected the collection of research journals that I follow and set it up as a shared collection in feedly. So anyone can easily subscribe to all of the same journals or select a subset of them to follow on feedly.There are about 90 journals on the list mostly in statistics but some from machine learning operations research and econometrics. I excluded probability journals and areas of application that are well outside my research interests (such as bioinformatics psychology and pharmacology).
2015,1,14,A Brief Overview of Deep Learning,http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html,"(This is a guest post by Ilya Sutskever on the intuition behind deep learning as well as some very useful practical advice.  Many thanks to Ilya for such a heroic effort!)Deep Learning is really popular these days.  Big and small companies are getting into it and making money off it.  It’s hot.  There is some substance to the hype too: large deep neural networks achieve the best results on speech recognition visual object recognition and several language related tasks such as machine translation and language modeling.But why?   What’s so special about deep learning? (from now on we shall use the term  Large Deep Neural Networks --- LDNN ---  which is what the vaguer term “Deep Learning” mostly refers to).  Why does it work now and how does it differ from neural networks of old?   Finally suppose you want to train an LDNN.  Rumor has it that it’s very difficult to do so that it is “black magic” that requires years of experience.  And while it is true that experience helps quite a bit the amount of “trickery” is surprisingly limited ---- one needs be on the lookout for only a small number well-known pitfalls.   Also there are many open-source implementations of various state-of-the-art neural networks (c.f. Caffe cuda-covnet Torch Theano) which makes it much easier to learn all the details needed to make it work.Why Does Deep Learning Work?It is clear that to solve hard problems we must use powerful models.  This statement is obvious.  Indeed if a model is not powerful then there is absolutely no chance that it can succeed in solving a hard problem no matter how good the learning algorithm is.   The other necessary condition for success is that our model is trainable.   That too is obvious for if we cannot train our model then its power is useless --- it will never amount to anything and great results will not be achieved.   The model will forever remain in a state of unrealized potential. Fortunately LDNNs are both trainable and powerful.    Why Are LDNNs Powerful?When I talk about LDNNs I’m talking about 10-20 layer neural networks (because this is what can be trained with today’s algorithms).   I can provide a few ways of looking at LDNNs that will illuminate the reason they can do as well as they do.Conventional statistical models learn simple patterns or clusters.  In contrast LDNNs learn computation albeit a massively parallel computation with a modest number of steps.  Indeed this is the key difference between LDNNs and other statistical models.To elaborate further:  it is well known that any algorithm can be implemented by an appropriate very deep circuit (with a layer for each timestep of the algorithm’s execution -- one example).   What’s more the deeper the circuit the more expensive are the algorithms that can be implemented by the circuit (in terms of runtime).    And given that neural networks are circuits as well deeper neural networks can implement algorithms with more steps ---- which is why depth = more power.N.B.: It is easy to see that a single neuron of a neural network can compute the conjunction of its inputs or the disjunction of its inputs by simply setting their connections to appropriate values. Surprisingly neural networks are actually more efficient than boolean circuits.  By more efficient I mean that a fairly shallow DNN can solve problems that require many more layers of boolean circuits.    For a specific example consider the highly surprising fact that a DNN with 2 hidden layer and a modest number of units can sort N  N-bit numbers!   I found the result shocking when I heard about it so I implemented a small neural network and trained it to sort 10  6-bit numbers which was easy to do to my surprise.    It is impossible to sort N   N-bit numbers with a boolean circuit that has two hidden layers and that are not gigantic.The reason DNNs are more efficient than boolean circuits is because neurons perform a threshold operation which cannot be done with a tiny boolean circuit.Finally human neurons are slow yet humans can perform lots of complicated tasks in a fraction of a second.   More specifically it is well-known that a human neuron fires no more than 100 times per second.  This means that if a human can solve a problem in 0.1 seconds then our neurons have enough time to fire only 10 times --- definitely not much more than that.   It therefore follows that a large neural network with 10 layers can do anything a human can in 0.1 seconds.  This is not scientific fact since it is conceivable that real neurons are much more powerful than artificial neurons but real neurons may also turn out to be much less powerful than artificial neurons.  In any event the above is certainly a plausible hypothesis.This is interesting because humans can solve many complicated perception problems in 0.1 seconds --- for example humans can recognize the identity of an object that’s in front of them recognize a face recognize an emotion and understand speech in a fraction of a second.    In fact if there exists even just one person in the entire world who has achieved an uncanny expertise in performing a highly complex task of some sort in a fraction of a second then this is highly convincing evidence that a large DNN could solve the same task --- if only its connections are set to the appropriate values. But won’t the neural network need to be huge?   Maybe.  But we definitely know that it won’t have to be exponentially large ---- simply because the brain isn’t exponentially large!   And if human neurons turn out to be noisy (for example) which means that many human neurons are required to implement a single real-valued operation that can be done using just one artificial neuron then the number of neurons required by our DNNs to match a human after 0.1 seconds is greatly diminished. These four arguments suggest (strongly in my opinion) that for a very wide variety of problems there exists a setting of the connections of a LDNN that basically solves the problem.    Crucially the number of units required to solve these problems is far from exponential --- on the contrary the number of units required is often so “small” that it is even possible using current hardware to train a network that achieves super-high performance on the task of interest.   It is this last point which is so important and requires additional elaboration:We know that most machine learning algorithms are consistent: that is they will solve the problem given enough data.   But consistency generally requires an exponentially large amount of data.  For example the nearest neighbor algorithm can definitely solve any problem by memorizing the correct answer to every conceivable input.  The same is true for a support vector machine --- we’d have a support vector for almost every possible training case for very hard problems.  The same is also true for a neural network with a single hidden layer:  if we have a neuron for every conceivable training case so that neuron fires for that training case and but not for any other then we could also learn and represent every conceivable function from inputs to outputs.  Everything can be done given exponential resources but it is never ever going to be relevant in our limited physical universe.And it is in this point that LDNNs differ from previous methods:  we can be reasonably certain that a large but not huge LDNN will achieve good results on a surprising variety of problems that we may want to solve.   If a problem can be solved by a human in a fraction of a second then we have a very non-exponential  super-pessimistic upper bound on the size of the smallest neural network that can achieve very good performance.   But I must admit that it is impossible to predict whether a given problem will be solvable by a deep neural network ahead of time although it is often possible to tell whenever we know that a similar problem can be solved by an LDNN of a manageable size. So that’s it then.  Given a problem such as visual object recognition all we need is to train a giant convolutional neural network with 50 layers.  Clearly a giant convnet with 50 layers can be configured to achieve human-level performance on object recognition --- right?   So we simply need to find these weights.  Once once we do the problem is solved.Learning.What is learning?   Learning is the problem of finding a setting of the neural network’s weights that achieves the best possible results on our training data.   In other words we want to “push” the information from the labelled data into the parameters so that the resulting neural network will solve our problem. The success of Deep Learning hinges on a very fortunate fact:  that well-tuned and carefully-initialized stochastic gradient descent (SGD) can train LDNNs on problems that occur in practice.   It is not a trivial fact since the training error of a neural network as a function of its weights is highly non-convex.  And when it comes to non-convex optimization we were taught that all bets are off.  Only convex is good and non-convex is bad.   And yet somehow SGD seems to be very good at training those large deep neural networks on the tasks that we care about.   The problem of training neural networks is NP-hard and in fact there exists a family of datasets such that the problem of finding the best neural network with three hidden units is NP-hard.  And yet SGD just solves it in practice.  This is the main pillar of deep learning.We can say fairly confidently that successful LDNN training relies on the “easy” correlation in the data which allows learning to bootstrap itself towards the more “complicated” correlations in the data.   I have done an experiment that seems to support this claim:   I found that training a neural network to solve the parity problem is hard.  I was able to train the network to solve parity for 25 bits 29 bits but never for 31 bits  (by the way I am not claiming that learning parity is impossible for over 30 bits --- only that I didn’t succeed in doing so).   Now we know that parity is a highly unstable problem that doesn’t have any linear correlations:  every linear function of the inputs is completely uncorrelated with the output which is a problem for neural networks since they are mostly linear at initialization time (so perhaps I should’ve used larger initial weights?  I will discuss the topic of weight initialization later in the text).   So my hypothesis (which is shared by many other scientists) is that neural networks start their learning process by noticing the most “blatant” correlations between the input and the output and once they notice them they introduce several hidden units to detect them which enables the neural network to see more complicated correlations.  Etc.  The process goes on.  I imagine some sort of a “spectrum” of correlations --- both easy and hard and the network jumps from a correlation to a more complicated correlation much like an opportunistic mountain climber.Generalization.While it is very difficult to say anything specific about the precise nature of the optimization of neural networks (except near a local minimum where everything becomes convex and uninteresting) we can say something nontrivial and specific about generalization.And the thing we can say is the following:  in his famous 1984 paper called ""A Theory of the Learnable"" Valiant proved roughly speaking that if you have a finite number of functions say N then every training error will be close to every test error once you have more than log N training cases by a small constant factor.   Clearly if every training error is close to its test error then overfitting is basically impossible  (overfitting occurs when the gap between the training and the test error is large).    (I am also told that this result was given in Vapnik’s book as small exercise).    This theorem is easy to prove but I won’t do it here.  But this very simple result has a genuine implication to any implementation of neural networks.  Suppose I have a neural network with N parameters.  Each parameter will be a float32.  So a neural network is specified with 32N bits which means that we have no more than 232N distinct neural networks and probably much less.   This means that we won’t overfit much once we have more than 32N training cases.  Which is nice.  It means that it’s theoretically OK to count parameters.  What’s more if we are quite confident that each weight only requires 4 bits (say) and that everything else is just noise then we can be fairly confident that the number of training cases will be a small constant factor of 4N rather than 32N.The Conclusion:If we want to solve a hard problem we probably need a LDNN which has many parameters.   So we need a large high-quality labelled training set to make sure that it has enough information to specify all the network’s connections.  And once we get that training set we should run SGD on it until the network solves the problem.  And it probably will if our neural network is large and deep.What Changed Since the 80s?In the old days people believed that neural networks could “solve everything”.  Why couldn’t they do it in the past?  There are several reasons.Computers were slow. So the neural networks of past were tiny.  And tiny neural networks cannot achieve very high performance on anything.   In other words small neural networks are not powerful.Datasets were small.   So even if it was somehow magically possible to train LDNNs there were no large datasets that had enough information to constrain their numerous parameters.   So failure was inevitable.Nobody knew how to train deep nets.   Deep networks are important.  The current best object recognition networks have between 20 and 25 successive layers of convolutions.  A 2 layer neural network cannot do anything good on object recognition.  Yet back in the day everyone was very sure that deep nets cannot be trained with SGD since that would’ve been too good to be true!It’s funny how science progresses and how easy it is to train deep neural networks especially in retrospect.Practical Advice.Ok.  So you’re sold.  You’re convinced that LDNNs are the present and the future and you want to train it.   But rumor has it that it’s so hard so difficult… or is it?   The reality is that it used to be hard but now the community has consolidated its knowledge and realized that training neural networks is easy as long as you keep the following in mind.   Here is a summary of the community’s knowledge of what’s important and what to look after:Get the data: Make sure that you have a high-quality dataset of input-output examples that is large representative and has relatively clean labels.   Learning is completely impossible without such a dataset.Preprocessing:  it is essential to center the data so that its mean is zero and so that the variance of each of its dimensions is one.   Sometimes when the input dimension varies by orders of magnitude it is better to take the log(1 + x) of that dimension.  Basically it’s important to find a faithful encoding of the input with zero mean and sensibly bounded dimensions.  Doing so makes learning work much better.   This is the case because the weights are updated by the formula: change in wij \propto  xidL/dyj (w denotes the weights from layer x to layer y and L is the loss function). If the average value of the x’s is large (say 100) then the weight updates will be very large and correlated which makes learning bad and slow.   Keeping things zero-mean and with small variance simply makes everything work much better.Minibatches: Use minibatches.  Modern computers cannot be efficient if you process one training case at a time.  It is vastly more efficient to train the network on minibatches of 128 examples because doing so will result in massively greater throughput.  It would actually be nice to use minibatches of size 1 and they would probably result in improved performance and lower overfitting;  but the benefit of doing so is outweighed the massive computational gains provided by minibatches.    But don’t use very large minibatches because they tend to work less well and overfit more.   So the practical recommendation is:  use the smaller minibatch that runs efficiently on your machine. Gradient normalization:  Divide the gradient by minibatch size.   This is a good idea because of the following pleasant property:  you won’t need to change the learning rate (not too much anyway) if you double the minibatch size (or halve it).Learning rate schedule:  Start with a normal-sized learning rate (LR) and reduce it towards the end.A typical value of the LR is 0.1.  Amazingly 0.1 is a good value of the learning rate for a large number of neural networks problems.   Learning rates frequently tend to be smaller but rarely much larger.Use a validation set ---- a subset of the training set on which we don’t train --- to decide when to lower the learning rate and when to stop training (e.g. when error on the validation set starts to increase).A practical suggestion for a learning rate schedule:   if you see that you stopped making progress on the validation set divide the LR by 2 (or by 5) and keep going.  Eventually the LR will become very small at which point you will stop your training.   Doing so helps ensure that you won’t be (over-)fitting the training data at the detriment of validation performance which happens easily and often.   Also lowering the LR is important and the above recipe provides a useful approach to controlling via the validation set.But most importantly worry about the Learning Rate.   One useful idea used by some researchers (e.g. Alex Krizhevsky) is to monitor the ratio between the update norm and the weight norm.  This ratio should be at around 10-3.  If it is much smaller then learning will probably be too slow and if it is much larger then learning will be unstable and will probably fail.Weight initialization.  Worry about the random initialization of the weights at the start of learning.   If you are lazy it is usually enough to do something like 0.02 * randn(num_params).    A value at this scale tends to work surprisingly well over many different problems.   Of course smaller (or larger) values are also worth trying.  If it doesn’t work well (say your neural network architecture is unusual and/or very deep) then you should initialize each weight matrix with the init_scale / sqrt(layer_width) * randn.   In this case init_scale should be set to 0.1 or 1 or something like that.   Random initialization is super important for deep and recurrent nets.  If you don’t get it right then it’ll look like the network doesn’t learn anything at all.  But we know that neural networks learn once the conditions are set.Fun story:  researchers believed for many years that SGD cannot train deep neural networks from random initializations.   Every time they would try it it wouldn’t work.  Embarrassingly they did not succeed because they used the “small random weights” for the initialization which works great for shallow nets but simply doesn’t work for deep nets at all.  When the nets are deep the many weight matrices all multiply each other so the effect of a suboptimal scale is amplified.But if your net is shallow you can afford to be less careful with the random initialization since SGD will just find a way to fix it.You’re now informed.  Worry and care about your initialization.  Try many different kinds of initialization.  This effort will pay off.   If the net doesn’t work at all (i.e. never “gets off the ground”) keep applying pressure to the random initialization. It’s the right thing to do.If you are training RNNs or LSTMs use a hard constraint over the norm of the gradient (remember that the gradient has been divided by batch size).  Something like 15 or 5 works well in practice in my own experiments.   Take your gradient divide it by the size of the minibatch and check if its norm exceeds 15 (or 5).  If it does then shrink it until it is 15 (or 5).  This one little trick plays a huge difference in the training of RNNs and LSTMs where otherwise the exploding gradient can cause learning to fail and force you to use a puny learning rate like 1e-6 which is too small to be useful.Numerical gradient checking:  If you are not using Theano or Torch you’ll be probably implementing your own gradients.   It is easy to make a mistake when we implement a gradient so it is absolutely critical to use numerical gradient checking.   Doing so will give you a complete peace of mind and confidence in your code.   You will know that you can invest effort in tuning the hyperparameters (such as the learning rate and the initialization) and be sure that your efforts are channeled in the right direction.If you are using LSTMs and you want to train them on problems with very long range dependencies you should initialize the biases of the forget gates of the LSTMs to large values.  By default the forget gates are the sigmoids of their total input and when the weights are small the forget gate is set to 0.5 which is adequate for some but not all problems.   This is the one non-obvious caveat about the initialization of the LSTM.Data augmentation:  be creative and find ways to algorithmically increase the number of training cases that are in your disposal.   If you have images then you should translate and rotate them;  if you have speech you should combine clean speech with all types of random noise;   etc.   Data augmentation is an art (unless you’re dealing with images).    Use common sense.Dropout.  Dropout provides an easy way to improve performance. It’s trivial to implement and there’s little reason to not do it.   Remember to tune the dropout probability and to not forget to turn off Dropout and to multiply the weights by (namely by 1-dropout probability) at test time.  Also be sure to train the network for longer.  Unlike normal training where the validation error often starts increasing after prolonged training dropout nets keep getting better and better the longer you train them.  So be patient.Ensembling.  Train 10 neural networks and average their predictions.  It’s a fairly trivial technique that results in easy sizeable performance improvements.   One may be mystified as to why averaging helps so much but there is a simple reason for the effectiveness of averaging.  Suppose that two classifiers have an error rate of 70%.   Then when they agree they are right.  But when they disagree one of them is often right so now the average prediction will place much more weight on the correct answer.  The effect will be especially strong whenever the network is confident when it’s right and unconfident when it’s wrong.I am pretty sure that I haven’t forgotten anything.  The above 13 points cover literally everything that’s needed in order to train LDNNs successfully.So to Summarize:LDNNs are powerful.LDNNs are trainable if we have a very fast computer.So if we have a very large high-quality dataset we can find the best LDNN for the task.Which will solve the problem or at least come close to solving it.The End.But what does the future hold?     Predicting the future is obviously hard but in general models that do even more computation will probably be very good.   The Neural Turing Machine is a very important step in this direction.   Other problems include unsupervised learning which is completely mysterious and incomprehensible in my opinion as of 8 Jan 2015.   Learning very complicated “things” from data without supervision would be nice.  All these problems require extensive research."
2015,1,12,Visualizing and forecasting big time series data,https://robjhyndman.com/seminars/visualizing-and-forecasting-big-time-series-data/,"Institute of Statistical Science Academia Sinica 時　間 2015/01/12 11:00 星期一 地　點 中研院-統計所 2F 交誼廳 備　註 茶 會：上午10：40統計所二樓交誼廳
Time series can often be naturally disaggregated in a hierarchical or grouped structure. For example a manufacturing company can disaggregate total demand for their products by country of sale retail outlet product type package size and so on. As a result there can be millions of individual time series to forecast at the most disaggregated level plus additional series to forecast at higher levels of aggregation."
2015,1,5,Seminars in Taiwan,https://robjhyndman.com/hyndsight/seminars-in-taiwan/,I&rsquo;m currently visiting Taiwan and I&rsquo;m giving two seminars while I&rsquo;m here &mdash; one at the National Tsing Hua University in Hsinchu and the other at Academia Sinica in Taipei. Details are below for those who might be nearby.Automatic Time Series Forecasting College of Technology Management Institute of Service Science National Tsing Hua University Hsinchu 時間及地點：2015.1.7 (Wed.) 5pm @ TSMC building 6F room 622. 台積館6F孫運璿紀念中心 Many applications require a large number of time series to be forecast completely automatically.
2014,12,24,Di Cook is moving to Monash,https://robjhyndman.com/hyndsight/di-cook-is-moving-to-monash/,"I&rsquo;m delighted that Professor Dianne Cook will be joining Monash University in July 2015 as a Professor of Business Analytics. Di is an Australian who has worked in the US for the past 25 years mostly at Iowa State University. She is moving back to Australia and joining the Department of Econometrics and Business Statistics in the Monash Business School as part of our initiative in Business Analytics.
Di is a world leader in data visu­al­iza­tion and is well-​​known for her work on inter­ac­tive graph­ics."
2014,12,20,Thoughts on NIPS 2014,http://yyue.blogspot.com/2014/12/thoughts-on-nips-2014.html,"NIPS 2014 happened last week and what a great conference it was.  Lots of great papers workshops and invited talks.  The NIPS ExperimentIn contrast to previous years the most talked about thing from NIPS this year was not any new machine learning approach but rather a reviewing experiment called the NIPS Experiment.In a nutshell about 10% of submissions were reviewed independently by two sets of reviewers (including two different Area Chairs).  The goal of the NIPS Experiment was to assess to what extent reviewers agreed on accept/reject decisions.  The outcome of the experiment has been a challenge to interpret properly.  The provocative and thought provoking blog post by Eric Price has garnered the most attention from the broader scientific community. Basically one reasonable way of interpreting the NIPS Experiment results is that of the papers accepted for publication at NIPS 2014 roughly half of them would be rejected if they were reviewed again by a different set of reviewers.  This of course highlights the degree of subjectivity and randomness (likely exacerbated by sub-optimal reviewing) inherent in reviewing for a such a broad field as machine learning.The most common way to analyze this is from a certain viewpoint about fairness.  I.e. if we had a budget for K papers did the top K submissions get published?  From that standpoint the answer seems to be a resounding no no matter how you slice it.  One can argue about the degree of unfairness which is a much murkier subject.Alternative Viewpoint via Regret MinimizationHowever as echoed in a blog post by Bert Huang NIPS was AWESOME this year.  The poster sessions had lots of great papers and the oral presentations were good. So I'd like to offer a different viewpoint about NIPS one based on regret minimization.  Let's assume that the accepted papers that were more likely to be rejected in a second review are ""borderline"" papers (seems like a reasonable assumption but perhaps there are arguments against it).  Then had we swapped out a bunch of borderline papers with other borderline papers that got rejected would the quality of the conference have been that much better?In other words given a budget of K papers to accept what is the collective quality of K papers actually accepted versus the quality of the ""optimal"" set of K papers we should've accepted?  It's conceivable that the regret on quality difference could be quite low despite the paper overlap being substantially different.One might even argue as alluded to here that long-term regret minimization (i.e. reviewing for NIPS over many years) requires some amount of randomness and/or disagreement between reviewers.  Otherwise there could be a more serious risk of group-think or intellectual inbreeding that can cause the field to stagnate.Not sure to what extent this viewpoint is appropriate.  For instance NIPS is also a venue by which junior researchers become established in the field.  Having a significant amount of randomness in the reviewing process can definitely be detrimental to the morale and career prospects of junior researchers. On to the Actual PapersThere were many great papers at NIPS this year.  Here are a few that caught my eye:Sequence to Sequence Learning with Neural Networks by Ilya Sutskever Oriol Vinyals & Quoc Le.Ilya gave hands down the best talk at NIPS this year.  Ever since it started becoming popular Deep Learning has carried with it the idea that only Geoff Hinton & company could make them work well. Ilya spent most of his talk describing how this is not the case anymore.  He also showed how to incorporate a type of gradient momentum called Long Short-Term Memory in order to do sequence-to-sequence prediction with deep neural networks.Learning Neural Network Policies with Guided Policy Search under Unknown Dynamicsby Sergey Levine & Pieter Abbeel.This paper combined reinforcement learning and neural networks in order to do policy search.  What's shocking about this approach is how few training examples they needed to train a neural network.  Granted the neural network wasn't very deep but still the low amount of training data is quite surprising.  Learning to Optimize via Information-Directed Samplingby Dan Russo & Benjamin Van Roy.Dan Russo has been doing some great work recently on analyzing bandit/MDP algorithms and proposing new algorithms. This paper proposes the first (mostly) fundamentally new bandit algorithm design philosophy that I've seen in a while.  It's not clear yet how to make this algorithm practical in a wide range of complex domains but it's definitely exciting to think about.Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Setsby Adarsh Prasad Stefanie Jegelka & Dhruv Batra.This paper deals with how to do submodular maximization when the ground set is exponentially large.  This paper exploits specific structure in the ground set e.g. it can be solved via cooperative cuts in order to arrive an efficient solution.  It would be interesting to try to learn the diversity/submodular objective function rather than hand-craft a relatively simple one (from a modeling perspective).From MAP to Marginals: Variational Inference in Bayesian Submodular Modelsby Josip Djolonga & Andreas Krause.Log submodular models are a new family of probabilistic models that generalizes things like associative Markov random fields.  This paper shows how to perform variational marginal inference on log submodular functions which might be wildly intractable when viewed through the lens of conventional graphical models (e.g. very large factors that obey a submodular structure).  Very cool stuff.Non-convex Robust PCAby Praneeth Netrapalli Niranjan U N Sujay Sanghavi Animashree Anandkumar & Prateek Jain.This paper gives a very efficient and provably optimal approach for robust PCA where a matrix is assumed to be low-rank but except for a few sparse components. This optimization problem is non-convex and convex relaxations can often give sub-optimal results.  They also have a cool demo.How transferable are features in deep neural networks?by Jason Yosinski Jeff Clune Yoshua Bengio & Hod Lipson.Along with a scientific study on the transferability of neural network features Jason Yosinski also developed a cool demo that can visualize the various hidden layers of a deep neural network.  Conditional Random Field Autoencoders for Unsupervised Structured Predictionby Waleed Ammar Chris Dyer & Noah A. Smith.This paper gives a surprisingly efficient approach for learning unsupervised auto-encoders that avoids making overly restrictive independence assumptions.  The approach is based off CRFs.  I wonder if one can do this with a more expressive model class such as structured decision trees.A* Samplingby Chris J. Maddison Daniel Tarlow & Tom Minka.I admit that I don't really understand what's going on in this paper.  But it seems like it's doing something quite new so there are perhaps many interesting connections to be made here.  This paper also won one of the Outstanding Paper Awards at NIPS this year."
2014,12,19,New curriculum design guidelines by American Statistical Association: Who will teach?,http://www.bzst.com/2014/12/new-curriculum-design-guidelines-by.html,"The American Statistical Association published new ""Curriculum Guidelines for Undergraduate Programs in Statistical Science"". This is the first update to the guidelines since 2000.
The executive..."
2014,12,17,New R package for electricity forecasting,https://robjhyndman.com/hyndsight/mefm/,Shu Fan and I have developed a model for electricity demand forecasting that is now widely used in Australia for long-term forecasting of peak electricity demand. It has become known as the &ldquo;Monash Electricity Forecasting Model&rdquo;. We have decided to release an R package that implements our model so that other people can easily use it. The package is called &ldquo;MEFM&rdquo; and is available on github. We will probably also put in on CRAN eventually.
2014,12,9,Am I a data scientist?,https://robjhyndman.com/hyndsight/am-i-a-data-scientist/,"Last night I gave a very short talk (less than 5 minutes) at the Melbourne Analytics Charity Christmas Gala a combined event of the Statistical Society of Australia Data Science Melbourne Big Data Analytics and Melbourne Users of R Network.
This is (roughly) what I said.Statisticians seem to go through regular periods of existential crisis as they worry about other groups of people who do data analysis. A common theme is: all these other people (usually computer scientists) are doing our job!"
2014,12,8,Honoring Herman Stekler,https://robjhyndman.com/hyndsight/honoring-herman-stekler/,"The first issue of the IJF for 2015 has just been published and I&rsquo;m delighted that it includes a special section honoring Herman Stekler. It includes articles covering a range of his forecasting interests although not all of them (sports forecasting is missing). Herman himself wrote a paper for it looking at &ldquo;Forecasting—Yesterday Today and Tomorrow&rdquo;.
He is in a unique position to write such a paper as he has been doing forecasting research longer than anyone else on the planet &mdash; his first published paper on forecasting appeared in 1959."
2014,12,8,Am I a data scientist?,https://robjhyndman.com/seminars/christmasgala2014/,"Talk given at the Melbourne Analytics Charity Christmas Gala a combined event of the Statistical Society of Australia Data Science Melbourne Big Data Analytics and Melbourne Users of R Network.
This is (roughly) what I said.Statisticians seem to go through regular periods of existential crisis as they worry about other groups of people who do data analysis. A common theme is: all these other people (usually computer scientists) are doing our job!"
2014,12,5,Prediction competitions,https://robjhyndman.com/hyndsight/prediction-competitions/,"Competitions have a long history in forecasting and prediction and have been instrumental in forcing research attention on methods that work well in practice. In the forecasting community the M competition and M3 competition have been particularly influential. The data mining community have the annual KDD cup which has generated attention on a wide range of prediction problems and associated methods. Recent KDD cups are hosted on kaggle.
In my research group meeting today we discussed our (limited) experiences in competing in some Kaggle competitions and we reviewed the following two papers which describe two prediction competitions:"
2014,11,27,New Australian data on the HMD,https://robjhyndman.com/hyndsight/hmd-australia/,"The Human Mortality Database is a wonderful resource for anyone interested in demographic data. It is a carefully curated collection of high quality deaths and population data from 37 countries all in a consistent format with consistent definitions. I have used it many times and never cease to be amazed at the care taken to maintain such a great resource.
The data are continually being revised and updated. Today the Australian data has been updated to 2011."
2014,11,21,Visualization of probabilistic forecasts,https://robjhyndman.com/hyndsight/visualization-of-probabilistic-forecasts/,"This week my research group discussed Adrian Raftery’s recent paper on “Use and Communication of Probabilistic Forecasts” which provides a fascinating but brief survey of some of his work on modelling and communicating uncertain futures. Coincidentally today I was also sent a copy of David Spiegelhalter’s paper on “Visualizing Uncertainty About the Future”. Both are well-worth reading.
It made me think about my own efforts to communicate future uncertainty through graphics."
2014,11,11,IJF review papers,https://robjhyndman.com/hyndsight/ijf-review-papers/,Review papers are extremely useful for new researchers such as PhD students or when you want to learn about a new research field. The International Journal of Forecasting produced a whole review issue in 2006 and it contains some of the most highly cited papers we have ever published. Now beginning with the latest issue of the journal we have started publishing occasional review articles on selected areas of forecasting. The first two articles are:
2014,11,7,Seasonal periods,https://robjhyndman.com/hyndsight/seasonal-periods/,"I get questions about this almost every week. Here is an example from a recent comment on this blog:
 I have two large time series data. One is separated by seconds intervals and the other by minutes. The length of each time series is 180 days. I’m using R (3.1.1) for forecasting the data. I’d like to know the value of the “frequency” argument in the ts() function in R for each data set."
2014,11,5,ABS seasonal adjustment update,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment-3/,"Since my last post on the seasonal adjustment problems at the Australian Bureau of Statistics I&rsquo;ve been working closely with people within the ABS to help them resolve the problems in time for tomorrow&rsquo;s release of the October unemployment figures.
Now that the ABS has put out a statement about the problem I thought it would be useful to explain the underlying methodology for those who are interested.The Labour Force Survey The unemployment rate is derived from the monthly Labour Force Survey."
2014,10,31,Jobs at Amazon,https://robjhyndman.com/hyndsight/jobs-at-amazon/,"I do not normally post job adverts but this was very specifically targeted to &ldquo;applied time series candidates&rdquo; so I thought it might be of sufficient interest to readers of this blog.Here is an excerpt from an email I received from someone at Amazon:
 Amazon is aggressively recruiting in the data sciences and we have found that applied economists compare quite favorably with the machine learning specialists and statisticians that are sometimes recruited for such roles."
2014,10,29,Becky’s and My Annotation Paper in TACL,https://lingpipe-blog.com/2014/10/29/beckys-and-my-annotation-paper-in-tacl/,Finally something published after all these years working on the problem: Rebecca J. Passonneau and Bob Carpenter. 2014. The Benefits of a Model of Annotation. Transactions of the Association for Comptuational Linguistics (TACL) 2(Oct):311−326. [pdf] (Presented at EMNLP 2014.) Becky just presented it at EMNLP this week. I love the TACL concept and lobbied Michael Collins [&#8230;]
2014,10,22,Prediction intervals too narrow,https://robjhyndman.com/hyndsight/narrow-pi/,Almost all prediction intervals from time series models are too narrow. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. In my 2002 IJF paper we measured the size of the problem by computing the actual coverage percentage of the prediction intervals on hold-out samples. We found that for ETS models nominal 95% intervals may only provide coverage between 71% and 87%.
2014,10,20,Optimally reconciling forecasts in a hierarchy,https://robjhyndman.com/publications/foresight-hts/,"This is an introduction to our approach to forecast reconciliation without using any matrices. The original research is available here:
 Hyndman Ahmed Athanasopoulos and Shang (CSDA 2011) Athanasopoulos Ahmed and Hyndman (IJF 2009)  The software is available in the hts package for R with some notes on usage in the vignette. There is also a gentle introduction in our forecasting textbook."
2014,10,20,hts with regressors,https://robjhyndman.com/hyndsight/hts-with-regressors/,"The hts package for R allows for forecasting hierarchical and grouped time series data. The idea is to generate forecasts for all series at all levels of aggregation without imposing the aggregation constraints and then to reconcile the forecasts so they satisfy the aggregation constraints. (An introduction to reconciling hierarchical and grouped time series is available in this Foresight paper.)
The base forecasts can be generated using any method with ETS models and ARIMA models provided as options in the forecast."
2014,10,16,"What's in a name? ""Data"" in Mandarin Chinese",http://www.bzst.com/2014/10/whats-in-name-data-in-mandarin-chinese.html,"The term ""data"" now popularly used in many languages is not as innocent as it seems. The biggest controversy that I've been aware of is whether the English term ""data"" is singular or plural. The..."
2014,10,15,Congratulations to Dr Souhaib Ben Taieb,https://robjhyndman.com/hyndsight/souhaib/,"Souhaib Ben Taieb has been awarded his doctorate at the Université libre de Bruxelles and so he is now officially Dr Ben Taieb! Although Souhaib lives in Brussels and was a student at the Université libre de Bruxelles I co-supervised his doctorate (along with Professor Gianluca Bontempi). Souhaib is the 19th PhD student of mine to graduate.
His thesis was on &ldquo;Machine learning strategies for multi-step-ahead time series forecasting&rdquo; and is now available online."
2014,10,13,How to Write an Academic Research Statement,http://yyue.blogspot.com/2014/10/how-to-write-academic-research-statement.html,"It's that time of year when junior researchers are preparing applications for academic positions.   One of the largest uncertainties that many people have is how to properly write a research statement that is typically part of the application package.  This post contains my thoughts on what a good Computer Science research statement should look like when applying to US and Canadian universities.  Please keep mind though that everyone's research profile is different so what worked for me may not exactly work for you.  To be perfectly honest the research statement is not the most important part of your application package -- the letters of recommendation are.  Your letter writers are accomplished researchers in your field of study and can place your work in context as well as compare you to other researchers (when they were at your current career stage).  Whether or not a hiring committee seriously considers you for an onsite interview is largely a function of your recommendation letters and any other research reputation you've managed acquire while disseminating your work (**).Nonetheless the research statement is still important especially once the hiring committee gets down to a short list and are basically trying to figure out which of the strong candidates seem like they would be the most interesting and impactful additions to the department.For reference here's my research statement when I was on the job market in Fall 2012.  I want to emphasize a few points:(1) As my history teacher Dr. Skinner would always say: ""Pithy and Erudition!"" In other words keep it short and to the point.  You have to optimize for the case when someone with very limited time is doing a quick read of your research statement.  No convoluted sentences and no long paragraphs.  As a general rule I'd say it's probably too long if it's more than 3 pages.  Optimize your wording to be as concise as possible.(2) Tell a Story.  Academics like to get excited by the potential of new research directions -- after all that's why many of us chose to pursue this line of work.  So make sure you have an overarching vision in mind.  For me I chose to talk about machine learning with humans in the loop as my central theme.  During my onsite interviews I was repeatedly asked to describe what my NSF CAREER proposal would look like.  The purpose of the question is so that the interviewer can get a sense of my research vision.  I quickly realized that I can just re-emphasize various aspects of my research statement as my answer.  This also helps create a consistent image of who you are as a researcher.(3) Don't Regurgitate Your CV.  Your letter writers will do a far better job of describing your previous accomplishments than you will in your research statement.  Trust in them to do that. Only describe your previous work to support the story you're trying to tell.  For me I used my previous work to demonstrate that machine learning with humans in the loop is both a broadly practical and an intellectually deep research area.  But I kept it to a bare minimum -- previous work took up just under 1 page in my research statement.  Your research statement is your one chance in your application package to describe your vision to the hiring committee.  Don't waste it all on dwelling in the past.  (4) It's OK to Stretch the Truth a Little Bit.  Because you're trying to keep the research statement concise you can't accurately describe all the details of your previous work.  For instance when I described my prior work I did not include all the caveats that necessarily come with any such research result.  That is OK; everyone understands that your research results have caveats.  People not in your area don't want to read a laundry list of assumptions and conditions that your result must be couched in.  And people who are interested will read your actual research papers.  You can explicitly highlight the more interesting limitations of your previous work when you talk about future research directions.(5) Don't Bullshit Too Much.  Of course you must be somewhat speculative when you're laying out your research vision and describing future research directions.  But make sure that your speculations are grounded in some kind of sound reasoning.  The easiest way to do this is to demonstrate that you've already done some preliminary work in the future directions you want to pursue.  For my research statement I listed one piece of preliminary work that I've done for each future direction.  This is also a nice way to incorporate the more interesting peripheral parts of your CV into your research vision.  (6) Get Lots of Feedback and Iterate.  I had  many great mentors and colleagues who contributed significantly in helping to sharpen the wording and focus of my research statement.  Again not all of these points may work for everyone and I'm sure there are plenty of other good tips that I didn't mention (examples here and here).  But hopefully this was useful to some.  Best of luck everyone!(**) This is not to say that your actual accomplishments are not important.  If there is no substance to your work then your letter writers won't write you strong letters and your research won't have garnered you much recognition and reputation.  Having done substantial work is assumed by default in this post."
2014,10,10,Explaining the ABS unemployment fluctuations,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment-2/,"Although the Guardian claimed yesterday that I had explained “what went wrong” in the July and August unemployment figures I made no attempt to do so as I had no information about the problems. Instead I just explained a little about the purpose of seasonal adjustment.
However today I learned a little more about the ABS unemployment data problems including what may be the explanation for the fluctuations. This explanation was offered by Westpac’s chief economist Bill Evans (see here for a video of him explaining the issue)."
2014,10,9,Seasonal adjustment in the news,https://robjhyndman.com/hyndsight/abs-seasonal-adjustment/,"It&rsquo;s not every day that seasonal adjustment makes the front page of the newspapers but it has today with the ABS saying that the recent seasonally adjusted unemployment data would be revised.
I was interviewed about the underlying concepts for the Guardian in this piece.
Further comment from me about users paying for the ABS data is here."
2014,10,7,Connect with local employers,https://robjhyndman.com/hyndsight/connect-with-local-employers/,I keep telling students that there are lots of jobs in data science (including statistics) and they often tell me they can&rsquo;t find them advertised. As usual you do have to do some networking and one of the best ways of doing it is via a Data Science Meetup. Many cities now have them including Melbourne Sydney London etc. It is the perfect opportunity to meet with local employers many of which are hiring due to the huge expansion in the use of data analysis in business (aka business analytics).
2014,10,6,IIF Sponsored Workshops,https://robjhyndman.com/hyndsight/iif-workshops/,The International Institute of Forecasters sponsors workshops every year each of which focuses on a specific theme. The purpose of these workshops is to facilitate small informal meetings where experts in a particular field of forecasting can discuss forecasting problems research and solutions. Over the years our workshops have covered topics from Predicting Rare Events ICT Forecasting and most recently Singular Spectrum Analysis. Often these workshops are associated with a special issue of the International Journal of Forecasting.
2014,10,6,TBATS with regressors,https://robjhyndman.com/hyndsight/tbats-with-regressors/,"I&rsquo;ve received a few emails about including regression variables (i.e. covariates) in TBATS models. As TBATS models are related to ETS models tbats() is unlikely to ever include covariates as explained here. It won&rsquo;t actually complain if you include an xreg argument but it will ignore it.
When I want to include covariates in a time series model I tend to use auto.arima() with covariates included via the xreg argument. If the time series has multiple seasonal periods I use Fourier terms as additional covariates."
2014,10,3,Caltech CMS Faculty Opening 2014,http://yyue.blogspot.com/2014/10/caltech-cms-faculty-opening-2014.html,The CMS department is growing!  We have an tenure-track faculty opening -- see the official ad here.  We are interested in outstanding candidates from all areas of applied math and computer science. We value high-impact and cross-cutting fundamental research more than the specific area or discipline.  However I personally would be delighted if we developed a stronger presence in computational linguistics and/or network science (with a healthy dose of machine learning sprinkled in of course).  Please apply!
2014,9,26,Humane and Socially Responsible Analytics: A new concentration at National Tsing Hua University,http://www.bzst.com/2014/09/humane-and-socially-responsible.html,This Fall I'm introducing two new elective courses at NTHU's Institute of Service Science: Business Analytics using Data Mining and Business Analytics using Forecasting (if you're wondering about...
2014,9,23,Forecasting: principles and practice (UWA course),https://robjhyndman.com/seminars/uwa/,"Workshop to be held on 23-25 September 2014.
Venue: The University Club University of Western Australia Nedlands WA.
Requirements: a laptop with R installed along with the fpp package and its dependencies. We will also use the hts and vars package on the third day.
Textbook [Hyndman and Athanasopoulos (2014)
Forecasting: principles and practice OTexts: Melbourne Australia.
Program  Introduction to forecasting [Slides R code Lab solutions] Forecasting tools [Slides R code Lab solutions] Exponential smoothing I [Slides R code Lab solutions] Exponential smoothing II [Slides R code Lab solutions] Time series decomposition and cross-validation [Slides R code Lab solutions] Transformations stationarity and differencing [Slides R code Lab solutions] Non-seasonal ARIMA models [Slides R code Lab solutions] Seasonal ARIMA models [Slides R code Lab solutions] State space models [Slides R code Lab solutions] Dynamic regression [Slides R code Lab solutions] Hierarchical forecasting [Slides R code Lab solutions] Advanced methods [Slides R code Lab solutions]  Course Notes"
2014,9,21,FPP now available as a downloadable e-book,https://robjhyndman.com/hyndsight/fpp-e-book/,"My forecasting textbook with George Athanasopoulos is already available online (for free) and in print via Amazon (for under $40). Now we have made it available as a downloadable e-book via Google Books (for $15.55). The Google Books version is identical to the print version on Amazon (apart from a few typos that have been fixed).
To use the e-book version on an iPad or Android tablet you need to have the Google Books app installed [iPad Android]."
2014,9,19,"India redefines ""reciprocity""; Israeli professionals pay the price",http://www.bzst.com/2014/09/india-redefines-reciprocity-israeli.html,After a few years of employment at the Indian School of Business (in 2010 as a visitor and later as a tenured SRITNE Chaired Professor of Data Analytics) the time has come for me to get a new...
2014,9,8,Tim Harford on forecasting,https://robjhyndman.com/hyndsight/tim-harford-on-forecasting/,A few weeks ago I had a Skype chat with Tim Harford the &ldquo;Undercover Economist&rdquo; for Britain&rsquo;s Financial Times. He was working on an article for the FT on forecasting and wanted my perspective as an academic forecaster. I mostly talked about what makes some things more predictable than others as discussed in this blog post. In the end his article headed in a different direction so I don&rsquo;t get quoted but it is still a good read!
2014,9,8,Generating quantile forecasts in R,https://robjhyndman.com/hyndsight/quantile-forecasts-in-r/,"From today’s email:
 I have just finished reading a copy of ‘Forecasting:Principles and Practice’ and I have found the book really interesting. I have particularly enjoyed the case studies and focus on practical applications.
After finishing the book I have joined a forecasting competition to put what I’ve learnt to the test. I do have a couple of queries about the forecasting outputs required. The output required is a quantile forecast is this the same as prediction intervals?"
2014,9,3,Resources for the FPP book,https://robjhyndman.com/hyndsight/fpp-resources/,"The FPP resources page has recently been updated with several new additions including
  R code for all examples in the book. This was already available within each chapter but the examples have been collected into one file per chapter to save copying and pasting the various code fragments.
  Slides from a course on Predictive Analytics from the University of Sydney.
  Slides from a course on Economic Forecasting from the University of Hawaii."
2014,9,1,A new candidate for worst figure,https://robjhyndman.com/hyndsight/worst-figure/,"Today I read a paper that had been submitted to the IJF which included the following figure

along with several similar plots. (Click for a larger version.) I haven&rsquo;t seen anything this bad for a long time. In fact I think I would find it very difficult to reproduce using R or even Excel (which is particularly adept at bad graphics).
A few years ago I produced &ldquo;Twenty rules for good graphics&rdquo;."
2014,9,1,Outdoor fungal spores are associated with child asthma hospitalisations - a case-crossover study,https://robjhyndman.com/publications/fungal-asthma/,"Introduction Asthma can be exacerbated by exposure to various fungal spores and Human Rhinovirus [HRV] but current understanding of the importance of fungal exposure to child asthma hospitalisations is limited. Moreover the interaction between HRV and fungal spore exposure on admission has not been examined.
Aim To investigate the role of outdoor fungal spores in child asthma hospitalisations and if HRV modifies any such effect.
Methods We conducted a case-crossover study of 644 child asthma hospitalisations in Melbourne Australia (2009–11)."
2014,8,26,NIPS 2014 Workshop on Personalization,http://yyue.blogspot.com/2014/08/nips-2014-workshop-on-personalization.html,"Call for PapersPersonalization: Methods and Applications a workshop in conjunction with the 28th Annual Conference on Neural Information Processing Systems (NIPS 2014)December 12 or 13 2014 – Montreal Canadahttps://sites.google.com/site/nips2014personalizationDeadline for Submissions: October 9 2014Overview--------From online news to online shopping to scholarly research we are inundated with a torrent of information on a daily basis. With our limited time money and attention we often struggle to extract actionable knowledge from this deluge of data. A common approach for addressing this challenge is personalization where results are automatically filtered to match the tastes and preferences of individual users.  This workshop aims to bring together researchers from industry and academia in order to describe recent advances and discuss future research directions pertaining to the personalization of digital systems broadly construed.  We aim to highlight new and emerging research opportunities for the machine learning community that arise from the evolving needs for personalization.Format and Submissions----------------------This is a one-day workshop. The program will feature five invited talks poster spotlights a poster session and a panel discussion.We welcome the following types of papers:1. Research papers that introduce new models or methodology or apply established models/methods to novel domains and data sets; or2. Research papers that explore theoretical and computational issues.We encourage submissions from a wide range of disciplines from machine learning to HCI to the social sciences.  Topics of interest include (but are not limited to):- Learning of fine-grained representations of user preferences- Large-scale personalization- Interpreting observable human behavior- Interactive algorithms for ""on-the-fly"" personalization- Learning to personalize using rich user interactions- Modeling complex sensemaking goals- Applications beyond conventional recommender systemsSubmissions should be 4-8 pages long and adhere to the NIPS format (http://nips.cc/Conferences/2014/PaperInformation/StyleFiles).Please make the author information visible on submissions. Submissions will be accepted through the following website:https://easychair.org/conferences/?conf=nipspersonalization2For up-to-date information on the workshop please check:https://sites.google.com/site/nips2014personalizationDeadline for Submissions: October 9 2014 [11:59pm Honolulu time]Notification of Decision: October 23 2014Organizers:-----------------Khalid El-Arini FacebookYisong Yue CaltechDilan Gorur MicrosoftContact: kelarini@fb.com"
2014,8,24,Forecasting with R in WA,https://robjhyndman.com/hyndsight/forecasting-with-r-in-wa/,"On 23-25 September I will be running a 3-day workshop in Perth on &ldquo;Forecasting: principles and practice&rdquo; mostly based on my book of the same name.
Workshop participants will be assumed to be familiar with basic statistical tools such as multiple regression but no knowledge of time series or forecasting will be assumed. Some prior experience in R is highly desirable.
Venue: The University Club University of Western Australia Nedlands WA."
2014,8,22,biblatex for statisticians,https://robjhyndman.com/hyndsight/biblatex-for-statisticians/,I am now using biblatex for all my bibliographic work as it seems to have developed enough to be stable and reliable. The big advantage of biblatex is that it is easy to format the bibliography to conform to specific journal or publisher styles. It is also possible to have structured bibliographies (e.g. divided into sections: books papers R packages etc.) Here is my default setting which should be suitable for almost all statistics and econometrics journals.
2014,8,18,GEFCom 2014 energy forecasting competition is underway,https://robjhyndman.com/hyndsight/gefcom-2014/,"GEFCom 2014 is the most advanced energy forecasting competition ever organized both in terms of the data involved and in terms of the way the forecasts will be evaluated.
So everyone interested in energy forecasting should head over to the competition webpage and start forecasting: www.gefcom.org.
This time the competition is hosted on CrowdANALYTIX rather than Kaggle.
Highlights of GEFCom2014:
  An upgraded edition from GEFCom2012
  Four tracks: electric load electricity price wind power and solar power forecasting."
2014,8,12,Visit of Di Cook,https://robjhyndman.com/hyndsight/visit-of-di-cook/,"Next week Professor Di Cook from Iowa State University is visiting my research group at Monash University. Di is a world leader in data visualization and is especially well-known for her work on interactive graphics and the XGobi and GGobi software. See her book with Deb Swayne for details.
For those wanting to hear her speak read on.Research seminar She will be giving a seminar at 2pm on Monday 18 August at the Monash Clayton campus (Rm E457 Menzies Building 11)."
2014,8,12,What not to say in a job interview,https://robjhyndman.com/hyndsight/what-not-to-say-in-a-job-interview/,"I&rsquo;ve interviewed a few people for jobs at Monash University and there&rsquo;s always someone who comes out with something surprising. Here are some real examples.For a post-doctoral research position:
  Q: What would you say were your major weaknesses?
  A: I don&rsquo;t have any.
  Q: Really? You can&rsquo;t think of anything that you could work on new skills you could develop anything at all that you might be able to improve?"
2014,8,11,Minimal reproducible examples,https://robjhyndman.com/hyndsight/minimal-reproducible-examples/,I occasionally get emails from people thinking they have found a bug in one of my R packages and I usually have to reply asking them to provide a minimal reproducible example (MRE). This post is to provide instructions on how to create a MRE.Bug reports on github not email First if you think there is a bug please don&rsquo;t send me emails. Instead use the bug-reporting facility on github. All eight of my R packages that are on CRAN have pre-release versions on github.
2014,8,6,Resnick Sustainability Institute Postdoctoral Fellowship,http://yyue.blogspot.com/2014/08/resnick-sustainability-institute.html,"The Resnick Sustainability Institute has openings for postdoctoral fellowships.  The Resnick Sustainability Institute is Caltech’s ""studio"" for sustainability science and is dedicated to supporting the cutting-edge science and creative problem solving necessary to change the balance of the world’s sustainability.The postdoctoral fellowship was created to attract outstanding recent graduates to Caltech working on projects that explore new directions in sustainability focused science and engineering research. The Resnick fellows will have support for up to two years to work on creative cross-catalytic research that complements the existing work of the Caltech faculty or that creates new research directions within the mission areas of the Resnick Sustainability Institute. Eligible candidates will have completed their PhD within five years of the start of the appointment and must have secured a commitment from one or more Caltech faculty member to serve as a mentor and provide office/lab space for the length of the fellowship. Candidates can come from any country provided they are proficient in English. Applications consisting of a research proposal cover letter recommendations and CV can be submitted electronically here.  Applications are due by October 13th 2014. Any questions can be directed to rpd@caltech.edu."
2014,8,1,Efficient identification of the Pareto optimal set,https://robjhyndman.com/publications/epic/,In this paper we focus on expensive multiobjective optimization problems and propose a method to predict an approximation of the Pareto optimal set using classification of sampled decision vectors as dominated or nondominated. The performance of our method called EPIC is demonstrated on a set of benchmark problems used in the multiobjective optimization literature and compared with state-of-the-art methods ParEGO and PAL. The initial results are promising and encourage further research in this direction.
2014,7,26,Student forecasting awards from the IIF,https://robjhyndman.com/hyndsight/iif-awards/,"At the IIF annual board meeting last month in Rotterdam I suggested that we provide awards to the top students studying forecasting at university level around the world to the tune of $100 plus IIF membership for a year. I&rsquo;m delighted that the idea met with enthusiasm and that the awards are now available. Even better my second year forecasting subject has been approved for an award.
The IIF have agreed to fund awards for 20 forecasting courses to start with."
2014,7,24,Coherent population forecasting using R,https://robjhyndman.com/hyndsight/coherent-population-forecasting/,"This is an example of how to use the demography package in R for stochastic population forecasting with coherent components. It is based on the papers by Hyndman and Booth (IJF 2008) and Hyndman Booth and Yasmeen (Demography 2013). I will use Australian data from 1950 to 2009 and forecast the next 50 years.
In demography &ldquo;coherent&rdquo; forecasts are where male and females (or other sub-groups) do not diverge over time."
2014,7,23,Plotting the characteristic roots for ARIMA models,https://robjhyndman.com/hyndsight/arma-roots/,When modelling data with ARIMA models it is sometimes useful to plot the inverse characteristic roots. The following functions will compute and plot the inverse roots for any fitted ARIMA model (including seasonal models).# Compute AR roots arroots &lt;- function(object) { if(!(&quot;Arima&quot; %in% class(object)) &amp; !(&quot;ar&quot; %in% class(object))) stop(&quot;object must be of class Arima or ar&quot;) if(&quot;Arima&quot; %in% class(object)) parvec &lt;- object$model$phi else parvec &lt;- object$ar if(length(parvec) &gt; 0) { last.
2014,7,21,I am not an econometrician,https://robjhyndman.com/hyndsight/statistics-vs-econometrics/,I am a statistician but I have worked in a department of predominantly econometricians for the past 17 years. It is a little like an Australian visiting the United States. Initially it seems that we talk the same language do the same sorts of things and have a very similar culture. But the longer you stay there the more you realise there are differences that run deep and affect the way you see the world.
2014,7,15,Variations on rolling forecasts,https://robjhyndman.com/hyndsight/rolling-forecasts/,Rolling forecasts are commonly used to compare time series models. Here are a few of the ways they can be computed using R. I will use ARIMA models as a vehicle of illustration but the code can easily be adapted to other univariate time series models.One-step forecasts without re-estimation The simplest approach is to estimate the model on a single set of training data and then compute one-step forecasts on the remaining test data.
2014,7,1,Fast computation of reconciled forecasts in hierarchical and grouped time series,https://robjhyndman.com/seminars/hgts-2/,International Symposium on Forecasting Rotterdam.
2014,6,24,Functional time series with applications in demography,https://robjhyndman.com/seminars/fts-berlin/,"This is a short course given at Humboldt University Berlin 24-25 June 2014.
Venue: LvB Library Room 401 Spandauerstr. 1 10178 Berlin
Time: 24 June 2014 09:30 - 12:30 and 14:00 - 17:00 25 June 2014 09:30 - 11:30
Functional time series are curves that are observed sequentially in time one curve being observed in each time period. In demography examples include curves formed by annual death rates as a function of age or annual fertility rates as a function of age."
2014,6,17,Challenges in forecasting peak electricity demand,https://robjhyndman.com/seminars/swiss-energy-forum/,"I am giving a two-part seminar at the Energy Forum Valais/Wallis Switzerland on 17 June 2014. (English brochure)
Abstract: Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. It is a challenging problem because of the different uncertainties including underlying population growth changing technology economic conditions prevailing weather conditions (and the timing of those conditions) as well as the general randomness inherent in individual usage."
2014,6,15,Varian on big data,https://robjhyndman.com/hyndsight/varian-2014/,"Last week my research group discussed Hal Varian&rsquo;s interesting new paper on &ldquo;Big data: new tricks for econometrics&rdquo; Journal of Economic Perspectives 28(2): 3-28.
It&rsquo;s a nice introduction to trees bagging and forests plus a very brief entree to the LASSO and the elastic net and to slab and spike regression. Not enough to be able to use them but ok if you&rsquo;ve no idea what they are. It was more disappointing on boosting (completely ignoring the fact that boosting can be applied in a regression context as well as a classification context) and his comments on causality seemed curiously naive."
2014,6,15,Specifying complicated groups of time series in hts,https://robjhyndman.com/hyndsight/gts/,"With the latest version of the hts package for R it is now possible to specify rather complicated grouping structures relatively easily.
All aggregation structures can be represented as hierarchies or as cross-products of hierarchies. For example a hierarchical time series may be based on geography: country state region store. Often there is also a separate product hierarchy: product groups product types packet size. Forecasts of all the different types of aggregation are required; e."
2014,6,14,European talks. June-July 2014,https://robjhyndman.com/hyndsight/europe2014/,"For the next month I am travelling in Europe and will be giving the following talks.
17 June. Challenges in forecasting peak electricity demand. Energy Forum Sierre Valais/Wallis Switzerland.
20 June. Common functional principal component models for mortality forecasting. International Workshop on Functional and Operatorial Statistics. Stresa Italy.
24-25 June. Functional time series with applications in demography. Humboldt University Berlin.
1 July. Fast computation of reconciled forecasts in hierarchical and grouped time series."
2014,6,11,Creating a handout from beamer slides,https://robjhyndman.com/hyndsight/beamer-handout/,"I&rsquo;m about to head off on a speaking tour to Europe (more on that in another post) and one of my hosts has asked for my powerpoint slides so they can print them. They have made two false assumptions: (1) that I use powerpoint; (2) that my slides are static so they can be printed.
Instead I produced a cut-down version of my beamer slides leaving out some of the animations and other features that will not print easily."
2014,6,5,"Low-dimensional decomposition, smoothing and forecasting of sparse functional data",https://robjhyndman.com/publications/ropes/,We propose a new generic method ROPES (Regularized Optimization for Prediction and Estimation with Sparse data) for decomposing smoothing and forecasting two-dimensional sparse data. In some ways ROPES is similar to Ridge Regression the LASSO Principal Component Analysis (PCA) and Maximum-Margin Matrix Factorisation (MMMF). Using this new approach we propose a practical method of forecasting mortality rates as well as a new method for interpolating and extrapolating sparse longitudinal data. We also show how to calculate prediction intervals for the resulting estimates.
2014,5,30,State space models,https://robjhyndman.com/seminars/abs-state-space-models/,"Exponential smoothing
 Slides R examples Lab session Solutions to lab session    Structural models
 Slides R examples Lab session Solutions to lab session    ARIMA and RegARMA models and dlm
 Slides R examples    (Updated: 2 June 2014)"
2014,5,26,Data science market places,https://robjhyndman.com/hyndsight/marketplaces/,Some new websites are being established offering &ldquo;market places&rdquo; for data science. Two I&rsquo;ve come across recently are Experfy and SnapAnalytx.Experfy provides a way for companies to find statisticians and other data scientists either for short-term consultancies or to fill full-time positions. They describe their &ldquo;providers&rdquo; as &ldquo;Data Engineers Data Scientists Data Mining Experts Data Analyst/Modelers Big Data Solutions Architects Visualization Designers Statisticians Applied Physicists Mathematicians Econometricians and Bioinformaticians.&rdquo; Data scientists can sign up as &ldquo;Providers&rdquo; companies can sign up as &ldquo;Clients&rdquo;.
2014,5,24,Common functional principal component models for mortality forecasting,https://robjhyndman.com/publications/cfpc-iwfos/,"We explore models for forecasting groups of functional time series data that exploit common features in the data. Our models involve fitting common (or partially common) functional principal component models and forecasting the coefficients using univariate time series methods. We illustrate our approach by forecasting age-specific mortality rates for males and females in Australia.
Slides for talk"
2014,5,23,Structural breaks,https://robjhyndman.com/hyndsight/structural-breaks/,"I&rsquo;m tired of reading about tests for structural breaks and here&rsquo;s why.
A structural break occurs when we see a sudden change in a time series or a relationship between two time series. Econometricians love papers on structural breaks and apparently believe in them. Personally I tend to take a different view of the world. I think a more realistic view is that most things change slowly over time and only occasionally with sudden discontinuous change."
2014,5,22,Monash Electricity Forecasting Model,https://robjhyndman.com/publications/mefm/,The model we developed for peak electricity demand forecasting in Hyndman and Fan (2010) is now widely used in practice around Australia and has undergone many improvements and developments. This document describes the current version of the model. It will be updated from time to time as the model continues to be modified and improved.
2014,5,22,Large-Scale Sports Analytics Workshop (KDD 2014),http://yyue.blogspot.com/2014/05/large-scale-sports-analytics-workshop.html,I'm delighted to be co-organizing the Large-Scale Sports Analytics Workshop at KDD 2014 in August later this year in New York City.  We have a great lineup of invited speakers and we're also inviting poster submissions for preliminary and recently published research.  See the Call for Papers below. In general KDD this year looks to be amazing so I encourage everyone to attend.KDD 2014 Workshop on Large-Scale Sports Analytics=== Call for Submissions === When: August 24th Where: New York City NYWebsite: http://large-scale-sports-analytics.org/Description: Virtually every aspect of sports analytics is now entering the “Big Data” phase and the interest in effectively mining modeling and learning from such data has also been correspondingly growing. Relevant data sources include detailed play-by-play game logs tracking data physiological sensor data to monitor the health of players social media and text-based content and video recordings of games.The objective of this workshop is to bring together researchers and analysts from academia and industry who work in sports analytics data mining and machine learning. We hope to enable meaningful discussions about state-of-the-art in sports analytics research and how it might be improved upon. We seek poster submissions (which can be both preliminary research as well as recently published work) on topics including but not limited to:* Spatiotemporal modeling* Video text and social media analysis* Feature selection and dimensionality reduction* Feature learning and latent factor models* Computational rationality* Real-time predictive modeling* Interactive analysis & visualization tools* Sensor technology and reliability* Labeling and annotation of events/activities/tactics* Real-time/deployed analytical systems* Knowledge discovery of player/team/league behaviors* Game theorySubmission Details:Poster submissions should be extended abstracts no more than 4 pages in length (in KDD format do not need to be anonymous).  Extended abstracts should be submitted by June 17th 11:59 PM PDT and can be submitted electronically via: https://cmt.research.microsoft.com/LSSA2014Important Dates:Submission - 17th June 2014 11:59 PM PDT Notification - 8th July 2014Workshop - 24th August 2014Organizers:Yisong Yue (Disney Research) Patrick Lucey (Disney Research) Peter Carr (Disney Research) Jenna Wiens (MIT) 
2014,5,19,To explain or predict?,https://robjhyndman.com/hyndsight/to-explain-or-predict/,"Last week my research group discussed Galit Shmueli&rsquo;s paper &ldquo;To explain or to predict?&rdquo; Statistical Science 25(3) 289-310. (See her website for further materials.) This is a paper everyone doing statistics and econometrics should read as it helps to clarify a distinction that is often blurred. In the discussion the following issues were covered amongst other things.
  The AIC is better suited to model selection for prediction as it is asymptotically equivalent to leave-one-out cross-validation in regression or one-step-cross-validation in time series."
2014,5,13,Questions on the business analytics jobs,https://robjhyndman.com/hyndsight/business-analytics-jobs-questions/,I&rsquo;ve received a few questions on the business analytics jobs advertised last week. I think it is best if I answer them here so other potential candidates can have the same information. I will add to this post if I receive more questions.1. What are your expectations in terms of outputs (KPIs)? Typically a person at Level B (Lecturer) in our department would be producing at least one refereed article in a good scholarly journal per year.
2014,5,8,ARIMA models with long lags,https://robjhyndman.com/hyndsight/arima-models-with-long-lags/,"Today&rsquo;s email question:
 I work within a government budget office and sometimes have to forecast fairly simple time series several quarters into the future. Auto.arima() works great and I often get something along the lines of: ARIMA(001)(110)[12] with drift as the lowest AICc.
  However my boss (who does not use R) takes issue with low-order AR and MA because &ldquo;you&rsquo;re essentially using forecasted data to make your forecast."
2014,5,4,New jobs in business analytics at Monash,https://robjhyndman.com/hyndsight/monash-business-analytics/,We have an exciting new initiative at Monash University with some new positions in business analytics. This is part of a plan to strengthen our research and teaching in the data science/computational statistics area. We are hoping to make multiple appointments at junior and senior levels. These are five-year appointments but we hope that the positions will continue after that if we can secure suitable funding.What is business analytics? You can think of &ldquo;business analytics&rdquo; as the application of data analysis to business problems.
2014,5,2,Great papers to read,https://robjhyndman.com/hyndsight/great-papers/,"My research group meets every two weeks. It is always fun to talk about general research issues and new tools and tips we have discovered. We also use some of the time to discuss a paper that I choose for them. Today we discussed Breiman&rsquo;s classic (2001) two cultures paper &mdash; something every statistician should read including the discussion.
I select papers that I want every member of research team to be familiar with."
2014,4,28,"Past, present, and future of statistical science",https://robjhyndman.com/hyndsight/ppfss/,"This is the title of a wonderful new book that has just been released courtesy of the Committee of Presidents of Statistical Societies.
It can be freely downloaded from the COPSS website or a hard copy can be purchased on Amazon (for only a little over 10c per page which is not bad compared to other statistics books).
The book consists of 52 chapters spanning 622 pages. The full table of contents below shows its scope and the list of authors (a veritable who&rsquo;s who in statistics)."
2014,4,24,Publishing an R package in the Journal of Statistical Software,https://robjhyndman.com/hyndsight/jss-rpackages/,I&rsquo;ve been an editor of JSS for the last few years and as a result I tend to get email from people asking me about publishing papers describing R packages in JSS. So for all those wondering here are some general comments.JSS prefers to publish papers about packages where the package is on CRAN and has been there long enough to have matured (i.e. obvious bugs ironed out and a few active users).
2014,4,22,Seven forecasting blogs,https://robjhyndman.com/hyndsight/seven-forecasting-blogs/,"There are several other blogs on forecasting that readers might be interested in. Here are seven worth following:
  No Hesitations by Francis Diebold (Professor of Economics University of Pennsylvania). Diebold needs no introduction to forecasters. He primarily covers forecasting in economics and finance but also xkcd cartoons graphics research issues etc.
  Econometrics Beat by Dave Giles. Dave is a professor of economics at the University of Victoria (Canada) formerly from my own department at Monash University (Australia) and a native New Zealander."
2014,4,16,Errors on percentage errors,https://robjhyndman.com/hyndsight/smape/,"The MAPE (mean absolute percentage error) is a popular measure for forecast accuracy and is defined as $$ \text{MAPE} = 100\text{mean}(|y_t - \hat{y}_t|/|y_t|) $$ where $y_t$ denotes an observation and $\hat{y}_t$ denotes its forecast and the mean is taken over $t$.
Armstrong (1985 p.348) was the first (to my knowledge) to point out the asymmetry of the MAPE saying that &ldquo;it has a bias favoring estimates that are below the actual values&rdquo;."
2014,4,14,Generating tables in LaTeX,https://robjhyndman.com/hyndsight/generating-tables-in-latex/,"Typing tables in LaTeX can get messy but there are some good tools to simplify the process. One I discovered this week is tablesgenerator.com a web-based tool for generating LaTeX tables. It also allows the table to saved in other formats including HTML and Markdown. The interface is simple but it does most things. For complicated tables some additional formatting may be necessary.
Similar functionality is available via plugins in Excel OpenOffice and Libreoffice &mdash; useful if the data for the table is already stored in a spreadsheet."
2014,4,10,Document Classification with Lucene,https://lingpipe-blog.com/2014/04/10/lucene4-document-classification/,As promised in my last post this post shows you how to use Lucene&#8217;s ranked search results and document store to build a simple classifier. Most of this post is excerpted from Text Processing in Java Chapter 7 Text Search with Lucene. The data and source code for this example are contained in the source [&#8230;]
2014,4,9,My forecasting book now on Amazon,https://robjhyndman.com/hyndsight/fpp-amazon/,"For all those people asking me how to obtain a print version of my book &ldquo;Forecasting: principles and practice&rdquo; with George Athanasopoulos you now can.

Order on Amazon.com
Order on Amazon.co.uk
Order on Amazon.fr
The online book will continue to be freely available. The print version of the book is intended to help fund the development of the OTexts platform.
The price is US$45 £27 or €35.
Compare that to $195 for my previous forecasting textbook $150 for Fildes and Ord or $182 for Gonzalez-Rivera."
2014,4,7,Job at Center for Open Science,https://robjhyndman.com/hyndsight/cos-job/,"This looks like an interesting job.
 Dear Dr. Hyndman
  I write from the Center for Open Science a non-profit organization based in Charlottesville Virginia in the United States which is dedicated to improving the alignment between scientific values and scientific practices. We are dedicated to open source and open science.
  We are reaching out to you to find out if you know anyone who might be interested in our Statistical and Methodological Consultant position."
2014,4,6,Interpreting noise,https://robjhyndman.com/hyndsight/interpreting-noise/,"When watching the TV news or reading newspaper commentary I am frequently amazed at the attempts people make to interpret random noise.
For example the latest tiny fluctuation in the share price of a major company is attributed to the CEO being ill. When the exchange rate goes up the TV finance commentator confidently announces that it is a reaction to Chinese building contracts. No one ever says &ldquo;The unemployment rate has dropped by 0."
2014,4,4,Getting a LaTeX system set up,https://robjhyndman.com/hyndsight/latex-setup/,"Today I was teaching the honours students in econometrics and economics about LaTeX. Here are some brief instructions on how to set up a LaTeX system on different operating systems.MS-Windows  Download and run the setup program for MikTeX. Choose the “basic” system. Download and run the installer program for TeXstudio.  Then run TeXstudio and start typing.
Mac OS  Download and install MacTeX.  Then run TeXshop and start typing."
2014,4,2,Parallel coordinate plot in Tableau: a workaround,http://www.bzst.com/2014/04/parallel-coordinate-plot-in-tableau.html,The parallel coordinate plot is useful for visualizing multivariate data in a dis-aggregated way where we have multiple numerical measurements for each record. A scatter plot displays two...
2014,4,1,A gradient boosting approach to the Kaggle load forecasting competition,https://robjhyndman.com/publications/kaggleloadforecasting/,We describe and analyse the approach used by Team TinTin (Souhaib Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the Kaggle Global Energy Forecasting Competition 2012. The competition involved a hierarchical load forecasting problem for a US utility with 20 geographical zones. The available data consisted of the hourly loads for the 20 zones and hourly temperatures from 11 weather stations for four and a half years.
2014,3,18,Cover of my forecasting textbook,https://robjhyndman.com/hyndsight/fpp-cover/,"We now have a cover for the print version of my forecasting book with George Athanasopoulos.

It should be on Amazon in a couple of weeks. The book is also freely available online.
The cover was produced by Scarlett Rugers who I can happily recommend to anyone wanting a book cover designed."
2014,3,17,Fast computation of cross-validation in linear models,https://robjhyndman.com/hyndsight/loocv-linear-models/,"The leave-one-out cross-validation statistic is given by $$ \text{CV} = \frac{1}{N} \sum_{i=1}^N e_{[i]}^2 $$ where ${e_{[i]} = y_{i} - \hat{y}_{[i]}} $ the observations are given by $y_{1}\dotsy_{N}$ and $\hat{y}_{[i]}$ is the predicted value obtained when the model is estimated with the $i\text{th}$ case deleted. This is also sometimes known as the PRESS (Prediction Residual Sum of Squares) statistic.
It turns out that for linear models we do not actually have to estimate the model $N$ times once for each omitted case."
2014,3,17,Grad School Decisions,http://yyue.blogspot.com/2014/03/grad-school-decisions.html,"In the computer science PhD world prospective students are just about wrapping up their school visits and they will soon make a decision on where to spend the next 5-6 years of their lives.  The decision can often be a difficult one and typically must be made with imperfect information conflicting advice from others and uncertainty about how to weigh the various influencing factors.  In my opinion the **single biggest indicator** of grad school success is how well you work/fit with your advisor. Ben Barres recently wrote a great article expounding on this very issue.  I won't bother with repeating all the great points he made but I want to emphasize some things from a computer science perspective.It's important to work with an advisor who has an exciting vision rather than just doggedly work someone whose research area matches the narrow scope of what you worked on as an undergrad.  Keep in mind that a significant fraction of students (perhaps as many as 50%) switch areas after starting grad school from what they declared in their applications.  Of course most switches are to neighboring areas that have significant overlap in the technical foundations.  But the lesson to take away here is that you should keep an open mind about what research you might find interesting.  It's important to have a good working relationship with your advisor.  Research is a very unpredictable process with a lot of highs and lows. You will almost invariably hit some bumps while working with your advisor and it's important to have a good rapport with your advisor when working through those bumps.  Assessing this somewhat intangible ""fit"" can be hard to do a priori but simply talking to a potential advisor can often yield some warning signs or illuminate that there's a good match.  Talking to other students who currently work or have previously worked with the potential advisor can also be useful.  But beware of students who have no first hand experience with the potential advisor -- sometimes students just like to say things =)Some professors are more hands on than others.  Some professors like their students to work in groups while others like their students to work alone.  Some professors want their students to take a deep dive for 1-2 years on a grand project while others want their students to be more focused on the next 4-6 month project.  All of these work styles can lead to successful research outcomes but it has to work for both the advisor and the student. And of course some professors are more flexible to adapt their advising style to match the student than others. Working with the right advisor is usually more important than quibbles over funding.  Most schools guarantee funding for PhD students.  But if your advisor is low on funds then you may have to spend a few extra terms as a teaching assistant rather than be funded directly by your advisor as a research assistant.  In my opinion this is completely worth it if it's for the right advisor (the one caveat being that the school should have reasonable TA workloads).  However if your advisor is completely broke and can't fund anything (including machines and travel) then that can become problematic.Good luck to everyone making decisions!"
2014,3,15,Can women be professors or doctors? Not according to Jet Airways,http://www.bzst.com/2014/03/can-women-be-professors-or-doctors-not.html,"I am already used to the comical scene at airports in Asia where a sign-holder with ""Professor Galit Shmueli"" sees us walk in his/her direction and right away rushes to my husband. Whether or not..."
2014,3,14,Probabilistic forecasting by Gneiting and Katzfuss (2014),https://robjhyndman.com/hyndsight/gneiting/,The IJF is introducing occasional review papers on areas of forecasting. We did a whole issue in 2006 reviewing 25 years of research since the International Institute of Forecasters was established. Since then there has been a lot of new work in application areas such as call center forecasting and electricity price forecasting. In addition there are areas we did not cover in 2006 including new product forecasting and forecasting in finance.
2014,3,12,Testing for trend in ARIMA models,https://robjhyndman.com/hyndsight/arima-trends/,"Today&rsquo;s email brought this one:
 I was wondering if I could get your opinion on a particular problem that I have run into during the reviewing process of an article.
  Basically I have an analysis where I am looking at a couple of time-series and I wanted to know if over time there was an upward trend in the series. Inspection of the raw data suggests there is but we want some statistical evidence for this."
2014,3,12,Unit root tests and ARIMA models,https://robjhyndman.com/hyndsight/unit-root-tests/,"An email I received today:
 I have a small problem. I have a time series called x :
   If I use the default values of auto.arima(x) the best model is an ARIMA(100)     However I tried the function ndiffs(x test=&ldquo;adf&rdquo;) and ndiffs(x test=&ldquo;kpss&rdquo;) as the KPSS test seems to be the default value and the number of difference is 0 for the kpss test (consistent with the results of auto."
2014,3,10,Using old versions of R packages,https://robjhyndman.com/hyndsight/old-r-packages/,"I received this email yesterday:
 I have been using your ‘forecast’ package for more than a year now. I was on R version 2.15 until last week but I am having issues with lubridate package hence decided to update R version to R 3.0.1. In our organization even getting an open source application require us to go through a whole lot of approval processes. I asked for R 3.0.1 before I get approval for 3."
2014,3,8,Lucene 4 Essentials for Text Search and Indexing,https://lingpipe-blog.com/2014/03/08/lucene-4-essentials-for-text-search-and-indexing/,Here&#8217;s a short-ish introduction to the Lucene search engine which shows you how to use the current API to develop search over a collection of texts. Most of this post is excerpted from Text Processing in Java Chapter 7 Text Search with Lucene. Lucene Overview Apache Lucene is a search library written in Java. It’s [&#8230;]
2014,3,7,IJF news,https://robjhyndman.com/hyndsight/ijf-news/,This is a short piece I wrote for the next issue of the Oracle newsletter produced by the International Institute of Forecasters. Special section topics We continue to publish special sections on selected topics. Because of the change in the way regular papers are now handled we do not publish whole special issues any more. Rather each issue has regular papers at the front and if there are any special sections they appear at the back.
2014,3,6,The use of dummy variables in predictive algorithms,http://www.bzst.com/2014/03/the-use-of-dummy-variables-in.html,Anyone who has taken a course in statistics that covers linear regression has heard some version of the rule regarding pre-processing categorical predictors with more than two categories and the need...
2014,3,6,Highlighting the web,https://robjhyndman.com/hyndsight/highlighting-the-web/,Users of my new online forecasting book have asked about having a facility for personal highlighting of selected sections as students often do with print books. We have plans to make this a built-in part of the platform but for now it is possible to do it using a simple browser extension. This approach allows any website to be highlighted so is even more useful than if we only had the facility on OTexts.
2014,3,4,Forecasting weekly data,https://robjhyndman.com/hyndsight/forecasting-weekly-data/,"This is another situation where Fourier terms are useful for handling the seasonality. Not only is the seasonal period rather long it is non-integer (averaging 365.25/7 = 52.18). So ARIMA and ETS models do not tend to give good results even with a period of 52 as an approximation.
Regression with ARIMA errors The simplest approach is a regression with ARIMA errors. Here is an example using weekly data on US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005."
2014,3,4,Fitting models to short time series,https://robjhyndman.com/hyndsight/short-time-series/,"Following my post on fitting models to long time series I thought I&rsquo;d tackle the opposite problem which is more common in business environments.
I often get asked how few data points can be used to fit a time series model. As with almost all sample size questions there is no easy answer. It depends on the number of model parameters to be estimated and the amount of randomness in the data."
2014,3,1,Fitting models to long time series,https://robjhyndman.com/hyndsight/long-time-series/,"I received this email today:
 I recall you made this very insightful remark somewhere that fitting a standard arima model with too much data ie. a very long time series is a bad idea.
  Can you elaborate why?
  I can see the issue with noise which compounds the ML estimation as the series gets too long. But is there anything else?
 I&rsquo;m not sure where I made a comment about this but it is true that ARIMA models don&rsquo;t work well for very long time series."
2014,2,25,The forecast mean after back-transformation,https://robjhyndman.com/hyndsight/backtransforming/,Many functions in the forecast package for R will allow a Box-Cox transformation. The models are fitted to the transformed data and the forecasts and prediction intervals are back-transformed. This preserves the coverage of the prediction intervals and the back-transformed point forecast can be considered the median of the forecast densities (assuming the forecast densities on the transformed scale are symmetric). For many purposes this is acceptable but occasionally the mean forecast is required.
2014,2,23,Statistical politicians,https://robjhyndman.com/hyndsight/statistical-politicians/,"Last week we had the pleasure of Professor Stephen Pollock (University of Leicester) visiting our Department best known in academic circles for his work on time series filtering (see his papers and his excellent book). But he has another career as a member of the UK House of Lords (under the name Viscount Hanworth &ndash; he is a hereditary peer).
It made me wonder how many other politicians have PhDs (or equivalent) in statistics or at least in mathematics."
2014,2,21,Forecasting within limits,https://robjhyndman.com/hyndsight/forecasting-within-limits/,It is common to want forecasts to be positive or to require them to be within some specified range \([ab]\). Both of these situations are relatively easy to handle using transformations.Positive forecasts To impose a positivity constraint simply work on the log scale. With the forecast package in R this can be handled by specifying the Box-Cox parameter \(\lambda=0\). For example consider the real price of a dozen eggs (1900-1993; in cents):
2014,2,20,Backcasting in R,https://robjhyndman.com/hyndsight/backcasting/,Sometimes it is useful to “backcast” a time series — that is forecast in reverse time. Although there are no in-built R functions to do this it is very easy to implement. Suppose x is our time series and we want to backcast for \(h\) periods. Here is some code that should work for most univariate time series. The example is non-seasonal but the code will also work with seasonal data.
2014,2,19,Global energy forecasting competitions,https://robjhyndman.com/hyndsight/gefcom2014/,"The 2012 GEFcom competition was a great success with several new innovative forecasting methods introduced. These have been published in the IJF as follows:  Hong Pinson and Fan. Global Energy Forecasting Competition 2012
  Charleton and Singleton. A refined parametric model for short term load forecasting
  Lloyd. GEFCom2012 hierarchical load forecasting: Gradient boosting machines and Gaussian processes
  Nedelec Cugliari and Goude: GEFCom2012: Electric load forecasting and backcasting with semi-parametric models"
2014,2,13,Automatic time series forecasting,https://robjhyndman.com/seminars/granada/,Talk presented at the conference &ldquo;New Trends on Intelligent Systems and Soft Computing 2014&rdquo; University of Granada Spain. 13-14 February 2014.Abstract Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In these circumstances it is not feasible for time series models to be developed for each series by an experienced analyst.
2014,2,12,Hierarchical forecasting with hts v4.0,https://robjhyndman.com/hyndsight/hts4/,A new version of my hts package for R is now on CRAN. It was completely re-written from scratch. Not a single line of code survived. There are some minor syntax changes but the biggest change is speed and scope. This version is many times faster than the previous version and can handle hundreds of thousands of time series without complaining. The speed-up is due to some new research I am doing with Alan Lee (University of Auckland).
2014,2,8,Detecting seasonality,https://robjhyndman.com/hyndsight/detecting-seasonality/,"I occasionally get email asking how to detect whether seasonality is present in a data set. Sometimes the period of the potential seasonality is known but in other cases it is not.
I’ve discussed before how to estimate an unknown seasonal period and how to measure the strength of the seasonality. In this post I want to look at testing if a series is seasonal when the potential period is known (e."
2014,2,5,Interview for the Capital of Statistics,https://robjhyndman.com/hyndsight/cos-interview/,"Earo Wang recently interviewed me for the Chinese website Capital of Statistics. The English transcript of the intervew is on Earo&rsquo;s personal website.
This is the third interview I&rsquo;ve done in the last 18 months. The others were for:
  Data Mining Research. Republished in Amstat News.
  DecisionStats."
2014,2,4,Top papers in the International Journal of Forecasting,https://robjhyndman.com/hyndsight/ijf-top-papers/,Every year or so Elsevier asks me to nominate five International Journal of Forecasting papers from the last two years to highlight in their marketing materials as &ldquo;Editor&rsquo;s Choice&rdquo;. I try to select papers across a broad range of subjects and I take into account citations and downloads as well as my own impression of the paper. That tends to bias my selection a little towards older papers as they have had more time to accumulate citations.
2014,2,3,Computational Actuarial Science with R,https://robjhyndman.com/hyndsight/caswithr/,I recently co-authored a chapter on &ldquo;Prospective Life Tables&rdquo; for this book edited by Arthur Charpentier. R code to reproduce the figures and to complete the exercises for our chapter is now available on github. Code for the other chapters should also be available soon. The book can be pre-ordered on Amazon.
2014,2,2,Monash Econometrics in the top 10,https://robjhyndman.com/hyndsight/monash-top10/,"Dave Giles pointed out on his blog yesterday that my department is currently ranked in the top 10 in the world for econometrics according to IDEAS. We are also ranked 13th in the world in forecasting. Since IDEAS only covers the economics literature the forecasting rank does not take account of our work in other areas such as demographic forecasting and electricity demand forecasting.
These rankings are only a rough indication of quality but it is nice to see the department being recognized."
2014,1,31,Automatic time series forecasting in Granada,https://robjhyndman.com/hyndsight/granada-workshop/,"In two weeks I am presenting a workshop at the University of Granada (Spain) on Automatic Time Series Forecasting.
Unlike most of my talks this is not intended to be primarily about my own research. Rather it is to provide a state-of-the-art overview of the topic (at a level suitable for Masters students in Computer Science). I thought I&rsquo;d provide some historical perspective on the development of automatic time series forecasting plus give some comments on the current best practices."
2014,1,31,New Book: Text Processing in Java,https://lingpipe-blog.com/2014/01/31/text-processing-in-java/,I&#8217;m pleased to announce the publication of Text Processing in Java ! This book teaches you how to master the subtle art of multilingual text processing and prevent text data corruption.  Data corruption is the all-too-common problem of words that are garbled into strings of question marks black diamonds or random glyphs.  In Japanese this [&#8230;]
2014,1,30,Free books on statistical learning,https://robjhyndman.com/hyndsight/free-books-on-statistical-learning/,Hastie Tibshirani and Friedman&rsquo;s Elements of Statistical Learning first appeared in 2001 and is already a classic. It is my go-to book when I need a quick refresher on a machine learning algorithm. I like it because it is written using the language and perspective of statistics and provides a very useful entry point into the literature of machine learning which has its own terminology for statistical concepts. A free downloadable pdf version is available on the website.
2014,1,28,Online collaborative writing,https://robjhyndman.com/hyndsight/online-collaborative-writing/,Everyone who has written a paper with another author will know it can be tricky making sure you don&rsquo;t end up with two versions that need to be merged. The good news is that the days of sending updated drafts by email backwards and forwards is finally over (having lasted all of 25 years &ndash; I can barely imagine writing papers before email).LaTeX solutions There has been a lot of activity in the development of online LaTeX tools over the last few years.
2014,1,27,New in forecast 5.0,https://robjhyndman.com/hyndsight/forecast5/,"Last week version 5.0 of the forecast package for R was released. There are a few new functions and changes made to the package which is why I increased the version number to 5.0. Thanks to Earo Wang for helping with this new version.
Handling missing values and outliers Data cleaning is often the first step that data scientists and analysts take to ensure statistical modelling is supported by good data."
2014,1,24,Thoughts on the Ljung-Box test,https://robjhyndman.com/hyndsight/ljung-box-test/,It is common to use a Ljung-Box test to check that the residuals from a time series model resemble white noise. However there is very little practical advice around about how to choose the number of lags for the test.
2014,1,23,New Caltech PhD Program,http://yyue.blogspot.com/2014/01/new-caltech-phd-program.html,"I'm excited to announce that Caltech will be starting a new Ph.D. option next year called the Computing and Mathematical Sciences (CMS) option.  This program will emphasize data-intensive algorithmic thinking broadly construed. Students will study topics such as optimization statistics machine learning economics privacy network science and optimal control.  Students will also be expected to perform cross-cutting research applied to various scientific disciplines such as geology chemistry biology and astronomy.  This program is similar in spirit to many other interdisciplinary ""data science"" and ""systems science & engineering"" programs now popping up in universities around the world.  One distinguishing feature of the Caltech program is that there will be a very heavy emphasis on the mathematical foundations as well as a close collaboration with scientists (which is possible due to Caltech's small size).  Students who find themselves conflicted between computer science economics and engineering should find this program particularly appealing. We hope this program will help bring about a unification of the mathematical foundations underpinning the many data-intensive science and engineering problems that we face today.See this post by Adam Wierman for more details.  Special thanks to Adam Katrina Ligett Venkat Chandrasekaran and Joel Tropp for doing all the heavy-lifting to make this program happen."
2014,1,22,Looking for a new post-doc,https://robjhyndman.com/hyndsight/postdoc/,"We are looking for a new post-doctoral research fellow to work on the project &ldquo;Macroeconomic Forecasting in a Big Data World&rdquo;. Details are given at the link below
jobs.monash.edu.au/jobDetails.asp?sJobIDs=519824
This is a two year position funded by the Australian Research Council and working with me George Athanasopoulos Farshid Vahid and Anastasios Panagiotelis. We are looking for someone with a PhD in econometrics statistics or machine learning who is well-trained in computationally intensive methods and who has a background in at least one of time series analysis macroeconomic modelling or Bayesian econometrics."
2014,1,21,Estimating a nonlinear time series model in R,https://robjhyndman.com/hyndsight/nlts/,"There are quite a few R packages available for nonlinear time series analysis but sometimes you need to code your own models. Here is a simple example to show how it can be done.
The model is a first order threshold autoregression:
 $$ y_t = \begin{cases} \alpha y_{t-1} + e_t & \text{if $y_{t-1} \le r$}\\ \beta y_{t-1} + \gamma e_t & \text{if $y_{t-1}r$} \end{cases} $$  where $e_t$ is a Gaussian white noise series with variance $\sigma^2$."
2014,1,11,Canceled: Help Build a Watson Clone–Participants Sought for LingPipe Code Camp,https://lingpipe-blog.com/2014/01/10/help-build-a-watson-clone-participants-sought-for-lingpipe-code-camp/,Code Camp is canceled. We are late on delivering a LingPipe recipes book to our publisher and that will have to be our project for March. But we could have testers/reviewers come and hang out. Less fun I think. Apologies. Breck &#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212; &#160; Dates: To be absolutely clear: Dates are 3/2/2014 to 3/31/14 in Driggs [&#8230;]
2014,1,9,Boosting multi-step autoregressive forecasts,https://robjhyndman.com/publications/boostingar/,Multi-step forecasts can be produced recursively by iterating a one-step model or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon.
2014,1,1,Prospective life tables,https://robjhyndman.com/publications/prospective-life-tables/,
2013,12,22,Judgmental forecasting experiment,https://robjhyndman.com/hyndsight/judgmental-forecasting-experiment/,The Centre for Forecasting at Lancaster University is conducting some research on judgmental forecasting and model selection. They hope to compare the performance of judgmental model selection with statistical model selection in order to learn how to best design forecasting support systems. They would like forecasting students practitioners and researchers to participate and are offering £50 Amazon Gift Cards as prizes. Here is a brief description from Fotios Petropoulos: We are inviting you to participate in a web-based judgmental forecasting exercise.
2013,12,19,Limits of Floating Point and the Asymmetry of 0 and 1,https://lingpipe-blog.com/2013/12/19/limits-of-floating-point-and-the-asymmetry-of-0-and-1/,If we are representing probabilities we are interested in numbers between 0 and 1. It turns out these have very different properties in floating-point arithmetic. And it&#8217;s not as easy to solve as just working on the log scale. Smallest Gaps between Floating Point Numbers of Different Magnitudes The difference between 0 and the next [&#8230;]
2013,12,17,Caltech Electrical Engineering Faculty Opening,http://yyue.blogspot.com/2013/12/caltech-electrical-engineering-faculty.html,The Electrical Engineering department at Caltech has a faculty opening and is targeting machine learning as a key area of interest.  Reviewing of applications has just begun (started December 15) but all applications received by January 15th will be reviewed.Caltech is an extremely small school (~300 faculty total) and departmental boundaries are almost meaningless.  I anticipate having many connections to the Electrical Engineering department so any machine learning person who joins will definitely not feel alone.  So please apply if you're interested!
2013,12,14,Thoughts on NIPS 2013,http://yyue.blogspot.com/2013/12/thoughts-on-nips-2013.html,I recently attended NIPS 2013 at South Lake Tahoe which was an absolutely fantastic conference (the limited dining choices and general casino debauchery notwithstanding).  Highlights included listening to an awesome talk by Daphne Koller on online education (and Coursera in particular) and meeting Mark Zuckerberg and learning about the new Facebook AI research lab. I also attended the Ben Taskar Memorial Session which while very saddening was also inspiring to myself and hopefully many others who attended.  Ben Taskar was a truly exceptional researcher and human being and it was very touching listening to his friends and collaborators talk about and celebrate his life and work.  Ben leaves behind a wife and daughter who requires around-the-clock care.  I encourage everyone who can to donate to his memorial fund.*EDIT* -- you can read more about my thoughts on this Quora answer.The NIPS workshops have consistently been my favorite machine learning event over the past several years.  This year I was privileged to be an invited speaker at the Discrete and Combinatorial Problems in Machine Learning workshop. The guest of honor at the workshop was Kazuo Murota who gave a great overview on his work on discrete convexity (slides here).  As was usual I jumped around between several workshops including Bayesian Optimization Machine Learning for Sustainability and Machine Learning for Clinical Data and Healthcare.  Daniel Russo presented a very interesting result at the Bayesian Optimization workshop showing a very simple way of analyzing Thompson sampling based on existing analysis for UCB-style algorithms.  He mainly showed results in a Bayesian regret setting and I wonder if one could extend his result to show account for the full spectrum between Bayesian expected regret and worst-case regret. There were many great papers at NIPS this year (more than I could get around to reading it seems like).  Here are a few that I found particularly interesting:Sequential Transfer in Multi-armed Bandit with Finite Set of Models by Mohammad Azar Alessandro Lazaric and Emma Brunskill.  This paper shows how to use knowledge learned from previous online-learning problems to learn faster for new online learning problems (i.e. transfer learning).  They leverage a recent result on learning tensor decompositions in order to arrive at a provably data-efficient approach.  Such problems are particularly relevant in many online systems which need to continuously personalize to new users and domains.High-Dimensional Gaussian Process Bandits by Josip Djolonga Andreas Krause and Volkan Cevher.  Like the above-mentioned paper this paper addresses how to transfer learning across different actions.  Rather than focusing on sequential transfer learning like above this paper instead treats all the actions (e.g.  items) and contexts (e.g. users) jointly as points in very high dimensional space.  This paper leverages a different result on low-rank matrix recovery.  I've thought about this type of approach a little while back and am really excited that the authors got this method to work.  One thing that kind of bugs me is that the first stage requires a very restrictive random sampling technique which makes the result less useful in practice.Eluder Dimension and the Sample Complexity of Optimistic Exploration by Daniel Russo and Benjamin Van Roy. This paper proposes a new measure of complexity for analyzing online learning problems.  This seems to be a refreshingly unique perspective on the problem although the relationship with more understood measures such as information gain is currently unclear.  But it does have the potential to help us analyze more complex prediction settings.Multi-Task Bayesian Optimization by Kevin Swersky Jasper Snoek and Ryan Adams.  This paper is very cool.  Hyper-parameter tuning is a big problem in machine learning. This method proposes a way to use a smaller dataset to extrapolate likely outcomes when running a learning algorithm (with a certain hyper-parameter setting) on the larger actual dataset.  The experiments show that it can offer SIGNIFICANT speedups compared to standard approaches.BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables by Cho-Jui Hsieh Matyas Sustik Inderjit Dhillon Pradeep Ravikumar and Russell Poldrack.  Estimating high-dimensional inverse covariance matrices is an increasingly more important problem in machine learning. This method absolutely blows the competition out of the water.Learning Adaptive Value of Information for Structured Prediction by David Weiss and Ben Taskar.  This paper is also very cool.  It is essentially a meta-learning algorithm that trains an adaptive policy on which features to compute for the base classifier to use.  For certain applications computing features can be very expensive and this paper shows how to achieve almost identical performance while using only a small fraction of the features.  Furthermore certain features can be harmful for certain classification instances and the authors show how this method can actually predict to avoid computing such features depending on the problem instance.  There's a lot about this setup that I don't quite understand from a theoretical perspective but it seems very powerful.Latent Structured Active Learning by Wenjie Luo Alex Schwing and Raquel Urtasun.  Active learning for structured prediction models is an important problem.  Most active learning results address only the unstructured setting e.g. binary labels.  This issue is even more important in structured models because acquiring structured labels is typically much more expensive than binary labels.  The authors propose a method for actively eliciting labels on components of the full structured prediction problem (thus leaving much of the structure latent).  One thing that's worth exploring here is a mixed-initiative approach rather than a purely system-driven approach.
2013,11,27,Running a data mining contest on Kaggle,http://www.bzst.com/2013/11/running-data-mining-contest-on-kaggle.html,Following the success last year I've decided once again to introduce a data mining contest in my Business Analytics using Data Mining course at the Indian School of Business. Last year I used two...
2013,11,25,How to get your paper rejected quickly,https://robjhyndman.com/hyndsight/quick-rejection/,"I sent this rejection letter this morning about a paper submitted to the International Journal of Forecasting.
 Dear XXXXX.
  I am writing to you regarding manuscript ????? entitled &ldquo;xxxxxxxxxxxx&rdquo; which you submitted to the International Journal of Forecasting.
  It so happens that I am aware that this paper was previously reviewed for the YYYYYYY journal. It seems that you have not bothered to make any of the changes recommended by the reviewers of your submission to YYYYYYY."
2013,11,21,The Scientific Value of Testing Predictive Performance,http://www.bzst.com/2013/11/the-value-of-testing-predictive.html,This week's NY Times article Risk Calculator for Cholesterol Appears Flawed and CNN article Does calculator overstate heart attack risk? illustrate the power of evaluating the predictive performance...
2013,11,19,Remembering Ben Taskar,http://yyue.blogspot.com/2013/11/remembering-ben-taskar.html,"Ben Taskar passed away recently.  It's a sudden and tragic loss for the machine learning community and our deepest condolences go out to his friends and family.  I've only known Ben professionally; we've chatted a few times over the years.  By all accounts he was a wonderful and kind person.  More information can be found here.I knew Ben mostly through his research which has been exceptional throughout his career as well as an inspiration for my own work.  The term ""rising star"" is reserved for a select few in the research community and Ben most definitely deserved that moniker. In graduate school I literally began my machine learning career reading Ben Taskar's papers.  Ben was a pioneer in the area of Structured Prediction.  His thesis work on Max Margin Markov Networks was a revelation and has stood the test of time as one of those foundational papers that people refer to over and over again. He's also done some great follow-up work extending it as well.But Ben was only just getting started. More recently his group has done some extremely elegant work extending the limits of what structured prediction models can be applied to.  For example his work on Structured Prediction Cascades (with David Weiss) was one of the first principled approaches for discriminatively learning efficient approximations of complex structured prediction models with learning-theoretic guarantees.  As another example his work on Structured Determinantal Point Processes (with Alex Kulesza) is quite possibly the most elegant way of building probabilistic models of redundancy that I've encountered thus far. Every year for the past several years I would regularly browse his website in anticipation of finding interesting new papers that his group has recently published. Although the shock I'm feeling must pale dramatically in comparison to that felt by his family and friends it is nonetheless profoundly saddening that this great star in the machine learning community has seen his chapter end so abruptly and prematurely."
2013,11,5,A Tale of Two (Business Analytics) Courses,http://www.bzst.com/2013/11/a-tale-of-two-business-analytics-courses.html,"I have been teaching two business analytics elective MBA-level courses at ISB. One is called ""Business Analytics Using Data Mining"" (BADM) and the other ""Forecasting Analytics"" (FCAS). Although we..."
2013,10,31,"Nonparametric and semiparametric response surface methodology: a review of designs, models and optimization techniques",https://robjhyndman.com/publications/rsm-review/,Abstract: Since the introduction of Response Surface Methodology in the 1950s there have been many developments with the aim of expanding the range of applications of the methodology. Various new design modeling and optimization techniques have been introduced for coping with unknown input-output relationships costly or time-consuming experimental studies and irregular experimental regions (e.g. non-cubic or non-spherical regions induced by constraints in the input variables). Such developments may involve many different research areas simultaneously (e.
2013,10,26,Caltech CMI Postdoc Openings,http://yyue.blogspot.com/2013/10/caltech-cmi-postdoc-openings.html,Caltech's Center for the Mathematics of Information (CMI) announces openings in the CMI Postdoctoral Fellowship Program starting in fall 2014. The CMI is dedicated to fundamental mathematical research with an eye to the roles of information and computation throughout science and engineering. Areas of interest include algorithms complexity algorithmic game theory applied combinatorics applied probability statistics machine learning information and coding theory control optimization networked systems geometry processing multi-resolution methods and molecular programming.Please apply and have three reference letters sent directly as instructed at:http://www.ist.caltech.edu/joinus/positions.htmlAll candidate materials are due by&nbsp; Friday December 20 2013 &nbsp;and reference letters are due by Monday December 23 2013.Positions are contingent upon completion of the PhD. Caltech is an affirmative action/equal opportunity employer; women minorities veterans and disabled persons are encouraged to apply.
2013,10,14,Probabilistic Energy Forecasting,https://robjhyndman.com/hyndsight/probabilistic-energy-forecasting/,The International Journal of Forecasting is calling for papers on probabilistic energy forecasting. Here are the details (taken from Tao Hong&rsquo;s blog). In today&rsquo;s competitive and dynamic environment more and more decision making processes in the energy industry are relying on probabilistic forecasts. The applications of probabilistic energy forecasts spread across planning and operations of the entire energy value chain. We are seeking papers from researchers working on the areas of probabilistic energy forecasting.
2013,10,11,Coherent mortality forecasting using functional time series,https://robjhyndman.com/seminars/coherent/,A talk given today at Macquarie University Sydney.
2013,10,10,Forecasting hierarchical time series,https://robjhyndman.com/seminars/hts-3/,Talk given at University of Sydney today.
2013,10,9,Robert G Brown (1923-2013),https://robjhyndman.com/hyndsight/rgbrown/,"Robert Goodell Brown was the father of exponential smoothing. He died last week at the age of 90. While I never met him I was indebted to him for exponential smoothing and his practical and insightful books.
Today I received this email from King Harrison III advising of his death. Twenty years ago I attended the ISF 93 conference in Pittsburgh which honored Bob Brown on his 70th birthday and his contributions to the forecasting world."
2013,9,25,Fokkens et al. on Replication Failure,https://lingpipe-blog.com/2013/09/25/fokkens-et-al-on-reproduction-replication-failure/,After ACL Becky Passonneau said I should read this: Antske Fokkens Marieke van Erp Marten Postma Ted Pedersen Piek Vossen Nuno Freire. 2013. Offspring from Reproduction Problems: What Replication Failure Teaches Us. ACL Proceedings. You should too. Or if you&#8217;d rather watch there&#8217;s a video of their ACL presentation. The Gist The gist of the [&#8230;]
2013,9,17,Forecasting with daily data,https://robjhyndman.com/hyndsight/dailydata/,"I&rsquo;ve had several emails recently asking how to forecast daily data in R. Unless the time series is very long the easiest approach is to simply set the frequency attribute to 7.
y &lt;- ts(x frequency=7) Then any of the usual time series forecasting methods should produce reasonable forecasts. For example
library(forecast) fit &lt;- ets(y) fc &lt;- forecast(fit) plot(fc) When the time series is long enough to take in more than a year then it may be necessary to allow for annual seasonality as well as weekly seasonality."
2013,9,11,MAXIMA research centre at Monash Uni,https://robjhyndman.com/hyndsight/maxima/,"The &ldquo;Monash Academy for Cross and Interdisciplinary Mathematical Applications&rdquo; (MAXIMA) is a new research centre that aims to maximise the potential of mathematics to deliver impact to society. It will be led by Kate Smith-Miles. I will also be involved along with several other mathematicians at Monash. Our mission at MAXIMA is to find solutions to 21st century problems by dismantling mathematical barriers.
MAXIMA will be launched on 25 September at a public lecture on &ldquo;The Role of Embedded Optimization in Smart Systems and Products&rdquo;."
2013,9,9,LingPipe Incubator Welcomes Seer,https://lingpipe-blog.com/2013/09/09/lingpipe-incubator-welcomes-getseer/,We have started an incubator program for start-ups with natural language processing (NLP) needs. Seer is our first egg. The company creates productivity apps based on unstructured data sources&#8211;that is where LingPIpe fits in. The guys (Conall and Joe) fit a pattern that we see quite often&#8211;smart people with a good idea that presupposes NLP [&#8230;]
2013,8,21,Token Lumpers and Splitters: Spider-Man vs. Superman,https://lingpipe-blog.com/2013/08/21/tokenization-lumpers-splitters-superman-spider-man/,One of the perennial problems in tokenization for search classification or clustering of natural language data is which words and phrases are spelled as compounds and which are separate words. For instance consider &#8220;dodgeball&#8221; (right) vs. &#8220;dodge ball&#8221; (wrong) and &#8220;golfball&#8221; (wrong) vs. &#8220;golf ball&#8221; (right)? It&#8217;s a classic lumpers vs. splitters problem. Alas the [&#8230;]
2013,7,31,"Disney, Caltech",http://yyue.blogspot.com/2013/07/disney-caltech.html,"As many of you know I was kept quite busy on the faculty job market this past spring when I spent three months traveling almost non-stop. My time as a postdoc at Carnegie Mellon University was a very stimulating and productive one but it is now coming to a close. Many thanks to my supervisors Carlos and Krishnan as well as all my other colleagues and collaborators. CMU truly is an amazing place with its sheer scale of high-impact research activity.I'm delighted to announce that I've accepted an assistant professor position in the Computing and Mathematical Sciences Department at the California Institute of Technology.When visiting Caltech I was very drawn to the technical rigor as well as the intense interdisciplinary focus that permeate the entire faculty. For instance I was pleasantly surprised to find common ground with geologists economists and biologists there.This interdisciplinary culture is in part due to Caltech's extremely small size (only ~300 faculty total). In fact I will be one of the very few machine learning researchers at Caltech and I'm looking forward to interacting and collaborating with a wide range of researchers there.However I won't be starting at Caltech just yet. Starting next week I will be spending one year at Disney Research&nbsp;(and defer my start at Caltech to fall 2014). I'm treating this as an chance to do a pre-sabbatical in industry before starting my tenure-track job.Disney Research tackles an impressive range of interesting data mining and modeling challenges (including advanced video analysis data-driven animation and cyber-physical sensor systems) and also currently do not have an overly large machine learning presence. I hope to use this opportunity to find ""untouched"" ground to push machine learning in new directions that I find interesting.Finally I'd like to thank all of my reference letter writers&nbsp;Thorsten&nbsp;Carlos&nbsp;Bobby&nbsp;and Krishnan&nbsp;for all their support and guidance throughout my job search process. It's been a hectic transitional year for me and I'm very much looking forward to diving back into research!"
2013,7,17,Interpretable Predictive Models,http://yyue.blogspot.com/2013/07/interpretable-predictive-models.html,"I recently attended a workshop on media at the NYU Center for Urban Science and Progress.  Many thanks to Arun Sundararajan and Maria Liakata for planning the workshop which facilitated a very interesting exchange of ideas from people in very different areas.One issue that was discussed extensively during the workshop is the need for interpretable predictive models.    Since the actual definition of ""interpretable"" is up for debate I concluded that (at least for now) it's more useful to talk about when and why a model isn't interpretable and what problems that might cause.The primary over-arching use-case is when an end-user is using the model not only for superior predictive power but also for deriving insight from a large dataset or problem domain. Examples include: Investigative analysis that aims to use a predictive model for policy making. Debugging/building a complicated predictive model for commercial purposes.In both examples the decision-making process of the model needs to be somehow understandable (at least at a high level) and thus trustworthy.  Unfortunately effective predictive models are typically very complex with potentially billions of parameters makes them difficult to interpret/understand using conventional means (i.e. inspecting the individual parameters). For many settings one effective approach could be to explain the behavior of the model on specific problem instances rather than the model as a whole.  There appears to be (at least) two types of approaches for this.Sensitivity AnalysisOne approach that Foster Provost mentioned at the workshop is to perform a form of sensitivity analysis to understand which alternative scenarios would cause the model predict something different.  For example weather patterns such as tropical storms are notoriously difficult to model and any prediction on the future behavior of such storms also come with a so-called cone of uncertainty.  One type of useful analysis would be to understand what factors might case the hurricane to fall within different regions of the cone of uncertainty (assuming the model isn't just using simple context-free stochastic process to explain uncertainty).  This type of sensitivity analysis can be done by varying the different input attributes into the model and then summarizing evaluating the likely outcomes.  Typically analysts would do a significant chunk of this work manually which significantly limits the scalability of such techniques.  It would be interesting to develop automated ""meta-analysis"" algorithms that can perform large-scale sensitivity analysis and summarization for large classes of predictive models.Structured ModelsAnother way for models to explain or justify their predictions is to actually build such capabilities into the predictive model.  Many prediction domains require structured models that can make complex predictions.  For example recent work by Yun Jiang use a model of hallucinated humans to understand a scene (such as a room) and how to predict where objects should be placed.  In this case the hallucinated human serves as a way for the model to explain that e.g.  a sofa should be placed opposite of a television.  When analyzing any particular scene it would be interesting to develop useful ways of exposing salient aspects of the hallucinated humans (which is typically referred to as a hidden or latent part of the model) to the end user.  Another example is my previous work on sentiment analysis with Ainur Yessenalina where we built a model to predict the sentiment of a movie review or congressional speech.  The model justifies its predictions by also extracting the sentiments that best explain the overall prediction.These models make the (sometimes implicit) assumption that for any particular problem instance a small set of factors contribute the bulk of reasoning behind the model's prediction for that instance.  Note that the set of contributing factors can vary for different problem instances.  Such ""structured"" models are essentially modeling the data at a level of granularity that is more expressive than a simple prediction (e.g. the likely human poses or the most opinionated sentences) but also less complex than the raw data.  It would be interesting to develop these types of models to be more amenable to human inspection and modification."
2013,7,12,Reflections on UseR! 2013,https://robjhyndman.com/hyndsight/user2013/,"This week I&rsquo;ve been at the R Users conference in Albacete Spain. These conferences are a little unusual in that they are not really about research unlike most conferences I attend. They provide a place for people to discuss and exchange ideas on how R can be used.
Here are some thoughts and highlights of the conference in no particular order.  Håvard Rue spoke on Bayesian computing with INLA and the R-INLA package."
2013,7,10,Asking good questions,https://robjhyndman.com/hyndsight/questions/,"I&rsquo;m currently attending my third conference in three weeks. So I&rsquo;ve heard a lot of talks and I&rsquo;ve heard a lot of questions asked after the talks. In this guest post Eran Raviv reflects on what makes a good question after a talk.
 A few weeks back I attended the excellent ISF conference. In one of the sessions the presenter was talking about a state-of-the-art method to prevent model overfitting an issue with an ever-growing importance in this era of increased computational power and data storage capabilities."
2013,7,3,Facts and fallacies of the AIC,https://robjhyndman.com/hyndsight/aic/,Akaike&rsquo;s Information Criterion (AIC) is a very useful model selection tool but it is not as well understood as it should be. I frequently read papers or hear talks which demonstrate misunderstandings or misuse of this important tool. The following points should clarify some aspects of the AIC and hopefully reduce its misuse.  The AIC is a penalized likelihood and so it requires the likelihood to be maximized before it can be calculated.
2013,7,3,R tools for hierarchical time series,https://robjhyndman.com/seminars/hts-2/,"Talk given at EURO/INFORMS Rome 1 July 2013
And at UseR! 2013 Albacete Spain 10 July 2013."
2013,6,29,End of Google Reader,https://robjhyndman.com/hyndsight/feedly/,"If you are reading this in Google Reader or you are reading in an RSS reader that uses Google Reader as a back-end then you probably need this reminder. Everyone else can stop reading now.Google Reader will cease to be available on 1 July and you will lose all your feeds and other data if you have not switched before then.
I have switched to using Feedly and it is a very convenient replacement."
2013,6,26,Future ISFs,https://robjhyndman.com/hyndsight/future-isfs/,"The next few locations for the International Symposium on Forecasting have been announced:
  2014: Rotterdam The Netherlands
  2015: Riverside California USA
  2016: Santander Spain
  2017: Cairns Australia
  The ISF is easily the best forecasting conference and is held every year in different locations. The 2013 conference finished today in Seoul South Korea.
I&rsquo;m especially delighted that it is coming back to Australia in 2017."
2013,6,24,Forecasting without forecasters,https://robjhyndman.com/seminars/forecasting-without-forecasters/,A keynote talk given at the International Symposium on Forecasting Seoul South Korea. 25 June 2013.
2013,5,25,Managing research ideas,https://robjhyndman.com/hyndsight/managing-research-ideas/,"I received this email today:
 Dear Professor Hyndman I was wondering if you could maybe give me some advice on how to organize your research process. I am able to search the literature on a certain topic and identify where there is a question to work with. My main difficult is to organize my paper annotations in order to help me to guide my research process i.e how to manage the information gathered in those papers to compose and structure a document which can represent the research developed so far."
2013,5,17,IJF quality indicators,https://robjhyndman.com/hyndsight/ijf-quality-indicators/,"I often receive email asking about IJF quality indicators. Here is one I received today.
 Dear Professor Hyndman
  I recently had a paper published in IJF entitled &ldquo;xxxxxxxxxxxx&rdquo;. I am very pleased with the publication and consider IJF to be an excellent outlet for my work in time-series econometrics.
  I have an unusual request but I hope you will consider responding. My research is judged by non-economists and IJF is not on their list of &ldquo;quality&rdquo; journals."
2013,5,15,Forecasting annual totals from monthly data,https://robjhyndman.com/hyndsight/forecasting-annual-totals/,"This question was posed on crossvalidated.com:
 I have a monthly time series (for 2009-2012 non-stationary with seasonality). I can use ARIMA (or ETS) to obtain point and interval forecasts for each month of 2013 but I am interested in forecasting the total for the whole year including prediction intervals. Is there an easy way in R to obtain interval forecasts for the total for 2013?
 I&rsquo;ve come across this problem before in my consulting work although I don&rsquo;t think I&rsquo;ve ever published my solution."
2013,5,13,Two Thoughts on the Academic Job Market,http://yyue.blogspot.com/2013/05/two-thoughts-on-academic-job-market.html,"As some of you know I'm on the academic job market this year. The whole process for me isn't over yet (so I can't divulge any details regarding my situation) but I wanted to talk about two issues about the academic job search that have been bothering me.Head Count Capped By EnrollmentAt almost every university the tenure-track faculty head count is more-or-less capped by undergraduate enrollment (or a professional school enrollment if you're looking for a job in say a business school).  Yet the dean of EVERY university I interviewed at told me that the primary responsibility of (new) tenure-track faculty is to engage in basic research.I'm sure that if one thought hard enough about the issue (and maybe perused some history books) then one could reverse engineer how this whole situation came to be.  Here are how things look from where I stand:  Tenure-track positions have historically been prestigious because of the job-security and this continues to be the case. The budget for tenure-track salaries have historically been computed based on student enrollment. For most schools this means primarily undergraduate enrollment.  It's unclear to what extent tenure-track salaries are currently paid for out of student tuition revenues but I can imagine that was the case in the past. Ever since the end of World War II the US federal government has invested in basic research (through funding agencies such as the National Science Foundation).  Intellectual scholarship is the first-order bit that dictates prestige in academia. These days most of that is based on scientific research.Not only are tenure-track positions few in number but most other research positions are significantly lacking (by comparison) in the way of salaries and benefits.  My sense is that it's difficult to find comfortable non-tenured (i.e. ""soft money"") research positions.  This situation strikes me as rather odd (and wrong) and I definitely think that the number of comfortable positions available for basic research is too low.The End GamePart 1: Short Fuse OffersCandidates spend months preparing their application.  Job search committees also spend a long time reviewing applications and interviewing short-listed candidates.  Yet the final few weeks of the academic job market are filled with rapid-fire offers that have short fuses (i.e. quick expiration dates).  For example someone I know received an offer with a one-week deadline well before he'd finished all his interviews.  From the schools' perspective an early-deadline offer puts pressure on candidates to make a decision.  Part of the rationale behind this strategy is because schools typically have a list of candidates they'd like to make offers to (and they often aren't allowed to make many offers in parallel).  Of course the problem with this strategy is that most candidates (including myself) don't like feeling pressured into making a decision that has such a large impact on our lives.  Part 2: DeadlockIn the end many candidates push back on (or at least they should) the few schools they're serious about claiming that they need more time to make a decision about each school.  There might be several plausible reasons why candidates need more time including: Managing added complexities to one's personal life Wanting to be absolutely sure given the life-changing nature of the decision Waiting for other schools to get back to them before making a final decisionSchools will typically allow deadlines to be extended if they realize that the candidate is serious about them. Of course knowing this fact didn't change the squirmy feeling I got when I asked for deadlines to be pushed back. Many candidates will get wait-listed by schools that they'd rather hear back from before making a final decision.  This situation arises since virtually all departments can only make a small number of outstanding offers even if they'd ideally like to make more (typically the dean limits headcount based on enrollment or other factors). Should one of the department's outstanding offers get declined then the department is free to make a second offer (hence the short-fuse deadlines from Part 1).  In most cases everyone is just waiting on a few candidates to make up their minds after which everything cascades and falls into place.  However a deadlock arises when a cycle is created: Candidate 1 gets an offer from School A but prefers School B Candidate 2 gets an offer from School B but prefers School A School A intends to give a second-round offer to Candidate 2 School B intends to give a second-round offer to Candidate 1In essence both candidates get offers from their second choice which causes them to push back their deadlines indefinitely (well not actually indefinitely but you get the idea). Given the limited amount of information sharing and cooperation between schools and candidates this type of situation can easily drag on for weeks. This process is in my opinion severely flawed.  However it's unclear if there's a good solution out there that doesn't require schools (and candidates) to more openly share information and cooperate in some fashion."
2013,5,6,Establishing priority,https://robjhyndman.com/hyndsight/establishing-priority/,The nature of research is that other people are probably working on similar ideas to you and it is possible that someone will beat you to publishing them.When I was working on my PhD I discovered another PhD thesis by Iris Yeung at UKC with almost exactly the same title as mine and published a year earlier. In those days a copy of a thesis had to be printed from microfiche and then posted by snail mail.
2013,4,23,Human Supremacy Bias,http://yyue.blogspot.com/2013/04/human-supremacy-bias.html,"Every once in a while I'm reminded of the implicit (or sometimes explicit) human bias that humans are at (or near) the proverbial center of the universe.For instance Thomas Nagel's recently published book Mind & Cosmos argues that the theory of evolution is false (and will soon be replaced by a more comprehensive theory). More notably the book argues that the main failing of current evolutionary science is that:It fails to account for how consciousness fits into the natural order. Instead it regards it as an afterthought an accidental quirk a trinket on the tree of life less important to life’s story than the random physical mutations of genes.The thing that I find notable about this argument is its elevation of [human] consciousness to the forefront of the argument.  This whole thing reminds me of a conversation I had a few years ago.  While discussing what rights a hypothetical general AI agent should have someone (let's call this person Person A) told me that even if one could construct an artificial intelligence whose cognitive capacity was indistinguishable from humans that Person A could not accept that being as an ""equal"".  I then rephrased the query supposing this general AI agent were to be (at least superficially) indistinguishable from real humans (think Bicentennial Man).  In that setting Person A felt it would be OK to grant such a being equal rights as humans.  Person A noted that in retrospect some kind of low-level emotional response played a significant role in his/her thought process.I suspect that a similar type of emotional response (e.g. regarding the supremacy of human consciousness) is driving Thomas Nagel's viewpoint.  The above linked article further describes Nagel's other views which lends credence to this suspicion -- e.g. talking about how human consciousness is ""part of the lengthy process of the universe gradually waking up and becoming aware of itself.""  I'll not waste time talking about my personal views on this issue (I am a materialist).  But I think it's important to realize when one's (subconscious) biases and emotional responses may be driving an entire intellectual agenda -- i.e. ""the theory of evolution must be wrong because it currently explains human consciousness as the result of unguided mutations and natural selection."""
2013,4,21,My new forecasting book is finally finished,https://robjhyndman.com/hyndsight/fpp-2/,"My new online forecasting book (written with George Athanasopoulos) is now completed. I previously described it on this blog nearly a year ago.
In reality an online book is never complete and we plan to continually update it. But it is now at the point where it is suitable for course work and contains exercises and references.
We hope that users (especially other lecturers) will submit materials such as slides and exercises that can be shared on the website."
2013,3,31,George E P Box (1919-2013),https://robjhyndman.com/hyndsight/gepbox/,"Last Thursday (28 March 2013) George Box passed away at the age of 93. He was one of the great statisticians of the last 100 years and leaves an astonishingly diverse legacy.
When I teach forecasting to my second year commerce students we cover Box-Cox transformations Box-Pierce and Ljung-Box tests and Box-Jenkins modelling and my students wonder if it is the same Box in all cases. It is. And we don&rsquo;t even go near his work on response surface modelling design of experiments quality control or random number generation."
2013,3,15,Reviewing Season,http://yyue.blogspot.com/2013/03/reviewing-season.html,"It's reviewing season for many machine learning conferences (and for conferences in other fields as well). &nbsp;My personal sample size is rather small but I must say that the quality of reviewing has improved noticeably over the recent years.I've been especially pleased with the ICML reviewing system this year which has taken steps towards becoming more of a hybrid conference/journal system. &nbsp;Being conference-driven (rather than journal-driven) once yielded significant benefits in terms of rapid dissemination of new research results. &nbsp;However given the rapidly growing number of submissions (as well as the fact that everything is submitted and reviewed electronically these days) it no longer makes sense to constrain submitting and reviewing to a few brief time intervals every year. &nbsp;This paper by H V Jagadish&nbsp;provides a nice discussion of the benefits of a fully hybridized conference/journal system.If I were to nitpick one thing it would be the lack of information regarding the levels of expertise and interest that reviewers have for various submissions. &nbsp;For example in ICML most reviewers seem to only express interest in a small number of submissions thus often requiring an Area Chair to ""infer"" whether a reviewer would be a good fit for reviewing any given submission. &nbsp;Although most conferences require submissions to enter in keywords or subjects this often does not provide well-calibrated information.One approach that I've enjoyed using in the past is the&nbsp;Toronto Matching System which is able to learn a matching between my areas of expertise and the technical areas contained in a submission. &nbsp;I hope that most conferences adopt something similar in the near-future because it reduces the amount of human effort needed to find good matchings between reviewers and submissions."
2013,3,12,The difference between prediction intervals and confidence intervals,https://robjhyndman.com/hyndsight/intervals/,"Prediction intervals and confidence intervals are not the same thing. Unfortunately the terms are often confused and I am often frequently correcting the error in students' papers and articles I am reviewing or editing.
A prediction interval is an interval associated with a random variable yet to be observed with a specified probability of the random variable lying within the interval. For example I might give an 80% interval for the forecast of GDP in 2014."
2013,2,28,ETS models now in EViews 8,https://robjhyndman.com/hyndsight/eviews8/,"The ETS modelling framework developed in my 2002 IJF paper (with Koehler Snyder and Grose) and in my 2008 Springer book (with Koehler Ord and Snyder) is now available in EViews 8. I had no idea they were even working on it so it was quite a surprise to be told that EViews now includes ETS models.Here is the blurb from the release notes:
 EViews 8 now offers support for exponential smoothing using the dynamic nonlinear model framework of Hyndman Koehler et al."
2013,2,22,Removing white space around R figures,https://robjhyndman.com/hyndsight/crop-r-figures/,"When I want to insert figures generated in R into a LaTeX document it looks better if I first remove the white space around the figure. Unfortunately R does not make this easy as the graphs are generated to look good on a screen not in a document.
There are two things that can be done to fix this problem.First you can reduce the white space generated by R. I use the following function when saving figures in R."
2013,2,15,Forecasting conferences,https://robjhyndman.com/hyndsight/forecasting-conferences/,This year there are no fewer than three forecasting conferences planned for June and July 2013. As well as the annual International Symposium on Forecasting there is WIPFOR (Workshop on Industry &amp; Practices for FORecasting) to be held in Clamart (near Paris) in June and a forecasting stream at the EURO2013 conference in Rome in early July. Some details follow taken from emails sent to me recently.WIPFOR (Clamart France 5-7 June 2013) We would like to bring to your attention the Second Workshop on Industry &amp; Practices for FORecasting (WIPFOR) with an emphasis on Modeling and Stochastic Learning for Forecasting in High Dimension.
2013,2,14,Hyndsight,https://robjhyndman.com/hyndsight/hyndsight/,"Originally I wrote this blog for my own PhD students and I covered issues to do with research. I called it &ldquo;Research tips&rdquo; because that is what it was meant to be.
However over time I&rsquo;ve started covering other things of interest to me and the readership has grown way beyond what I ever expected. So I decided it was time to acknowledge the change of focus with a change of name."
2013,2,13,Out-of-sample one-step forecasts,https://robjhyndman.com/hyndsight/out-of-sample-one-step-forecasts/,"It is common to fit a model using training data and then to evaluate its performance on a test data set. When the data are time series it is useful to compute one-step forecasts on the test data. For some reason this is much more commonly done by people trained in machine learning rather than statistics.
If you are using the forecast package in R it is easily done with ETS and ARIMA models."
2013,2,11,Statistical consulting in Australia,https://robjhyndman.com/hyndsight/consulting-groups/,"There must be dozens of statistical consulting businesses and organizations in Australia each specializing in different areas.
I do some consulting work myself mostly in the forecasting area but sometimes in other areas of applied statistics including expert witness work in court cases. Email me if you have a project you would like me to take on. However I often refer potential clients to other statistical consulting groups as I only have a limited amount of time I can spend on consulting projects."
2013,2,6,Man vs wild data,https://robjhyndman.com/seminars/man-vs-wild-data/,"Keynote address. Young Statisticians Conference 2013.
Abstract: For 25 years I have been an intrepid statistical consultant tackling the wild frontiers of real data real problems and real time constraints. I have faced problems ranging from linguistics to river beds from making paper plates to selling pies at the MCG from tax office audits to surveys about the colour purple. University education helps prepare you to be a statistical consultant in the same way that Google maps helps prepare you to cross the Simpson Desert."
2013,2,1,Coherent mortality forecasting: the product-ratio method with functional time series models,https://robjhyndman.com/publications/coherentfdm/,When independence is assumed forecasts of mortality for subpopulations are almost always divergent in the long term. We propose a method for coherent forecasting of mortality rates for two or more subpopulations based on functional principal components models of simple and interpretable functions of rates. The product-ratio functional forecasting method models and forecasts the geometric mean of subpopulation rates and the ratio of subpopulation rates to product rates. Coherence is imposed by constraining the forecast ratio function through stationary time series models.
2013,1,7,Batch forecasting in R,https://robjhyndman.com/hyndsight/batch-forecasting/,"I sometimes get asked about forecasting many time series automatically. Here is a recent email for example:
 I have looked but cannot find any info on generating forecasts on multiple data sets in sequence. I have been using analysis services for sql server to generate fitted time series but it is too much of a black box (or I don’t know enough to tweak/manage the inputs). In short what package should I research that will allow me to load data generate a forecast (presumably best fit) export the forecast then repeat for a few thousand items."
2013,1,1,A change of editors,https://robjhyndman.com/publications/a-change-of-editors/,
2012,12,21,Man vs Wild Data,https://robjhyndman.com/hyndsight/ysc2013/,"I&rsquo;m speaking on this topic at the Young Statisticians Conference 7-8 February 2013.
If you&rsquo;re a young statistician and live in Australia please book in. It promises to be a great couple of days. Early registrations close on 2 January.
Abstract for my talk:
For 25 years I have been an intrepid statistical consultant tackling the wild frontiers of real data real problems and real time constraints. I have faced problems ranging from linguistics to river beds from making paper plates to selling pies at the MCG from tax office audits to surveys about the colour purple."
2012,12,12,Thoughts on NIPS 2012,http://yyue.blogspot.com/2012/12/thoughts-on-nips-2012.html,I've recently returned from this year's NIPS conference which is the largest annually held machine learning conference.  The program was excellent but the venue left something to be desired.NIPS is currently undergoing a transition period.  During the previous decade NIPS was always held in Vancouver (main conference) and Whistler (workshops) which were excellent locations.  However because of increased costs (in part due to the 2010 winter Olympics) the two recent NIPS conferences were held in Grenada (last year) and South Lake Tahoe (this year) respectively.This year's conference was held jointly at the Harveys and Harrah's hotels.  The two hotels are located across the street from each other and are joined via an underground floor filled with restaurants casinos arcades and the like. I suspect the rather confusing path from one venue to the other was done quite deliberately by the hotels in an effort to promote gambling.  Unfortunately for them rather few of the 1000+ NIPS attendees participated in the smoke-filled debauchery (which I heard peeved the hotels quite a bit).  Nonetheless the winding maze was successful at limiting my exposure to the outdoors. I actually spent my first 36 hours completely indoors after which I started feeling claustrophobic. In fact one fellow attendee spent 5 days (that's 120 hours) straight indoors before finally venturing outside.  The conference program was fantastic. Here are some of the papers that I found particularly interesting:Discriminative Learning of Sum-Product Networks -- sum-product networks are a new deep learning architecture that yields tractable inference.  Deep architectures are the most expressive machine learning models in existence but are notoriously difficult to train.  This paper shows how to discriminatively train sum-product networks (this is a bit of misnomer -- max-product networks really) which leads to significantly improved prediction accuracy. Imitation Learning by Coaching -- imitation learning is a learning approach where a human expert teaches a computer program how to behave within some environment.  Often times the actions of a human expert are too difficult for a computer program to initially learn and so the program might be better served by first learning how to perform easier actions.  This paper proposes such an approach (which they call Coaching) and demonstrates improved theoretical guarantees and empirical performance.Near-Optimal MAP Inference for Determinantal Point Processes -- determinental point processes (DPPs) have recently gained visibility in the machine learning community.  A DPP is a probabilistic model that encourages predictions to be diverse.  This paper shows how to perform MAP inference with DPPs which were previously difficult to do well.  One thing I'm not sold on is why one would want to do MAP inference with a DPP. DPPs spend considerable model capacity learning a probability distribution which MAP inference throws away.  If one wanted to do MAP inference why not just use a discriminatively trained model instead?  Practical Bayesian Optimization of Machine Learning Algorithms -- let's face it parameter tuning is a pain in the ass. The naive thing to do (which I have been guilty of on several occasions) is parameter grid search which scales exponentially with the number of tuning parameters. This paper shows how to frame parameter tuning as a goal-oriented active learning problem (my terminology).  The main difference with classical active learning is that the final performance of the model is evaluated on the best action the model can take (i.e. predicting the best parameter setting) rather than predicting the performance of all actions (i.e. predicting the response variable of all inputs).  This style of approach could potentially be useful for more structured active learning problems as well.A Spectral Algorithm for Latent Dirichlet Allocation -- spectral algorithms have become increasingly popular for learning various latent variable models in machine learning (started by this paper).  In contrast to the common alternative Expectation Maximization spectral algorithms are exact learning algorithms subject to there being sufficient training data.  This paper shows how to do spectral learning for Latent Dirichlet Allocation which is pretty cool.  This particular spectral learning approach is largely a theoretical result so it will be interesting to see how practical it is (or if it could be made sufficiently practical).And as usual the NIPS workshops were fantastic.  I particularly enjoyed the Bayesian Optimization & Decision Making and Personalizing Education With Machine Learning workshops. Andrew Ng gave an inspiring presentation on Coursera and the potential of online education. Online education (in its current form) certainly has its flaws but it definitely seems that they will be an integral part of the education process moving forward.
2012,12,3,New in forecast 4.0,https://robjhyndman.com/hyndsight/forecast4/,A few days ago I released version 4.0 of the forecast package for R. There were quite a few changes and new features so I thought it deserved a new version number. I keep a list of changes in the Changelog for the package but I doubt that many people look at it. So for the record here are the most important changes to the forecast package made since v3.
2012,11,20,"SimpleR: tips, tricks and tools",https://robjhyndman.com/seminars/simpler/,"Melbourne R Users' Group
Deloitte Level 11 550 Bourke Street Melbourne
Examples"
2012,10,31,Makefiles for R/LaTeX projects,https://robjhyndman.com/hyndsight/makefiles/,"Updated: 21 November 2012
Make is a marvellous tool used by programmers to build software but it can be used for much more than that. I use make whenever I have a large project involving R files and LaTeX files which means I use it for almost all of the papers I write and almost of the consulting reports I produce.If you are using a Mac or Linux you will already have make installed."
2012,10,29,YSC2013 funding support,https://robjhyndman.com/hyndsight/ysc2013-funding-support/,"Young Victorian statisticians should be attending the YSC conference in Melbourne in February 2013. It promises to be a great event and the speaker line-up looks first-class (apart from one dodgy keynote speaker).
If you think you can&rsquo;t afford it there is good news! The local SSA Branch is offering financial support to selected young statisticians. Applicants must be student members of the Victorian Branch (to join see www.statsoc.org.au/join.htm). Successful applicants will be given free conference registration."
2012,10,23,LaTeX loops,https://robjhyndman.com/hyndsight/latex-loops/,"Today I was writing a report which included 20 figures with the names demandplot1.pdf demandplot2.pdf &hellip; demandplot20.pdf and all with similar captions. Clearly a loop was required. After all LaTeX is a programming language so we should be able to take advantage of its capabilities.I found the forloop package which handled the problem perfectly. My first attempt looked like this:
\newcommand{\demandplot}[1]{\begin{figure}\centering \includegraphics[width=\textwidth]{./figs/demandplot#1.pdf} \caption{Hourly demand (GW) for zone #1.} \end{figure}} \newcounter{loop} \forloop{loop}{1}{\value{loop}&lt;21}{\demandplot{\arabic{loop}}}  The magic happens in the last line providing a loop from 1 to 20 generating the commands \demandplot{1} \demandplot{2} &hellip; \demandplot{20}."
2012,10,7,The Young Stats Communication Challenge,https://robjhyndman.com/hyndsight/ysc2013comp/,"The Australian Young Statisticians Conference (Feb 2013) is organizing a communication competition. They invite all early-career statisticians (studying or within 5 years of graduation) to produce a short (3-5 minute) video for the ABS YSC2013 Video Competition or a static infographic for the ABS YSC2013 Infographic Competition.
Both competitions have a 1st prize of $500 and 2nd prize of $250.
Entries close 16th November and winners will be notified by mid-December."
2012,10,3,Forecasting research grants,https://robjhyndman.com/hyndsight/iifsas/,The International Institute of Forecasters and SAS have an annual research grant scheme that has been offered for the past ten years. The amounts offered are small (a total of $10K per year usually split between 2 or 3 projects) but it might be useful for young researchers wanting a bit of funding to help with their forecasting research. The deadline for 2012 has just been extended to 26 October which is a sure sign that they don&rsquo;t have enough applications yet.
2012,9,18,Why are some things easier to forecast than others?,https://robjhyndman.com/hyndsight/hardforecasts/,Forecasters are often met with skepticism. Almost every time I tell someone that I work in forecasting they say something about forecasting the stock market or forecasting the weather usually suggesting that such forecasts are hopelessly inaccurate. In fact forecasts of the weather are amazingly accurate given the complexity of the system while anyone claiming to forecast the stock market deserves skepticism. So what is the difference between these two types of forecasts and can we say anything about what can be reasonably be forecast and what can&rsquo;t?
2012,9,3,Multivariate time series modelling and forecasting workshop,https://robjhyndman.com/hyndsight/workshop2013/,"We are planning a workshop on multivariate time series modelling and forecasting to be held at Monash University on 18-19 February 2013.
Call for papers: all submissions should be sent in pdf format to george.athanasopoulos@monash.edu by 20 November 2012.
Keynote speakers   Professor Helmut Lütkepohl (DIW Berlin)
  Professor Massimiliano Marcellino (European University Institute Florence)
  Associate Professor Andrew Patton (Duke University Durham NC)
  Professor George Kapetanios (Queen Mary University of London)"
2012,9,1,Recursive and direct multi-step forecasting: the best of both worlds,https://robjhyndman.com/publications/rectify/,We propose a new forecasting strategy called rectify that seeks to combine the best properties of both the recursive and direct forecasting strategies. The rationale behind the rectify strategy is to begin with biased recursive forecasts and adjust them so they are unbiased and have smaller error. We use linear and nonlinear simulated time series to investigate the performance of the rectify strategy and compare the results with those from the recursive and the direct strategies.
2012,8,28,COMPSTAT2012,https://robjhyndman.com/hyndsight/compstat2012/,"This week I&rsquo;m in Cyprus attending the COMPSTAT2012 conference. There&rsquo;s been the usual interesting collection of talks and interactions with other researchers. But I was struck by two side comments in talks this morning that I&rsquo;d like to mention.
Stephen Pollock: Don&rsquo;t imagine your model is the truth Actually Stephen said something like &ldquo;economists (or was it econometricians?) have a bad habit of imagining their models are true&rdquo;. He gave the example of people asking whether GDP &ldquo;has a unit root&rdquo;?"
2012,8,19,Flat forecasts,https://robjhyndman.com/hyndsight/flat-forecasts/,"About once a week someone will tell me there is a bug in my forecast package for R because it gives forecasts that are the same for all future horizons. To save answering the same question repeatedly here is my response.
A point forecast is (usually) the mean of the distribution of a future observation in the time series conditional on the past observations of the time series. It is possible even likely in some circumstances that the future observations will have the same mean and then the forecast function is flat."
2012,8,9,Interviews,https://robjhyndman.com/hyndsight/interviews/,"I&rsquo;ve been interviewed twice in the last year:
  For DecisionStats 9 August 2012.
  For Data Mining Research 21 October 2011. Republished in Amstat News 1 December 2011.
  Some readers of this blog might find them interesting. I said a few things in today&rsquo;s interview that I don&rsquo;t think I&rsquo;ve previously said publicly."
2012,8,9,Blogs about research,https://robjhyndman.com/hyndsight/blogs-about-research/,"If you find this blog helpful (or even if you don&rsquo;t but you&rsquo;re interested in blogs on research issues and tools) there are a few other blogs about doing research that you might find useful. Here are a few that I read.
  Patter &ndash; Pat Thomson.
  The Thesis Whisperer &ndash; Inger Mewburn.
  The Research Whisperer &ndash; several RMIT researchers.
  the (research) supervisor&rsquo;s friend &ndash; Geof Hill."
2012,8,3,Read the literature,https://robjhyndman.com/hyndsight/read-the-literature/,"I&rsquo;ve just finished another reviewer report for a journal and yet again I&rsquo;ve had to make comments about reading the literature. It&rsquo;s not difficult. Before you write a paper read what other people have done. A simple search on Google scholar will usually do the trick. And before you submit a paper check again that you haven&rsquo;t missed anything important.
The paper I reviewed today did not cite a single reference from either of the two most active research groups in the area in the last ten years."
2012,8,2,Put your pre-prints online,https://robjhyndman.com/hyndsight/preprints/,"I have argued previously that research papers should be posted online at the same time as they are submitted to a journal. Sometimes people claim that journals don&rsquo;t allow it which is nonsense. Almost every journal allows it and many also allow the published version of a paper to appear on your personal website.
Today I discovered a new tool (thanks to the IMU newsletter) which makes it easy to check a journal&rsquo;s policy on this."
2012,8,1,Bare bones beamer,https://robjhyndman.com/hyndsight/beamer/,"Beamer is far and away the most popular software for presentations amongst researchers in mathematics and statistics. Most conference and seminar talks I attend these days use beamer. Unfortunately they all look much the same. I think people find beamer themes too hard to modify easily so a small number of templates get shared around. Even the otherwise wonderful LaTeX Templates site has no beamer examples.
The beamer user guide explains how to make changes but it is not for the faint-hearted (although it is a fantastic resource once you have some expertise)."
2012,7,31,Forecasting the Olympics,https://robjhyndman.com/hyndsight/olympics/,"Forecasting sporting events is a growing research area. The International Journal of Forecasting even had a special issue on sports forecasting a couple of years ago.
The London 2012 Olympics has attracted a few forecasters trying to predict medal counts world records etc. Here are some of the articles I&rsquo;ve seen.
 Which Olympic records get shattered? Nate Silver New York Times. Statisticians predict the number of Olympic records that will fall at London 2012 Physics arXiv blog."
2012,6,25,A case-crossover design to examine the role of aeroallergens and respiratory viruses on childhood asthma exacerbations requiring hospitalisation: The MAPCAH study,https://robjhyndman.com/publications/mapcah/,"Background: Few case-control studies of time dependent environmental exposures and respiratory outcomes have been performed. Small sample sizes pose modeling challenges for estimating interactions. In contrast case cross-over studies are well suited where control selection and responses are low time consuming and costly.
Objective: To demonstrate the feasibility and validity of a case crossover study of children admitted to hospital for asthma to examine interacting effects of time varying environmental exposures."
2012,6,19,Advances in automatic time series forecasting,https://robjhyndman.com/seminars/automaticforecasting/,Invited talk Australian Statistical Conference Adelaide 10 July 2012. COMPSTAT 2012 Cyprus 29 August 2012. Seminar Lancaster University 10 September 2012.  Many applications require a large number of time series to be forecast completely automatically. For example manufacturing companies often require weekly forecasts of demand for thousands of products at dozens of locations in order to plan distribution and maintain suitable inventory stocks. In population forecasting there are often a few hundred time series to be forecast representing various components that make up the population dynamics.
2012,6,6,Constants and ARIMA models in R,https://robjhyndman.com/hyndsight/arimaconstants/,"This post is from my new book Forecasting: principles and practice available freely online at OTexts.org/fpp/.
 A non-seasonal ARIMA model can be written as \begin{equation}\label{eq:c} (1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)e_t \end{equation} or equivalently as \begin{equation}\label{eq:mu} (1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d (y_t - \mu t^d/d!) = (1 + \theta_1 B + \cdots + \theta_q B^q)e_t \end{equation} where $B$ is the backshift operator $c = \mu(1-\phi_1 - \cdots - \phi_p )$ and $\mu$ is the mean of $(1-B)^d y_t$."
2012,5,23,My new forecasting textbook,https://robjhyndman.com/hyndsight/fpp/,"After years of saying that I was going to write a book to replace Makridakis Wheelwright and Hyndman (1998) I&rsquo;m finally ready to make an announcement!
My new book is Forecasting: principles and practice co-authored with George Athanasopoulos. It is available online and free-of-charge. We have written about 2/3 of the book so far (all of which is already available online) and we plan to finish it by the end of 2012."
2012,5,15,Blog aggregators,https://robjhyndman.com/hyndsight/blog-aggregators/,"A very useful way of keeping up with blogs in a particular area is to subscribe to a blog aggregator. These will syndicate posts from a large number of blogs and provide links back to the original sources. So you only need to subscribe once to get all the good stuff in that area.
There are now several blog aggregators available that might be of interest to readers here. And this blog is now syndicated on several other sites including those listed below."
2012,5,8,Seeking help,https://robjhyndman.com/hyndsight/seeking-help/,Every day I receive emails or comments on this blog asking for help with R forecasting LaTeX possible research topics how to install software or some other thing I&rsquo;m supposed to know something about. Unfortunately I cannot provide a one-man help service to the rest of the world. I used to reply to all the requests explaining where to go for help but I stopped replying a while ago as it took too much time to do even that.
2012,5,2,Measuring time series characteristics,https://robjhyndman.com/hyndsight/tscharacteristics/,"A few years ago I was working on a project where we measured various characteristics of a time series and used the information to determine what forecasting method to apply or how to cluster the time series into meaningful groups. The two main papers to come out of that project were:
Wang Smith and Hyndman (2006) Characteristic-​​based clustering for time series data. Data Mining and Knowledge Discovery 13(3) 335-364."
2012,4,2,Google scholar metrics,https://robjhyndman.com/hyndsight/google-scholar-metrics/,"Google has produced another great tool for researchers this time providing some metrics on journal citations. Google Scholar Metrics allows you to search on journals rather than articles and to see the average or median h-index for each journal.
For example a search on &ldquo;forecasting&rdquo; yields the following results:

The h-index is the largest number $h$ such that at least $h$ articles in that publication were cited at least $h$ times each."
2012,3,4,Data visualization,https://robjhyndman.com/hyndsight/data-visualization/,"For those who have not read the seminal works of Tufte and Cleveland please hang your heads in shame. To salvage some sense of self-worth you can then head over to Solomon Messing&rsquo;s blog where he is starting a series on data visualization based on the principles developed by Tufte and Cleveland (with R examples).
The classics are also worth reading and remain relevant despite the 20 or 30 years that have elapsed since they appeared."
2012,2,28,Exponential smoothing and regressors,https://robjhyndman.com/hyndsight/ets-regressors/,I have thought quite a lot about including regressors (i.e. covariates) in exponential smoothing (ETS) models and I have done it a couple of times in my published work. See my 2008 exponential smoothing book (chapter 9) and my 2008 Tourism Management paper. However there are some theoretical issues with these approaches which have come to light through the research of Ahmad Farid Osman one of our PhD students at Monash University.
2012,2,22,Academia StackExchange,https://robjhyndman.com/hyndsight/academia/,"There&rsquo;s a new StackExchange site that might be useful to readers: Academia. It is a Q&amp;A site for academics and those enrolled in higher education.
The draft FAQ says it will cover:
  Life as a graduate student postdoctoral researcher university professor
  Transitioning from undergraduate to graduate researcher
  Inner workings of research departments
  Requirements and expectations of academicians
  Judging from the first 89 questions this is going to be extremely helpful especially for PhD students."
2012,2,16,Mailing lists,https://robjhyndman.com/hyndsight/mailing-lists/,"Staying in touch with other researchers is important. You need to know about up-coming conferences seminars jobs etc. To this end it is worth finding a few key email lists to join. A long list of lists in econometrics and statistics is provided by EconometricLinks. Browse through it to find topics of interest. No doubt researchers in other disciplines have their own lists but I don&rsquo;t know about them.
For those in Australia and New Zealand there are two local lists that every statistician and econometrician should be on:"
2012,2,13,Table design,https://robjhyndman.com/hyndsight/tables/,Almost every research paper and thesis in statistics contains at least some tables yet students are rarely taught how to make good tables. While the principles of good graphics are slowly becoming part of a statistical education (although not an econometrics education!) the principles of good tables are often ignored. Perhaps people think they are obvious although the results I see in papers and theses suggest otherwise.Lately the topic seems to have been getting some much-needed attention and the following resources may be useful.
2012,2,1,Short-term load forecasting based on a semi-parametric additive model,https://robjhyndman.com/publications/stlf/,Short-term load forecasting is an essential instrument in power system planning operation and control. Many operating decisions are based on load forecasts such as dispatch scheduling of generating capacity reliability analysis and maintenance planning for the generators. Overestimation of electricity demand will cause a conservative operation which leads to the start-up of too many units or excessive energy purchase thereby supplying an unnecessary level of reserve. On the contrary underestimation may result in a risky operation with insufficient preparation of spinning reserve causing the system to operate in a vulnerable region to the disturbance.
2012,1,31,Following authors on Google Scholar,https://robjhyndman.com/hyndsight/googlescholar2/,"A great new feature has been added to Google Scholar Citations. For those authors who have set up a citations page it is now possible to get email alerts for any new articles they publish or for any new citations of their articles. So you can track citations to your own work this way and stay up-to-date with key authors in your field.
Setting up a Google Citations page is super-easy and was already worth doing."
2012,1,29,Forecasts of COPD mortality in Australia: 2006-2025,https://robjhyndman.com/publications/copdaustralia/,"Background: Chronic Obstructive Pulmonary Disease (COPD) is currently the fifth leading cause of death in Australia and there are marked differences in mortality trends between men and women. In this study we have sought to model and forecast age related changes in COPD mortality over time for men and women separately over the period 2006–2025.
Methods: Annual COPD death rates in Australia from 1922 to 2005 for age groups (50–54 55–59 60–64 65–69 70–74 75–79 80–84 85+) were used."
2012,1,20,Refereeing a journal article,https://robjhyndman.com/hyndsight/refereeing2/,"I&rsquo;ve written briefly on this before. For an excellent and more detailed discussion of what is involved there is a series of excellent posts on Pat Thomson&rsquo;s blog:
  Refereeing a journal article part 1: reading
  Refereeing a journal article part 2: making a recommendation
  Refereeing a journal article part 3: writing the feedback
  If every reviewer followed her advice my life as an editor would be much easier and the quality of research would be improved."
2012,1,18,Internet surveys,https://robjhyndman.com/hyndsight/surveys/,"I received the following email today:
 I am preparing a thesis &hellip; I need to conduct the widest possible poll and it occurred to me that perhaps you could guide me toward an internet-based way in which this can be done easily. I have a ten-question questionnaire prepared that I wish to have an random sample of the population respond to. I have no budget for this so I hope you can suggest a way in which a good number of responses can be harvested using blogs or sites you may be aware of."
2011,12,31,Forecasting time series with complex seasonal patterns using exponential smoothing,https://robjhyndman.com/publications/complex-seasonality/,A new innovations state space modeling framework incorporating Box-Cox transformations Fourier series with time varying coefficients and ARMA error correction is introduced for forecasting complex seasonal time series that cannot be handled using existing forecasting models. Such complex time series include time series with multiple seasonal periods high frequency seasonality non-integer seasonality and dual-calendar effects. Our new modelling framework provides an alternative to existing exponential smoothing models and is shown to have many advantages.
2011,12,23,Are we getting better at forecasting?,https://robjhyndman.com/hyndsight/bostonglobe/,"I was interviewed recently for the Boston Globe. The interview was by email and I thought it might be useful to post here.Here are the questions from the journalist.
 Are we better at predicting future events than we used to be? Or are there obstacles inherent to the endeavor that prevent us from ever really being able to do it accurately? If we are better then what allowed it? Computational power?"
2011,12,19,Organizing travel,https://robjhyndman.com/hyndsight/travel/,Whether travelling to a seminar or conference or just having a holiday using a travel organizer can make the process simpler and easier. A good travel organizer keeps all your travel details (flights hotels car rentals meetings weather forecasts etc.) organized and synced to whatever devices you use (two computers an iPad and an iPhone in my case).I am aware of four travel organizers that do this: TripIt TripCase WorldMate and Wipolo.
2011,12,16,Forecasting time series using R,https://robjhyndman.com/hyndsight/melburntalk/,"I gave this talk on Forecasting time series using R for the Melbourne Users of R Network (MelbURN) on Thursday 27 October 2011.
Slides Examples
Abstract I look at the various facilities for time series forecasting available in R concentrating on the forecast package. This package implements several automatic methods for forecasting time series including forecasts from ARIMA models ARFIMA models and exponential smoothing models. I also look more generally at how to go about forecasting non-seasonal data seasonal data seasonal data with high frequency and seasonal data with multiple frequencies."
2011,12,14,Cyclic and seasonal time series,https://robjhyndman.com/hyndsight/cyclicts/,These terms get confused all the time (e.g. this question on CrossValidated.com) and so I thought it might be helpful to try to summarize the distinction and some of the associated models.Definitions A seasonal pattern exists when a series is influenced by seasonal factors (e.g. the quarter of the year the month or day of the week). Seasonality is always of a fixed and known period. Hence seasonal time series are sometimes called periodic time series.
2011,11,30,The art of R programming,https://robjhyndman.com/hyndsight/matloff/,This is a gem of a book. It will become the book I give PhD students when they are learning how to write good R code. That is if I ever see it again. I had hoped to write a review of it but I haven&rsquo;t seen it since it arrived in the mail a couple of weeks ago because a research student or research assistant has always had it on loan.
2011,11,28,Kaggle on TV,https://robjhyndman.com/hyndsight/kaggle-on-tv/,"It is good to see forecasting algorithms getting some mainstream exposure on ABC Catalyst.
Update: See also this great talk by Jeremy Howard a data scientist from Melbourne and now part of Kaggle."
2011,11,26,Researcher portals,https://robjhyndman.com/hyndsight/portals/,A researcher portal is a website that attempts to list all the publications of a given researcher. Some portals also allow sharing papers interacting with other researchers calculating citation statistics etc. Every researcher wants their work read and cited so these websites can be useful tools for getting your work noticed. They can also function as a de facto home page if you don&rsquo;t already have a personal website. Conversely they can be a good way to find new work by researchers in your field.
2011,11,11,What you wish you knew before you started a PhD,https://robjhyndman.com/hyndsight/wishlist/,I asked my research group recently what they wished they had learned before they started work on a PhD. Here are some of the responses.  More mathematics. Particular topics they named included real analysis functional analysis measure theory algebra linear algebra. That would have been my response also. I still wish I knew more mathematics than I do. I did quite a lot of mathematics as an undergraduate but every year I need to learn some more.
2011,10,27,Forecasting time series using R,https://robjhyndman.com/seminars/melbournerug/,"Melbourne R Users' Group Thursday October 27 2011 6:00 PM Deloitte Level 11 (Culture Room) 550 Bourke Street Melbourne
I will look at the various facilities for time series forecasting available in R concentrating on the forecast package. This package implements several automatic methods for forecasting time series including forecasts from ARIMA models ARFIMA models and exponential smoothing models. I will also look more generally at how to go about forecasting non-seasonal data seasonal data seasonal data with high frequency and seasonal data with multiple frequencies."
2011,10,3,Forecasting electricity demand distributions using a semiparametric additive model,https://robjhyndman.com/seminars/electricity-forecasting/,"Talk given at
 University of Melbourne 11 October 2011. University of Adelaide 16 March 2012 Monash University 16 May 2012 La Trobe University 24 May 2012 EDF Paris. 4 September 2012 University of New South Wales 1 November 2012  Abstract: Electricity demand forecasting plays an important role in short-term load allocation and long-term planning for future generation facilities and transmission augmentation. Planners must adopt a probabilistic view of potential peak demand levels therefore density forecasts (providing estimates of the full probability distributions of the possible future values of the demand) are more helpful than point forecasts and are necessary for utilities to evaluate and hedge the financial risk accrued by demand variability and forecasting uncertainty."
2011,9,25,Help for forecasting practitioners,https://robjhyndman.com/hyndsight/forecasting-help/,I often get email from forecasters wanting assistance. As much as I&rsquo;d like to provide a free forecasting advice service to the world that&rsquo;s not what I&rsquo;m paid to do and I choose to spend my unpaid time on other things. However there are some very helpful resources available for forecasting practitioners.First every practicing forecaster should be reading Foresight. It is far and away the best journal or magazine for forecast practitioners.
2011,8,30,The scourge of the academic publishers,https://robjhyndman.com/hyndsight/scourge/,Academic publishing is built on an old model where publishers were needed to print and distribute journals to libraries. Under this system it makes sense that the journals are distributed by publishing companies who charge fees for their work. On the other hand the academics who write for the journals the peer reviewers and (almost all) editors have always contributed their time and expertise without cost. Essentially they are being paid by universities and other research organizations to do this work.
2011,8,26,Time series cross-validation: an R example,https://robjhyndman.com/hyndsight/tscvexample/,I was recently asked how to implement time series cross-validation in R. Time series people would normally call this &ldquo;forecast evaluation with a rolling origin&rdquo; or something similar but it is the natural and obvious analogue to leave-one-out cross-validation for cross-sectional data so I prefer to call it &ldquo;time series cross-validation&rdquo;.Here is some example code applying time series CV and comparing 1-step 2-step &hellip; 12-step forecasts using the Mean Absolute Error (MAE).
2011,8,26,Major changes to the forecast package,https://robjhyndman.com/hyndsight/forecast3/,The forecast package for R has undergone a major upgrade and I&rsquo;ve given it version number 3 as a result. Some of these changes were suggestions from the forecasting workshop I ran in Switzerland a couple of months ago and some have been on the drawing board for a long time. Here are the main changes in version 3 plus a few earlier additions that I thought deserved a mention.
2011,8,24,Crowd sourcing forecasts,https://robjhyndman.com/hyndsight/crowd-sourcing-forecasts/,"Forecasting Ace is looking for participants to develop improved methods for predicting future events and outcomes. Their goal is to develop methods for aggregating many individual judgments in a manner that yields more accurate predictions than any one person or small group alone could provide. Potential applications of the system include forecasting economic conditions political changes technological development and medical breakthroughs.
They are currently seeking to recruit individuals who have interest (and/or expertise) in any of a wide range of current affairs including but not limited to politics economics technology military affairs social trends and science and technology."
2011,8,16,Learn Machine Learning at Stanford for free,https://robjhyndman.com/hyndsight/stanford-ml/,Andrew Ng&rsquo;s machine learning course at Stanford is being offered free to anyone online in the (northern) fall of 2011. I&rsquo;ve seen some of the notes from this course and it looks to be an excellent broad introduction to machine learning and data mining. For example support vector machines neural networks kernels clustering dimension reduction etc.Statisticians should know something about this area (just as computer scientists working in machine learning should know some statistical modelling) and this would be a great way to learn it.
2011,8,12,Beware of junk journals and publishers,https://robjhyndman.com/hyndsight/junkjournals/,"Today I received the following email:
 Dear Professor
 Antarctica Journal of Mathematics Archimedes Journal of Mathematics Bessel Journal of Mathematics Cayley Journal of Mathematics Diophantus Journal of Mathematics  We are charging only $3 per page which is very cheap when compared to some money oriented journals.
Further we request you to withdraw your paper from other journals keeping in view of high page charges.
 You can submit your research papers to our online journals."
2011,7,25,Recommended survey papers,https://robjhyndman.com/hyndsight/surveypapers/,Survey articles are particularly helpful in getting a foothold in a new research area or in looking for important papers that you may have overlooked. Whatever area of research you are in look out for survey papers and journals dedicated to publishing survey papers.The journal Statistics Surveys is relatively new (volume 1 in 2007) and provides very helpful survey articles of areas of statistical research. It is sponsored by the American Statistical Association the Bernoulli Society the Institute of Mathematical Statistics and by the Statistical Society of Canada.
2011,7,21,Social networking for researchers,https://robjhyndman.com/hyndsight/socialnetworks/,It would be nice to have a place to share ideas links comments in a very informal way with others involved in research in statistical methodology and data science. CrossValidated.com is great for specific questions but is not suitable for commenting on papers or sharing ideas and links.Although I have a facebook account this seems the wrong place to carry out a research discussion because my family and friends are generally not interested and it is messy to restrict posts to groups of friends.
2011,7,16,Investigating the influence of synoptic-scale circulation on air quality using self-organizing maps and generalized additive modelling,https://robjhyndman.com/publications/synoptic-gams/,The influence of synoptic-scale circulations on air quality is an area of increasing interest to air quality management in regards to future climate change. This study presents an analysis where the dominant synoptic &lsquo;types&rsquo; over the region of Melbourne Australia are determined and linked to regional air quality. First a self-organising map (SOM) is used to generate a time series of synoptic charts that classify the annual daily circulation affecting Melbourne into 20 different synoptic types.
2011,7,14,Point and interval forecasts of mortality rates and life expectancy: a comparison of ten principal component methods,https://robjhyndman.com/publications/mortality-forecast-comparison/,Abstract: Using the age- and sex-specific data of 14 developed countries we compare the point and interval forecast accuracy and bias of ten principal component methods for forecasting mortality rates and life expectancy. The ten methods are variants and extensions of the Lee-Carter method. Based on one-step forecast errors the weighted Hyndman-Ullah method provides the most accurate point forecasts of mortality rates and the Lee-Miller method is the least biased. For the accuracy and bias of life expectancy the weighted Hyndman-Ullah method performs the best for female mortality and the Lee-Miller method for male mortality.
2011,7,14,Method for optimizing coating properties based on an evolutionary algorithm approach,https://robjhyndman.com/publications/emma/,In industry as well as many areas of scientific research data collected often contain a number of responses of interest for a chosen set of exploratory variables. Optimization of such multivariable multiresponse systems is a challenge well suited to genetic algorithms as global optimization tools. One such example is the optimization of coating surfaces with the required absolute and relative sensitivity for detecting analytes using devices such as sensor arrays. High-throughput synthesis and screening methods can be used to accelerate materials discovery and optimization; however an important practical consideration for successful optimization of materials for arrays and other applications is the ability to generate adequate information from a minimum number of experiments.
2011,6,22,Giving a useR! talk,https://robjhyndman.com/publications/usertalk/,Giving a useR! talk at the international R user conference is a balancing act in which you have to try to impart some new ideas provide sufficient background and keep the audience interested all in a very short period of time.
2011,6,15,Evaluating extreme quantile forecasts,https://robjhyndman.com/seminars/maep/,Talk to be given at the International Symposium on Forecasting Prague 26–29 June 2011.
2011,5,29,Comparing HoltWinters() and ets(),https://robjhyndman.com/hyndsight/estimation2/,"I received this email today:
 I have a question about the ets() function in R which I am trying to use for Holt-Winters exponential smoothing. My problem is that I am getting very different estimates of the alpha beta and gamma parameters using ets() compared to HoltWinters() and I can&rsquo;t figure out why.
 This is a common question so I thought the answer might be of sufficient interest to post here."
2011,4,29,Tourism forecasting: an introduction,https://robjhyndman.com/publications/tourism-forecasting-an-introduction/,Introduction to the special issue on Tourism Forecasting.
2011,3,31,The price elasticity of electricity demand in South Australia,https://robjhyndman.com/publications/electricity-price-elasticity/,In this paper the price elasticity of electricity demand representing the sensitivity of customer demand to the price of electricity has been estimated for South Australia. We first undertake a review of the scholarly literature regarding electricity price elasticity for different regions and systems. Then we perform an empirical evaluation of the historic South Australian price elasticity focussing on the relationship between price and demand quantiles at each half-hour of the day.
2011,3,29,I'm switching to TeXstudio,https://robjhyndman.com/hyndsight/texstudio/,I&rsquo;ve happily used WinEdt for all my LaTeX editing for about 15 years and I&rsquo;ve encouraged my whole research team to use it. But I&rsquo;m tired of problems with WinEdt that take up my time.I regularly have requests for help from one of my research team because something in WinEdt is not working properly &mdash; such as pdf synchronization problems or it is using an old version of MikTeX that no longer updates or that it has switched to using another pdf viewer without warning.
2011,3,25,Looking after your supervisor,https://robjhyndman.com/hyndsight/looking-after-your-supervisor/,Some good advice here: The care and maintenance of your adviser.
2011,3,15,Optimal combination forecasts for hierarchical time series,https://robjhyndman.com/publications/hierarchical/,"In many applications there are multiple time series that are hierarchically organized and can be aggregated at several different levels in groups based on products geography or some other features. We call these &ldquo;hierarchical time series''. They are commonly forecast using either a &ldquo;bottom-up'' or a &ldquo;top-down'' method.
In this paper we propose a new approach to hierarchical forecasting which provides optimal forecasts that are better than forecasts produced by either a top-down or a bottom-up approach."
2011,3,14,Ten rules for data analysis,https://robjhyndman.com/hyndsight/ten-rules-for-data-analysis/,Peter Kennedy was an associate editor of the International Journal of Forecasting and a superb applied econometrician. He died unexpectedly in August 2010. He was best known for his excellent book A Guide to Econometrics as well as his &ldquo;Ten Commandments of Applied Econometrics&rdquo;. He provided a variation on his ten commandments in advice to his students in the form of the following ten rules:  Use common sense (and economic theory)
2011,3,14,Statistical tests for variable selection,https://robjhyndman.com/hyndsight/tests2/,"I received an email today with the following comment:
 I’m using ARIMA with Intervention detection and was planning to use your package to identify my initial ARIMA model for later iteration however I found that sometimes the auto.arima function returns a model where AR/MA coefficients are not significant. So my question is: Is there a way to filter the search for ARIMA models that only have significant coefficients. I can remove the non-significant coefficients but I think it would be better to search for those models that only have significant coefficients."
2011,3,10,Improved interval estimation of long run response from a dynamic linear model: a highest density region approach,https://robjhyndman.com/publications/dlm-hdr/,This paper proposes a new method of interval estimation for the long run response (or elasticity) parameter from a general linear dynamic model. We employ the bias-corrected bootstrap in which small sample biases associated with the parameter estimators are adjusted in two stages of the bootstrap. As a means of bias-correction we use alternative analytic and bootstrap methods. To take atypical properties of the long run elasticity estimator into account the highest density region (HDR) method is adopted for the construction of confidence intervals.
2011,3,1,RStudio: just what I've been looking for,https://robjhyndman.com/hyndsight/rstudio/,"For many years I used RWinEdt as my text editor for R code but when WinEdt 6.0 came out RWinEdt stopped working. So I&rsquo;ve been looking for something to replace it. I&rsquo;ve tried Tinn-R NppToR Eclipse with StatET and a couple of other editors but nothing was quite right.
Then yesterday out of the blue RStudio was announced and it looks fantastic! A screenshot is given below.

The window contains a smart editor with code completion and tabbing console workspace with viewable objects plotting panel with history etc."
2011,2,17,Nonparametric time series forecasting with dynamic updating,https://robjhyndman.com/publications/dynamic-updating/,Abstract We present a nonparametric method to forecast a seasonal univariate time series and propose four dynamic updating methods to improve point forecast accuracy. Our methods consider a seasonal univariate time series as a functional time series. We propose first to reduce the dimensionality by applying functional principal component analysis to the historical observations and then to use univariate time series forecasting and functional principal component regression techniques. When data in the most recent year are partially observed we improve point forecast accuracy using dynamic updating methods.
2011,2,9,The value of feedback in forecasting competitions,https://robjhyndman.com/publications/kaggle/,In this paper we challenge the traditional design used for forecasting competitions. We implement an online competition with a public leaderboard that provides instant feedback to competitors who are allowed to revise and resubmit forecasts. The results show that feedback significantly improves forecasting accuracy.
2011,1,14,"Lies, damn lies and statistics",https://robjhyndman.com/hyndsight/lewandowsky/,There&rsquo;s a nice article with this title by Stephan Lewandowsky on the ABC website today exploring the difference between anecdotes and data and the dangers of cherry-picking evidence.
2011,1,11,Six places left for the forecasting workshop,https://robjhyndman.com/hyndsight/swissworkshop2011a/,There are six places left for the forecasting workshop I am giving in Switzerland in June. If you were thinking of going book in fast!
2011,1,11,Authorship ethics,https://robjhyndman.com/hyndsight/authorship/,"With the constant pressure on academics to publish research papers there is a temptation for research groups to include &ldquo;coauthors&rdquo; who have not really made any contribution to the manuscript. This seems more prevalent in some fields (e.g. the health sciences) than others.
Occasionally I am asked to add an author to a paper that has already been accepted for publication in the International Journal of Forecasting. I am very reluctant to do this as it is hard to imagine how someone could be left off a paper while it goes through several revisions only to be remembered after the paper is accepted."
2011,1,9,Becoming a referee,https://robjhyndman.com/hyndsight/refereeing/,"I regularly get emails from people wanting to be referees for the International Journal of Forecasting usually with an accompanying CV. This is not how the process works.
Referees are almost always selected because they have previously written papers on a similar topic to the manuscript under review. If you want to be a referee then write good papers and get them published in scholarly journals. Very quickly you will be invited to referee papers in the same journals."
2011,1,9,Tips for academic talks,https://robjhyndman.com/hyndsight/talks/,"There is a nice post on Matt Might&rsquo;s blog entitled &ldquo;10 tips on how to give an academic talk&rdquo;. Check it out.
He recommends the following two books by Joey Asher.
See also my article on &ldquo;Giving an academic talk&rdquo;."
2011,1,1,The tourism forecasting competition,https://robjhyndman.com/publications/the-tourism-forecasting-competition/,We evaluate the performance of various methods for forecasting tourism demand. The data used include 366 monthly series 427 quarterly series and 518 yearly series all supplied to us by tourism bodies or by academics from previous tourism forecasting studies. The forecasting methods implemented in the competition are univariate and multivariate time series approaches and econometric models. This forecasting competition differs from previous competitions in several ways: (i) we concentrate only on tourism demand data; (ii) we include approaches with explanatory variables; (iii) we evaluate the forecast interval coverage as well as point forecast accuracy; (iv) we observe the effect of temporal aggregation on forecasting accuracy; and (v) we consider the mean absolute scaled error as an alternative forecasting accuracy measure.
2011,1,1,Quantifying the influence of local meteorology on air quality using generalized additive modelling,https://robjhyndman.com/publications/local-gams/,Quantifying the observed relationships between local meteorology and air pollution provides air quality managers with a knowledge base that may prove useful in understanding how climate change may potentially impact air quality. This paper presents the estimated response of ozone (O3) particulate matter ≤ 10 μm (PM10) and nitrogen dioxide (NO2) to individual local meteorological variables in Melbourne Australia over the period of 1999 to 2006. The relationships have been quantified after controlling for long-term trends seasonality weekly emissions spatial variation and temporal persistence using the framework of a generalized additive modelling (GAM).
2010,12,23,In praise of Dropbox,https://robjhyndman.com/hyndsight/dropbox/,"Every couple of years a new technology has a big impact on how I work. Gmail was one. My iPhone was another. And I rank Dropbox in the same category.
I get three huge benefits in using Dropbox:
  All my files are backed up online. The house can burn down and I know I can still get my files. Also if I&rsquo;m away from my desktop or laptop I can still access my files on my iPhone."
2010,12,22,CrossValidated Journal Club,https://robjhyndman.com/hyndsight/cvjournalclub/,"Journal Clubs are a great way to learn new research ideas and to keep up with the literature. The idea is that a group of people get together every week or so to discuss a paper of joint interest. This can happen within your own research group or department or virtually online.
There is now a virtual journal club operating in conjunction with CrossValidated.com. The first paper discussed was on text data mining."
2010,12,10,Hamming on research,https://robjhyndman.com/hyndsight/hamming/,Richard Hamming was an excellent mathematician who worked at the interface of mathematics and computer science. In 1986 he gave a wonderful talk entitled You and Your Research. Derek Smith on the AMS Graduate Student blog reminded me of it today. If you haven&rsquo;t read it previously stop work immediately and read it now.
2010,12,6,"Forecasting workshop: Switzerland, June 2011",https://robjhyndman.com/hyndsight/swissworkshop2011/,I will be running a workshop on Statistical Forecasting: Principles and Practice in Switzerland 20-22 June 2011. Check out the venue: Waldhotel Doldenhorn Kandersteg! So if you fancy a trip to the beautiful Swiss Alps next June read on&hellip;Outline Forecasting is required in many situations: deciding whether to build another power generation plant in the next five years requires forecasts of future demand; scheduling staff in a call centre next week requires forecasts of call volume; stocking an inventory requires forecasts of stock requirements.
2010,11,30,Data visualization videos,https://robjhyndman.com/hyndsight/data-visualization-videos/,"Probably everyone has seen Hans Rosling&rsquo;s famous TED talk by now. If not here it is:
  I recently came across a couple of other exceptional talks on data visualization:
Hans Rosling again: &ldquo;Let my dataset change your mindset&rdquo;. If only all statistics lecturers were this dynamic!
  David McCandless: &ldquo;The beauty of data visualization&rdquo;. Not so exciting as Hans but some great examples.
  And here&rsquo;s an hour-length documentary hosted by Hans Rosling called &ldquo;The Joy of Stats&rdquo;."
2010,11,30,Initializing the Holt-Winters method,https://robjhyndman.com/hyndsight/hw-initialization/,"The Holt-Winters method is a popular and effective approach to forecasting seasonal time series. But different implementations will give different forecasts depending on how the method is initialized and how the smoothing parameters are selected. In this post I will discuss various initialization methods.
Suppose the time series is denoted by $y_1\dotsy_n$ and the seasonal period is $m$ (e.g. $m=12$ for monthly data). Let $\hat{y}_{t+h|t}$ be the $h$-step forecast made using data to time $t$."
2010,11,27,A LaTeX template for a CV,https://robjhyndman.com/hyndsight/cv/,"Every researcher needs a Curriculum Vitae (Latin for &ldquo;course of life&rdquo;) or CV. You will need it for job applications for annual performance appraisal and just for keeping track of your publications. A CV typically contains lists of achievements including qualifications publications presentations awards plus teaching experience.
I&rsquo;ve created a LaTeX style for a CV to make it easy to produce something that looks good and is easy to maintain. You will need an up-to-date implementation of LaTeX because I&rsquo;m using the wonderful biblatex package (more on that in another post) which has only just become available as part of MikTeX and TeXLive."
2010,11,24,Tourism forecasting competition ends,https://robjhyndman.com/hyndsight/tfc2/,"And the winners are &hellip; Jeremy Howard and Lee C Baker. (See my earlier post for information about the competition.)
Jeremy describes his approach to seasonal time series in a blog post on Kaggle.com. Lee described his approach to annual time series in an earlier post.
A few lessons that come out of this:
  For data from a single industry using a global trend (i.e. estimated across all series) can be useful."
2010,11,20,Free open-source forecasting using R,https://robjhyndman.com/publications/r-foresight/,Summary  R is a free open source statistical computing environment which can be used for forecasting. It is available at www.r-project.org. Advantages of R include its (zero) price the large number of user-contributed packages its production-quality graphics and the possibility of extending it by linking fast compiled C/C++/Fortran code. Disadvantages of R include a steep learning curve and a certain slowness when dealing with truly massive amounts of data. R can be used to advantage by forecasters with a technical background who are comfortable with the command line and who may want to use R for additional Business Intelligence analyses and graphics beyond forecasting.
2010,11,15,The vector innovations structural time series framework: a simple approach to multivariate forecasting,https://robjhyndman.com/publications/vists/,The vector innovations structural time series framework is proposed as a way of modelling a set of related time series. Like all multivariate approaches the aim is to exploit potential inter-series dependencies to improve the fit and forecasts. The model is based around an unobserved vector of components representing features such as the level and slope of each time series. Equations that describe the evolution of these components through time are used to represent the inter-temporal dependencies.
2010,11,10,"Forecast estimation, evaluation and transformation",https://robjhyndman.com/hyndsight/forecastmse/,"I&rsquo;ve had a few emails lately about forecast evaluation and estimation criteria. Here is one I received today along with some comments.
 I have a rather simple question regarding the use of MSE as opposed to MAD and MAPE. If the parameters of a time series model are estimated by minimizing MSE why do we evaluate the model using some other metric e.g. MAD and MAPE. I could see that MAPE is not scale dependent."
2010,11,4,CrossValidated launched!,https://robjhyndman.com/hyndsight/crossvalidated/,"The CrossValidated Q&amp;A site is now out of beta and the new design and site name is live.

New design The new design looks great thanks to Jin Yang our designer-in-residence. Note the normal density icon for accepted answers and the site icon depicting a 5-fold cross-validation (light green for the test set and dark green for the training set). There is a faint background graphic in the header and footer from a program that tracks and plots a person&rsquo;s mouse movement."
2010,10,26,Different results from different software,https://robjhyndman.com/hyndsight/estimation/,"I&rsquo;ve had a few questions on this topic lately. Here is an email received today:
 I use Eviews to estimate time series but I have been checking out R recently and your Forecast package.
I cannot understand why 2 similar equations in Eviews and R are giving different estimated output. Your insights will be invaluable for my work.
The equations are:
 R: Arima(log(fee) order=c(110) seasonal=list(order=c(100) period=4) include.drift=TRUE) Eviews: dlog(fee) c ar(1) sar(1) Even with include."
2010,10,23,Getting started with XeLaTeX,https://robjhyndman.com/hyndsight/xelatex/,"By now most LaTeX users have probably heard of XeLaTeX if only because it is an option in the latest versions of the standard LaTeX editors such as TeXnicCenter WinEdt and TeXWorks. But most LaTeXers have probably not yet become XeLaTeXers. Why should you?
XeLaTeX is essentially a replacement for pdfLaTeX. It was primarily developed to enable better font handling especially non-Roman scripts. If you want to write in Telugu then XeLaTeX is going to make your life much easier."
2010,10,22,How to avoid annoying a referee,https://robjhyndman.com/hyndsight/how-to-avoid-annoying-a-referee/,"It&rsquo;s not a good idea to annoy the referees of your paper. They make recommendations to the editor about your work and it is best to keep them happy. There is an interesting discussion on stats.stackexchange.com on this subject. This inspired my own list below.
 Explain what you&rsquo;ve done clearly avoiding unnecessary jargon. Don&rsquo;t claim your paper contributes more than it actually does. (I refereed a paper this week where the author claimed to have invented principal component analysis!"
2010,10,19,Happy World Statistics Day!,https://robjhyndman.com/hyndsight/wsd/,"The United Nations has declared today &ldquo;World Statistics Day&rdquo;. I&rsquo;ve no idea what that means or why we need a WSD. Perhaps it is because the date is 20.10.2010 (except in North America where it is 10.20.2010). But then what happens from 2013 to 2099? And do we just forget the whole idea after 3112?
In any case if we are going to have a WSD let&rsquo;s use it to do something useful."
2010,10,17,Do something else,https://robjhyndman.com/hyndsight/do-something-else/,I&rsquo;ve written about taking a break from research before. Along the same lines there is some good advice on &ldquo;The Importance of Making Time for &ldquo;Real World&rdquo; Activities in Grad School&rdquo; over at the excellent AMS Graduate Student Blog.
2010,10,13,Animated plots in R and LaTeX,https://robjhyndman.com/hyndsight/animations/,"I like to use animated plots in my talks on functional time series partly because it is the only way to really see what is going on with changes in the shapes of curves over time and also because audiences love them! Here is how it is done.
For LaTeX you need to create every frame as a separate graphics file. Here is an example. First the R code:"
2010,10,13,Always listen to reviewers,https://robjhyndman.com/hyndsight/always-listen-to-reviewers/,This week I was asked to review a paper that I had seen before. It had been submitted to a journal a few months ago and I had written a detailed report describing some problems with the paper and noting a large number of typos that needed fixing. That journal had rejected the paper the authors had submitted it to a second journal and the paper ended up on my desk again for review.
2010,10,7,Joining an editorial board,https://robjhyndman.com/hyndsight/editorialboards/,"Being on the editorial board of a journal is a lot of work. I&rsquo;m currently Editor-in-Chief of the International Journal of Forecasting and previously I&rsquo;ve been Theory &amp; Methods Editor of the Australian &amp; New Zealand Journal of Statistics. Although it is time-consuming and often goes un-noticed there are some important rewards that make it worthwhile in my opinion.
  You are forced to read carefully a lot of papers in your area of interest."
2010,10,5,Why R is better than Excel for teaching statistics,https://robjhyndman.com/hyndsight/rvsexcel/,"This was the topic of a recent conversation on the Australian and New Zealand R mailing list. Here is an edited list of some of the comments made.
  R is free.
  R is well-documented.
  R runs (really well) on *nix as well as Windows and Mac OS.
  R is open-source. Trust in the R software is evident by its support among distinguished statisticians."
2010,10,4,The ARIMAX model muddle,https://robjhyndman.com/hyndsight/arimax/,There is often confusion about how to include covariates in ARIMA models and the presentation of the subject in various textbooks and in R help files has not helped the confusion. So I thought I&rsquo;d give my take on the issue. To keep it simple I will only describe non-seasonal ARIMA models although the ideas are easily extended to include seasonal terms. I will include only one covariate in the models although it is easy to extend the results to multiple covariates.
2010,10,4,Why every statistician should know about cross-validation,https://robjhyndman.com/hyndsight/crossvalidation/,"Surprisingly many statisticians see cross-validation as something data miners do but not a core statistical technique. I thought it might be helpful to summarize the role of cross-validation in statistics especially as it is proposed that the Q&amp;A site at stats.stackexchange.com should be renamed CrossValidated.com.
Cross-validation is primarily a way of measuring the predictive performance of a statistical model. Every statistician knows that the model fit statistics are not a good guide to how well a model will predict: high $R^2$ does not necessarily mean a good model."
2010,9,30,That syncing feeling,https://robjhyndman.com/hyndsight/syncing/,"Like many people I use more than one computer and I like to have all my files bookmarks and other settings synchronized across my computers. Fortunately that is getting easier as more tools are made available for keeping computers synchronized. So I thought it might be timely to review how to keep computers &ldquo;synced&rdquo;.
 Files: By far the best service is Dropbox. All files within the &ldquo;My Dropbox&rdquo; directory are backed up online and synchronized with any other computer associated with your account."
2010,9,29,Forecasting with long seasonal periods,https://robjhyndman.com/hyndsight/longseasonality/,"I am often asked how to fit an ARIMA or ETS model with data having a long seasonal period such as 365 for daily data or 48 for half-hourly data. Generally seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data.
The ets() function in the forecast package restricts seasonality to be a maximum period of 24 to allow hourly data but not data with a larger seasonal frequency."
2010,9,20,Tourism forecasting competition results: part one,https://robjhyndman.com/hyndsight/tfc1/,"The first stage of the tourism forecasting competition on kaggle has finished. This stage involved forecasting 518 annual time series. Twenty one teams beat our Theta method benchmark which is a great result and well beyond our expectations. Congratulations to Lee Baker for winning stage one.
I am yet to learn what methods the top teams were using but we hope to write up a paper for the IJF describing the results."
2010,9,16,Your name is your brand,https://robjhyndman.com/hyndsight/names/,"As a researcher you want to become known as an expert in your field. You need people to recognize your name and associate it with your research. Consequently it is important to be consistent in the name you use on publications.
For example I could write under &ldquo;R Hyndman&rdquo; &ldquo;R J Hyndman&rdquo; &ldquo;Rob Hyndman&rdquo; &ldquo;Rob J Hyndman&rdquo; etc. I&rsquo;ve chosen the last of these and I try to use it on all publications."
2010,9,7,Demographic forecasting using functional data analysis,https://robjhyndman.com/seminars/demographic-forecasting/,"University of Wollongong 8 September 2010. Statistical Society of Australia Victorian Branch 28 September 2010. Updated version. September 2012.  Abstract:
Functional time series are curves that are observed sequentially in time. In demography such data arise as the curves formed by annual death rates as a function of age or annual fertility rates as a function of age. I will discuss methods for describing modelling and forecasting such functional time series data."
2010,9,2,How to fail a PhD,https://robjhyndman.com/hyndsight/phdfail/,"I read an interesting post today by Matt Might on &ldquo;10 reasons PhD students fail&rdquo; and I thought it might be helpful to reflect on some of the barriers to PhD completion that I&rsquo;ve seen. Matt&rsquo;s ideas are not all relevant to Australian PhDs so I have come up with my own list below. Here are the seven steps to failure.
1. Wait for your supervisor to tell you what to do A good supervisor will not tell you what to do."
2010,8,31,Econometrics and R,https://robjhyndman.com/hyndsight/econometrics-and-r/,"Econometricians seem to be rather slow to adopt new methods and new technology (compared to other areas of statistics) but slowly the use of R is spreading. I&rsquo;m now receiving requests for references showing how to use R in econometrics and so I thought it might be helpful to post a few suggestions here.
[![](/files/Farnsworth.png)](http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)  A useful on-line and free resource is ""[Econometrics in R](http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)"" by Grant Farnsworth. It covers some common econometric methods including heteroskedasticity in regression probit and logit models tobit regression and quantile regression."
2010,8,26,Job advertisements,https://robjhyndman.com/hyndsight/job-advertisements/,"Employers often contact me asking how to find a good statistician econometrician or forecaster for their organization. Students also ask me how to go about finding a job when they finish their degree. This post is for both groups hopefully making it easier for them to pair up appropriately.
First the mainstream media outlets are not usually good places to advertise. It seems that few people read printed newspapers anymore."
2010,8,25,Benchmarks for forecasting,https://robjhyndman.com/hyndsight/benchmarks/,"Every week I reject papers submitted to the International Journal of Forecasting because they present new methods without ever attempting to demonstrate that the new methods are better than existing methods. It is a policy of the journal that every new method must be compared to standard benchmarks and existing methods before the paper will even be considered for publication.
For univariate time series methods it is not difficult. As a minimum comparisons should be made against a naive method and a standard method such as an ARIMA model."
2010,8,17,Phenological change detection while accounting for abrupt and gradual trends in satellite image time series,https://robjhyndman.com/publications/bfast2/,A challenge in phenology studies is understanding what constitutes significant phenological change amidst background variation (e.g. noise) and ecosystem disturbances (e.g. fires). The majority of phenological studies have focussed on extracting critical points in the seasonal growth cycle (e.g. Start-of-spring) without exploiting the full temporal detail. Moreover the high degree of phenological variability between years demonstrates the necessity of distinguishing long term phenological change from temporal variability. Here we evaluate the phenological change detection ability of a method for detecting Breaks For Additive Seasonal and Trend (BFAST).
2010,8,13,Transforming data with zeros,https://robjhyndman.com/hyndsight/transformations/,"I&rsquo;m currently working with a hydrologist and he raised a question that occurs quite frequently with real data &mdash; what do you do when the data look like they need a log transformation but there are zero values?
I asked the question on stats.stackexchange.com and received some useful suggestions. What follows is a summary based on these answers my own experience plus a few papers I discovered that deal with the topic."
2010,8,9,The tourism forecasting competition,https://robjhyndman.com/hyndsight/tourism-forecasting-competition/,Recently I wrote a paper entitled &ldquo;The tourism forecasting competition&rdquo; in which we (i.e. George Athanasopoulos Haiyan Song Doris Wu and I) compared various forecasting methods on a relatively large set of tourism-related time series. The paper has been accepted for publication in the International Journal of Forecasting. (When I submit a paper to the IJF it is always handled by another editor. In this case Mike Clements handled the paper and it went through several revisions before it was finally accepted.
2010,8,6,Twenty rules for good graphics,https://robjhyndman.com/hyndsight/graphics/,One of the things I repeatedly include in referee reports and in my responses to authors who have submitted papers to the International Journal of Forecasting are comments designed to include the quality of the graphics. Recently someone asked on stats.stackexchange.com aboutbest practices for producing plots. So I thought it might be helpful to collate some of the answers given there and add a few comments of my own taken from things I&rsquo;ve written for authors.
2010,8,3,Exploratory graphics for functional data,https://robjhyndman.com/publications/interface2010/,"We survey some graphical tools for visualizing large sets of functional data represented by smooth curves. These graphical tools include the phase-plane plot singular value decomposition plot rainbow plot functional variants of the bagplot and the highest density region boxplot. The latter two techniques utilize the first two robust principal component scores Tukey&rsquo;s halfspace location depth and highest density regions.
The computer code and datasets are collected in the rainbow package for R which is available at the Comprehensive R Archive Network (CRAN)."
2010,7,27,Statistical Analysis StackExchange site now available,https://robjhyndman.com/hyndsight/stats-stackexchange/,"The Q&amp;A site for statistical analysis data mining data visualization and everything else to do with data analysis has finally been launched. Please head over to
stats.StackExchange.com and start asking and answering questions.
Also spread the word to everyone else who may be interested &mdash; work colleagues students etc. The more people who use the site the better it will be. There are already 170 questions 513 answers and 387 users."
2010,7,21,Short-term load forecasting based on a semi-parametric additive model,https://robjhyndman.com/publications/aupec2010/,Short-term load forecasting is an essential instrument in power system planning operation and control. Many operating decisions are based on load forecasts such as dispatch scheduling of generating capacity reliability analysis and maintenance planning for the generators. Overestimation of electricity demand will cause a conservative operation which leads to the start-up of too many units or excessive energy purchase thereby supplying an unnecessary level of reserve. On the contrary underestimation may result in a risky operation with insufficient preparation of spinning reserve causing the system to operate in a vulnerable region to the disturbance.
2010,7,17,More StackExchange sites,https://robjhyndman.com/hyndsight/more-stackexchange-sites/,"The StackExchange site on Statistical Analysis is about to go into private beta testing. This is your last chance to commit if you want to be part of the private beta testing. Don&rsquo;t worry if you miss out &mdash; it will only be a week before it is then open to the public.
There is also a StackExchange site proposal for TeX LaTeX and friends. Presumably that means that most of the LaTeX questions on StackOverflow will then move to this new site."
2010,7,15,The falling standard of English in research,https://robjhyndman.com/hyndsight/english/,"It seems that most journals no longer do any serious copy-editing and the standard of English is falling. Today I was reading an article from the European Journal of Operational Research which is supposedly a good OR journal (current impact factor over 2). Take this for an example from the first page of this paper:
 If the learned patterns are unstable the learning tools would produce inconsistent concepts. To overcome this difficult situation we employed artificial neural networks (ANNs NNs) for helping the learning task."
2010,7,11,Academic citations in the popular press,https://robjhyndman.com/hyndsight/academic-citations-in-the-popular-press/,It is very unusual for a newspaper article to cite an academic paper unless it is in Nature Science or the Lancet. Mostly what we write is too technical and assumes too much background knowledge for it to be accessible to anyone but specialists. So I was pleasantly surprised to find a reference to the International Journal of Forecastingin a recent Wall Street Journalarticle. It is a citation of a 1996 article so in terms of scientific research it is a bit like quoting the Magna Carta but a citation nevertheless.
2010,7,6,Forecasting with Exponential Smoothing: the State Space Approach,https://robjhyndman.com/expsmooth/,"Rob J Hyndman Anne B Koehler J Keith Ord Ralph D Snyder (Springer 2008).
 .verticalhorizontal { display: table-cell; vertical-align: middle; }        
Exponential smoothing methods have been around since the 1950s and are still the most popular forecasting methods used in business and industry. However a modelling framework incorporating stochastic models likelihood calculation prediction intervals and procedures for model selection was not developed until relatively recently."
2010,7,6,Forecasting: methods and applications,https://robjhyndman.com/forecasting/,"This book was published in 1998 and for nearly 20 years I maintained an associated website at this address.
The data sets from the book can be found in the fma package for R.
The solutions to exercises can be downloaded here.
The book is now out-of-date. I recommend my new book entitled Forecasting: principles and practice."
2010,7,6,Musings,https://robjhyndman.com/musings/,"I used to be a Christian and for several years I wrote a blog at this address exploring ideas of faith and belief. I eventually lost my faith as explained in my book Unbelievable. When I reorganized this website there didn&rsquo;t seem much point spending time converting those old posts so they are no longer here.
Some of the posts were reworked into my book including the last one announcing my deconversion."
2010,6,11,Use fake data and real data,https://robjhyndman.com/hyndsight/use-fake-data-and-real-data/,"When developing new statistical methods it is very useful to test them on both fake data (i.e. simulations) and real data.
Testing on fake data is useful because then you know the &ldquo;true&rdquo; answer and can check the procedure under ideal conditions. If your method doesn&rsquo;t work when the data are designed for the task it is unlikely to work in real conditions. Fake data also enables you to test the robustness of your method when the conditions aren&rsquo;t perfect &ndash; for example try adding some nasty outliers and see if the method still works."
2010,6,9,Should you make your working papers public?,https://robjhyndman.com/hyndsight/working-papers/,"There seems to be two points of view on this with different practices in different disciplines.
  Some researchers do not make their work public until after it has been accepted for publication in a journal. Until that time drafts of papers are only circulated to close confidants and usually marked &ldquo;Do not distribute&rdquo;.
  Working papers are published on web sites and in web repositories (such as arXiv or RePEc) as soon as they are finished at about the same time they are submitted to a journal."
2010,6,9,Coherent functional forecasts of mortality rates and life expectancy,https://robjhyndman.com/seminars/isf2010/,Talk to be given at the International Symposium on Forecasting San Diego 20-23 June 2010.
2010,6,2,Update on a StackExchange site for statistical analysis,https://robjhyndman.com/hyndsight/stackexchange2/,"About six weeks ago I proposed that there should be a Stack Exchange site for questions on data analysis statistics data mining machine learning etc. I can finally report that there has been substantial progress on this.
The formal proposal is now at Area 51 where the scope of the new site is being developed and voted on in a democratic way. The site has been in a private beta state for a week or so but is now open for anyone to join in."
2010,5,26,Google scholar alerts,https://robjhyndman.com/hyndsight/google-scholar-alerts/,"A couple of weeks ago Google scholar added a facility to provide email alerts on new articles associated with specific search queries. First do the search then click the envelope at top left of screen. For example here is a search on &ldquo;exponential smoothing&rdquo; since 2000.

Note the envelope at the top marked New! Click it to get the following screen.

Those results show some of the flaws in Google Scholar &ndash; the dates are not always correct (the first paper listed above appeared in 2004) and there are unresolved duplicates."
2010,5,20,Online mathematical resources,https://robjhyndman.com/hyndsight/online-mathematical-resources/,DLMF For nearly 50 years a standard reference in mathematical work has been Abramowitz and Stegun&rsquo;s (1964) Handbook of Mathematical Functions with Formulas Graphs and Mathematical Tables. It has provided a marvellous collection of results and tables that have been indispensable for a generation of mathematicians. I&rsquo;ve used it to look up computationally efficient methods for calculating Bessel functions or gamma functions or to find one of those trigonometric identities I learned in high school and no longer remember.
2010,5,9,Scheduling meetings,https://robjhyndman.com/hyndsight/scheduling-meetings/,"I don&rsquo;t go to many meetings as I find they are largely a waste of time. In fact I have the following poster on my office wall to remind everyone who walks in not to ask me to attend a meeting!
But I&rsquo;m now a chief investigator of an NHMRC grant and I have to meet with other members of the team from time to time. We&rsquo;ve started using Doodle to schedule our meetings and it is so good I thought I should share it."
2010,5,6,Forecasting age-related changes in breast cancer mortality among white and black US women,https://robjhyndman.com/publications/brca-bwus/,The disparity in breast cancer mortality rates among white and black US women is widening with higher mortality rates among black women. We apply functional time series models on age-specific breast cancer mortality rates for each group of women and forecast their mortality curves using exponential smoothing state-space models with damping. The data were obtained from the Surveillance Epidemiology and End Results (SEER) program of the US. Mortality data were obtained from the National Centre for Health Statistics (NCHS) available on the SEER*Stat database.
2010,4,20,A StackExchange site for statistical analysis?,https://robjhyndman.com/hyndsight/stackexchange/,Regular readers of this site will know I&rsquo;m a fan of using Stack Overflow for questions about LaTeX R and other areas of programming. Now the people who produce Stack Overflow are planning on setting up several new sites for asking questions about other topics and are seeking proposals. I have proposed that there should be a site for questions on data analysis statistics data mining machine learning etc.
2010,3,25,Making a poster in beamer,https://robjhyndman.com/hyndsight/beamer-poster/,"This week I made my first poster. Although I&rsquo;ve been an academic for more than 20 years I&rsquo;ve never had to make a poster before. Some of my coauthors have made posters about our joint research and two of them have even won prizes (although I can&rsquo;t take any credit for them). But this week our department is displaying posters from all research staff about our recent work.
Here is my poster (click for pdf version):"
2010,3,21,My standard LaTeX preamble,https://robjhyndman.com/hyndsight/latex-preamble/,When I was a PhD student I found I needed a lot of LaTeX functionality that did not then exist. So I wrote my own package which has served me well for about 20 years. It is called HyTeX.sty (the name being a shameless take-off of LaTeX from Leslie Lamport as well as a homonym of High-Tech). The advantage of having my own package is that almost every file starts with
2010,3,10,Using the command line in Windows,https://robjhyndman.com/hyndsight/using-the-command-line-in-windows/,Jeromy Anglim is a local blogger who covers a lot of the same territory as this blog. His latest post on running command line programs in Windows is particularly helpful.
2010,2,28,"Rainbow plots, bagplots and boxplots for functional data",https://robjhyndman.com/publications/rainbow-fda/,"We propose new tools for visualizing large numbers of functional data in the form of smooth curves or surfaces. The proposed tools include functional versions of the bagplot and boxplot and make use of the first two robust principal component scores Tukey&rsquo;s data depth and highest density regions.
By-products of our graphical displays are outlier detection methods for functional data. We compare these new outlier detection methods with existing methods for detecting outliers in functional data and show that our methods are better able to identify the outliers."
2010,2,15,Top four LaTeX mistakes,https://robjhyndman.com/hyndsight/top-four-latex-mistakes/,There is a nice post today by John Cook on the top four LaTeX mistakes. I see these all the time in draft papers by my students and co-authors.
2010,2,10,Why referee?,https://robjhyndman.com/hyndsight/why-referee/,"There are several reasons why researchers should be willing to provide referee reports.
  You learn a lot. If the paper is in your area then writing a referee report forces you to read it very carefully and engage closely with the research of other people in your field. There&rsquo;s no better way of understand what is going on in your field.
  You get better known by the research leaders in your area."
2010,2,10,Writing a referee report,https://robjhyndman.com/hyndsight/referee-reports/,"As an editor I like to see referee reports comprising three sections:
  A general summary of the paper and the contribution it makes. You need to highlight here what is new and interesting about the paper as well as give a summary in a few sentences.
  The major problems that need addressing. This is probably the most important section of your report where you explain the main problems."
2010,2,7,Functionalization of microarray devices: process optimization using a multiobjective PSO and multiresponse MARS modeling,https://robjhyndman.com/publications/microarray-optimization/,An evolutionary approach for the optimization of microarray coatings produced via sol-gel chemistry is presented. The aim of the methodology is to face the challenging aspects of the problem: high dimensional variable space constraints on the independent variables multiple responses expensive or time-consuming experimental trials expected complexity of the functional relationships between independent and response variables. The proposed approach iteratively select a set of experiments by combining a multiobjective Particle Swarm Optimization (PSO) and a multiresponse Multivariate Adaptive Regression Spines (MARS) model.
2010,2,5,Using functional data analysis models to estimate future time trends of age-specific breast cancer mortality for the United States and England-Wales,https://robjhyndman.com/publications/brca-usew/,"Background: Mortality/incidence predictions are used for planning public health resources and need to accurately reflect age-related changes through time. We present a new forecasting model to estimate future trends in age-related breast cancer mortality for the United States and England-Wales.
Material and methods: We use functional data analysis techniques to model breast cancer mortality-age relationships in the United States from 1950 to 2001 and England-Wales from 1950 to 2003 and estimate 20-year predictions using a new forecasting method."
2010,1,14,Detecting trend and seasonal changes in satellite image time series,https://robjhyndman.com/publications/bfast1/,A wealth of remotely sensed time series covering large areas is now available to the earth science community. Change detection methods are often not capable of detecting land cover changes within time series that are heavily influenced by seasonal climatic variations. Detecting change within the trend and seasonal components of time series enables the detection of different types of changes. Changes occurring in the trend component indicate disturbances (e.g. insect attack) while changes occurring in the seasonal component indicate phenological changes (e.
2010,1,2,Density forecasting for long-term peak electricity demand,https://robjhyndman.com/publications/peak-electricity-demand/,Abstract: Long-term electricity demand forecasting plays an important role in planning for future generation facilities and transmission augmentation. In a long term context planners must adopt a probabilistic view of potential peak demand levels therefore density forecasts (providing estimates of the full probability distributions of the possible future values of the demand) are more helpful than point forecasts and are necessary for utilities to evaluate and hedge the financial risk accrued by demand variability and forecasting uncertainty.
2010,1,1,Business Forecasting Methods,https://robjhyndman.com/publications/iess2/,
2010,1,1,Forecasting Overview,https://robjhyndman.com/publications/iess3/,
2010,1,1,Moving Averages,https://robjhyndman.com/publications/iess1/,
2010,1,1,Encouraging replication and reproducible research,https://robjhyndman.com/publications/replication/,
2010,1,1,Changing of the guard,https://robjhyndman.com/publications/changing-of-the-guard/,
2009,12,27,Using DOIs,https://robjhyndman.com/hyndsight/doi/,"Almost all papers these days have a DOI and it is worth knowing how to use them.
At the top or bottom of the first page of a paper you will see something like this:
doi:10.1016/j.csda.2006.07.028  This is a unique and permanent identifier for the paper known as a &ldquo;Digital Object Identifier&rdquo;. The part before the forward slash (10.1016 in the example above) identifies the naming authority (in this case Elsevier) and the part after the forward slash (j."
2009,12,2,Replications and reproducible research,https://robjhyndman.com/hyndsight/replications/,"Reproducible research One of the best ways to get started with research in a new area is to try to replicate some existing research. In doing so you will usually gain a much better understanding of the topic and you will often discover some problems with the research or develop ideas that will lead to a new research paper.
Unfortunately a lot of papers are not reproducible because the data are not made available or the description of the methods are not detailed enough."
2009,11,25,Exponential smoothing and non-negative data,https://robjhyndman.com/publications/expsmooth-nonnegative/,"The most common forecasting methods in business are based on exponential smoothing and the most common time series in business are inherently non-negative. Therefore it is of interest to consider the properties of the potential stochastic models underlying exponential smoothing when applied to non-negative data. We explore exponential smoothing state space models for non-negative data under various assumptions about the innovations or error process.
We first demonstrate that prediction distributions from some commonly used state space models may have an infinite variance beyond a certain forecasting horizon."
2009,11,24,Learning by video,https://robjhyndman.com/hyndsight/video/,"There are some nice online videos available on various aspects of statistics and mathematics that might be helpful to students trying to learn about new areas.
A search on YouTube will lead to a few fairly basic videos.
 Statistics playlists Mathematics playlists  A better place to go is YouTube EDU which contains material from universities.
Something similar is offered at iTunesU
But the best stuff is on Academic Earth."
2009,11,11,Controlling figure and table placement in LaTeX,https://robjhyndman.com/hyndsight/latex-floats/,"It can be frustrating trying to get your figures and tables to appear where you want them in a LaTeX document. Sometimes they just seem to float off onto another page of their own accord. Here is a collection of tools and ideas that help you get control of those pesky floats.Use the placement options: h t b and p. For example
\begin{figure}[htb]  causes LaTeX to try to fit the float &ldquo;here&rdquo; or at the &ldquo;top&rdquo; of the current page (or the next page) or at the &ldquo;bottom&rdquo; of the current page (or the next page)."
2009,11,9,More on the evils of statistical tests,https://robjhyndman.com/hyndsight/value-of-p-values/,"Check out the two posts by Galit Shmueli over at Bzst on hypothesis tests: one on the value of p-values and another on one-sided tests.
She says &ldquo;Shockingly enough people seem to really want to use p-values even if they don&rsquo;t understand them.&rdquo; That mirrors my experience too. Confidence intervals are much more useful because they provide a measure of the size of an effect rather than testing if it is equal to some prespecified value."
2009,10,15,"""Elements of Statistical Learning"" now online",https://robjhyndman.com/hyndsight/esl2/,"In the past couple of days the authors of several blogs have noted that the wonderful book The Elements of Statistical Learning: Data Mining Inference and Prediction by Hastie Tibshirani and Friedman (2nd ed. 2009) is now available for free download in pdf format.
Of course it is also nice to have a hard copy. Click the image to purchase from Amazon."
2009,10,15,Using personal pronouns in research writing,https://robjhyndman.com/hyndsight/personal-pronouns/,"Should you use &ldquo;I&rdquo; or &ldquo;we&rdquo; or neither in your thesis or paper?
Thoughts on this have changed over the years. Traditionally using personal pronouns like &ldquo;I&rdquo; and &ldquo;we&rdquo; was frowned on. Instead of saying &ldquo;In Section 3 I have compared the results from method X with those of method Y&rdquo; you were expected to write &ldquo;In section 3 the results from method X are compared with those from method Y&rdquo;."
2009,10,14,Attending research seminars,https://robjhyndman.com/hyndsight/attending-research-seminars/,"Most research students don&rsquo;t seem to attend seminars. When asked they usually say the seminars are not on their topic or they don&rsquo;t understand them or they find them boring or some other similar reason. I think this is because students don&rsquo;t understand the purpose of research seminars and have not learned how to listen to them.
Admittedly many research seminars are badly presented and seminar speakers also frequently misunderstand the purpose of the seminar which makes the problem worse."
2009,10,14,Squeezing space with LaTeX,https://robjhyndman.com/hyndsight/squeezing-space-with-latex/,I&rsquo;ve been writing a grant application with a 10-page limit and as usual it is difficult to squeeze everything in. No I can&rsquo;t just change the font as it has to be 12 point with at least 2 cm margins on an A4 page. Fortunately LaTeX is packed full of powerful features that help in squeezing it all in. Here are some of the tips I&rsquo;ve used over the years.
2009,10,1,Converting eps to pdf,https://robjhyndman.com/hyndsight/converting-eps-to-pdf/,"Simply include the package epstopdf. Then when you use pdflatex the eps files will be automatically converted to pdf at compile time. (The conversion only happens the first time you process the file and is skipped if there is already a pdf file with the same name.)
For example:
\documentclass{article} \usepackage{graphicxepstopdf} \begin{document} \includegraphics[width=\textwidth]{fig1} \end{document}  Then even though the only graphics file available is fig1.eps this will still be processed ok using pdflatex or pdftexify."
2009,9,28,The 7 secrets of highly successful PhD students,https://robjhyndman.com/hyndsight/7secrets/,It seems everyone has 7 secrets to success and now someone has hopped on the 7-secrets bandwagon with something for PhD students. Thinkwell is an Australian company offering a seminar and associated work book on &ldquo;The 7 secrets of highly successful PhD students&rdquo;. I bought the book out of curiosity but &ldquo;book&rdquo; is a gross exaggeration &ndash; only eleven pages of fairly simplistic advice. I hope the seminar has more substance.
2009,9,27,Writing an abstract,https://robjhyndman.com/hyndsight/abstracts/,The abstract is probably the most important part of a paper. Many readers will not read anything else so you need to grab their attention and get your main message across as clearly and succinctly as possible. It is not meant to be an introduction to the paper but a summary of the paper. In a single paragraph a reader can learn the purpose of the research your general approach to the problem your main results and the most important conclusions.
2009,9,18,Workflow in R,https://robjhyndman.com/hyndsight/workflow-in-r/,"This came up recently on StackOverflow. One of the answers was particularly helpful and I thought it might be worth mentioning here. The idea presented there is to break the code into four files all stored in your project directory. These four files are to be processed in the following order.
 load.R This file includes all code associated with loading the data. Usually it will be a short file reading in data from files."
2009,9,18,Writing mathematics,https://robjhyndman.com/hyndsight/writing-mathematics/,"Mathematics has its own particular conventions and rules when it comes to writing. There have been several attempts to write them down. The most famous is Halmos&rsquo;s excellent essay &ldquo;How to write mathematics&rdquo;. Other good sources of advice are the following two books:
(I reviewed these two books in the Australian and New Zealand Journal of Statistics a few years ago.)
But if you just want a quick summary I recommend Dave Richeson&rsquo;s blog entry &ldquo;The nuts and bolts of writing mathematics&rdquo;."
2009,9,13,Take a break,https://robjhyndman.com/hyndsight/take-a-break/,"Occasionally the best research is done in long periods of concentrated effort. Allegedly Isaac Newton used to sometimes write for eight hours standing up without a break.
At other times taking a break helps the research process. Think of Archimedes and his Eureka moment. Many of my best ideas come while walking or taking a shower. In fact I once suggested to my head of department that we should have showers installed in every office as it would increase the quality of our research."
2009,9,13,Finding an R function,https://robjhyndman.com/hyndsight/finding-an-r-function/,"Suppose you want a function to fit a neural network. What&rsquo;s the best way to find it? Here are three steps that help to find the elusive function relatively quickly.
First use help.search(&quot;neural&quot;) or the shorthand ??neural. This will search the help files of installed packages for the word &ldquo;neural&rdquo;. Actually fuzzy matching is used so it returns pages that have words similar to &ldquo;neural&rdquo; such as &ldquo;natural&rdquo;. For a stricter search use help."
2009,9,2,Statistics education journals,https://robjhyndman.com/hyndsight/statistics-education-journals/,"In many research universities there can be a tension that arises when great teachers don&rsquo;t publish much. I believe there is a place for excellent teachers who do limited research within a strong research university but their contribution is considerably enhanced if they share their teaching insights. There are at least three reputable research journals for publishing articles on statistics education:
  Journal of Statistics Education
  Statistics Education Research Journal"
2009,9,2,Mathematical research and the internet,https://robjhyndman.com/hyndsight/tao-lecture/,"On Monday night I attended a lecture by Terry Tao on &ldquo;Mathematical research and the internet&rdquo;. Terry is Australia&rsquo;s most famous mathematician our only Field&rsquo;s medalist and one of the most active mathematical bloggers in the world. He has been described as the &ldquo;Mozart of mathematics&rdquo; for his remarkable precocity and prolific output. The slides of his talk are available on his blog site.
It was an interesting talk with excellent slides marred only by the poor sound system and his bad habit of mumbling."
2009,8,27,How good are economic forecasts?,https://robjhyndman.com/hyndsight/how-good-are-economic-forecasts/,I wrote last week that &ldquo;macroeconomic forecasts are little better than shooting blindfold&rdquo;. I don&rsquo;t know if it was connected or not but on the same day a journalist (Richard Pullin) from Reuters phoned me to ask about assessing some economic forecasts. He wanted to compare the accuracy of several economic forecasts for Japan and he wasn&rsquo;t sure how to go about it. I helped him to calculate the MASE for the different forecasts and the results have now been published.
2009,8,26,Research supervision workshop,https://robjhyndman.com/hyndsight/supervision-workshop/,"Today I gave a workshop for supervisors of postgraduate students. Mostly I talked about creating a team environment for postgraduate students rather than the traditional model (at least in statistics and econometrics) of each student working in isolation.
The slides are available here in presentation form or in handout form. Actually these are an edited version of the slides as I accidentally left out a couple of the photographs in the workshop and I&rsquo;ve omitted slides that I didn&rsquo;t end up covering in the workshop."
2009,8,25,Seek help when it's needed,https://robjhyndman.com/hyndsight/seek-help/,I don&rsquo;t think I&rsquo;ve had a research student who did not think about giving up at some point. It was part through my second year when I felt like giving up. I felt I was not going to be able to finish my thesis and that I would be better off throwing in the towel and doing something else. Fortunately I couldn&rsquo;t think of anything better to do plus I hate giving up on anything so I persevered and it turned out ok.
2009,8,24,Why I don't like statistical tests,https://robjhyndman.com/hyndsight/tests/,"It may come as a shock to discover that a statistician does not like statistical tests. Isn&rsquo;t that what statistics is all about? Unfortunately in some disciplines statistical analysis does seem to consist almost entirely of hypothesis testing and therein lies the problem.
The standard practice is to construct a hypothesis test to determine if some attribute of the data is &ldquo;significant&rdquo; or not with the standard p-value threshold of 5%."
2009,8,23,R help on StackOverflow,https://robjhyndman.com/hyndsight/r-help-on-stackoverflow/,"Ever since I began using R about ten years ago the best place to find R help was on the R-help mailing list. But it is time-consuming searching through the archives trying to find something from a long time ago and there is no way to sort out the good advice from the bad advice.
But now there is a new tool and it is very neat. Head over to the R tag on StackOverflow."
2009,8,21,Backing up,https://robjhyndman.com/hyndsight/backing-up/,"Ever since I deleted the only copy of my honours thesis one week before it was due to be handed in I&rsquo;ve been obsessive about backups often to the amusement of my family and colleagues. But every time one of them loses a file or has a hard-disk fail the smiles fade and they ask for advice.
I&rsquo;ve used many systems over the years each one a little better than the last."
2009,8,17,Forecasting the recession,https://robjhyndman.com/hyndsight/forecasting-the-recession/,"Forecasters are under the pump with a recession that many didn&rsquo;t see coming. As I don&rsquo;t do any macroeconomic forecasting I can sit back and smile smugly at some of my colleagues while I work on simpler problems such as forecasting in epidemiology demography and energy demand.
Some of those colleagues are cited in the Wall Street Journal today. The following quotation is interesting:
 The spate of cloudy crystal balls highlighted an uncomfortable reality about telling the future: It is hardest when it is most important."
2009,8,14,Maintaining local LaTeX files,https://robjhyndman.com/hyndsight/localtexmf/,If you use LaTeX then you probably have a bib file — a data base of all the papers and books that you have cited. It is much more efficient to keep one database in one location than have multiple copies of it floating around your hard drive. (Or even worse have different bib files created for different papers.) You might also have a few of your own style files and again it is best to keep these in a central location and not have duplicates all over the place.
2009,8,13,Sight what you cite,https://robjhyndman.com/hyndsight/sight-what-you-cite/,"There seems to be a widespread practice of researchers citing papers they have never even seen let alone read. For example
  Some papers claim to do something new when it has already been done in one of the papers cited.
  Some articles are cited that apparently have little to do with the reason given for the citation or which argue the opposite point of view to what is claimed."
2009,8,5,Songs of Statistics,https://robjhyndman.com/hyndsight/songs-of-statistics/,"If you love statistics (don&rsquo;t we all?) and can write Chinese (which rules me out) you might like to contribute to the Chinese National Bureau of Statistics celebrations of the 60th anniversary of the &ldquo;founding of New China&rdquo;. They are calling for submissions of prose poetry or song which will &ldquo;enhance people&rsquo;s patriotic feelings statistics and confidence&rdquo;. Here is an English translation of the page.
Some further translations are on the WSJ page."
2009,8,3,Writing responses to referee reports,https://robjhyndman.com/hyndsight/responses/,"I&rsquo;ve been spending time writing response letters lately. I&rsquo;ve also been reading lots of response letters from authors wanting their stuff published in the International Journal of Forecasting. I thought it might be useful to collate a few thoughts on the subject.
  No grovelling. I sometimes get response letters that start off with a paragraph of inane and obsequious fawning. The real response only begins after a paragraph of flattery which makes me wonder why I&rsquo;ve never won the Nobel prize."
2009,8,2,Managing a bibliographic database,https://robjhyndman.com/hyndsight/managing-a-bibliographic-database/,"All researchers need to maintain a database of papers they have read cited or simply noted for later reference. For those of us using LaTeX the database is in the BibTeX format and is stored as a simple text file (a bib file) that can be edited using a text editor such as WinEdt.
But it is often easier to edit the file using specialist software. My current favourite tool is JabRef."
2009,7,27,Finding LaTeX symbols,https://robjhyndman.com/hyndsight/finding-latex-symbols/,"All LaTeX users will sometimes need a symbol for which they don&rsquo;t know the command. If you use WinEdt there is a neat drop-down menu of some common symbols that can be helpful but it is necessarily limited. What do you do when you want the male and female symbols?
For several years I&rsquo;ve been using the comprehensive symbol list whenever I need a symbol in LaTeX that I can&rsquo;t recall."
2009,7,23,Forecasting functional time series,https://robjhyndman.com/publications/forecasting-functional-time-series-2/,We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions assign higher weights to more recent data and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006 and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts.
2009,7,21,Why Word is a bad choice for academic writing,https://robjhyndman.com/hyndsight/why-word-is-a-bad-choice-for-academic-writing/,For years I&rsquo;ve been telling everyone who would listen that MS-Word may sometimes be useful for short notes or for making a &ldquo;Back in 5 minutes&rdquo; sign to stick on your door but if you want to write a serious document like an academic paper a book or a thesis then you should use a serious tool such as LaTeX. For those who are not yet convinced Ben Klemens has a nice article entitled &ldquo;Why Word is a terrible program&rdquo;.
2009,7,15,Mathematical genealogy,https://robjhyndman.com/hyndsight/mathematical-genealogy/,"Having a PhD student is like having a child. I have had many such &ldquo;children&rdquo; graduate and have another few &ldquo;on the way&rdquo;. (See here for my offspring.)
Going in the other direction here is my family tree (or one branch of it) compiled from the Mathematical Genealogy Project. Each successive person is the doctoral student of the person before him. (No females here!)
     Erhard Weigel Ph."
2009,7,13,Searching the research literature,https://robjhyndman.com/hyndsight/searching-the-research-literature/,"Most students seem to go to Google first. This is not a good strategy. Google Scholar is much better as it filters out all the junk. Scopus is another engine that aims to do a similar thing. It is better organized but not so complete. ISI WOK is also not as complete as Google Scholar but is particularly good at tracking citations.
  Google scholar
  Scopus
  ISI Web of Knowledge"
2009,7,12,Nonparametric time series forecasting with dynamic updating,https://robjhyndman.com/publications/dynamic-updating1/,
2009,7,5,Monitoring processes with changing variances,https://robjhyndman.com/publications/monitoring-processes/,Statistical process control (SPC) has evolved beyond its classical applications in manufacturing to monitoring economic and social phenomena. This extension requires consideration of autocorrelated and possibly non-stationary time series. Less attention has been paid to the possibility that the variance of the process may also change over time. In this paper we use the innovations state space modeling framework to develop conditionally heteroscedastic models. We provide examples to show that the incorrect use of homoscedastic models may lead to erroneous decisions about the nature of the process.
2009,6,25,English academic writing,https://robjhyndman.com/seminars/english-academic-writing/,Presentation to College of Management University of Fuzhou China. 25 June 2009.
2009,6,23,Extreme forecasting,https://robjhyndman.com/seminars/extreme-forecasting/,"Keynote address International Symposium on Forecasting June 2009. Abstract
Extremely bad data extremely poor methods and extremely difficult problems will be used as the basis of some extremely useful lessons. I will describe three cases from my consulting experience and draw some general lessons that are widely applicable.
The first case involved forecasting passenger traffic on an Australian airline. The data showed variations due to school holidays major sporting events competitor activity industrial disputes changes in fare structures and other factors."
2009,6,1,Clive Granger (1934-2009),https://robjhyndman.com/hyndsight/granger/,Sir Clive Granger has died at the age of 74. There are some nice obituaries in the New York Times and the Daily Telegraph. Also his Wikipedia page has some good information. I met Clive on several occasions and he was &ldquo;a scholar and a gentleman&rdquo; a remarkably humble man given his outstanding achievements and someone who was always willing to help young researchers. The world of forecasting will miss him.
2009,5,28,Akram's story,https://robjhyndman.com/hyndsight/akrams-story/,Muhammad Akram was my PhD student a few years ago and has remained a good friend since he moved on. Here is an interview he recently gave about moving to Australia. Thanks Akram for the kind words about me!
2009,5,18,Prediction markets,https://robjhyndman.com/hyndsight/prediction-markets/,Andrew Leigh has a nice piece in today&rsquo;s AFR on forecasting via prediction markets
2009,5,1,Statistical support for HDR students,https://robjhyndman.com/seminars/statistical-support-for-hdr-students/,Presentation to a meeting of Australian Deans and Directors of Graduate Studies 1 May 2009.
2009,4,8,Neil Postman on technological change,https://robjhyndman.com/hyndsight/neil-postman-on-technological-change/,"Neil Postman was Professor of Communication at New York University until his death in 2003. He wrote many wonderfully insightful and thought-provoking articles and books about television education technology and childhood. I recently came across a speech he gave in 1998 on &ldquo;Five things we need to know about technological change&rdquo;. Here is an online transcript. The five things are:
  That we always pay a price for technology; the greater the technology the greater the price."
2009,3,12,Accessing journal articles online,https://robjhyndman.com/hyndsight/accessing-journal-articles-online/,"When searching for research articles online I often find that the article is unavailable unless I go through the Monash library website especially when working from home. Here are two solutions to the problem
  Within Google scholar go to &ldquo;Scholar preferences&rdquo; and under library links search for &ldquo;Monash&rdquo;. Tick the entry &ldquo;Monash University - Check for full text&rdquo;. Then save your preferences (button at bottom of page). Next time you do a Google scholar search a link labelled &ldquo;Check for full text&rdquo; will appear beside each entry."
2009,2,1,Statistician: the dream job,https://robjhyndman.com/hyndsight/statistician-the-dream-job/,"So what is the ultimate job? According to Hal Varian chief economist at Google it is being a statistician (see this interview)
Here he is on YouTube with a longer comment:
  Statistics - Dream Job of the next decade
From a keynote presentation to the 2008 Almaden Institute - &ldquo;Innovating with Information&rdquo;.
The full presentation is available at http://www.almaden.ibm.com/institute/agenda.shtml
Hal Varian makes the argument that with data in huge supply and statisticians in short supply being a statistician has to be the &lsquo;really sexy job for the 2010s&rsquo;."
2009,1,16,Rule induction for forecasting method selection: meta-learning the characteristics of univariate time series,https://robjhyndman.com/publications/forecast-rules/,For univariate forecasting there are various statistical models and computational algorithms available. In real-world exercises too many choices can create difficulties in selecting the most appropriate technique especially for users lacking sufficient knowledge of forecasting. This study focuses on rule induction for forecasting method selection by understanding the nature of historical forecasting data. A novel approach for selecting a forecasting method for univariate time series based on measurable data characteristics is presented that combines elements of data mining meta-learning clustering classification and statistical measurement.
2009,1,16,Hierarchical forecasts for Australian domestic tourism,https://robjhyndman.com/publications/hierarchical-tourism/,In this paper we explore the hierarchical nature of tourism demand time series and produce short-term forecasts for Australian domestic tourism. The data and forecasts are organized in a hierarchy based on disaggregating the data for different geographical regions and for different purposes of travel. We consider five approaches to hierarchical forecasting: two variations of the top-down approach the bottom-up method a newly proposed top-down approach where top-level forecasts are disaggregated according to forecasted proportions of lower level series and a recently proposed optimal combination approach.
2009,1,1,A multivariate innovations state space Beveridge-Nelson decomposition,https://robjhyndman.com/publications/vists-beveridge-nelson/,The Beveridge-Nelson vector innovations structural time series framework is a new formulation that decomposes a set of variables into their permanent and transitory components. The proposed framework is flexible modelling inter-series relationships and common features in a simple manner. In particular it is shown that this new specification is simpler than conventional state space and cointegration approaches. The approach is illustrated using a trivariate data set comprising the GDP of Australia the USA and the UK.
2008,11,16,Forecasting time series with multiple seasonal patterns,https://robjhyndman.com/publications/multiple-seasonal-patterns/,A new approach is proposed for forecasting a time series with multiple seasonal patterns. A state space model is developed for the series using the innovation approach which enables us to develop explicit models for both additive and multiplicative seasonality. Parameter estimates may be obtained using methods from exponential smoothing. The proposed model is used to examine hourly and daily patterns in hourly data for both utility loads and traffic flows.
2008,11,5,Forecasting without significance tests?,https://robjhyndman.com/publications/forecasting-without-significance-tests/,Statistical significance testing has little useful purpose in business forecasting and other tools are to be preferred. For selecting or ranking forecasting methods (especially those based on models) there exist simple but powerful and practical alternative approaches that are not tests in any sense. It is suggested that forecasters place less emphasis on p-values and more emphasis on the predictive ability of models.
2008,10,16,Time series packages on R,https://robjhyndman.com/hyndsight/time-series-packages-on-r/,There is now an official CRAN Task View for Time Series. This will replace my earlier list of time series packages for R and provide a more visible and useful entry point for people wanting to use R for time series analysis. If I have missed anything on the list please let me know.
2008,9,3,LaTeX tips,https://robjhyndman.com/hyndsight/latex-tips/,"While reading students' theses and papers recently I came across various examples of poor latex-ing that I thought would be useful to catalogue.
  Don&rsquo;t set both width and height when using \includegraphics. It distorts the figure. Instead I suggest using \includegraphics[width=\textwidth]{..}
  Be consistent in capitalizing section and subsection headings and titles in the bibliography. My preferences is to use sentence case throughout.
  When using functions such as max min log exp sin cos etc."
2008,8,20,Tracking changes in LaTeX files,https://robjhyndman.com/hyndsight/tracking-changes-in-latex-files/,"When I write a paper it usually goes through many versions before being submitted to a journal. I keep track of the different versions by renaming the file when I&rsquo;m about to make major changes or when I receive a new version from a coauthor. The files are named file1.tex file2.tex etc. where &ldquo;file&rdquo; is replaced by something more meaningful.
It is often useful to be able to compare versions to see what changes have been made especially when working with coauthors."
2008,8,20,Tracking changes in text files,https://robjhyndman.com/hyndsight/tracking-changes-in-text-files/,"A common issue that arises with text files (e.g. R code) is to identify changes that have been made between versions. I usually number my R files as file1.R file2.R etc. (with &ldquo;file&rdquo; replaced by something more meaningful)with the number indicating the version of the file. Version numbers change whenever I send the file to someone else to modify or whenever I make major changes myself.
I often need to know what changes have been made between successive versions."
2008,8,19,Supervision award,https://robjhyndman.com/hyndsight/supervision-award/,Last night I received the Vice-Chancellor&rsquo;s postgraduate supervision award at a function at Government House. I am deeply honoured that my students thought to nominate me for the award. I think I was as surprised as anyone to win and some people have asked me what I did to deserve it. Actually I&rsquo;m not sure that I did deserve it but I can tell you what I told the award committee who chose me.
2008,7,30,LaTeX books,https://robjhyndman.com/hyndsight/latex-books/,"amzn_assoc_placement = ""adunit0""; amzn_assoc_tracking_id = ""otexts-20""; amzn_assoc_ad_mode = ""manual""; amzn_assoc_ad_type = ""smart""; amzn_assoc_marketplace = ""amazon""; amzn_assoc_region = ""US""; amzn_assoc_title = ""LaTeX books""; amzn_assoc_rows = ""6""; amzn_assoc_linkid = ""837c44f97120fac94a76c36f9781f970""; amzn_assoc_asins = ""18471998603642238157032117385602013629961784395145081764131933192379500201529831"";   If you cannot see any books above please turn off your ad-blocker."
2008,7,25,Time management,https://robjhyndman.com/hyndsight/time-management/,"I am frequently asked how I manage my time and how I manage to get so much done. I don&rsquo;t know that my approach is right for everyone but in case it helps here are some comments on how I work.
One of the main traps that people fall into is to do things that are urgent but not necessarily important. They react to the urgency of deadlines and demands of colleagues and end up spending a lot of time on things that don&rsquo;t really matter much."
2008,7,18,Forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/forecasting-and-the-importance-of-being-uncertain/,Indian Institute of Management Calcutta. Melbourne 18 July 2008.
2008,7,16,"Stochastic population forecasts using functional data models for mortality, fertility and migration",https://robjhyndman.com/publications/stochastic-population-forecasts/,Age-sex-specific population forecasts are derived through stochastic population renewal using forecasts of mortality fertility and net migration. Functional data models with time series coefficients are used to model age-specific mortality and fertility rates. As detailed migration data are lacking net migration by age and sex is estimated as the difference between historic annual population data and successive populations one year ahead derived from a projection using fertility and mortality data. This estimate which includes error is also modeled using a functional data model.
2008,7,16,Automatic time series forecasting: the forecast package for R,https://robjhyndman.com/publications/automatic-forecasting/,Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovation state space models that underly exponential smoothing methods. The second is based on ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data and are compared and illustrated using four real time series.
2008,6,29,Building R packages for Windows,https://robjhyndman.com/seminars/building-r-packages-for-windows/,"R workshop. Melbourne 29 June 2008. There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day.
Building R packages for Windows
 handout slides"
2008,6,29,Time series and forecasting in R,https://robjhyndman.com/seminars/time-series-and-forecasting-in-r/,"R workshop. Melbourne 29 June 2008. There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day.
Time series and forecasting in R
 handout slides"
2008,6,19,"Bagplots, boxplots and outlier detection for functional data",https://robjhyndman.com/seminars/fboxplot-talk/,Australian Statistics Conference. Melbourne July 2008.  Where: First International Workshop on Functional and Operatorial Statistics Toulouse Where : Australian Statistical Conference Melbourne Australia Speakers: Professor Rob J Hyndman Monash University &amp; Han Lin Shang Monash University  Abstract: We propose some new tools for visualizing functional data and for identifying functional outliers. The proposed tools make use of robust principal component analysis data depth and highest density regions. We compare the proposed outlier detection methods with the existing “functional depth” method and show that our methods have better performance on identifying outliers in French male age-specific mortality data.
2008,6,16,The admissible parameter space for exponential smoothing models,https://robjhyndman.com/publications/the-admissible-parameter-space-for-exponential-smoothing-models/,We discuss the admissible parameter space for some state space models including the models that underly exponential smoothing methods. We find that the usual parameter restrictions (requiring all smoothing parameters to lie between 0 and 1) do not always lead to stable models. We also find that all seasonal exponential smoothing methods are unstable as the underlying state space models are neither reachable nor observable. This instability does not affect the forecasts but does corrupt the state estimates.
2008,6,15,Exponential smoothing and non-negative data,https://robjhyndman.com/seminars/exponential-smoothing-and-non-negative-data/,Where: International Symposium on Forecasting Nice France  Abstract: The most common forecasting methods in business are based on exponential smoothing and the most common time series in business are inherently non-negative. Therefore it is of interest to consider the properties of the potential stochastic models underlying exponential smoothing when applied to non-negative data. We explore nonlinear exponential smoothing state space models for non-negative data under various assumptions about the innovations or error process.
2008,6,6,LaTeX workshop,https://robjhyndman.com/hyndsight/latex-workshop/,"I gave a one-day LaTeX workshop today.
Here is the blurb:
 LaTeX is an extremely powerful markup language for creating structured documents. It is particularly well-suited for documents containing mathematics but can be used for any document. Those whose publications involve a large number of mathematical equations often use LaTeX rather than MS-Word or some other word processing package. It is the standard writing tool for most research in the mathematical sciences."
2008,5,28,Words to avoid,https://robjhyndman.com/hyndsight/words-to-avoid/,"According to Andrew Gelman we should avoid these words in research writing:
  Note that
  Interestingly
  Obviously
  It is clear that
  It is interesting to note that
  very
  quite
  of course
  Notice that
  I agree with him that all of these are overused but that doesn&rsquo;t mean they should be banned. The words &ldquo;very&rdquo; and &ldquo;quite&rdquo; are useful for conveying the strength of a statement."
2008,5,15,"Bagplots, boxplots and outlier detection for functional data",https://robjhyndman.com/publications/bagplots-boxplots-and-outlier-detection-for-functional-data/,We propose some new tools for visualizing functional data and for identifying functional outliers. The proposed tools make use of robust principal component analysis data depth and highest density regions. We compare the proposed outlier detection methods with the existing &ldquo;functional depth&rdquo; method and show that our methods have better performance on identifying outliers in French male age-specific mortality data.
2008,5,7,Giving a research seminar,https://robjhyndman.com/hyndsight/giving-a-research-seminar/,"An expanded version of this post is available in my article on “Giving an academic talk”.
With conference season almost upon us it is timely to discuss what makes a good conference presentation. Here is a suggested structure.
 A motivating example demonstrating the problem you are trying to solve. Explain existing approaches to the problem and their weaknesses. Describe your main contributions. Show how your ideas solve the problem/example you started with."
2008,4,28,Forecasting and time series books,https://robjhyndman.com/hyndsight/forecasting-and-time-series-books/,"People often ask me for recommendations on forecasting books and time series books. So here is list of eight good books to which I often refer.
(Updated 8 November 2017)
amzn_assoc_placement = ""adunit0""; amzn_assoc_tracking_id = ""otexts-20""; amzn_assoc_ad_mode = ""manual""; amzn_assoc_ad_type = ""smart""; amzn_assoc_marketplace = ""amazon""; amzn_assoc_region = ""US""; amzn_assoc_title = ""Forecasting and time series books""; amzn_assoc_rows = ""6""; amzn_assoc_linkid = ""837c44f97120fac94a76c36f9781f970""; amzn_assoc_asins = ""098750710935407191640999064908B004UW0PA4047136164X11186750290792374010144197864X"";   Two are my own books of course (after all I wrote them because I thought I had something to say)."
2008,3,28,R workshop,https://robjhyndman.com/hyndsight/r-workshop/,"There was an R workshop on 28-29 June just before the Australian Statistical Conference. I put in an appearance on the second day giving two talks.
Time series and forecasting in R
 handout slides  Building R packages for Windows
 handout slides"
2008,3,27,Creating a BibTeX file from a Google Library,https://robjhyndman.com/hyndsight/creating-a-bibtex-file-from-a-google-library/,"As you will have seen if you poke around these pages I have a Google Library of books in statistics and forecasting. This is intended to be a complete copy of what is on the shelves in my office (about 400 books) plus books that I would like on my shelves if I had more space.
I find the library useful for keeping track of books that my students and colleagues borrow (I just add a tag containing their name to the book)."
2008,3,26,The maths/stats crisis in Australian education,https://robjhyndman.com/hyndsight/the-mathsstats-crisis-in-australian-education/,"There is a great article in today&rsquo;s Australian by my co-author Peter Hall on the crisis in mathematics &amp; statistics education (with student numbers falling at the same time as the number of jobs is rising).
The most interesting comment is the last paragraph:
 For a nation in the grip of a serious skills shortage in mathematics and statistics another review is not needed. Action is needed. It should be possible to drive university behaviour in the national interest by foreshadowing that the Government&rsquo;s proposed compacts will include a requirement that mathematics and statistics curriculums be offered in science and technology education business and economics courses across the country taught by professionals in mathematics and statistics."
2008,3,13,Dodgy forecasting,https://robjhyndman.com/hyndsight/dodgy-forecasting/,"A few years ago I did some forecasting work for a commonwealth government department and found that they were forecasting a $5 billion budget using the FORECAST command in Excel. Worse they were fitting a regression through only three observations and they were not even the most recent observations.
It seems a similar thing has happened again. The Victorian government is projecting water consumption based on a regression through three observations."
2008,2,24,About Hyndsight,https://robjhyndman.com/hyndsight/about/,I was thinking of writing a book on doing research in statistics. Instead I decided to write a blog covering the same material plus other things that might be of interest to my research team. Topics covered include LaTeX R writing and preparing a thesis writing a journal article submitting an article to a refereed journal how to convince editors to publish your work and writing referee reports. Topics of more specific interest to my research team include forecasting data visualization and functional data and local events such as meetups or statistics conferences.
2008,2,22,Forecasting functional time series,https://robjhyndman.com/seminars/forecasting-functional-time-series/,Where: Australian Frontiers of Science  Abstract: Functional time series are curves that are observed sequentially in time. For example the curve of death rate as a function of age is observed annually. Yield curves in finance (essentially interest rates as a function of the term of investment) are observed each week or each day. Electricity consumption as a function of temperature is observed every month. These are all high dimensional functional data indexed by time.
2008,2,1,Modelling and forecasting Australian domestic tourism,https://robjhyndman.com/publications/aus-domestic-tourism/,In this paper we model and forecast Australian domestic tourism demand. We use a regression framework to estimate important economic relationships for domestic tourism demand. We also identify the impact of world events such as the 2000 Sydney Olympics and the 2002 Bali bombings on Australian domestic tourism. To explore the time series nature of the data we use innovation state space models to forecast the domestic tourism demand. Combining these two frameworks we build innovation state space models with exogenous variables.
2008,1,25,Generation of synthetic sequences of half-hourly temperatures,https://robjhyndman.com/publications/generation-of-synthetic-sequences-of-half-hourly-temperatures/,"We present tools to generate synthetic sequences of half-hourly temperatures with similar statistical characteristics to observed historical data. Temperatures are generated using a combination of daily and half-hourly temperature models which account for intra-day and intra-year seasonality as well as short- and long-term serial correlations. Details of the model estimation are given as well as a description of the synthetic generation.
Keywords: temperature data time series Fourier series ARMA models seasonal block-bootstrap synthetic generation."
2007,11,27,Population forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/population-forecasting-and-the-importance-of-being-uncertain/,Where: Knibbs Lecture Statistical Society of Australia  Abstract: Forecasters had an inauspicious beginning dabbling with divination sheep&rsquo;s livers and vapour-ridden caves in the mountains of Greece. Then there was a time when forecasters could be charged with vagrancy! Their reputations are still tarnished but their tools are rather more effective. Professor Rob Hyndman will argue for the importance of statistical modelling in forecasting and demonstrate the dangers that occur when uncertainty is ignored.
2007,11,16,Indexing in LaTeX,https://robjhyndman.com/hyndsight/indexing-in-latex/,"I&rsquo;m in the final stages of preparing my new exponential smoothing book for publication and have been learning about some LaTeX indexing tools.
The standard subject index is created using the following procedure:
 Include \index{entry} commands wherever you want an index entry. Include \usepackage{makeidx} and \makeindex in the preamble. Put a \printindex command where the index is to appear normally before the \end{document} command.  The details are well-documented in this tutorial (starting on p9)."
2007,11,1,Tables in LaTeX,https://robjhyndman.com/hyndsight/tables-in-latex/,"(Updated May 2017)
Making tables in LaTeX is one of the few areas where LaTeX can be more difficult than a WYSIWYG editor.
Here are some pointers to tools and packages that I have found useful.
  tablesgenerator.com: a web-based tool for generating LaTeX tables.
  Excel2LaTeX: this excel add-in makes it easy to copy a rectangular array of cells in a spreadsheet into a LaTeX document.
  Calc2LaTeX: a similar extension for LibreOffice and OpenOffice."
2007,10,25,Graduation address,https://robjhyndman.com/seminars/graduation-address/,"Mr Chancellor Madam Deputy Vice-Chancellor colleagues guests and especially graduates.
I would like to congratulate all of you who are graduating tonight.
It is a great achievement to have completed a university degree and you should all feel very very proud of your accomplishment. This is one of the six great milestones in your life: the others being birth death marriage parenthood and the day you finally pay off your HECS debt."
2007,9,12,Searching the statistical literature,https://robjhyndman.com/hyndsight/searching-the-statistical-literature/,"Last week Google books introduced a facility whereby users can create their own &ldquo;library&rdquo; containing a subset of books to search. I have set up a library of statistical books to aid in searching the statistical literature.
My Google library is intended to include all the books on my office shelves plus a whole lot more that I would like to buy if I had more money (and more shelf space)."
2007,9,6,Organization and R,https://robjhyndman.com/hyndsight/organization-and-r/,"Many R users seem to get themselves in a bit of a mess with R files and workspaces scattered across different directories. The R files themselves also get messy and hard to follow. So here is some advice on keeping organized with R:
  Try to keep code strictly indented based on the code structure such as loops if statements etc. Every left brace { should be followed by an extra level of indentation which continues until the matching right brace }."
2007,8,31,Debugging in R,https://robjhyndman.com/hyndsight/debugging-in-r/,"Anyone who starts writing serious R code (i.e. code that involves user-written functions) soon finds the need to use debugging tools. There are a few basic tools in R that are worth knowing about.
The function debug() allows one to step through the execution of a function line by line. At any point you can print out values of variables or produce a graph of the results within the function. While debugging you can simply type &ldquo;c&rdquo; to continue to the end of the current section of code (e."
2007,7,16,Measurement of changes in antihypertensive drug utilization following primary care educational interventions,https://robjhyndman.com/publications/measurement-of-changes-in-antihypertensive-drug-utilization-following-primary-care-educational-inte/,"Abstract: Purpose To measure changes in drug utilization following a national general practice education program aimed at improving prescribing for hypertension.
Methods A series of nationally implemented multifaceted educational interventions using social marketing principles focusing on prescribing for hypertension was commenced in October 1999 and repeated in September 2001 and August 2003. The target group was all primary care prescribers in Australia and interventions were both active (voluntary) and passive. Newsletter and prescribing feedback was mailed in October 1999 September 2001 (newsletter only) and August 2003."
2007,7,16,Robust forecasting of mortality and fertility rates: a functional data approach,https://robjhyndman.com/publications/funcfor/,Abstract: A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age is robust for outlying years due to wars and epidemics and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error.
2007,6,29,Do levels of airborne grass pollen influence asthma hospital admissions?,https://robjhyndman.com/publications/do-levels-of-airborne-grass-pollen-influence-asthma-hospital-admissions/,"Background: The effects of environmental factors and ambient concentrations of grass pollen on allergic asthma are yet to be established.
Objective: We sought to estimate the independent effects of grass pollen concentrations in the air over Melbourne on asthma hospital admissions for the 1992-1993 pollen season.
Methods: Daily grass pollen concentrations were monitored over a 24 hr period at three stations in Melbourne. The outcome variable was defined as all-age asthma hospital admissions with ICD9-493 codes."
2007,6,25,Forecasting medium- and long-term peak electricity demand,https://robjhyndman.com/seminars/forecasting-medium-and-long-term-peak-electricity-demand/,"Where: International Symposium on Forecasting New York  Abstract: Peak electricity demand forecasting is important in medium and long-term planning of electricity supply. Extreme demand often leads to supply failure with consequential business and social disruption. Forecasting extreme demand events is therefore an important problem in energy management.
Electricity demand at a given time is subject to a range of influences including the ambient temperature recent past temperatures time of day day of week holidays economic conditions and so on."
2007,5,29,A state space model for exponential smoothing with group seasonality,https://robjhyndman.com/publications/a-state-space-model-for-exponential-smoothing-with-group-seasonality/,We present an approach to improve forecast accuracy by simultaneously forecasting a group of products that exhibit similar seasonal demand patterns. Better seasonality estimates can be made by using information on all products in a group and using these improved estimates when forecasting at the individual product level. This approach is called the group seasonal indices (GSI) approach and is a generalization of the classical Holt-Winters procedure. This article describes an underlying state space model for this method and presents simulation results that show when it yields more accurate forecasts than Holt-Winters.
2007,4,1,Half-life estimation based on the bias-corrected bootstrap: a highest density region approach,https://robjhyndman.com/publications/half-life-estimation-based-on-the-bias-corrected-bootstrap-a-highest-density-region-approach/,The half-life is defined as the number of periods required for the impulse response to a unit shock to a time series to dissipate by half. It is widely used as a measure of persistence especially in international economics to quantify the degree of mean-reversion of the deviation from an international parity condition. Several studies have proposed bias-corrected point and interval estimation methods. However they have found that the confidence intervals are rather uninformative with their upper bound being either extremely large or infinite.
2007,3,16,Minimum sample size requirements for seasonal forecasting models,https://robjhyndman.com/publications/minimum-sample-size-requirements-for-seasonal-forecasting-models/,How much data do you need to forecast using a seasonal model? The answer depends on the type of model being used and the amount of random variation in the data. We discuss the mathematical limits for estimating various common seasonal forecasting models from data. These limits apply when the amount of random variation is very small. Real data often contain a lot of random variation and then many more observations are required.
2007,2,22,Stochastic population forecasts using functional data models,https://robjhyndman.com/seminars/stochastic-population/,When: 12.00noon Thu 22nd February 2007 Where: Room 213 Richard Berry Building The University of Melbourne When: 2.30pm Fri 1 June 2007 Where: Room 457 Menzies Building Monash University (Clayton)  Abstract: I will present a new approach to age-specific forecasting of population based on separate forecasts of mortality fertility and net migration. Functional data models with time series coefficients will be used to model mortality and fertility rates and net migration.
2007,2,16,Forecasting age-specific breast cancer mortality using functional data models,https://robjhyndman.com/publications/forecasting-age-specific-breast-cancer-mortality-using-functional-data-models/,Accurate estimates of future age-specific incidence and mortality are critical for allocation of resources to breast cancer control programs and evaluation of screening programs. The purpose of this study is to apply functional data analysis techniques to model age-specific breast cancer mortality time trends and forecast entire age-specific mortality functions using a state-space approach. We use annual unadjusted breast cancer mortality rates in Australia from 1921 to 2001 in five year age groups (45 to 85+).
2006,11,16,Another look at measures of forecast accuracy,https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/,"We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition and the M3-competition and many of the measures recommended by previous authors on this topic are found to be degenerate in commonly occurring situations. Instead we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.
Keywords: forecast accuracy forecast evaluation forecast error measures M-competition mean absolute scaled error."
2006,10,26,Forecasting and the importance of being uncertain,https://robjhyndman.com/seminars/forecasting-and-the-importance-of-being-uncertain-2/,What: 2006 Belz lecture Statistical Society of Australia (Victorian branch) When: 6.15pm 24 October 2006 Where: Old Geology Theatre 1 University of Melbourne  Forecasters had an inauspicious beginning dabbling with divination sheep&rsquo;s livers and vapour-ridden caves in the mountains of Greece. Then there was a time when forecasters could be charged with vagrancy! Their reputations are still tarnished but their tools are rather more effective. Professor Rob Hyndman will argue for the importance of statistical modelling in forecasting and demonstrate the dangers that occur when uncertainty is ignored.
2006,10,20,Lee-Carter mortality forecasting: a multi-country comparison of variants and extensions,https://robjhyndman.com/publications/lee-carter-mortality-forecasting-a-multi-country-comparison-of-variants-and-extensions/,We compare the short- to medium- term accuracy of five variants or extensions of the Lee-Carter method for mortality forecasting. These include the original Lee-Carter the Lee-Miller and Booth-Maindonald-Smith variants and the more flexible Hyndman-Ullah and De Jong-Tickle extensions. These methods are compared by applying them to sex-specific populations of 10 developed countries using data for 1986-2000 for evaluation. All variants and extensions are more accurate than the original Lee-Carter method for forecasting log death rates by up to 61%.
2006,9,17,Projection pursuit estimator for multivariate conditional densities,https://robjhyndman.com/publications/projection-pursuit-estimator-for-multivariate-conditional-densities/,
2006,9,16,Another look at measures of forecast accuracy for intermittent demand,https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy-for-intermittent-demand/,"Some of the proposed measures of forecast accuracy for intermittent demand can give infinite or undefined values. This makes them unsuitable for general use. I summarize the various measures and demonstrate what can go wrong. Then I describe a new measure (the mean absolute scaled error) which does not have these flaws. I believe it should become the standard measure for comparing forecast accuracy for multiple intermittent-demand series.
Errata: In the series shown in Table 2 the sixth value should be 11 (not 1)."
2006,8,16,A note on the categorization of demand patterns,https://robjhyndman.com/publications/a-note-on-the-categorization-of-demand-patterns/,"We revisit the problem of categorizing demand patterns in order to select the best forecasting method. We improve the categorization scheme of Syntetos Boylan and Croston (2004) by deriving an exact result for the boundary between type and giving a simple approximation to the boundary that is better than that previously published.
Keywords: categorization; forecasting; inventory control; intermittent demand."
2006,7,30,Useful LaTeX links,https://robjhyndman.com/hyndsight/useful-latex-links/,"Learning LaTeX   Introduction to LaTeX: notes and other materials from a one-day workshop.
  Getting started with LaTeX: an excellent online introduction from David Wilkins.
  Great reference book from the Indian TeX Users Group.
  Excellent on-line tutorials from Andy Roberts.
  More tutorials from the Indian TeX Users Group.
  CTAN for finding packages.
  Getting help The first and best place to go is TeX."
2006,7,20,A Bayesian approach to bandwidth selection for multivariate kernel density estimation,https://robjhyndman.com/publications/bandwidth-selection-for-multivariate-kernel-density-estimation-using-mcmc/,Kernel density estimation for multivariate data is an important technique that has a wide range of applications. However it has received significantly less attention than its univariate counterpart. The lower level of interest in multivariate kernel density estimation is mainly due to the increased difficulty in deriving an optimal data-driven bandwidth as the dimension of the data increases. We provide Markov chain Monte Carlo (MCMC) algorithms for estimating optimal bandwidth matrices for multivariate kernel density estimation.
2006,7,16,25 years of time series forecasting,https://robjhyndman.com/publications/25-years-of-time-series-forecasting/,We review the past 25 years of research into time series forecasting. In this silver jubilee issue we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982-1985; International Journal of Forecasting 1985-2005). During this period over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period.
2006,7,15,Twenty-five years of forecasting,https://robjhyndman.com/publications/ijf-editorial-twenty-five-years-of-forecasting/,
2006,6,26,Automatic time series forecasting,https://robjhyndman.com/seminars/automatic-time-series-forecasting/,UseR! conference Vienna Austria
2006,6,20,Optimal combination forecasts for hierarchical time series,https://robjhyndman.com/seminars/hierarchical2/,"International Symposium on Forecasting Santander Spain
Speakers: Rob J Hyndman and Roman A. Ahmed"
2006,5,16,Characteristic-based clustering for time series data,https://robjhyndman.com/publications/ts-clustering/,With the growing importance of time series clustering research particularly for similarity searches amongst long time series such as those arising in medicine or finance it is critical for us to find a way to resolve the outstanding problems that make most clustering methods impractical under certain circumstances. When the time series is very long some clustering algorithms may fail because the very notation of similarity is dubious in high dimension space; many methods cannot handle missing data when the clustering is based on a distance metric.
2006,5,16,Measuring change in prescription drug utilization in Australia,https://robjhyndman.com/publications/measuring-change-in-prescription-drug-utilization-in-australia/,"Purpose: The National Prescribing Service Ltd (NPS) aims to improve prescribing and use of medicines consistent with evidence-based best practice. This study compares two statistical methods used to determine whether multiple educational interventions influenced antibiotic prescribing in Australia.
Methods Monthly data (July 1996 to June 2003) were obtained from a national administrative claims database. The outcome measures were the median number of antibiotic prescriptions per 1000 consultations for each general practitioner (GP) each month and the mean proportion (across GPs) of each subgroup of antibiotics (e."
2006,5,1,Local linear multivariate regression with variable bandwidth in the presence of heteroscedasticity,https://robjhyndman.com/publications/local-linear-multivariate-regression-with-variable-bandwidth-in-the-presence-of-heteroscedasticity/,We present a local linear estimator with variable bandwidth for multivariate nonparametric regression. We prove its consistency and asymptotic normality in the interior of the observed data and obtain its rates of convergence. This result is used to obtain practical direct plug-in bandwidth selectors for heteroscedastic regression in one and two dimensions. We show that the local linear estimator with variable bandwidth has better goodness-of-fit properties than the local linear estimator with constant bandwidth in the presence of heteroscedasticity.
2006,1,16,The accuracy of television network rating forecasts: the effects of data aggregation and alternative models,https://robjhyndman.com/publications/the-accuracy-of-television-network-rating-forecasts-the-effects-of-data-aggregation-and-alternative-models/,This paper investigates the effect of aggregation in relation to the accuracy of television network rating forecasts. We compare the forecast accuracy of network ratings using population rating models rating models for demographic/behavioural segments and individual viewing behaviour models. Models are fitted using neural networks decision trees and regression. The most accurate forecasts are obtained by aggregating forecasts from segment rating models with neural networks being used to fit these models.
2005,12,16,Sensitivity of the estimated air pollution-respiratory admissions relationship to statistical model,https://robjhyndman.com/publications/sensitivity-of-the-estimated-air-pollution-respiratory-admissions-relationship-to-statistical-model/,"Abstract: Study objective: The objective of this study is to demonstrate the methodological shortcomings of currently available analytical methods for single-city time series data one of the most commonly used ecological study designs in air pollution and respiratory disease research.
Design and Methods: We analyse single city epidemiological time series of daily Chronic Obstructive Pulmonary Disease (COPD) (ICD codes 490-492 494 496) and daily asthma (ICD codes 493) hospital admissions in Melbourne Australia from July 1989 to December 1992."
2005,10,16,Empirical information criteria for time series forecasting model selection,https://robjhyndman.com/publications/empirical-information-criteria-for-time-series-forecasting-model-selection/,In this paper we propose a new Empirical Information Criterion (EIC) for model selection which penalizes the likelihood of the data by a function of the number of parameters in the model. It is designed to be used where there are a large number of time series to be forecast. However a bootstrap version of the EIC can be used where there is a single time series to be forecast. The EIC provides a data-driven model selection tool that can be tuned to the particular forecasting task.
2005,7,16,Stochastic models underlying Croston's method for intermittent demand forecasting,https://robjhyndman.com/publications/croston/,Intermittent demand commonly occurs with inventory data with many time periods having no demand and small demand in the other periods. Croston&rsquo;s method is a widely used procedure for intermittent demand forecasting. However it is an ad~hoc method with no properly formulated underlying stochastic model. In this paper we explore possible models underlying Croston&rsquo;s method and three related methods and we show that any underlying model will be inconsistent with the properties of intermittent demand data.
2005,7,16,"Book Review of ""Data Analysis and Graphics Using R: An Example-based Approach"" (Maindonald and Braun, 2003)",https://robjhyndman.com/publications/maindonald-and-braun-data-analysis-and-graphics-using-r-an-example-based-approach/,
2005,5,22,Dimension reduction for clustering time series using global characteristics,https://robjhyndman.com/publications/dimension-reduction-for-clustering-time-series-using-global-characteristics/,Existing methods for time series clustering rely on the actual data values can become impractical since the methods do not easily handle dataset with high dimensionality missing value or different lengths. In this paper a dimension reduction method is proposed that replaces the raw data with some global measures of time series characteristics. These measures are then clustered using a self-organizing map. The proposed approach has been tested using benchmark time series previously reported for time series clustering and is shown to yield useful and robust clustering.
2005,4,16,Robust forecasting of mortality and fertility rates: a functional data approach,https://robjhyndman.com/publications/robust-forecasting-of-mortality-and-fertility-rates-a-functional-data-approach/,We propose a new method for forecasting age-specific mortality and fertility rates observed over time. We combine ideas from functional data analysis nonparametric smoothing and robust statistics to form a methodology that is widely applicable to any functional time series data and age-specific mortality and fertility in particular. Our approach provides a modelling framework that is easily adapted to allow for constraints and other information. The model used can be considered a generalization of the Lee-Carter model commonly used in mortality and fertility forecasting.
2005,4,2,Time series forecasting: the case for the single source of error state space approach,https://robjhyndman.com/publications/322/,The state space approach to modelling univariate time series is now widely used both in theory and in applications. However the very richness of the framework means that quite different model formulations are possible even when they purport to describe the same phenomena. In this paper we examine the single source of error [SSOE] scheme which has perfectly correlated error components. We then proceed to compare SSOE to the more common version of the state space models for which all the error terms are independent; we refer to this as the multiple source of error [MSOE] scheme.
2005,1,16,Prediction intervals for exponential smoothing using two new classes of state space models,https://robjhyndman.com/publications/prediction-intervals-for-exponential-smoothing-using-two-new-classes-of-state-space-models/,Three general classes of state space models are presented based upon the single source of error formulation. The first class is the standard linear state space model with homoscedastic errors the second retains the linear structure but incorporates a dynamic form of heteroscedasticity and the third allows for non-linear structure in the observation equation as well as heteroscedasticity. These three classes provide stochastic models for a wide variety of exponential smoothing methods.
2005,1,16,Local linear forecasts using cubic smoothing splines,https://robjhyndman.com/publications/splinefcast/,We show how cubic smoothing splines fitted to univariate time series data can be used to obtain local linear forecasts. Our approach is based on a stochastic state space model which allows the use of a likelihood approach for estimating the smoothing parameter and which enables easy construction of prediction intervals. We show that our model is a special case of an ARIMA(022) model and we provide a simple upper bound for the smoothing parameter to ensure an invertible model.
2005,1,15,Editorial,https://robjhyndman.com/publications/ijf-editorial/,
2004,10,16,The interaction between trend and seasonality,https://robjhyndman.com/publications/the-interaction-between-trend-and-seasonality/,A contribution to the discussion of Miller and Williams (2004).
2004,7,16,Nonparametric confidence intervals for receiver operating characteristic curves,https://robjhyndman.com/publications/nonparametric-confidence-intervals-for-receiver-operating-characteristic-curves/,We study methods for constructing confidence intervals and confidence bands for estimators of receiver operating characteristics. Particular emphasis is placed on the way in which smoothing should be implemented when estimating either the characteristic itself or its variance. We show that substantial undersmoothing is necessary if coverage properties are not to be impaired. A theoretical analysis of the problem suggests an empirical plug-in rule for bandwidth choice optimising the coverage accuracy of interval estimators.
2004,5,16,Exponential smoothing models: Means and variances for lead-time demand,https://robjhyndman.com/publications/exponential-smoothing-models-means-and-variances-for-lead-time-demand/,Exponential smoothing is often used to forecast lead-time demand for inventory control. In this paper formulae are provided for calculating means and variances of lead-time demand for a wide variety of exponential smoothing methods. A feature of many of the formulae is that variances as well as the means depend on trends and seasonal effects. Thus these formulae provide the opportunity to implement methods that ensure that safety stocks adjust to changes in trend or changes in season.
2004,1,16,Spline interpolation for demographic variables: the monotonicity problem,https://robjhyndman.com/publications/monotonic-splines-2/,"In demography it is often necessary to obtain a monotonic interpolation of data. A solution to this problem is available using the Hyman filter for cubic splines. However this does not seem to be well-known amongst demographers and no implementation of the procedure is readily available. We remedy these problems by outlining the relevant ideas here and providing a function for the R package.
R code"
2003,7,16,Normative data for the Test of Visual Analysis Skills on an Australian population,https://robjhyndman.com/publications/normative-data-for-the-test-of-visual-analysis-skills-on-an-australian-population/,Purpose: The purpose of this study was to produce normative data for Rosner&rsquo;s Test of Visual Analysis Skills (TVAS). Methods: 886 unselected children aged 5 to 10 years and in the first 4 years of school in Australia were tested to threshold on the TVAS. Percentiles means and standard deviations for each age group were calculated. Results: We found a steady increase in scores with grade and age and a significant difference in scores between each of the ages and grades.
2003,7,15,Statistical jokes,https://robjhyndman.com/hyndsight/statistical-jokes/,"Most of these jokes were posted to Usenet news groups. People who read such things collected them and put them on their web sites. I have shamelessly borrowed them edited them and posted them here for the light relief of other statisticians.
The Biologist the Statistician the Mathematician and the Computer Scientist A biologist a statistician a mathematician and a computer scientist are on a photo-safari in Africa. They drive out into the savannah in their jeep stop and scour the horizon with their binoculars."
2003,4,16,Unmasking the Theta method,https://robjhyndman.com/publications/unmasking-the-theta-method/,"The &ldquo;Theta method&rdquo; of forecasting performed particularly well in the M3-competition and is therefore of interest to forecast practitioners. The description of the method given by Assimakopoulos and Nikolopoulos (2000) involves several pages of algebraic manipulation and is difficult to comprehend. We show that the method can be expressed much more simply; furthermore we show that the forecasts obtained are equivalent to simple exponential smoothing with drift.
Keywords: exponential smoothing forecasting competitions state space models."
2003,2,16,Improved methods for bandwidth selection when estimating ROC curves,https://robjhyndman.com/publications/improved-methods-for-bandwidth-selection-when-estimating-roc-curves/,"The receiver operating characteristic (ROC) curve is used to describe the performance of a diagnostic test which classifies observations into two groups. We introduce new methods for selecting bandwidths when computing kernel estimates of ROC curves. Our techniques allow for interaction between the distributions of each group of observations and gives substantial improvement in MISE over other proposed methods especially when the two distributions are very different.
Keywords: bandwidth selection; binary classification; kernel estimator; ROC curve."
2002,11,16,Mixed model-based hazard estimation,https://robjhyndman.com/publications/mixed-model-based-hazard-estimation/,We propose a new method for estimation of the hazard function from a set of censored failure time data with a view to extending the general approach to more complicated models. The approach is based on a mixed model representation of penalized spline hazard estimators. One payoff is the automation of the smoothing parameter choice through restricted maximum likelihood. Another is the option to use standard mixed model software for automatic hazard estimation.
2002,7,16,Nonparametric estimation and symmetry tests for conditional density functions,https://robjhyndman.com/publications/nonparametric-estimation-and-symmetry-tests-for-conditional-density-functions/,We suggest two new methods for conditional density estimation. The first is based on locally fitting a log-linear model and is in the spirit of recent work on locally parametric techniques in density estimation. The second method is a constrained local polynomial estimator. Both methods always produce non-negative estimators. We propose an algorithm suitable for selecting the two bandwidths for either estimator. We also develop a new bootstrap test for the symmetry of conditional density functions.
2002,7,16,A state space framework for automatic forecasting using exponential smoothing methods,https://robjhyndman.com/publications/hksg/,We provide a new approach to automatic busineswwwwws forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods can be shown to be equivalent to the forecasts obtained from a state space model. This allows (1) the easy calculation of the likelihood the AIC and other model selection criteria; (2) the computation of prediction intervals for each method; and (3) random simulation from the underlying state space model.
2002,7,15,Kalman filter,https://robjhyndman.com/publications/kalman-filter/,
2002,7,15,Box-Jenkins modelling,https://robjhyndman.com/publications/box-jenkins-modelling/,
2002,7,15,ARIMA processes,https://robjhyndman.com/publications/arima-processes/,
2002,3,16,Using R to Teach Econometrics,https://robjhyndman.com/publications/using-r-to-teach-econometrics/,R an open-source programming environment for data analysis and graphics has in only a decade grown to become a de-facto standard for statistical analysis against which many popular commercial programs may be measured. The use of R for the teaching of econometric methods is appealing. It provides cutting-edge statistical methods which are by R&rsquo;s open-source nature available immediately. The software is stable available at no cost and exists for a number of platforms including various flavors of Unix and Linux Windows (9x/NT/2000) and the MacOS.
2001,11,16,Cycles and synchrony in the Collared Lemming (Dicrostonyx groenlandicus) in Arctic North America,https://robjhyndman.com/publications/cycles-and-synchrony-in-the-collared-lemming-dicrostonyx-groenlandicus-in-arctic-north-america/,Lemming populations are generally characterised by their cyclic nature yet empirical data to support this are lacking for most species largely because of the time and expense necessary to collect long-term population data. In this study we use the relative frequency of yearly willow scarring by lemmings as an index of lemming abundance allowing us to plot population changes over a 34-year period. Scars were collected from 18 sites in Arctic North America separated by 2-1647 km to investigate local synchrony among separate populations.
2001,10,16,It's time to move from 'what' to 'why',https://robjhyndman.com/publications/its-time-to-move-from-what-to-why/,"(Invited commentary on M3 competition.)
We provide a new approach to automatic business forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods can be shown to be equivalent to the forecasts obtained from a state space model. This allows (1) the easy calculation of the likelihood the AIC and other model selection criteria; (2) the computation of prediction intervals for each method; and (3) random simulation from the underlying state space model."
2001,8,16,Data visualization for time series in environmental epidemiology,https://robjhyndman.com/publications/data-visualization-for-time-series-in-environmental-epidemiology/,Data visualization has become an integral part of statistical modelling. Exploratory graphical analysis allows insight into the underlying structure of observations in a data set and graphical methods for diagnostic purposes after model fitting provide insight into the fitted model and its inadequacies. In this paper we present visualization methods for preliminary exploration of time series data and graphical diagnostic methods for modelling relationships between time series data in medicine. We will use exploratory graphical methods to better understand the relationship between a time series response and a number of potential covariates.
2001,7,2,Statistical methodological issues in studies of air pollution and respiratory disease,https://robjhyndman.com/publications/statistical-methodological-issues-in-studies-of-air-pollution-and-respiratory-disease/,Epidemiological studies have consistently shown short term associations between levels of air pollution and respiratory disease in countries of diverse populations geographical locations and varying levels of air pollution and climate. The aims of this paper are: (1) to assess the sensitivity of the observed pollution effects to model specification with particular emphasis on the inclusion of seasonally adjusted covariates; and (2) to study the effect of air pollution on respiratory disease in Melbourne Australia.
2001,6,16,Bandwidth selection for kernel conditional density estimation,https://robjhyndman.com/publications/bandwidth-selection-for-kernel-conditional-density-estimation/,"We consider bandwidth selection for the kernel estimator of conditional density with one explanatory variable. Several bandwidth selection methods are derived ranging from fast rules-of-thumb which assume the underlying densities are known to relatively slow procedures which use the bootstrap. The methods are compared and a practical bandwidth selection strategy which combines the methods is proposed. The methods are compared using two simulation studies and a real data set.
Keywords: density estimation; kernel smoothing; conditioning; bandwidth selection."
2000,11,16,Non-Gaussian conditional linear AR(1) models,https://robjhyndman.com/publications/non-gaussian-conditional-linear-ar1-models/,We give a general formulation of a non-Gaussian conditional linear AR(1) model subsuming most of the non-Gaussian AR(1) models that have appeared in the literature. We derive some general results giving properties for the stationary process mean variance and correlation structure and conditions for stationarity. These results highlight similarities and differences with the Gaussian AR(1) model and unify many separate results appearing in the literature. Examples illustrate the wide range of properties that can appear under the conditional linear autoregressive assumption.
2000,11,16,Residual diagnostic plots for model mis-specification in time series regression,https://robjhyndman.com/publications/residual-diagnostic-plots-for-model-mis-specification-in-time-series-regression/,This paper considers residuals for time series regression. Despite much literature on visual diagnostics for uncorrelated data there is little on the autocorrelated case. In order to examine various aspects of the fitted time series regression model three residuals are considered. The fitted regression model can be checked using orthogonal residuals; the time series error model can be analysed using marginal residuals; and the white noise error component can be tested using conditional residuals.
2000,8,9,Seasonal adjustment methods for the analysis of respiratory disease in environmental epidemiology,https://robjhyndman.com/publications/seasonal-adjustment-methods-for-the-analysis-of-respiratory-disease-in-environmental-epidemiology/,We study the relationship between daily hospital admissions for respiratory disease and various pollutant and climatic variables looking particularly at the effect of seasonal adjustment on the estimated models. Often time series exhibit seasonal behaviour and adequate control for the presence of a seasonal component is essential before one attempts to model the complex pollution-health association. We show that if these factors are not adequately controlled for spurious effects of pollutants and climate on morbidity/mortality can be induced.
2000,7,16,"Book review of ""Nonparametric econometrics"" (Pagan and Ullah, 1999)",https://robjhyndman.com/publications/pagan-and-ullah-nonparametric-econometrics/,
2000,5,16,Generalized additive modelling of mixed distribution Markov models with application to Melbourne's rainfall,https://robjhyndman.com/publications/gam-rainfall/,We consider modelling time series using a generalized additive model with first-order Markov structure and mixed transition density having a discrete component at zero and a continuous component with positive sample space. Such models have application for example in modelling daily occurrence and intensity of rainfall and in modelling the number and size of insurance claims. We show how these methods extend the usual sinusoidal seasonal assumption in standard chain-dependent models by assuming a general smooth pattern of occurrence and intensity over time.
1999,7,16,"Book Review of ""A primer of mathematical writing"" (Krantz, 1997) and ""Handbook of writing for the mathematical sciences"" (Higham, 1998)",https://robjhyndman.com/publications/krantz-a-primer-of-mathematical-writing-higham-handbook-of-writing-for-the-mathematical-sciences/,
1999,7,16,"Book review of ""Statistically speaking: a dictionary of quotations"" (Gaither and Cavazos-Gaither, 1996)",https://robjhyndman.com/publications/gaither-and-cavazos-gaither-statistically-speaking-a-dictionary-of-quotations/,
1999,7,15,"Book review of ""Chance encounters: a first course in data analysis and inference"" (Wild & Seber, 2000)",https://robjhyndman.com/publications/wild-c-j-and-seber-g-a-f-chance-encounters-a-first-course-in-data-analysis-and-inference/,
1999,7,7,Nonparametric additive regression models for binary time series,https://robjhyndman.com/publications/logitar/,I consider models for binary time series starting with autoregression models and then developing generalizations of them which allow nonparametric additive covariates. I show that several apparently different binary AR(1) models are equivalent. Three possible nonparametric additive regression models which allow for autocorrelation are considered; one is a generalization of an ARX model the other two are generalizations of a regression model with AR errors. One of the models is applied to two data sets: IBM stock transactions and Melbourne&rsquo;s rainfall.
1998,7,16,Smoothing non-Gaussian time series with autoregressive structure,https://robjhyndman.com/publications/smoothing-non-gaussian-time-series-with-autoregressive-structure/,We consider nonparametric smoothing for time series which are clearly non-Gaussian and which are subject to an autoregressive random component. This generalizes methods for smoothing Gaussian series with autoregressive errors but in the non-Gaussian case the autoregressive structure is not always additive. The problem can be formulated in a general way to include many common non-Gaussian autoregressive models. The amount of smoothing can be chosen by penalized likelihood methods and we give simulations and parametric bootstrap methods for studying and empirically estimating the penalty function.
1998,7,15,"Book Review of ""Smoothing methods in Statistics"" (Simonoff, 1996)",https://robjhyndman.com/publications/simonoff-smoothing-methods-in-statistics/,
1998,7,15,"Book review of ""Leading personalities in the Statistical Sciences: from the seventeenth century to the present"" (Johnson and Kotz, 1998)",https://robjhyndman.com/publications/johnson-and-kotz-eds-leading-personalities-in-the-statistical-sciences-from-the-seventeenth-century-to-the-present/,
1997,12,16,Nonparametric autocovariance function estimation,https://robjhyndman.com/publications/nonparametric-autocovariance-function-estimation/,"Nonparametric estimators of autocovariance functions for non-stationary time series are developed. The estimators are based on straightforward nonparametric mean function estimation ideas and allow use of any linear smoother (e.g. smoothing spline local polynomial). We study the properties of the estimators and illustrate their usefulness through application to some meteorological and seismic time series.
Keywords: bandwidth; correlated errors; kernel smoothing; local polynomial; nonparametric regression; non-stationary model; time series.
Data: Melbourne maximum temperatures Kobe earthquake seismograph"
1997,7,16,Some properties and generalizations of non-negative Bayesian time series models,https://robjhyndman.com/publications/some-properties-and-generalizations-of-non-negative-bayesian-time-series-models/,We study the most basic Bayesian forecasting model for exponential family time series the power steady model (PSM) of Smith in terms of observable properties of one-step forecast distributions and sample paths. The PSM implies a constraint between location and spread of the forecast distribution. Including a scale parameter in the models does not always give an exact solution free of this problem but it does suggest how to define related models free of the constraint.
1997,1,16,The pricing and trading of options using a hybrid neural network model with historical volatility,https://robjhyndman.com/publications/the-pricing-and-trading-of-options-using-a-hybrid-neural-network-model-with-historical-volatility/,"(Later known as Journal of Computational Intelligence in Finance)
The residuals between conventional option pricing models and market prices have persistent patterns or biases. The &ldquo;hybrid&rdquo; method models the residuals using an artificial neural network. The pricing accuracy of the hybrid method is demonstrated on real data using the Australian All Ordinaries Share Price Index options on futures and is compared with all major competing conventional models. The hybrid method is found to be both statistically and economically superior to the conventional models alone."
1996,11,16,Sample quantiles in statistical packages,https://robjhyndman.com/publications/quantiles/,There are a large number of different definitions used for sample quantiles in statistical computer packages. Often within the same package one definition will be used to compute a quantile explicitly while other definitions may be used when producing a boxplot a probability plot or a QQ-plot. We compare the most commonly implemented sample quantile definitions by writing them in a common notation and investigating their motivation and some of their properties.
1996,7,16,Estimating and visualizing conditional densities,https://robjhyndman.com/publications/estimating-and-visualizing-conditional-densities/,We consider the kernel estimator of conditional density and derive its asymptotic bias variance and mean-square error. Optimal bandwidths (with respect to integrated mean-square error) are found and it is shown that the convergence rate of the density estimator is order n-2/3. We also note that the conditional mean function obtained from the estimator is equivalent to a kernel smoother. Given the undesirable bias properties of kernel smoothers we seek a modified conditional density estimator which has mean equivalent to some other nonparametric regression smoother with better bias properties.
1996,7,16,Computing and graphing highest density regions,https://robjhyndman.com/publications/computing-and-graphing-highest-density-regions/,Many statistical methods involve summarizing a probability distribution by a region of the sample space covering a specified probability. One method of selecting such a region is to require it to contain points of relatively high density. Highest density regions are particularly useful for displaying multimodal distributions and in such cases may consist of several disjoint subsets &mdash; one for each local mode. In this paper I propose a simple method for computing a highest density region from any given (possibly multivariate) density f(x) which is bounded and continuous in x.
1996,7,15,"Book review of ""Kernel smoothing"" (Wand and Jones, 1995)",https://robjhyndman.com/publications/wand-and-jones-kernel-smoothing/,
1996,6,16,A unified view of linear AR(1) models,https://robjhyndman.com/publications/a-unified-view-of-linear-ar1-models/,We review and synthesize the wide range of non-Gaussian first order linear autoregressive models that have appeared in the literature. Models are organized into broad classes to clarify similarities and differences and facilitate application in particular situations. General properties for process mean variance and correlation are derived unifying many separate results appearing in the literature. Examples illustrate the wide range of properties that can appear even under the autoregressive assumption. These results are used in analysing a variety of real data sets illustrating general methods of estimation model diagnostics and model selection.
1995,7,16,Highest density forecast regions for non-linear and non-normal time series models,https://robjhyndman.com/publications/highest-density-forecast-regions-for-non-linear-and-non-normal-time-series-models/,"Many modern time series methods such as those involving non-linear models or non-normal data frequently lead to forecast densities which are asymmetric or multi-modal. The problem of obtaining forecast regions in such cases is discussed and it is proposed that highest density forecast regions be used. A graphical method for presenting the results is discussed.
Keywords: non-linear time series non-normal time series highest density regions forecast intervals threshold models.
R code"
1995,7,15,The use of information technology in the research process,https://robjhyndman.com/publications/the-use-of-information-technology-in-the-research-process/,
1995,7,5,The problem with Sturges' rule for constructing histograms,https://robjhyndman.com/publications/sturges/,Most statistical packages use Sturges' rule (or an extension of it) for selecting the number of classes when constructing a histogram. Sturges' rule is also widely recommended in introductory statistics textbooks. It is known that Sturges' rule leads to oversmoothed histograms but Sturges' derivation of his rule has never been questioned. In this note I point out that the argument leading to Sturges' rule is wrong.
1994,7,16,Approximations and boundary conditions for continuous time threshold autoregressive processes,https://robjhyndman.com/publications/approximations-and-boundary-conditions-for-continuous-time-threshold-autoregressive-processes/,"Continuous time threshold autoregressive (CTAR) processes have been developed in the past few years for modelling non-linear time series observed at irregular intervals. Several approximating processes are given here which are useful for simulation and inference. Each of the approximating processes implicitly defines conditions on the thresholds thus providing greater understanding of the way in which boundary conditions arise.
Keywords: continuous time autoregression threshold autoregression non-linear stochastic differential equations unequally spaced time series."
1993,7,16,Yule-Walker estimates for continuous-time autoregressive models,https://robjhyndman.com/publications/yule-walker-estimates-for-continuous-time-autoregressive-models/,I consider continuous time autoregressive (CAR) processes of order p and develop estimators of the model parameters based on Yule&ndash;Walker type equations. For continuously recorded data it is shown that these estimators are least squares estimators and have the same asymptotic distribution as maximum likelihood estimators. In practice though data can only be observed discretely. For discrete data I consider approximations to the continuous time estimators. It is shown that some of these discrete time estimators are asymptotically biased.
1992,12,17,Continuous-time threshold autoregressive modelling,https://robjhyndman.com/publications/phd/,"This thesis considers continuous time autoregressive processes defined by stochastic differential equations and develops some methods for modelling time series data by such processes.
The first part of the thesis looks at continuous time linear autoregressive (CAR) processes defined by linear stochastic differential equations. These processes are well-understood and there is a large body of literature devoted to their study. I summarise some of the relevant material and develop some further results."
1992,7,16,On continuous-time threshold autoregression,https://robjhyndman.com/publications/on-continuous-time-threshold-autoregression/,The use of non-linear models in time series analysis has expanded rapidly in the last ten years with the development of several useful classes of discrete-time non-linear models. One family of processes which has been found valuable is the class of self-exciting threshold autoregressive (SETAR) models discussed extensively in the books of Tong (1983 1990). In this paper we consider problems of modelling and forecasting with continuous-time threshold autoregressive (CTAR) processes.
1991,2,16,Continuous time threshold autoregressive models,https://robjhyndman.com/publications/continuous-time-threshold-autoregressive-models/,The importance of non-linear models in time series analysis has been recognized increasingly over the past ten years. A number of discrete time non-linear processes have been introduced and found valuable for the modelling of observed series. Among these processes are the discrete time threshold models discussed extensively in the book of Tong (1983). The purpose of this paper is to define a continuous time analogue of the threshold AR(p) process and to discuss some of its properties.
1987,9,15,Calculating the odds,https://robjhyndman.com/publications/calculating-the-odds/,
1,1,1,About me,https://robjhyndman.com/about/,"The following bio and this photo may be used in media releases without further permission.
 Rob J Hyndman FAA FASSA is Professor of Statistics and Head of the Department of Econometrics and Business Statistics at Monash University. From 2005 to 2018 he was Editor-in-Chief of the International Journal of Forecasting and a Director of the International Institute of Forecasters. Rob is the author of over 200 research papers and 5 books in statistical science."
1,1,1,Handling papers as an IJF associate editor,https://robjhyndman.com/ijf/associate-editor-instructions/,"Using ManuscriptCentral When you log into ManuscriptCentral you will see an &ldquo;Associate Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. You will mostly be using the &ldquo;Associate Editor Center&rdquo;.
Please log in every week or two to check on papers you are handling and to handle any new papers you have been allocated. You will receive email notifications about new papers but you won&rsquo;t necessarily receive any notifications about papers where the reviewers are overdue."
1,1,1,Handling papers as an IJF editor,https://robjhyndman.com/ijf/editor-instructions/,"Using ManuscriptCentral When you log into https://mc.manuscriptcentral.com/ijf you will see an &ldquo;Associate Editor Center&rdquo; and a &ldquo;Handling Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. You will mostly be using the &ldquo;Handling Editor Center&rdquo;.
Please log in every week to check on papers you are handling and to handle any new papers you have been allocated. You will receive email notifications about new papers but you won&rsquo;t necessarily receive any notifications about papers where the associate editors are overdue."
1,1,1,Handling papers as an IJF guest editor,https://robjhyndman.com/ijf/guest-editor-instructions/,"Using ManuscriptCentral When you log into ManuscriptCentral you will see an &ldquo;Associate Editor Center&rdquo; along with your &ldquo;Reviewer Center&rdquo; and &ldquo;Author Center&rdquo;. These are for different roles that you have with the journal. When acting as a guest editor you will be using the &ldquo;Associate Editor Center&rdquo;.
Please log in every week or two to check on papers you are handling and to handle any new papers you have been allocated."
1,1,1,In the news,https://robjhyndman.com/in-the-news/,"&ldquo;All eyes online&rdquo; &ldquo;Monash Awards&rdquo; 10 June 2021.
  “This is really a marathon.” Meet the statistician helping forecast the spread of COVID-19 Monash Impact 25 May 2021.
  The Australia Academy of Science Elects ACEMS Chief Investigator as New Fellow ACEMS News 25 May 2021.
  Leading Monash University scientists elected Fellows of Australian Academy of Science Monash News 25 May 2021.
  Distinguished statistician elected Fellow of the Australian Academy of Science Monash News 25 May 2021."
1,1,1,Research team,https://robjhyndman.com/research-team/,Potential PhD students: please read Advice to PhD Applicants Potential research fellows: All available positions are filled.  Current research fellows  Puwasala Gamakumara  Stuart Lee  Zhuo Li  Current PhD students  Lakshan Bernard. Analytical tools for future power networks (PhD begun 2020). Fan Cheng. Manifold learning on empirical probability distributions (PhD begun 2019).  Sayani Gupta. Visualization of probability distributions of deconstructed temporal data (PhD begun 2018).
1,1,1,In the news,https://robjhyndman.com/in-the-news/,"&ldquo;Rob Hyndman awarded the Pitman Medal 2021&rdquo; Statistical Society of Australia 8 July 2021.
  &ldquo;IIF Fellow &amp; Distinguished Researcher: Rob Hyndman&rdquo; International Institute of Forecasters 2 July 2021.
  &ldquo;International Journal of Forecasting Announces Best Papers from the M4 special issue&rdquo; International Institute of Forecasters 30 June 2021.
  &ldquo;All eyes online&rdquo; &ldquo;Monash Awards&rdquo; 10 June 2021.
  “This is really a marathon.” Meet the statistician helping forecast the spread of COVID-19 Monash Impact 25 May 2021."
